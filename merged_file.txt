



Grecia (en griego moderno, Ελλάδα, Elláda, AFI: [eˈlaða] ( escuchar); en griego antiguo, Ἑλλάς, Hellás), oficialmente República Helénica (en griego, Ελληνική Δημοκρατία, Ellinikí Dimokratía, AFI: [eliniˈci ðimokɾaˈtia]), es uno de los veintisiete estados soberanos que forman la Unión Europea.[4]​ En este país viven alrededor de once millones de habitantes que conforman una sociedad muy homogénea, donde principalmente se habla griego y se practica el cristianismo ortodoxo.[8]​

Atenas, la capital, es la ciudad más poblada del país y la segunda es Tesalónica. Otras ciudades como El Pireo, Patras, Heraclión y Lárisa, son centros políticos, económicos y culturales a nivel regional.[9]​

Grecia está estratégicamente ubicada entre Europa, Asia y África, y comparte fronteras terrestres al noroeste con Albania, al norte con Macedonia del Norte y Bulgaria, y al noreste con Turquía.[10]​[11]​[12]​ Al este se encuentra el mar Egeo, al oeste el mar Jónico y en el sur, el Mediterráneo; estos tres mares bañan sus 13 676 km de costas, el 11.º litoral más largo del mundo.[4]​ El territorio griego está conformado por siete archipiélagos con unas 1400 islas, de las que 227 están habitadas.[13]​ Cerca de un 80 % de su relieve consta de montañas, de las cuales la más alta es el monte Olimpo, con 2917 metros sobre el nivel del mar.[4]​

La Grecia moderna tiene su origen en la civilización de la antigua Grecia, cuna de la civilización occidental. Para Occidente  es el lugar de nacimiento de la democracia, la filosofía occidental, los Juegos Olímpicos, la literatura y el estudio de la historia, la política y los más importantes principios de las matemáticas y la ciencia.[14]​[15]​[16]​[17]​ El Estado griego moderno, que comprende la mayor parte del núcleo histórico de la civilización griega, se estableció en 1830, luego de una guerra de independencia del Imperio otomano.[18]​ El legado de su larga historia se refleja en el arte, la arquitectura, gastronomía, literatura y otros aspectos culturales.[19]​

En la actualidad Grecia es un Estado democrático, desarrollado y un Índice de Desarrollo Humano muy alto.[20]​[21]​[22]​[7]​ Grecia es además miembro de la Unión Europea desde 1981 y utiliza el euro desde 2001, forma parte de la OTAN desde 1952 y de la Agencia Espacial Europea desde 2005.[23]​ Es también socio fundador de las Naciones Unidas, la OCDE y la Organización de Cooperación del Mar Negro.[24]​[25]​[26]​ Sin embargo, Grecia es el país que más ha visto afectada su economía durante la crisis económica de 2008-2015, cuando redujo su PIB en un 25% durante cinco años.[27]​ También han aumentado mucho las desigualdades sociales, el Coeficiente de Gini y la pobreza.[28]​ No obstante, el Eurogrupo pronosticó en 2015 un aumento del PIB griego en los siguientes años.[29]​[30]​

Los nombres utilizados para referirse a la nación de Grecia y al pueblo griego varían dependiendo del idioma, la ubicación y la cultura. Aunque los griegos llaman al país Hellás o Ellada (en griego, Ελλάς o Ελλάδα) y su nombre oficial es República Helénica, en español se le conoce como Grecia, que proviene del término en latín Graecia. Este fue utilizado por los romanos, literalmente significa «la tierra de los griegos» y se deriva del nombre griego Graikós (Γραικός), cuya etimología aún se desconoce.[31]​ En español se emplea ocasionalmente el término Hélade para referirse tanto a la Grecia actual como a la antigua.[32]​ Aristóteles fue el primero en utilizar el nombre graeci (γραικοί, es decir, «griegos») en su obra Meteorología, donde afirma que el área cerca de Dodona y Aqueloo estaba habitada por los selli y por un pueblo anteriormente llamado graeci, pero que en su tiempo se llamaban helenos.[33]​

La evidencia de presencia humana más antigua hallada en los territorios de la actual Grecia se encuentra en la caverna de Petralona, en la península Calcídica, donde se halló un cráneo conocido como hombre de Petralona, cuya datación es discutida.[34]​ Dentro del territorio griego existen vestigios de asentamientos de las tres etapas de la Edad de Piedra —paleolítico, mesolítico y neolítico—; algunos sitios, como la cueva Franchthi estuvieron ocupados durante estos tres periodos.[35]​ Dado que el país se ubica en la ruta por la cual la agricultura se expandió desde el Cercano Oriente hacia Europa,[36]​ los asentamientos neolíticos en Grecia son los más antiguos en el continente, pues datan del séptimo milenio a. C.[34]​

En el actual territorio griego surgieron las primeras civilizaciones de Europa, por lo que se considera el lugar de nacimiento de la civilización occidental.[37]​[38]​[39]​[40]​[41]​ Las primeras en aparecer fueron la civilización cicládica en las islas del mar Egeo (alrededor del 3200 a. C.),[42]​ la civilización minoica en Creta (2700-1500 a. C.)[41]​[43]​ y la civilización micénica en el continente (1900-1100 a. C.).[43]​ Estas sociedades poseían un sistema de escritura: los minoicos utilizaron uno aún sin descifrar conocido como Lineal A, mientras que los micénicos desarrollaron el Lineal B, una forma primitiva del griego. Los micénicos gradualmente absorbieron a los minoicos, pero su cultura colapsó violentamente alrededor del 1200 a. C., durante un periodo de inestabilidad regional conocido como el colapso de la Edad de Bronce.[44]​ Esto condujo a una era conocida como la Edad Oscura, de la que no se conservan registros escritos.[45]​

Tradicionalmente se fija el final de la Edad Oscura, e inicio de la Época Arcaica, en el 776 a. C., año durante el cual se celebraron los primeros Juegos Olímpicos.[46]​ Se piensa que entre los siglos siglo VIII y VII a. C. Homero escribió la Ilíada y la Odisea, los textos fundacionales de la literatura occidental.[47]​[48]​ Con el final de la Edad Oscura surgieron varios reinos y ciudades-estado, los cuales se extendieron hasta las costas del mar Negro, el sur de Italia (Magna Graecia) y Asia menor. Estos estados y sus colonias alcanzaron un gran nivel de prosperidad que dio paso a un florecimiento cultural sin precedentes —periodo conocido como la Grecia clásica— más evidente en la arquitectura, el teatro, la ciencia, las matemáticas y la filosofía. En el 508 a. C., Clístenes introdujo el primer sistema democrático del mundo en Atenas.[49]​[50]​

Para el 500 a. C. el Imperio persa controlaba el territorio entre el actual Irán hasta las zonas que hoy forman parte del norte de Grecia, Macedonia del Norte, el sur de Ucrania, Bulgaria y Rumania, por lo que se convirtió en una amenaza para los griegos.[51]​ Las ciudades-estado helénicas ubicadas en Asia Menor fracasaron en sus intentos por expulsar a los persas; en 492 a. C. el ejército persa invadió los estados de la Grecia continental, pero se vio forzado a retirarse luego de su derrota en la batalla de Maratón en 490 a. C. Diez años más tarde lanzaron una segunda ofensiva. Pese a la heroica resistencia de los espartanos y otros griegos en la batalla de las Termópilas, las fuerzas persas lograron llegar a Atenas.[52]​

Luego de una serie de victorias griegas entre el 480 y 479 a. C. en las batallas de Salamina, Platea y Mícala, los persas se vieron forzados a retirarse por segunda ocasión.[53]​ Estos conflictos militares, conocidos como las Guerras Médicas, fueron liderados en gran parte por Atenas y Esparta. El hecho de que Grecia no fuese un país unificado dio lugar a varios conflictos entre los estados helénicos.[52]​

Dentro de estos, el enfrentamiento más importante fue la guerra del Peloponeso (431-404 a. C.), donde la victoria de Esparta marcó el final de la supremacía del Imperio ateniense sobre la Antigua Grecia.[54]​ Posteriormente, la batalla de Leuctra (371 a. C.) le brindó el poder hegemónico a Tebas, pero poco después le fue arrebatado por Macedonia. Este reino logró unificar al mundo griego en la liga de Corinto —también conocida como la «liga helénica»—, bajo el mando del Filipo II, líder del primer estado griego unificado en la historia.[55]​

Luego del asesinato de Filipo II, su hijo Alejandro Magno asumió el liderazgo de la liga de Corinto, y en 334 a. C. lanzó una invasión al Imperio persa con las fuerzas combinadas de los estados griegos. Cuatro años después y tras salir victoriosos en las batallas de Gránico, Issos y Gaugamela, los griegos marcharon hacia Susa y tomaron Persépolis, la capital ceremonial de Persia.[56]​ El imperio creado por Alejandro Magno se extendió desde Grecia en el oeste hasta el actual Pakistán en el este y Egipto en el sur.[57]​

La repentina muerte de Alejandro Magno, acaecida en el 323 a. C., condujo al colapso del Imperio, que se dividió en varios reinos: el Imperio seléucida, el Egipto Ptolemaico, el Reino grecobactriano y el Reino indogriego. Muchos griegos emigraron a Alejandría, Antioquía, Seleucia y a muchas otras ciudades helenísticas en Asia y África.[58]​ Aunque no se pudo mantener la unidad política del Imperio de Alejandro Magno, este trajo consigo el dominio de la civilización helenística y el idioma griego a todos los territorios conquistados por al menos dos siglos, y en el caso de algunas regiones del este del Mediterráneo, por un periodo mayor.[59]​

Alejandro Magno (busto siglo II a. C., Alexandría). Continuó el plan de su padre Filipo II en afirmar el dominio sobre Grecia, extendió los territorios hasta el sureste de Europa, y puso en marcha en plan panhelénico para arrebatarle el imperio a Persia: las ciudades griegas de Asia menor hasta Egipto, y finalmente conquistó el imperio persa, hasta llegar a Afganistán y límites de la India. Falleció a los treinta y dos años de edad.

En Grecia, la muerte de Alejandro Magno fue seguida por un periodo de confusión. En el 276 a. C. la dinastía Antigónida, descendientes de uno de los generales de Alejandro, tomó el poder en Macedonia y en la mayor parte de las ciudades-estado griegas.[60]​ Desde el siglo II a. C. la participación de la república romana en los asuntos internos de los helenos desembocó en las guerras macedónicas.[61]​ La derrota de Macedonia en la batalla de Pidna (168 a. C.) puso fin al poder Antigónido en Grecia.[62]​ En 146 a. C. Roma se anexionó Macedonia como una provincia, y el resto de su territorio se convirtió en un protectorado romano.[61]​[63]​

El proceso terminó en el 27 a. C. cuando el emperador romano César Augusto se hizo con el resto de Grecia para convertirla en la provincia senatorial de Acaya.[63]​ Pese a su supremacía militar, los romanos admiraron y estuvieron fuertemente influidos por los logros de la cultura griega, de ahí la famosa frase de Horacio: Graecia capta ferum victorem cepit («la Grecia conquistada, conquistó al bárbaro conquistador»).[64]​ Generalmente se considera que las matemáticas, la ciencia y tecnología griegas alcanzaron su apogeo durante el periodo helenístico.[65]​

Las comunidades greco-parlantes del Oriente helenizado tuvieron un papel clave en la expansión del cristianismo durante los siglos II y III,[66]​ pues varios de los primeros líderes y autores de la cristiandad, como Pablo de Tarso, hablaban griego.[67]​ Sin embargo, la población griega como tal tuvo una tendencia a apegarse al paganismo y el país no fue uno de los pilares principales del cristianismo primitivo: de hecho, algunas de las prácticas de la religión griega antigua continuaron vigentes hasta finales del siglo IV,[68]​ y algunas áreas del sureste del Peloponeso no se convirtieron al cristianismo hasta el siglo X.[69]​

Tras la división y caída del Imperio romano, Grecia pasó a formar parte del Imperio bizantino, el Imperio romano de Oriente, que perduró desde el siglo V hasta 1453. Su capital se ubicó en Constantinopla, su idioma y literatura se basaron en la lengua griega y la religión predominante fue el cristianismo ortodoxo.[70]​

Desde el siglo IV, los territorios balcánicos del imperio, incluida Grecia, sufrieron del constante embate de las invasiones bárbaras. Los asaltos y la devastación de los godos y hunos durante los siglos IV y V, y la invasión eslava del siglo VII, provocaron un colapso dramático de la autoridad imperial en la península.[71]​ Luego de la invasión eslava, el gobierno imperial mantuvo el control únicamente en las islas y algunas zonas costeras, particularmente las ciudades como Atenas, Corinto y Tesalónica, mientras que algunas de las zonas montañosas del interior mostraron cierta resistencia a la ocupación y siguieron reconociendo la autoridad imperial.[72]​ Se cree que existió cierto número de asentamientos eslavos fuera de estas regiones, aunque a una escala mucho menor de lo que se pensaba anteriormente.[73]​[74]​

A finales del siglo VIII, el Imperio bizantino comenzó a recuperar gradualmente sus territorios perdidos, y para el siglo IX la mayor parte de la Grecia actual se encontraba nuevamente bajo el control bizantino.[75]​[76]​ Las grandes migraciones de griegos desde Sicilia y Asia Menor hacia la península Balcánica facilitaron este proceso, al mismo tiempo que muchos de los eslavos fueron capturados y re-ubicados en Asia Menor y aquellos que permanecieron en Grecia fueron asimilados.[73]​ Durante los siglos XI y XII el regreso de la paz y estabilidad al territorio griego fueron las bases para un fuerte crecimiento económico, mucho más grande que el de la región de Anatolia.[75]​

Luego de la Cuarta Cruzada y la caída de Constantinopla ante los latinos en 1204, la mayor parte del territorio griego pasó a manos de los francos —un periodo conocido como «Francocracia»—,[77]​  y algunas islas fueron tomadas por Venecia.[78]​ En 1261 el restablecimiento del Imperio bizantino en Constantinopla hizo posible la recuperación de casi todas estas regiones. Sin embargo, el principado franco de Acaya en el Peloponeso siguió siendo una potencia regional importante hasta el siglo XIV, mientras que varios archipiélagos permanecieron bajo el control de Génova y Venecia.[77]​

En el siglo XIV el Imperio bizantino perdió varias zonas de la actual Grecia ante los ataques de los serbios y los otomanos.[79]​ A principios del siglo XV, el avance otomano significó que el control bizantino sobre Grecia se redujo al Despotado de Morea en el Peloponeso.[79]​ Luego de la caída de Constantinopla ante los otomanos en 1453, Morea fue el último remanente del Imperio bizantino que se opuso a la invasión turca, pues se mantuvo en pie hasta 1460.[80]​ Con la conquista otomana, muchos académicos greco-bizantinos —responsables de preservar gran parte del conocimiento de la Grecia Clásica— emigraron a Occidente llevando consigo un gran número de obras literarias, y contribuyeron con ellas al desarrollo del Renacimiento.[81]​

Hacia finales del siglo XV, la mayor parte de Grecia y las islas del mar Egeo estaban bajo control otomano, mientras que Chipre y Creta permanecieron bajo el dominio veneciano y no formaron parte del Imperio otomano hasta 1571 y 1670, respectivamente.[83]​ La única parte del mundo grecoparlante que no fue conquistada por los turcos fueron las islas Jónicas, que se mantuvieron bajo el control de Venecia hasta su conquista por la Primera República Francesa en 1797 y luego pasaron al Reino Unido en 1809 hasta su unificación con Grecia en 1864.[84]​

Los griegos de las Islas Jónicas y Constantinopla vivieron en prosperidad, incluso los que habitaban dicha ciudad alcanzaron puestos importantes dentro de la administración otomana.[85]​ Por el contrario, la mayor parte del pueblo griego sufrió las consecuencias económicas de la conquista turca. Los turcos fijaron impuestos elevados, y en años posteriores promulgaron una política para la creación de títulos hereditarios, prácticamente convirtiendo a los habitantes rurales en siervos.[86]​

El gobierno otomano consideró a la Iglesia ortodoxa de Grecia y al Patriarcado Ecuménico de Constantinopla como las principales autoridades de toda la población cristiana ortodoxa del Imperio, sin importar su origen étnico.[87]​ Aunque el Estado otomano no obligó a la población a convertirse al islam, los cristianos enfrentaron varias formas de discriminación, encaminadas a señalar su estatus inferior dentro del Imperio. Esta discriminación hacia los cristianos, sumada al maltrato de las autoridades otomanas locales, fueron la causa de muchas conversiones al islam, al menos superficialmente. En el siglo XIX, muchos «cripto-cristianos» regresaron a sus antiguas prácticas religiosas.[85]​

La naturaleza de la administración otomana en Grecia variaba de un lugar a otro, pero siempre se caracterizó por su negligencia y arbitrariedad.[85]​ Algunas ciudades tenían gobernadores nombrados por el sultán, mientras que otras, como Atenas, eran municipalidades autogobernadas. Las regiones montañosas en el interior permanecieron prácticamente autónomas del gobierno central otomano durante varios siglos.[85]​

Cuando los conflictos militares estallaban entre el Imperio otomano y otros países, los griegos usualmente se levantaban en contra de los turcos, con algunas excepciones. Antes de la independencia, los griegos lucharon contra los otomanos en varios enfrentamientos. Cabe destacar la participación griega en la batalla de Lepanto en 1571, las revueltas de campesinos en Epiro de 1600-1601, la Guerra de Morea de 1684-1699 y la rebelión de Orlov en 1770, esta última instigada por el Imperio ruso.[85]​ Estos levantamientos fueron reprimidos con gran dureza por el ejército otomano.[88]​[89]​

Los siglos XVI y XVII son considerados por algunos autores como una especie de «edad oscura» en la historia griega, pues la esperanza de expulsar a los otomanos parecía remota. Incluso el ejército turco intentó ocupar las islas Jónicas en varias ocasiones; Corfú resistió tres grandes asedios en 1537, 1571 y 1716.[82]​ Durante el siglo XVIII, en esta isla, surgió una clase mercantil griega rica y dispersa. Estos comerciantes dominaron el comercio dentro del Imperio otomano, y establecieron comunidades a lo largo del Mediterráneo, los Balcanes y Europa Occidental.[90]​ Aunque el dominio turco había dejado a Grecia fuera de los grandes movimientos intelectuales europeos como la Reforma protestante y la Ilustración, estas ideas, junto a los ideales de la Revolución francesa y el nacionalismo romántico, comenzaron a penetrar en el mundo griego gracias a esta diáspora mercantil.[91]​ A finales del siglo XVIII, Rigas Feraios, el primer revolucionario en buscar la creación de un estado griego independiente, publicó en Viena una serie de documentos relativos a la independencia de Grecia, incluidos un himno nacional y el primer mapa detallado del país; fue asesinado en 1798 por agentes del Imperio otomano.[91]​[92]​

En 1814, se fundó una organización secreta llamada Filikí Etería —en griego: Sociedad de Amigos—, cuya finalidad era la independencia de Grecia. Filikí Etería planeaba lanzar una revolución en el Peloponeso, los principados del Danubio y Constantinopla. La primera de estas revueltas comenzó el 6 de marzo de 1821 en los principados del Danubio bajo el liderazgo de Alexandros Ypsilantis, pero los otomanos rápidamente la sofocaron. Los eventos en el norte alentaron a los griegos del Peloponeso a levantarse en armas y el 17 de marzo de 1821 declararon la guerra a los otomanos.[18]​

Para finales del mes, el Peloponeso se hallaba envuelto en una rebelión contra los otomanos y para octubre de 1821, los griegos, al mando de Theodoros Kolokotronis, tomaron Tripolitsa. La rebelión del Peloponeso inmediatamente fue seguida por otros movimientos en Creta, Macedonia y Grecia Central pero todos ellos fueron rápidamente sofocados. Mientras tanto, una Armada griega improvisada derrotó a la Armada otomana en el mar Egeo, con esto evitó que llegaran por mar refuerzos turcos. En 1822 y 1824 los turcos y los egipcios invadieron las islas, incluyendo Quíos y Psará, donde cometieron una masacre entre la población civil.[18]​ Por esta causa, la opinión pública de Europa occidental se puso en favor de los rebeldes griegos.[85]​

Sin embargo, comenzaron a surgir problemas entre las distintas facciones griegas, lo que llevó a dos guerras civiles consecutivas. Por su parte, el sultán negoció con el gobernador egipcio Mehmet Alí, quien accedió a enviar a su hijo Ibrahim bajá a Grecia con un ejército para sofocar la rebelión a cambio de ciertos territorios.[93]​ Ibrahim llegó al Peloponeso en febrero de 1825 y obtuvo una victoria inmediata: para finales de ese año, la mayor parte de la región se hallaba bajo control egipcio, y la ciudad de Mesolongi —sitiada por los turcos desde abril de 1825— cayó un año después. Aunque Ibrahim fue derrotado en Mani, consiguió expulsar a los rebeldes de gran parte del Peloponeso y retomó el control sobre Atenas.[94]​

Luego de años de negociaciones, tres de las Grandes Potencias —Rusia, el Reino Unido y Francia— decidieron intervenir en el conflicto, y cada nación envió una flota a Grecia. Tras conocer que una flota otomano-egipcia se dirigía a la isla de Hidra, la flota aliada los interceptó en Pilos. Después de una semana de tensión, comenzó la batalla que acabó con la destrucción de la flota otomano-egipcia.[95]​ Una fuerza expedicionaria francesa supervisó la evacuación del ejército egipcio del Peloponeso, mientras que los griegos prosiguieron con la toma de Grecia Central en 1828. Tras dos años de negociaciones, la Primera República Helénica recibió el reconocimiento internacional en el Protocolo de Londres.[96]​

En 1827 Ioannis Kapodistrias fue elegido como primer gobernador de la nueva República.[97]​ Sin embargo, luego de su asesinato en 1831, las Grandes Potencias instauraron una monarquía encabezada por Otón I, de la Casa de Wittelsbach. En 1843, un levantamiento obligó al rey a promulgar una constitución y establecer una asamblea representativa.[98]​
Debido a su actitud autoritaria, fue destronado en 1862 y, un año más tarde, reemplazado por el príncipe Guillermo de Dinamarca, quien tomó el nombre de Jorge I y trajo consigo las islas Jónicas, regalo de coronación por parte del Reino Unido.[99]​ En 1875 Charilaos Trikoupis, a quien se le atribuye una mejora importante en la infraestructura del país, limitó el poder de la monarquía a interferir en la asamblea y promulgar la norma del voto de confianza para el primer ministro.[100]​ La corrupción y los incrementos en los gastos de Trikoupis para construir obras de infraestructura necesarias para el país —como el canal de Corinto—, debilitaron la frágil economía griega. En 1893 el gobierno se declaró en bancarrota y aceptó las demandas de una autoridad Internacional de Control Financiero para pagar a sus deudores.[101]​

Otro problema político del siglo XIX, único de Grecia, fue la cuestión lingüística. La población general hablaba una forma de griego llamada demótico. Muchos intelectuales de la élite lo veían como un dialecto campesino y estaban determinados a restaurar la gloria del griego antiguo.[102]​ Los documentos del gobierno y diarios eran publicados en griego katharévousa («purificado»), una variante que poca gente podía leer. Los liberales promovieron el reconocimiento del demótico como el idioma nacional, pero los conservadores y la Iglesia ortodoxa estaban en contra de dicha declaración.[103]​ La situación llegó al punto en el que, cuando el Nuevo Testamento se tradujo al demótico en 1901, estallaron una serie de manifestaciones en Atenas que terminaron derrocando al gobierno, hecho conocido como Evangeliaka.[104]​ El problema del idioma perduró en el ambiente político hasta la década de 1970.[102]​

No obstante, todos los griegos estaban unidos en su determinación por liberar las provincias grecohablantes del Imperio otomano. En Creta, una prolongada revuelta entre 1866-1869 exacerbó los ánimos nacionalistas. Cuando estalló la guerra ruso-turca de 1877-1878, el pueblo griego se mostró a favor de apoyar a los rusos; pero debido a la precaria situación económica y a la posibilidad de una intervención británica, Grecia nunca entró en la guerra. Cuando los rusos derrotaron a los turcos en 1881, el Tratado de Berlín obligó al Imperio a ceder Tesalia y algunas partes de Epiro a Grecia, pero no así la isla de Creta.[105]​

Por su parte, los cretenses continuaron orquestando una serie de rebeliones, y en 1897 el gobierno griego de Theodoros Deligiannis, cediendo a la presión del pueblo, declaró la guerra a los otomanos.[106]​ Así, en la guerra greco-turca de 1897 los otomanos derrotaron al mal entrenado y equipado ejército griego. Gracias a la intervención de las Grandes Potencias, Grecia solo perdió una pequeña parte de su territorio en la frontera con Turquía, mientras que en Creta se estableció un estado autónomo presidido por el príncipe Jorge de Grecia.[107]​[108]​

Al final de la guerra de los Balcanes, la superficie y población de Grecia habían aumentado. En los años siguientes, la lucha entre el rey Constantino I y el primer ministro Eleftherios Venizelos por el control de la política exterior, dominó el escenario político y dividió al país.[110]​ Durante la Primera Guerra Mundial, Grecia llegó a tener dos gobiernos: uno proalemán a favor del rey, ubicado en Atenas; el otro probritánico a favor de Venizelos, con sede en Tesalónica.[111]​ Los dos gobiernos se unieron en 1917, cuando Grecia ingresó oficialmente a la guerra del lado de la Triple Entente.[112]​

Poco después de terminada la Primera Guerra Mundial y con la partición del Imperio otomano, Grecia intentó extender sus territorios hacia Asia Menor, que en ese tiempo era una región con una gran población de origen griego, pero salió derrotada en la guerra greco-turca de 1919-1922. Como consecuencia del conflicto y la firma del Tratado de Lausana, ambos países sufrieron un gran intercambio poblacional: los griegos que vivían en territorio turco emigraron a Grecia, y viceversa.[113]​[114]​ Además, miles de griegos pónticos murieron durante la guerra, en un episodio a menudo referido como genocidio de los griegos pónticos, casi medio millón de griegos pónticos fueron asesinados por lo turcos.[115]​[116]​[117]​ Los años siguientes estuvieron caracterizados por la inestabilidad, sumada a la enorme tarea de integrar a más de 1,5 millones de griegos refugiados provenientes de los territorios que se mantuvieron en poder de Turquía dentro de la sociedad de la República Griega. La población reconocida como griega de Estambul (en griego llamada milenariamente Bizancio o Constantinopla),  pasó de 300 000 habitantes en 1900 a cerca de 3000 en 2001.[118]​

Tras los eventos catastróficos en Asia Menor, en 1924 se celebró un referéndum para abolir la monarquía y proclamar la Segunda República Helénica.[119]​ El primer ministro Georgios Kondilis tomó el poder en 1935 y prácticamente abolió la república al traer de vuelta la monarquía con otro referéndum.[120]​ Al año siguiente Ioannis Metaxas dio un golpe de Estado e implantó una dictadura conocida como el régimen del 4 de agosto. A pesar de ser una dictadura de corte fascista, Grecia permaneció en buenos términos con el Reino Unido y se mantuvo alejada de los países del Eje.[121]​

El 28 de octubre de 1940 Italia exigió la rendición de Grecia, pero el gobierno griego se negó y le declaró la guerra.[122]​ En la Guerra greco-italiana, Grecia repelió las fuerzas italianas hacia Albania, la primera victoria de los Aliados en una batalla terrestre.[123]​ Sin embargo, poco después el país cayó derrotado ante las fuerzas alemanas durante la batalla de Grecia. Aunque la ocupación alemana tuvo que lidiar con la resistencia griega, más de 100 000 civiles murieron de inanición durante el invierno de 1941-1942, y la gran mayoría de los judíos griegos fue deportada y asesinada en los campos de concentración nazis.[124]​

Luego de su liberación, Grecia entró en una guerra civil entre las fuerzas comunistas y anticomunistas, lo que trajo consigo un debilitamiento económico y tensiones políticas entre los partidos de derecha y de izquierda.[125]​ Tras la victoria de los anticomunistas, las siguientes dos décadas se caracterizaron por una gran marginalización de los izquierdistas en las esferas políticas y sociales, pero también por un rápido crecimiento económico impulsado en gran parte por el Plan Marshall.[126]​

En julio de 1965, la dimisión ante el rey Constantino II del gobierno centrista de Yorgos Papandréu creó una agitación política que culminó en el golpe de Estado del 21 de abril de 1967 por un grupo de coroneles que estableció una dictadura militar.[127]​ El 17 de noviembre de 1973, la brutal supresión de la revuelta de la Politécnica de Atenas debilitó el régimen, por lo que un consejo nombró al brigadier Dimitrios Ioannidis como dictador.[128]​ El 20 de julio de 1974, mientras Turquía invadía Chipre, el régimen colapsó.[129]​

El antiguo primer ministro Constantinos Karamanlís fue invitado a regresar de su exilio en París y dar comienzo a la era Metapolítefsi.[130]​ En el primer aniversario de la revuelta de la Politécnica de Atenas se celebraron las primeras elecciones multipartidistas desde 1964. El 11 de junio se promulgó una constitución democrática.[131]​ Andreas Papandréu fundó el Movimiento Panhelénico Socialista (PASOK) en respuesta al partido conservador de Karamanlis Nueva Democracia. Desde entonces, ambos partidos se alternaron en el gobierno hasta el año 2015.[132]​

Grecia se convirtió en el décimo miembro de la Comunidad Económica Europea —antecesora de la Unión Europea— el 1 de enero de 1981, impulsada por un periodo de crecimiento constante.[133]​ Las múltiples inversiones en empresas industriales e infraestructura, así como los fondos de la Unión Europea y los ingresos crecientes del turismo, el transporte y el sector servicios, elevaron los niveles de vida del país a una altura sin precedentes. Aunque tradicionalmente las relaciones con Turquía habían sido bastante tensas, estas mejoraron luego de que dos terremotos azotaran a ambos países en 1999.[134]​ Grecia adoptó el euro como moneda en 2001 y albergó los Juegos Olímpicos de Atenas 2004.[135]​[136]​

La economía griega se vio muy afectada por la Gran Recesión de 2008 y fue el protagonista principal en la crisis del euro en 2010.[137]​ La situación se agravó al descubrir que el gobierno derechista de Nueva Democracia presidido por Kostas Karamanlís ocultó durante años, con la ayuda del banco de inversión Goldman Sachs,  algunos datos macroeconómicos, entre ellos el verdadero monto de su deuda externa y el déficit público.[138]​[139]​ El enorme déficit real provocó importantes recortes en el sector público. Esto, unido a la grave crisis económica que durante más de un lustro redujo el PIB del país en más de un cuarto del de 2008,[140]​ situó el paro en un pico del 27 %,[141]​ la población en la pobreza o en riesgo de padecerla superó el 35 % y tres millones de personas se quedaron sin asistencia sanitaria,[141]​ entre otros indicadores. Como respuesta, se produjeron manifestaciones y disturbios en las principales ciudades griegas y más de una treintena de huelgas generales entre 2009 y 2014. (véase Crisis de la deuda soberana en Grecia).[142]​[143]​ En enero de 2015, en el marco de la crisis sin precedentes en el país, el partido SYRIZA ganó las elecciones parlamentarias, siendo la primera vez en la historia de Grecia que un partido a la izquierda de la socialdemocracia alcanzaba el gobierno democráticamente.[144]​

Grecia es una república parlamentaria[145]​ donde el jefe de Estado es el presidente de la República, quien es electo por el Parlamento para un periodo de cinco años.[145]​ La constitución fue redactada y promulgada por el Quinto Parlamento de Revisión de los Helenos y entró en vigor en 1975, luego de la caída de la junta militar que gobernaba al país desde 1967. Desde entonces, se le han hecho tres enmiendas: en 1986, 2001 y 2008. La constitución, que consta de 120 artículos, establece la división de poderes en la ramas ejecutiva, legislativa y judicial, y estipula de manera extensa y específica las garantías de las libertades civiles y los derechos sociales.[146]​[147]​ 


De acuerdo a la constitución, el poder ejecutivo está representado por el presidente de la República y su gabinete.[145]​ La enmienda constitucional de 1986 limitó muchas de las tareas del presidente, y ahora la mayor parte de sus funciones son solo ceremoniales; por lo tanto, la mayor parte del poder político recae en las manos del primer ministro.[148]​ El puesto del primer ministro, el jefe de Gobierno de Grecia, pertenece al líder actual del partido político que obtenga el voto de confianza del Parlamento. El presidente nombra formalmente al primer ministro y, basado en sus recomendaciones, elige o destituye a los otros miembros del Gobierno.[145]​
El poder legislativo está representado por un parlamento unicameral compuesto por 300 miembros elegidos por el pueblo.[145]​ Los estatutos aprobados por el Parlamento son promulgados por el presidente de la República.[145]​ Las elecciones parlamentarias se celebran cada cuatro años, pero el presidente está obligado a disolver el Parlamento un poco antes, durante la elección del gabinete, en vistas de lidiar con un problema nacional de importancia excepcional.[145]​ El presidente también está obligado a disolver el Parlamento antes si la oposición aprueba una moción de no confianza.[145]​ Desde que se restauró la democracia, el sistema bipartidista griego ha sido dominado por los liberales-conservadores de Nueva Democracia (Grecia) (ND) y los socialdemócratas del Movimiento Panhelénico Socialista (PASOK).[149]​

El poder judicial es independiente del ejecutivo y legislativo, y está encabezado por tres Cortes Supremas: la Corte de casación, el Consejo de Estado y la Corte de Cuentas, los tribunales más altos en el país.[150]​ El sistema judicial también está integrado por las cortes civiles, que juzgan los casos civiles y penales, y las cortes administrativas, las cuales resuelven las disputas entre los ciudadanos y las autoridades administrativas.[4]​

La Policía Helénica es la fuerza policíaca nacional de Grecia. Es una agencia muy grande cuyas responsabilidades abarcan desde el control del tráfico vehicular hasta las acciones contra el terrorismo. Se estableció en 1984 por la Ley 1481/1-10-1984, como resultado de la fusión de las fuerzas de la Gendarmería y la Policía Citadina.[151]​

El Ministerio de Relaciones Exteriores es el encargado de la política exterior de Grecia. El ministro de Relaciones Exteriores afirma que los objetivos principales del ministerio son: representar a Grecia ante otros Estados y organizaciones internacionales; salvaguardar los intereses del Estado griego y de sus ciudadanos en el extranjero; promover la cultura griega; buscar relaciones más estrechas con la diáspora griega; y promover la cooperación internacional.[152]​  Además, Grecia desarrolló una política exterior regional en la que se compromete a promover la paz y la estabilidad en los Balcanes, el Mediterráneo y el Medio Oriente.[153]​

A su vez, el ministerio identifica tres situaciones que son de particular importancia para el gobierno griego: los reclamos de Turquía sobre lo que el ministerio define como «la soberanía griega sobre el mar Egeo y el espacio aéreo correspondiente»; la legitimidad de la República Turca del Norte de Chipre en la isla de Chipre; y la disputa sobre el nombre de Macedonia con el pequeño país balcánico que comparte su nombre con la región más grande y la segunda más poblada de Grecia.[154]​ Grecia es miembro de numerosas organizaciones internacionales, incluyendo el Consejo de Europa, la Unión del Mediterráneo, la Organización del Tratado del Atlántico Norte y la Organización de las Naciones Unidas, del cual es un miembro fundador.[4]​

Las Fuerzas Armadas de Grecia son controladas por el Estado Mayor de la Defensa Nacional de Grecia y por el Ministerio de la Defensa Nacional. Las fuerzas armadas consisten de tres ramas: el Ejército, la Armada y la Fuerza Aérea.[155]​ Además de las anteriores, Grecia también cuenta con la Guardia Costera Helénica para la aplicación de la ley en el mar y operaciones de búsqueda y rescate.[156]​

El servicio militar es obligatorio en Grecia para todos los hombres. Aunque las mujeres están exentas de este, pueden alistarse en el ejército. Todos los hombres entre 19 y 45 años deben servir un total de nueve meses o un año.[4]​ Sin embargo, a medida que las fuerzas armadas se van convirtiendo cada vez más en un ejército profesional, el gobierno piensa en la posibilidad de reducir o abolir el servicio militar obligatorio.[157]​

La Guardia Costera puede requerir el servicio de todos aquellos hombres de entre 18 y 60 años que vivan en zonas estratégicamente sensibles. Este servicio no es de tiempo completo y es pagado. Como miembro de la OTAN, Grecia participa en ejercicios militares bajo el patrocinio de sus aliados. En 2012 Grecia invirtió más de US$ 7000 millones en la milicia, es decir, 1,7 % de su PIB.[4]​

Grecia mantiene pleno interés en lo que respecta a derechos humanos y el tema del calentamiento global. En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Grecia ha firmado o ratificado:

Para su administración, Grecia se divide en siete administraciones descentralizadas (αποκεντρωμένες διοικήσεις), divididas a su vez en trece periferias (περιφέρειες), estas en setenta y cuatro unidades periféricas (περιφερειακές ενότητες) y estas últimas en 325 municipios (δήμοι). Existe una región aparte, el Monte Athos, que posee una autonomía propia bajo soberanía griega.[164]​

El territorio griego es principalmente montañoso y conforma una península que sobresale del extremo sur de los Balcanes y termina en la península del Peloponeso, la cual está separada del resto del continente por el canal de Corinto, que atraviesa el istmo de Corinto.[4]​ Debido a su litoral tan accidentado y a las numerosas islas, Grecia cuenta con la décima línea costera más extensa del mundo, con 13 676 km.[165]​ Al norte, comparte frontera con Albania, Macedonia del Norte y Bulgaria, y al noreste con Turquía; sus fronteras terrestres abarcan más de 1160 km.[4]​ El país yace aproximadamente entre las latitudes 34° y 42° N y las longitudes 19° y 30° E.[4]​

El territorio griego también comprende entre 1200 y 6000 islas —el número depende de la definición de isla—, 227 de las cuales se encuentran habitadas.[13]​ De estas, Creta es la isla más grande y más poblada; Euboea, separada del continente por los 60 m del estrecho de Euripo, es la segunda más extensa, seguida de Rodas y Lesbos.[166]​

Comúnmente las islas griegas se agrupan en seis conjuntos: las islas Sarónicas, ubicadas en el golfo Sarónico, cerca de Atenas; las Cícladas, una colección grande y densa que ocupa la parte central del mar Egeo; las islas del Egeo Norte, un grupo disperso frente a la costa occidental de Turquía; el Dodecaneso, otra colección dispersa en el sureste entre Creta y Turquía; las Espóradas, un pequeño grupo en la costa nororiental de Euboea; y las islas Jónicas, localizadas al oeste del continente en el mar Jónico.[167]​[168]​

Entre el 70 % y el 80 % del territorio griego está cubierto por montañas y colinas, por lo que es uno de los países más montañosos de Europa.[169]​ El monte Olimpo, la mítica morada de los dioses griegos, es el punto más alto del país, pues la cima Mytikas alcanza los 2917 m s. n. m..[4]​ En el oeste de Grecia hay varios lagos y pantanos, pero el terreno está dominado por la cordillera del Pindo. El Pindo, una continuación de los Alpes Dináricos, alcanza los 2637 m s. n. m. en el monte Smolikas —el segundo punto más alto en Grecia— e históricamente ha sido una barrera importante para los viajes que cruzan de este a oeste entre Tesalia y Epiro.[170]​

La cordillera del Pindo continúa su trayecto hacia el centro del Peloponeso, cruza las islas de Citera y Anticitera hacia el suroeste del Egeo, hasta llegar a la isla de Creta, donde termina. Las islas del mar Egeo son las cimas de montañas submarinas, que alguna vez fueron la continuación de las cordilleras continentales.[171]​ El Pindo se caracteriza por su cumbres altas y escarpadas, a menudo interrumpidas por numerosos cañones y una gran variedad de paisajes kársticos. El Libro Guinness de los récords reconoce a la barranca de Vikos, parte del parque nacional del Vikos-Aoos, como el cañón más profundo del mundo.[172]​

En el noreste de Grecia se encuentra otra cadena montañosa de gran altitud, los montes Ródope, que atraviesa la región de Macedonia Oriental y Tracia. Esta zona está cubierta por varios bosques extensos y frondosos, incluyendo el famoso bosque de Dadia.[171]​ Las grandes llanuras generalmente se encuentran en las regiones de Tesalia, Macedonia Central y Tracia. Como son uno de los pocos terrenos cultivables en el país, estas regiones constituyen una parte importante de la economía.[173]​

El clima de Grecia es en su mayor parte mediterráneo con inviernos templados y húmedos y veranos cálidos y secos. Este clima predomina en todas las regiones costeras, incluyendo Atenas, las Cícladas, el Dodecaneso, Creta, el Peloponeso, las islas Jónicas y partes de la región de Grecia Central.[174]​ La cordillera del Pindo afecta enormemente el clima del país, pues las zonas al oeste de las montañas son considerablemente más húmedas que el resto, debido a la mayor exposición a los sistemas suroccidentales que traen consigo más humedad, mientras que las zonas orientales sufren del efecto de la sombra orográfica.[174]​[171]​

Las zonas montañosas del noroeste griego —partes de Epiro, Grecia Central, Tesalia, Macedonia Occidental— así como los montes de las partes centrales del Peloponeso —incluidas algunas áreas de Acaya, Arcadia y Laconia— tienen un clima alpino con nevadas intensas. El interior de las regiones del norte, como Macedonia Occidental y Macedonia Oriental y Tracia, tienen un clima templado con veranos cálidos y secos e inviernos fríos y húmedos con tormentas eléctricas frecuentes.[174]​ Las nevadas ocurren todos los años en el norte y en las zonas montañosas, incluso las nevadas pequeñas se pueden presentar en regiones más al sur y con menos altitud, como Atenas.[175]​

El 30 % de Grecia está cubierta por bosques con vegetación, que varía desde coníferas alpinas a la vegetación mediterránea.[176]​  Según el WWF, el territorio de Grecia se reparte entre ocho ecorregiones diferentes. El bosque templado de frondosas se divide en el bosque mixto balcánico de las tierras bajas del norte y en el bosque mixto de los montes Ródope.[177]​  El bosque mediterráneo se puede clasificar en cuatro tipos: bosque caducifolio de Iliria en el extremo noroeste, bosque mixto de los montes Pindo, bosque mediterráneo de Creta y el bosque esclerófilo y mixto del Egeo y Turquía occidental.[178]​

Gracias a la variedad de climas y paisajes, en Grecia se encuentran más de 5500 especies de plantas.[176]​ En las tierras bajas es común encontrar árboles frutales como naranjos, olivos, dátiles, almendros, granadas, higueras y la vid. En terrenos más altos abundan los pinos, robles y castaños. En las zonas montañosas por encima de los 1070 m s. n. m. crecen árboles como la haya y el abeto.[179]​

A principios de los años 2000, había 116 especies de mamíferos, 422 de aves, 126 de peces, 60 de reptiles y 20 de anfibios, aunque se espera que estas cifras vayan en descenso.[176]​ En los mares que rodean a la Grecia continental habitan algunas especies marinas únicas como las focas pinípedas y la tortuga boba. Por su parte, en los bosques densos habitan algunas especies de mamíferos amenazadas como el oso pardo, el lince y el corzo.[171]​ La cabra salvaje, extinta en el resto de Europa, aún vive en algunas montañas e islas del país.[179]​

Los ríos de Grecia son pequeños y suelen desembocar en el mar Jónico y Adriático. Los griegos antiguos trataban a sus ríos como dioses por su belleza excepcional. Los ríos más largos de Grecia son el Evros, el Axiós y el Haliacmón.[180]​

Según datos del Fondo Monetario Internacional, la economía de Grecia es la 50.ª más grande del mundo, con US$ 226 774 millones por su PIB nominal o paridad de poder adquisitivo, respectivamente. Además, Grecia es la 15.ª economía más grande entre los 27 miembros de la Unión Europea.[181]​

Grecia es considerado un país desarrollado con altos estándares de vida. Su economía se compone por el sector servicios (85 %), la industria (12 %) y el sector primario (3 %).[182]​ Las industrias griegas más importantes incluyen el turismo —con 15,5 millones de turistas internacionales en 2012, el séptimo país más visitado en la Unión Europea y el 16° a nivel internacional—[183]​ y la industria marítima —con 16,2 % de la capacidad mundial total, la marina mercante griega es la más grande del mundo—.[184]​ Grecia también es un productor de materias primas importante dentro de la Unión Europea, especialmente en relación a la pesca.[185]​[186]​

Con una economía más grande que la de todos los países balcánicos combinados, también es la economía más grande de la región,[24]​[25]​[26]​ y un importante inversor regional.[24]​[25]​ Grecia es el segundo inversor más importante en Albania, el tercero en Bulgaria, Rumania y Serbia y el socio comercial más importante de Macedonia del Norte. Se estima que cada semana un banco griego abre una nueva sucursal en algún lugar de los Balcanes.[187]​[188]​[189]​[187]​

El Fondo Monetario Internacional clasifica a Grecia como una economía avanzada[190]​[191]​[192]​ de ingresos altos.[193]​ Es un miembro fundador de la Organización para la Cooperación y el Desarrollo Económico (OCDE) y de la Cooperación Económica del Mar Negro (CEMN). En 1979 accedió a la Comunidad Económica Europea y a su mercado común, un proceso que terminó en 1982. En enero de 2001 adoptó el euro para reemplazar su antigua moneda, la dracma, a una tasa de cambio de 340,75 dracmas por euro.[194]​

Para finales de 2009, como resultado de una combinación de factores locales e internacionales, la economía griega enfrentó su crisis más severa desde la restauración de la democracia en 1974, pues el gobierno corrigió la cifra de su déficit, que pasó de un estimado de 6 % del PIB a un 12,7 %.[195]​[196]​

A principios de 2010, se descubrió que los gobiernos de Grecia, Italia y otros países europeos, mediante la asistencia de Goldman Sachs, JPMorgan Chase y otros bancos, desarrollaron varios instrumentos financieros que les permitieron esconder todos sus préstamos.[197]​[138]​ De este modo, cada país fue capaz de gastar más de su presupuesto y al mismo tiempo seguir respetando el déficit límite impuesto por la Unión Europea.[138]​[139]​ En mayo de 2010, el déficit del gobierno griego fue revisado nuevamente y se estimó que alcanzó el 13,6 % del PIB,[198]​ el segundo déficit más alto del mundo, solo después del de Islandia.[199]​ De acuerdo a una serie de estimaciones, se predijo que la deuda pública se elevaría hasta un 120 % del PIB durante 2010.[200]​

Como consecuencia, hubo una crisis de confianza internacional sobre la capacidad de Grecia para pagar su deuda pública. Para prevenir dicha situación, en mayo de 2010 otros países de la eurozona, junto al FMI, acordaron un paquete de rescate en el que le entregarían a Grecia alrededor de €45 000 millones en bonos de manera inmediata, para posteriormente seguir otorgando más préstamos hasta totalizar €110 000 millones.[201]​[202]​ Para asegurar estos fondos, Grecia tuvo que adoptar una serie de medidas de austeridad extremas para mantener su déficit bajo control.[203]​

En 2011 se volvió aparente que el rescate financiero sería insuficiente, por lo que se aprobó un segundo paquete de €130 000 millones para 2012, sujeto a condiciones estrictas, incluyendo reformas financieras y más medidas de austeridad.[204]​ En 2013, Grecia alcanzó un superávit en el presupuesto gubernamental primario. En abril del siguiente año, el país regresó al mercado de bonos global y según el FMI, tras cinco años de declives, se espera que el PIB crezca un 0,6 % para 2014.[143]​

En 2010, Grecia fue el principal productor de algodón (183 800 t) y pistaches (8000 t) en la Unión Europea;[185]​ fue el segundo mayor productor de arroz (229 500 t)[185]​ y aceitunas (147 500 t);[186]​ el tercero en higos (11 000 t),[186]​ almendras (44 000 t),[186]​ tomates (1 400 000 t)[186]​ y sandías (578 400 t);[186]​ y el cuarto en tabaco (22 000 t).[185]​ La agricultura contribuye con el 3,8 % del PIB y emplea a 12,4 % de la fuerza laboral.[4]​

Grecia es uno de los principales beneficiarios de la Política Agrícola Común de la Unión Europea.[205]​ Como resultado de su acceso a la Comunidad Europea, gran parte de su infraestructura agrícola se actualizó para aumentar su producción. Entre 2000 y 2007 el número de granjas orgánicas aumentó un 885 %, el incremento más alto dentro de la Unión Europea.[206]​

La industria marítima es un elemento clave de la economía griega que data desde tiempos antiguos.[207]​ Conforma el 4,5 % del PIB, da empleo a cerca de 160 000 personas (4 % de la fuerza de trabajo) y representa un tercio del déficit comercial del país.[208]​

Durante los años 1960, el tamaño de la flota mercantil griega casi se duplicó, principalmente debido a las inversiones hechas por los magnates de la navegación Aristóteles Onassis y Stavros Niarchos.[209]​ La base de la industria marítima griega moderna se estableció después de la Segunda Guerra Mundial, cuando los empresarios griegos fueron capaces de obtener ganancias de los botes vendidos por el gobierno de los Estados Unidos, gracias a la Ley de Venta de Botes de los años 1940.[209]​

De acuerdo a la Conferencia de las Naciones Unidas sobre Comercio y Desarrollo, en 2013 la marina mercante griega era la más grande del mundo, con 15,1 % de la capacidad mundial total,[184]​ una disminución del 16,2 % de 2011.[210]​  También posee el mayor tonelaje total en su flota mercantil, con 244 millones de TPM.[184]​

En términos de número de navíos, la marina mercante griega es la cuarta más grande a nivel internacional, con 3695 buques —825 de ellos están registrados en Grecia y los restantes 2870 en otros puertos—.[184]​ En términos de categorías, Grecia tiene el mayor número de cargueros y graneleros, y ocupa el cuarto lugar por número de portacontenedores.[211]​ No obstante, la flota actual es mucho más pequeña en comparación a los años 1970, cuando llegó a tener más de 5000 navíos.[207]​ Además, el número total de buques que portan la bandera griega es de 885, es decir, 5,3 % del TPM mundial.[184]​

Un porcentaje importante de los ingresos nacionales provienen del turismo, que aporta el 16 % del PIB.[212]​ De acuerdo a estadísticas de Eurostat, Grecia recibió a más de 17,1 millones de turistas en 2011,[213]​ lo cual representa un decremento de los 17,7 millones reportados en 2007.[214]​

En 2007, la gran mayoría de los turistas provenían del continente europeo, unos 15,7 millones.[215]​ De estos, los visitantes del Alemania —2,2 millones— y el Reino Unido —1,8 millones— fueron los más numerosos.[215]​ En 2010, la región más visitada de Grecia fue Macedonia Central, que acaparó el 18 % del flujo turístico total —3,6 millones de turistas—, seguida por Ática —2,6 millones— y el Peloponeso —1,8 millones—.[213]​ 

En 2012, Lonely Planet colocó a Tesalónica como la quinta mejor ciudad para festejar en el mundo, comparable a otras ciudades como Dubái y Montreal.[216]​ En 2014, los lectores de Travel + Leisure votaron a Santorini como «La mejor isla del mundo».[217]​ La isla vecina de Miconos quedó en el quinto puesto en la categoría europea.[217]​

La producción de energía en Grecia está dominada por la Compañía Pública de Energía —conocida por su acrónimo griego ΔΕΗ, o DEI—. En 2009 DEI suministró el 85,6 % de toda la demanda energética del país,[218]​ cifra que cayó el 77,3 % en 2010.[218]​

Un 90 % de la electricidad de Grecia se genera en instalaciones termoeléctricas de lignito, carbón o derivados del petróleo.[218]​ En 2010, la capacidad generadora de energía de Grecia era de unos 15,1 millones de kW, y su producción anual de 56 200 millones kWh.[4]​ El 12 % de la electricidad proviene de la energía hidroeléctrica y otro 20 % del gas natural.[219]​ Entre 2009 y 2010, la producción de energía de las compañías independientes se incrementó en un 56 %,[218]​ pues pasó de 2709 kWh en 2009 a 4232 GWh en 2010.[218]​

En 2012 la energía renovable representó el 13,8 % del consumo total de energía nacional.[220]​ La cifra aumentó del 10,6 % en 2011,[220]​ y casi igualó el promedio de la Unión Europea del 14,1 %.[220]​ Un 10 % de la energía renovable del país proviene de la energía solar,[206]​ mientras que la mayor parte proviene de la biomasa y otros desechos reciclables.[206]​ De acuerdo a la Dirección de Energía Renovable de la Comisión Europea, Grecia intenta obtener el 18 % de su energía en 2020 de fuentes renovables.[221]​ Grecia no cuenta con ninguna central nuclear en operación, aunque en 2009 la Academia de Atenas sugirió iniciar el proceso de construcción de la primera planta nuclear del país.[222]​

Desde los años 1980, la red de carreteras y ferrocarriles de Grecia se ha ido modernizando. Las obras más sobresalientes incluyen la autopista Egnatía Odós, que conecta el noroeste de Grecia (Igoumenitsa) con el noreste (Kipoi); y el puente de Río-Antírio, el puente suspendido por cable más largo de Europa (2250 m), que conecta el Peloponeso desde Río con Antírio en Grecia Central.[223]​

Además, hay varios proyectos importantes que se encuentran en desarrollo como: la conversión de la carretera GR-8A —que conecta Atenas con Patras y Pirgos en el oeste del Peloponeso— en una autopista moderna en toda su extensión; la modernización de algunas secciones de la autopista A1 —que comunica a Atenas con Tesalónica—; y la construcción del metro de Tesalónica.[224]​[225]​

El sistema de transporte del área metropolitana de Atenas se transformó para mejorar los problemas de tráfico y contaminación que afectaban a la región, gracias a la construcción y modernización de obras como la red de carreteras de Attiki Odos, el metro y el Aeropuerto Internacional de Atenas.[226]​

Las dos aerolíneas más grandes del país, Olympic Air y Aegean Airlines, conectan por aire a la mayoría de las islas griegas y las ciudades principales del continente.[227]​ Se han mejorado las conexiones marítimas con naves modernas de alta velocidad, tales como el hidroala y el catamarán.[228]​

En comparación con otros países europeos, el ferrocarril juega un papel menor en Grecia, pero también se ha expandido gracias a los nuevos trenes suburbanos que recorren los alrededores de Atenas, Tesalónica y Patras.[229]​ En 2014 se volvieron a abrir las líneas internacionales que conectan a Grecia con el resto de Europa, los Balcanes y Turquía, que anteriormente habían sido suspendidas debido a la crisis financiera.[230]​

Grecia cuenta con servicios de radio y televisión tanto privados como estatales. A pesar de contar con más de 150 estaciones privadas de televisión, solo diez de ellas tienen alcance nacional; el gobierno solo opera una transmisora, NERIT. De manera similar, hay más de 1500 estaciones de radio, solo dos de ellas pertenecen al gobierno.[4]​ La mayoría de la prensa diaria griega se publica en Atenas y Tesalónica. Entre los diarios de mayor circulación están el Kathimeriní, el Eleftherotypia y el Ta Nea, todos ellos impresos en Atenas.[231]​ La Organización Helénica de la Telecomunicación OTE es el operador histórico de telecomunicaciones en Grecia.[232]​

En todo el país existen más de 35 000 km de cable de fibra óptica y una gran sistema de red abierta. También existen más de 2 252 653 conexiones a Internet de banda ancha, lo que representa una penetración del 20 %.[233]​ En 2012, 53,6 % de los trabajadores utilizaban el Internet de manera regular, 94,8 % de ellos tenía una conexión de banda ancha.[234]​

Los cibercafés que ofrecen acceso a Internet, servicios de oficina y videojuegos también son comunes por todo el país, mientras que las conexiones móviles por 3G y Wi-Fi pueden encontrarse prácticamente en cualquier punto del territorio griego.[235]​ El uso del Internet móvil de 3G ha tenido un aumento vertiginoso en años recientes, pues entre 2011 y 2012 su uso se incrementó un 340 %.[236]​ La Unión Internacional de Telecomunicaciones colocó a Grecia entre los primeros treinta países con la mejor infraestructura de información y telecomunicaciones.[237]​

La Secretaría General de Investigación y Tecnología del Ministerio de Desarrollo es la responsable a nivel nacional del diseño, implementación y supervisión de las políticas de investigación y tecnología.[238]​ En 2012, el gasto público en investigación y desarrollo (I+D) fue de US$ 1600 millones, es decir, un 0,5 % del PIB.[239]​ El gasto total en I+D como porcentaje del PIB se incrementó considerablemente desde inicios de los años 1990, pues pasó de 0,38 % en 1989 a 0,65 % en 2001. Aunque el gasto en I+D permaneció por debajo del promedio de la Unión Europea (1,93 %), el crecimiento que tuvo la inversión griega en esta zona fue uno de los más importantes en el continente, solo por detrás de Finlandia e Irlanda.[240]​ Gracias a su localización estratégica y a su mano de obra calificada, muchas compañías multinacionales como Microsoft, Nokia y Telekom cuentan con cuarteles regionales de investigación y desarrollo en Grecia.[241]​

Los parques tecnológicos de Grecia con facilidades para incubadoras incluyen: el Parque de Ciencia y Tecnología de Creta, el Parque Tecnológico de Tesalónica, el Parque Tecnológico de Lavrio, el Parque de Ciencias de Patras y el Parque de Ciencia y Tecnología de Epiro.[242]​ Grecia es miembro de la Agencia Espacial Europea (ESA) desde 2005.[243]​ La cooperación entre la ESA y el Comité Nacional Espacial de Grecia comenzó en 1994, cuando Grecia y la agencia firmaron su primer acuerdo de colaboración. Como miembro de ESA, Grecia participa en las actividades de telecomunicaciones y tecnología de la agencia, y de la iniciativa Global Monitoring for Environment and Security.[244]​

En 2011, 26 % de la población de Grecia acudió a una institución de educación superior, una cifra debajo del promedio de la OCDE (32 %).[245]​ Sin embargo, también se debe considerar que miles de estudiantes griegos asisten a las universidades de Europa Occidental cada año.[246]​ Algunos de los científicos griegos más notables de los tiempos modernos incluyen a Dimitrios Galanos, Georgios Papanikolaou, Nicholas Negroponte, Constantin Carathéodory, Manolis Andronikos, Michael Dertouzos, John Argyris, Panagiotis Kondylis, John Iliopoulos, Joseph Sifakis, Christos Papadimitriou, Mihalis Yannakakis y Dimitri Nanopoulos.[247]​

De acuerdo al censo realizado por la Autoridad Estadística Helénica en 2011, la población total del país sumaba los 10 815 197 habitantes.[248]​ El censo arrojó que había 9 903 268 ciudadanos griegos (91,56 %), 480 824 albaneses (4,44 %), 75 915 búlgaros (0,7 %), 46 523 rumanos (0,43 %), 34 177 pakistaníes (0,32 %), 27 400 georgianos (0,25 %) y 247 090 personas de origen desconocido (2,3 %).[8]​ 189 000 personas del total de la población albanesa eran étnicamente griegos del norte de Epiro.[249]​ La tasa de nacimiento en 2014 fue de 8,8 por cada 1000 habitantes, significativamente menor a la tasa de 14,5 por cada 1000 reportada en 1981. Al mismo tiempo, la tasa de mortalidad se incrementó de 8,9 por cada 1000 habitantes en 1981 a 11 en 2014.[4]​

La sociedad griega ha cambiado rápidamente en las últimas décadas. La caída de su índice de fertilidad llevó a un incremento en la edad promedio, que coincide con el envejecimiento de Europa. En 2001, el 16,71 % de la población era mayor de 65 años, el 68,12 % tenía entre 15 y 64 años y el 15,18 % era menor de 14 años.[250]​ El número de matrimonios descendió de casi 71 por cada 1000 habitantes en 1981 hasta 51 en 2004.[250]​ Además, el número de divorcios aumentó de 191,2 por cada 1000 matrimonios en 1991 a 239,5 en 2004.[250]​ 

Durante el siglo XX, millones de griegos emigraron a Estados Unidos, Reino Unido, Australia, Canadá y Alemania, fenómeno conocido como diáspora griega. La migración neta empezó a mostrar números positivos desde la década de 1970, pero hasta inicios de los años 1990, el principal movimiento migratorio era el de los griegos que regresaban a su país.[249]​

Un estudio del Observatorio Mediterráneo de Migración mantiene que el censo de 2001 registró a 762 191 personas que residían en Grecia sin tener la ciudadanía, es decir, el 7 % de la población. De estos, 48 560 provenían de la UE o de la Asociación Europea de Libre Comercio y 17 426 eran chipriotas con estatus privilegiado. El resto provenía principalmente de países de Europa oriental: Albania (56 %), Bulgaria (5 %) y Rumania (3 %), mientras que los inmigrantes de la antigua Unión Soviética —Georgia, Rusia, Ucrania, Moldavia, etc.— comprendían 10 % del total.[251]​

Los grandes centros urbanos contienen las principales concentraciones de los inmigrantes que no provienen de la UE, especialmente en el municipio de Atenas, donde los 27 000 inmigrantes representan el 7 % de la población. También hay un número considerable de co-étnicos que provienen de comunidades griegas en Albania o la antigua Unión Soviética.[249]​

Grecia, junto con Italia y España, se enfrenta a un gran número de inmigrantes que intentan entrar a la UE. Muchos de los inmigrantes ilegales que entran a Grecia lo hacen por el río Maritsa, en la frontera con Turquía. En 2012, la mayoría de ellos provenían de Afganistán, Pakistán y Bangladés.[252]​ Desde ese año, diariamente se llevan a cabo varias operaciones policíacas —llamadas «Xenios Zeus»— en Atenas y otras ciudades griegas con el fin de detener a los inmigrantes ilegales. Como resultado, más de 15 000 personas han sido detenidas y varios miles han tenido que verificar su estatus de residencia. De esta forma, se estima que los inmigrantes solo constituyen el 4 % de la población.[253]​

La constitución reconoce a la fe cristiana ortodoxa como la religión «predominante» en el país, al mismo tiempo que garantiza la libertad de culto a sus ciudadanos.[145]​ El gobierno griego no mantiene estadísticas sobre grupos religiosos y en los censos no interroga sobre la afiliación religiosa. De acuerdo al Departamento de Estado de los Estados Unidos, se estima que el 98 % de los griegos se identifica como cristiano ortodoxo, perteneciente a la Iglesia ortodoxa de Grecia.[254]​

El Eurobarómetro de 2010 mostró que el 79 % de los griegos respondieron que creían que «existe un Dios».[255]​ De acuerdo a otras fuentes, el 15,8 % de los griegos se califica de «muy religioso», el porcentaje más alto entre los países europeos. En contraste, solo el  3,5 % no va nunca a la iglesia, comparado con el 4,9 % en Polonia y el 59,1 % en la República Checa.[256]​

Se estima que cerca de 50 000 ciudadanos griegos son católicos,[254]​[257]​ quienes junto con otros inmigrantes católicos suman más de 200 000 personas.[254]​ Los veteranocalendaristas griegos tienen más de medio millón de seguidores.[257]​  Los protestantes, incluida la Iglesia evangélica griega y las Iglesias evangélicas libres, suman más de 30 000 practicantes.[254]​[257]​ Las Asambleas de Dios, la Iglesia Internacional del Evangelio Cuadrangular y otras iglesias pentecostales tienen más de 12 000 miembros.[258]​ La Iglesia Apostólica Libre de Pentecostés es la denominación protestante más grande del país, con más de 120 templos.[259]​ No existen estadísticas oficiales sobre esta Iglesia, pero los cristianos ortodoxos estiman que tiene más de 20 000 seguidores.[260]​ Por su parte, los testigos de Jehová reportan que cuentan con más de 28 859 miembros activos.[254]​

Las estimaciones que reconocen a la minoría musulmana en Grecia, en su mayor parte localizada en Tracia, varían de 98 000 a 140 000 personas —cerca del 1 % de la población—,[254]​[257]​ mientras que la comunidad inmigrante musulmana ronda entre los 200 000 y 300 000. Los inmigrantes albaneses se suelen relacionar con el islam, aunque la mayoría tiende a una orientación más secular.[261]​ Luego de la guerra greco-turca de 1919-1922 y el Tratado de Lausana de 1923, Grecia y Turquía accedieron a intercambiar su población, basados en su identidad cultural y religiosa. Cerca de 500 000 musulmanes de Grecia, en su mayoría turcos, fueron intercambiados por aproximadamente 1,5 millones de griegos de Anatolia.[262]​

El judaísmo ha existido en Grecia desde hace más de 2000 años. Los judíos sefardíes solían tener una gran presencia en la ciudad de Tesalónica: en 1900, unas 80 000 personas, la mitad de la población, eran judíos.[263]​ Sin embargo, actualmente la comunidad greco-judía, que sobrevivió a la ocupación alemana y al Holocausto durante la Segunda Guerra Mundial, no sobrepasa los 5500 personas.[254]​[257]​

En Grecia también se practica el dodecateísmo o neohelenismo, versión moderna de la religión practicada en la antigua Grecia (creencia en los dioses olímpicos), con unos 100.000 seguidores, según estimaciones de los líderes del movimiento.[264]​[265]​[266]​[267]​

Grecia es un país homogéneo en términos lingüísticos, pues la mayor parte de la población utiliza el griego como su primera o única lengua.[268]​[269]​ Entre los grecoparlantes, los que hablan el dialecto póntico llegaron de Grecia asiática luego del genocidio griego, y constituyen un grupo considerable.[115]​ En la zona oriental de la península del Peloponeso, a relativamente pocos (para los tiempos modernos) kilómetros al suroeste de Atenas, aún se habla el dialecto griego tsacónico.

La primera evidencia textual del idioma griego data del siglo XV a. C., una escritura conocida como Lineal B que se asocia con la civilización micénica. Durante la antigüedad clásica el griego fue una lingua franca muy utilizada en el mundo mediterráneo y más allá, y se convirtió en el idioma oficial del Imperio bizantino.[270]​

Durante los siglos XIX y XX hubo una gran disputa conocida como la cuestión lingüística griega, sobre si el idioma oficial de Grecia debiese ser el katharévousa, creado en el siglo XIX y utilizado como el lenguaje académico y por el gobierno, o el demótico, la forma del griego que evolucionó naturalmente del griego bizantino y era el idioma utilizado por el pueblo. La disputa finalmente se resolvió en 1976, cuando el demótico se declaró la única variación oficial del griego, y el katharévousa cayó en desuso.[102]​

La minoría musulmana en Tracia comprende hablantes del turco, búlgaro (pomacos), romaní («gitano»), y en una pequeña región de la Macedonia limítrofe con Macedonia del Norte aún se habla (desde al parecer la Edad Media) la lengua romance llamada meglenítica.[269]​ Los gitanos cristianos de otras partes del país también hablan el romaní.[271]​ Así, en el actual relativamente pequeño territorio del Estado griego aún existen otros idiomas minoritarios tradicionalmente hablados por grupos regionales pequeños, aunque uso ha disminuido en el transcurso del último siglo por a la asimilación de la mayoría grecoparlante. Hoy en día solo se mantienen por las generaciones mayores y están en peligro de desaparecer.[272]​ Entre ellos están los arbanitas, un grupo de habla albanesa ubicado en las zonas rurales alrededor de Atenas, los aromunes y los ya citados meglenorrumanos. Los dos últimos también son conocidos como valacos, sus idiomas están muy ligados al rumano y solían habitar las zonas montañosas del centro de Grecia. Los miembros de estos grupos se identifican étnicamente como griegos y todos pueden hablar griego.[273]​[272]​

Cerca de la frontera norte también hay algunos hablantes de lenguas eslavas, conocidos localmente como «eslavomacedonios», un grupo donde la mayoría de sus miembros se identifica étnicamente como griego. Sus dialectos pueden ser clasificados lingüísticamente como variantes del macedonio eslavo o del búlgaro.[274]​[275]​ Se cree que tras el intercambio poblacional de 1923, Macedonia tenía entre doscientos y cuatrocientos mil hablantes eslavos.[118]​ La comunidad judía de Grecia tradicionalmente habló el judeoespañol, que hoy en día se mantiene solo por unos cuantos miles de hablantes.[276]​

Los griegos tienen una tradición muy antigua por la evaluación y el desarrollo educativo (paideia).[277]​ La educación era uno de los valores sociales más importantes dentro del mundo griego clásico y helenístico, además de que aquí surgieron las primeras universidades en Europa durante el siglo V, las cuales siguieron en operación hasta la caída de Constantinopla en 1453.[278]​ La Universidad de Constantinopla fue la primera institución de educación superior secular, pues no se impartían cátedras teológicas.[279]​

La educación obligatoria en Grecia comprende el preescolar (Νηπιαγωγείο, Nipiagogeío), la primaria (Δημοτικό Σχολείο, Dimotikó Scholeío) y el gimnasio (Γυμνάσιο, Gymnásio).[280]​  Las guarderías (Παιδικός σταθμός, Paidikós Stathmós) son populares pero su asistencia no es obligatoria. La educación preescolar es obligatoria para cualquier niño mayor de cuatro años. La escuela primaria comienza para los niños con seis años de edad y comprende seis grados. La asistencia al gimnasio comienza a los doce años y continúa durante tres años.[281]​ El sistema de educación griego también provee escuelas para estudiantes con capacidades diferentes o dificultades para el aprendizaje. También existen secundarias y gimnasios especializados que ofrecen educación física, musical y tecnológica.[282]​

La educación pos-secundaria en Grecia no es obligatoria y hay dos tipos: las escuelas secundarias superiores unificadas (Γενικό Λύκειο, Genikό Lýkeio) y las escuelas técnicas-vocacionales (Τεχνικά και Επαγγελματικά Εκπαιδευτήρια, TEE).[281]​  La educación pos-secundaria también incluye institutos de entrenamiento vocacional (Ινστιτούτα Επαγγελματικής Κατάρτισης, IEK) que proveen un nivel de educación formal pero sin clasificar. Como pueden aceptar alumnos de Gymnásio —graduados de la escuela secundaria— y de Lýkeio —graduados de la escuela secundaria superior—, el nivel de educación que brindan estos institutos no puede ser clasificado.[283]​

De acuerdo al Marco Legal (3549/2007), la educación superior se imparte en los Institutos de Educación Superior (Ανώτατα Εκπαιδευτικά Ιδρύματα, Anótata Ekpaideftiká Idrýmata, AEI), que consiste de dos sectores paralelos: el sector universitario —universidades, politécnicas, escuelas de Bellas Artes, universidades abiertas— y el sector tecnológico —Institutos de Educación Tecnológica (TEI) y la Escuela de Pedagogía y Educación Tecnológica—.[281]​ También hay otros Institutos Autónomos no Universitarios que ofrecen cursos vocacionales de corta duración —dos a tres años— que operan bajo la supervisión de otros ministerios.[280]​ Los estudiantes ingresan a las instituciones de educación superior de acuerdo a su desempeño en los exámenes nacionales aplicados al finalizar el tercer grado de Lýkeio.[281]​  Además, los estudiantes mayores de 22 años pueden ser admitidos en la Universidad Abierta de Grecia por medio de una especie de sorteo.[284]​

Grecia cuenta con asistencia sanitaria universal. El Informe sobre la salud en el mundo 2000 de la Organización Mundial de la Salud colocó a su sistema de salud en la posición 14ª de 191 países enlistados.[285]​ En un análisis de 2013 hecho por Save the Children, se coloca a Grecia en el puesto 19º de 176 países con los mejores cuidados maternos y del recién nacido.[286]​ En 2010, había 138 hospitales con 31 000 camas, pero el 1 de julio de 2011, el Ministerio de Salud y Seguridad Social anunció sus planes de reducir el número a 77 hospitales con 36 035 camas, una reforma necesaria para reducir gastos y mejorar los estándares de salud.[287]​ El gasto del gobierno en salud como porcentaje del PIB fue de 9,3 % en 2012, igual al promedio de los países de la OCDE.[288]​  Además, Grecia tiene el mayor índice de médicos por habitante en la organización.[288]​

La esperanza de vida es de 80,3 años, una de las más altas en el mundo.[288]​ De hecho, la isla de Icaria tiene el mayor porcentaje de personas mayores de 90 años en el mundo, pues cerca de un tercio de los isleños rebasa dicha edad.[289]​[290]​  Como en otros países europeos, dos de los principales problemas de salud pública en Grecia son el tabaquismo y la obesidad; en 2010, 19,6 % de los adultos griegos tenían algún grado de sobrepeso.[288]​ La tasa de mortalidad infantil es una de las más bajas entre los países desarrollados, de 3,1 muertes por cada 1000 nacidos vivos.[288]​

Casi dos tercios de los griegos viven en zonas urbanas. Los centros urbanos más grandes e influyentes de Grecia son Atenas y Tesalónica, cuya población metropolitana supera los cuatro millones y el millón de habitantes, respectivamente. Otras ciudades importantes cuya población rebasa los 100 000 residentes son Patras, Heraclión, Lárisa, Volos, Rodas, Ioánina, La Canea y Calcis.[9]​

Atenas
Tesalónica
Patras

Heraclión
Lárisa
Volos

La cultura de Grecia ha evolucionado a través de miles de años, iniciando con la Grecia micénica y pasando más notablemente por la Grecia clásica, a través de la influencia del Imperio romano y su continuación oriental, el Imperio bizantino. Otras culturas y naciones, como los Estados latinos y francos, el Imperio otomano, la República de Venecia, la República de Génova y el Imperio británico también han dejado su influencia en la cultura griega moderna.[19]​

En tiempos antiguos, Grecia fue la cuna de la civilización occidental.[291]​ Las democracias modernas deben su existencia a las ideas griegas sobre el gobierno del pueblo, el juicio con un jurado y la igualdad ante la ley. Los antiguos griegos fueron pioneros en muchos campos que dependen del pensamiento sistemático, como la biología, geometría, historia, filosofía y física.[292]​ Introdujeron muchas formas literarias como la poesía épica y lírica, la narrativa histórica, la tragedia y la comedia. En su búsqueda por el orden y la proporción, los griegos crearon un ideal de belleza que influyó fuertemente el arte occidental.[293]​

El teatro nació en Grecia.[17]​ La ciudad-Estado de Atenas, que se convirtió en la potencia cultural, política y militar más importante durante esta época, fue su centro, y fue el lugar donde se institucionalizó como parte de un festival llamado dionisias, una fiesta en honor al dios Dionisio. Los tres géneros dramáticos que surgieron aquí fueron: la sátira, la tragedia (finales del siglo VI a. C.) y la comedia (486 a C.).[17]​

Durante el periodo bizantino, el teatro sufrió un declive importante. De acuerdo a Marios Ploritis, la única forma que sobrevivió fue el teatro folclórico —Mimos y Pantomimos— pese a la hostilidad del Estado oficial.[294]​ Más tarde, durante el periodo otomano, el principal arte dramático folclórico fue el Karagöz. El renacimiento que dio origen al teatro griego moderno tuvo lugar en la Creta veneciana. Algunos de los dramaturgos más importantes incluyen Vitsentzos Kornaros y Georgios Chortatzis.[295]​

El teatro griego moderno comenzó a desarrollarse luego de la guerra de independencia, a principios del siglo XIX, e inicialmente estuvo influenciado por el teatro heptaneso y los melodramas, como la ópera italiana. El Nobile Teatro di San Giacomo de Corfú fue el primer teatro y casa de la ópera construido en la Grecia moderna, y también fue el lugar donde se estrenó la primera ópera griega, El candidato parlamentario de Spyridon Xyndas.[296]​ Durante finales del siglo XIX y principios del siglo XX, el teatro ateniense estuvo dominado por revistas, comedias musicales, operetas y nocturnos, escritas por personajes como Spyridon Samaras, Dionysios Lavrangas, Theophrastos Sakellaridis, entre otros.[297]​

El Teatro nacional de Grecia se fundó en 1880. Algunos de los escritores y directores de teatro griegos más destacados incluyen a Grigorios Xenópulos, Nikos Kazantzakis, Pantelís Horn, Alekos Sakellarios y Iakovos Kambanelis; mientras que entre los actores más sobresalientes están Cybele Andrianú, Marika Kotopouli, Aimilios Veakis, Orestis Makrís, Katina Paxinou, Manos Katrakis y Dimitris Horn.[297]​

La cocina griega es citada como un ejemplo de dieta mediterránea. Tiene influencias de la italiana, balcánica y turca.[298]​ Los vegetales están entre los ingredientes más utilizados en la gastronomía griega, principalmente el tomate, la berenjena, la patata, las judías verdes, la cebolla, el pimiento y la espinaca.[299]​ La carne de cordero es la más utilizada, solo superada por el pescado en las zonas costeras. El aceite de oliva se utiliza para cocinar y aderezar la mayoría de los platillos.[298]​ A partir de la leche de cabra se preparan distintos tipos de quesos, como el feta, kefalotyri, kasseri y mizithra.[299]​ También se utiliza para elaborar el yogur griego, que ha ganado popularidad internacional por ser más cremoso y denso que otras variedades de yogur, además de que se utiliza para la elaboración de sopas, ensaladas, platos fuertes y postres.[300]​

Algunos de los platos locales más populares son la musaca, un horneado con capas de carne y berenjena bañado en salsa de tomate; el gyros, una especie de sándwich con carne, salsa tzatziki y otros vegetales a elección, el stifado, un guiso de carne de res con cebollitas; la spanakopita, un pastel salado relleno de queso feta y espinacas; y el souvlaki, carne cocinada con vegetales, salsa de yogur, patatas y aderezos.[301]​ Entre los platillos dulces está el galaktoboureko, el baklava y el kataifi.[299]​ Las bebidas tradicionales griegas incluyen el ouzo, el metaxá y una variedad de vinos incluyendo el retsina.[302]​

Los orígenes de la música griega se extienden hasta la Antigua Grecia, cuando los coros cantaban por razones espirituales, de celebración o por entretenimiento.[303]​ Los instrumentos musicales de este periodo incluían el aulos, la lira y la cítara. La música jugaba un papel importante en el sistema educativo, pues los varones aprendían sobre música desde los seis años. Posteriormente, los romanos, musulmanes y bizantinos influyeron en el desarrollo de la música griega.[304]​

En el Imperio bizantino la música religiosa se caracterizó por ser una monodia vocal sin acompañamiento instrumental. Pese a estas limitaciones, el canto bizantino desarrolló una variedad rítmica y un gran poder expresivo.[305]​ Por su parte, el pueblo griego creó varias canciones folclóricas que se pueden dividir en dos tipos: las acríticas y las kleftes. Las acríticas nacieron entre los siglos IX y X y expresan la vida y luchas de los akritai, los guardias de las fronteras bizantinas, la más conocida de ellas es la que narra la historia de Digenís Akritas. Las kleftes surgieron entre el final del Imperio bizantino y el inicio de la guerra de independencia. Las kleftes, junto con las canciones históricas o parálogos, las canciones de amor, las mantinadas, las canciones de boda y del exilio, hablan sobre la vida diaria del pueblo griego.[306]​

Los  cánticos de las  Islas Jónicas se convirtieron en los antecesores de las canciones griegas modernas, pues influenciaron su desarrollo de manera considerable.[295]​ Durante la primera mitad del siglo XIX, varios compositores griegos tomaron elementos del estilo heptaneso. Las composiciones más famosas del periodo entre 1870 y 1930 son las llamadas «serenatas atenienses» y las canciones interpretadas en obras de teatro.[307]​

El rebético, inicialmente un género relacionado con las clases sociales inferiores, ganó una gran aceptación general luego de que algunos de sus elementos fueron suavizados y adaptados para eliminar su esencia subcultural. De esta forma se convirtió en la base del laïkó («canción del pueblo»). Los principales representantes de este género incluyen a Apostolos Kaldaras, Grigoris Bithikotsis, Stelios Kazantzidis, George Dalaras, Haris Alexiou y Glykería.[306]​

En cuanto a la música clásica, fue por medio de las Islas Jónicas —que estuvieron bajo el dominio e influencia occidental— que la mayor parte de los avances en la música clásica de la Europa occidental llegaron a Grecia. El archipiélago es notable por el nacimiento de la primera escuela moderna de música clásica en Grecia, la Escuela Heptantesa (Επτανησιακή Σχολή), establecida en 1815.[308]​ Algunos de los mejores ejemplos de este género incluyen a Nicolaos Mantzaros, Spyridon Xyndas, Spyridon Samaras y Pavlos Carrer. Manolis Kalomiris es considerado el fundador de la Escuela Nacional de Música de Grecia.[309]​ Durante el siglo XX, los compositores griegos tuvieron un impacto importante en el desarrollo del avant-garde y la música clásica contemporánea, con figuras como Iannis Xenakis, Nikos Skalkottas y Dimitri Mitrópoulos, que alcanzaron relevancia internacional.[306]​

Los restos de la arquitectura de la Antigua Grecia aún perviven o están bien documentados. Los antiguos griegos desarrollaron tres estilos arquitectónicos primarios, llamados «órdenes clásicos»: el sobrio y sólido dórico, el refinado y decorativo jónico y el elegante y ornamentado corintio.[310]​

La forma rectangular de los antiguos templos griegos, rodeados de columnatas que soportan un frontón triangular, construido de piedra caliza o mármol, es un modelo que aún se utiliza. Aunque el arco era familiar a los troyanos, su uso no estaba ampliamente extendido, en contraste con las posteriores edificaciones romanas. Las obras representativas que perviven de la arquitectura griega son el Partenón y el Erecteón de Atenas, y las estructuras romanas basadas en el modelo griego, como el Panteón de Roma, que se atribuye al arquitecto griego Apolodoro de Damasco.[311]​

La arquitectura bizantina fue un modo de construcción común hasta la caída de Constantinopla en 1453 a manos de los turcos otomanos. Son característicos la cruz griega, el capitel de estilo bizantino o capitol —una mezcla de capiteles corintios y jónicos— y una cúpula central rodeada por varias cúpulas pequeñas.[312]​ En los años siguientes a la guerra de independencia, Grecia experimentó también el resurgimiento neobizantino tras la revolución griega y también el auge de la arquitectura neoclásica; esto vino a ponerla en contacto e interacción con la tradicional villa bizantina para producir una forma específica en la Grecia contemporánea.[313]​

Como otras capitales contemporáneas, Atenas tiene obras de arquitectura modernistas y postmodernistas. Resultado de un concurso internacional, el nuevo Museo de la Acrópolis, de Bernard Tschumi, es un ejemplo de la internacionalización del mercado arquitectónico mundial.[314]​ Algunas de las últimas obras fueron para las Olimpiadas de Atenas de 2004, con la participación de arquitectos extranjeros como Santiago Calatrava.[315]​

En contraste con otras formas ilustradas, las pinturas conservadas de la antigua Grecia son muy escasas.[311]​ Los pintores griegos trabajaban principalmente en paneles de madera, y las obras finales fueron admiradas durante cientos de años después de su creación. Sin embargo, estas pinturas desaparecieron después del siglo IV cuando no fueron suficientemente protegidas. Las principales muestras de la pintura griega que aún se preservan incluyen las copias romanas, por ejemplo las de Pompeya, las escasas muestras conservadas halladas en las tumbas de los reyes de Macedonia en Vergina, en Leúcade también en la antigua Macedonia y las de Kazanlak en la antigua Tracia.[316]​

Las obras conservadas de la antigua escultura griega son más comunes, en particular las de los maestros escultores, como Fidias y Praxíteles. Estos artistas y sus seguidores fueron frecuentemente emulados por los romanos.[311]​ Sin embargo, los cristianos de los siglos IV y V vieron la destrucción de los ídolos paganos como un «acto de piedad». Muchas esculturas antiguas de mármol fueron quemadas con cal en la Edad Media, y estatuas de bronce fueron fundidas para obtener el metal. Las estatuas de mármol que escaparon a la destrucción fueron olvidadas, o en el caso de los bronces, perdidos en el mar.[317]​

En el periodo bizantino, el arte religioso era el tema dominante, con mosaicos e iconos muy trabajados adornando los edificios religiosos.[318]​ El artista renacentista, El Greco (Domenikos Theotocopoulos), respondía al bizantino y en el siglo XVI el arte manierista, produciendo escultura y pinturas de forma libre, luz y color que inspiraría a artistas del siglo XX como Pablo Picasso y Jackson Pollock.[319]​

Durante los siglos XVIII y XIX, los artistas de las islas Jónicas jugaron un papel importante, y a menudo pionero, que explotaron las conquistas del Renacimiento italiano y de los talleres barrocos.[295]​ Con persistentes esfuerzos hacia nuevas direcciones y objetivos, los artistas griegos afloraron al mundo durante las primeras décadas del siglo XIX conectando el arte griego con su antigua tradición, así como la búsqueda de talleres europeos, sobre todo los de la Escuela de Múnich, con ejemplos definitorios del arte contemporáneo griego del periodo que incluye la obra de Theodoros Vryzakis y Nikiphoros Lytras.[320]​

A comienzos del siglo XX, Dimitrios Galanis, contemporáneo y amigo de Picasso, consiguió un amplio reconocimiento en Francia y fue miembro vitalicio de la Academia Francesa, tras la alabanza del crítico André Malraux como un artista capaz «de provocar fuertes emociones como Giotto».[321]​  Durante el siglo XX, los artistas griegos siguieron las tendencias occidentales. Uno de los primeros en obtener el reconocimiento internacional fue el poeta y pintor Nikos Engonopoulos, por sus trabajos surrealistas. En los años 1960, otros pintores griegos, como Dimitris Mytaras y Yiannis Psychopedis, se sumaron al movimiento europeo del realismo crítico. Por su parte, en la escultura modernista y postmodernista, cabe destacar la participación de artistas como Costas Axelos y Constantine Andreou.[322]​

La primera sala de cine apareció por primera vez en Grecia en 1897. La primera producción data de 1914, cuando la compañía Asty Film fue fundada y comenzó a producir largometrajes. Golfo (Γκόλφω), una tradicional historia de amor, fue la primera película producida totalmente en el país, aunque hubo antes varias producciones menores como los noticiarios.[323]​

El cine griego ha tenido una historia agitada, desde momentos de relativo estancamiento hasta muy memorables producciones. Desde la década de 1920 hasta finales de los años 1940 hubo algunas producciones bastante notables, como Έρως και κύματα (Amor y olas), dirigida en 1927 por D. Gaziadis, y Χειροκροτήματα (Aplausos), dirigida en 1944 por G. Tzavelas;[324]​ También se encuentra Por quién doblan las campanas de 1944, por la que Katina Paxinou fue premiada con el Óscar a la mejor actriz de reparto.[325]​

La edad de oro del cine griego fue la década de 1950, en la que se produjeron hasta sesenta películas al año.[326]​ Notables actores y directores de este periodo fueron Michael Cacoyannis, Alekos Sakelarios, Nikos Tsiforos, Ellie Lambeti, Dinos Iliopoulos e Irene Papas. Cacoyannis, en particular, que dirigió películas como Stella (Στέλλα) en 1955 y Zorba el griego (Αλέξης Ζορμπάς) en 1964, llegó a ganar tres premios Óscar.[325]​

Desde esta época el cine griego ha estado relativamente parado, debido a la fuerte competencia extranjera y el aumento de la popularidad de la televisión.[326]​ En los años 1980, Loafing and Camoflage (Λούφα και Παραλλαγή) logró un éxito moderado con el uso de la comedia egea. En años siguientes otros directores tocaron varios temas políticos, como The Suspended Step of the Stork (Μετέωρο βήμα του πελαργού, Το) dirigida por Theo Angelopoulos en 1991, que se relaciona con la inmigración desde Albania.[327]​

En los años 2000, cintas como Un toque de canela (Πολίτικη Κουζίνα) y la comedia sexual tabú Safe Sex señaló la tendencia al alza de la calidad del cine griego. Esto pudo estar relacionado, en gran parte, al periodo de prosperidad económica en Grecia, el cual ha conducido a un incremento de la producción cultural en todas las artes, tanto física como visual.[328]​ Este crecimiento frenó en la década de 2010, cuando a causa de la crisis financiera, solo se realizaron un promedio de veinte producciones por año.[329]​

Grecia tiene una destacable, rica y fuerte tradición literaria que abarca varias épocas a través de 2800 años. La época clásica es la que más comúnmente se relaciona con la literatura griega, que comienza en el 800 a. C. y mantiene su influencia durante el periodo bizantino, no obstante la influencia del cristianismo comenzó a engendrar un nuevo desarrollo de la palabra escrita. Muchos elementos de la antigua tradición milenaria están reflejados en la moderna literatura griega, incluyendo a las obras de los laureados con el Nobel, Odysseas Elytis y Giorgos Seferis.[330]​

Las primeras obras de la tradición literaria occidental son los poemas épicos de Homero y Hesíodo.[331]​ La primera poesía lírica, la representativa por poetas como Safo y Píndaro, fue la responsable de la definición del género lírico como es entendido en la actualidad en la literatura occidental.[332]​ Esopo escribió sus Fábulas en el siglo VI a. C.[333]​ La literatura de este periodo tuvo una profunda influencia no solo en los poetas romanos, sino que se extendió a través de toda Europa.[330]​

En la Grecia clásica se establece el nacimiento del teatro tal y como lo entendemos. Esquilo introdujo las ideas de diálogo y dramatizó las relaciones de los personajes,  al hacerlo inventó el drama: su trilogía Orestíada es considerada la cima de su carrera.[334]​ Otros grandes dramaturgos fueron Sófocles y Eurípides. Aristófanes, un escritor de comedias, definió y desarrolló el concepto de comedia como forma teatral.[334]​ Heródoto y Tucídides son considerados los pioneros del moderno estudio de la historia en el campo de la búsqueda filosófica, literaria, y científica.[292]​ Polibio fue el primero en introducir en su estudio el concepto de militar.[335]​

La filosofía griega produjo literatura con los diálogos de Platón, mientras que su discípulo Aristóteles, en su obra Poética, formuló el primer criterio de la crítica literaria. Ambas figuras literarias, en el contexto de las contribuciones de la filosofía griega en las épocas clásica y helenística, dieron nacimiento al concepto de ciencia política, el estudio de la evolución política y la crítica de los sistemas de gobierno.[336]​

La expansión del cristianismo por todo el mundo grecorromano en los siglos IV, V y VI, junto a la Helenización del Imperio bizantino que se produjo en el período, llevó a la creación de una forma literaria única, que combinó influencias cristianas, griegas, romanas y orientales.[337]​ A su vez, esto permitió que se desarrollase la poesía cretense, la saltaría poética en el Oriente griego, y el género histórico, con varios historiadores prominentes como Procopio de Cesarea.[338]​

Se considera de manera convencional que la literatura griega moderna apareció a partir de 1453 (año de la caída de Constantinopla), si bien la producción literaria es muy reducida.[339]​ Se pueden señalar algunas obras ilustradas, aunque destacan las canciones populares y las novelas acríticas del Imperio bizantino. La producción aumenta grandemente a partir de la guerra de independencia en 1821, por lo que la literatura griega del período está fuertemente influida por temas revolucionarios.[296]​

En el siglo XX, la tradición literaria griega moderna abarca la obra de Constantino Cavafis, considerado una figura clave de la poesía del siglo XX. Giorgos Seferis, cuyas obras y poemas aspiraron a unir la literatura de la antigua y moderna Grecia y Odysseas Elytis ganaron el Premio Nobel de Literatura. Nikos Kazantzakis es también una importante figura, que recibió reconocimiento internacional con obras como La última tentación de Cristo y Cristo de nuevo crucificado.[340]​[296]​

Grecia es el lugar del nacimiento de los Juegos Olímpicos, celebrados por primera vez en el 776 a. C hasta su prohibición en el 393 d. C.[341]​ El estadio Panathinaikó en Atenas, que fue esencialmente reconstruido en 1895, fue la sede de los primeros Juegos Olímpicos modernos en 1896. El estadio Panathinaikó también fue la sede de los juegos de 1906 y albergó algunos eventos de los Juegos de 2004.[136]​

Los dos deportes más practicados en Grecia son el fútbol y el baloncesto.[342]​ En 2008, la selección de fútbol de Grecia llegó a ocupar la octava posición en la Clasificación mundial de la FIFA,[343]​ y fue la sorpresa de la Eurocopa 2004 tras ganar dicho campeonato.[344]​ La Super Liga de Grecia es la máxima liga de fútbol profesional y comprende dieciocho equipos, de los cuales los más exitosos son Olympiacos, Panathinaikos, Aris de Tesalónica, PAOK de Tesalónica y AEK Atenas.[345]​

La selección de baloncesto de Grecia es considerada como una de las mejores en el mundo, pues en 2012 se colocó en el número cuatro de la clasificación internacional de la FIBA y en el segundo lugar de la clasificación europea.[346]​  Ganaron el campeonato europeo en 1987 y 2005,[347]​ y han terminado entre los cuatro mejores en dos de los últimos cuatro campeonatos mundiales. La liga nacional de baloncesto, A1 Ethniki, está conformada por catorce equipos. Los más exitosos son los Olympiacos, Panathinaikos, Aris de Tesalónica, PAOK de Tesalónica y AEK Atenas.[348]​ Los equipos griegos han sido los más exitosos en el baloncesto europeo de los últimos 25 años, pues ganaron nueve Euroligas desde 1988.[349]​

Otros deportes populares en el país incluyen el polo acuático, el voleibol, el atletismo, el levantamiento de peso, el boxeo y la gimnasia.[342]​ El tenis y el golf, que fueron introducidos en el siglo XIX por la clase burguesa, aún atraen a un gran número de jugadores.[350]​ El críquet y el balonmano son muy practicados en Corfú y Véria, respectivamente.[322]​

Consumo es la acción y efecto de consumir o gastar, sean productos, bienes o servicios,  como por ejemplo la energía, entendiendo por consumir, como el hecho de utilizar estos productos y servicios para satisfacer necesidades primarias y secundarias. El consumo masivo ha dado lugar al consumismo y a la denominada sociedad de consumo. En términos puramente económicos se entiende por consumo la etapa final del proceso económico de producción, definida como el momento en que un bien o servicio produce alguna utilidad al consumidor. En este sentido hay bienes y servicios que directamente se destruyen en el acto del consumo, mientras que con otros lo que sucede es que su consumo consiste en su transformación en otro tipo de bienes o servicios diferentes.


Para el antropólogo García-Canclini el consumo es «el conjunto de procesos socioculturales en los que se realizan la apropiación y los usos de los productos». 

Para el sociólogo Jeremy Rifkin el fomento del consumo se produjo en la década de 1920 en Estados Unidos para aliviar la sobreproducción en Estados Unidos -motivada por un aumento de la productividad y una bajada de la demanda por la existencia de un alto número de desempleados debido a los cambios tecnológicos- que encontró en el marketing (mercadotecnia y publicidad) la herramienta para incrementar, dirigir y controlar el consumo.[1]​[2]​

En macroeconomía, el consumo constituye uno de los componentes fundamentales del producto interno bruto (PIB) (desde el punto de vista del gasto o demanda). Este consumo puede ser dividido entre:

Por otra parte, existen también dos clases básicas de consumo:

La función de consumo privado fue desarrollada por John Maynard Keynes en su obra Teoría General del Empleo, Interés y Dinero. 

Según esta hipótesis, el factor determinante del consumo es La renta disponible de cada año. Estadísticamente se ha comprobado que la renta y el consumo son variables que avanzan 
conjuntamente. Tanto la observación como los estudios estadísticos muestran que el nivel de renta disponible anual es el factor más importante que determina el consumo de un país.

Esta hipótesis, formulada por Milton Friedman, estipula que el consumo es una función que depende de la renta o ingreso permanente y no solo de su renta anual, entendiendo como renta permanente la que el consumidor espera cobrar a lo largo de un conjunto amplio de años. Por ejemplo un agricultor que por una mala cosecha tiene un descenso de su renta, no bajará su nivel de consumo en la misma proporción porque entiende que al ser una bajada de carácter temporal, marcará su consumo en función de su renta a largo plazo. La evidencia ha indicado que los consumidores eligen su nivel de consumo con la vista puesta en las "perspectivas" de la renta que tiene en cada momento como de la renta a largo plazo.

La teoría del ciclo vital fue expuesta por Franco Modigliani, que señaló que existen diferentes etapas en la vida de las personas. En los primeros años de vida se consume más de lo que se gana, al acceder a la vida laboral se comienza a ahorrar para poder mantener el consumo en la jubilación, cuando bajarán los ingresos. Según esta hipótesis el consumo se mueve de acuerdo a los patrones de este ciclo vital.

Esta teoría establece que la parte de la renta de una familia dedicada al consumo depende del nivel de su renta relativa frente a la renta de las familias vecinas o de otras familias con las que aquella se identifica y no del nivel absoluto de la renta de la familia. Esta teoría desarrollada por Duesenberry trata de recalcar el carácter imitativo o emulativo del consumo.

El consumo es un proceso económico asociado a la satisfacción de las necesidades de los agentes. No todo consumo procede de la satisfacción de necesidades, ya que también los deseos producen consumo. Desde un punto de vista económico es frecuente no distinguir estrictamente entre necesidades y deseos. El consumo como tal se produce en todos los sistemas económicos.

El consumismo por otra parte es una característica de determinados sistemas económicos, en los que las decisiones de producción están asociadas al supuesto de que los agentes económicos trabajarán para obtener su renta, por encima de sus necesidades estrictas de consumo, y por tanto tomarán decisiones para poder disponer de una renta disponible mayor y aumentar sus niveles de satisfacción personal a través del consumo asociado a la satisfacción de deseos. Se considera ha llegado a acuñar el término sociedad de consumo, para designar a sociedades donde una de las actividades de ocio principales de la población es la adquisición de bienes materiales o servicios adicionales, con los que satisfacen sus deseos de estatus social o satisfacción material.

En las llamadas sociedades de consumo, cierto número de individuos pueden desarrollar un trastorno de compra compulsiva. Para los individuos que desarrollan este trastorno acto de adquirir productos y servicios que están al alcance de los consumidores y usuarios, se convierte en un acto de abusar. En ocasiones, el consumismo se entiende como la adquisición o compra desaforada, que asocia la compra con la obtención de la satisfacción personal e incluso de la felicidad personal. En las sociedades de consumo, ciertos individuos están dispuestos a trabajar más horas y reducir el número total de horas de ocio, a cambio de mayores salarios y rentas, que les permitan en un tiempo de ocio menor adquirir mayor cantidad de productos y bienes.

El trastorno de compra compulsiva (TCC) es un trastorno psicológico del control de impulsos, caracterizado por preocupaciones excesivas relacionadas con las compras y por la necesidad irresistible de comprar de forma masiva objetos superfluos, acompañados de sentimientos de ansiedad, irritabilidad o malestar, y consecuencias adversas como el endeudamiento. Tras el alivio momentáneo al realizar la conducta adictiva, la persona experimenta sentimientos de culpabilidad. La mayoría de las personas con TCC cumplen con los criterios de los trastorno del Eje II o de personalidad. En Estados Unidos, se estima que el TCC tienen una prevalencia del 5,8 % de la población, y aproximadamente el 80 % de las personas afectadas son mujeres y personas mayores



El término consumerismo, usado por los agentes sociales que están en contacto con la defensa de los intereses de los consumidores y usuarios, como pueden ser las organizaciones de consumidores, engloba un consumo responsable, ético y solidario, que consiste en consumir con criterios responsables, teniendo en cuenta la historia de los productos que compramos y las repercusiones medioambientales y sociales de ese consumo.

La definición más aceptada de Consumo Sostenible es aquella propuesta en el Simposio de Oslo en 1994 y adoptada por la tercera sesión de la Comisión para el Desarrollo Sostenible (CSD III) en 1995: "El uso de bienes y servicios que responden a necesidades básicas y proporcionan una mejor calidad de vida, al mismo tiempo que minimizan el uso de recursos naturales, materiales tóxicos y emisiones de desperdicios y contaminantes sobre el ciclo de vida, de tal manera que no se ponen en riesgo las necesidades de futuras generaciones".
Entre las diferentes interpretaciones y acepciones del concepto, existen una serie de elementos comunes en todas ellas, que caracterizan el Consumo Sostenible por:

Los productos y servicios utilizados en este tipo de consumo se caracterizan por el respeto al medioambiente en todo el proceso, es decir, en los componentes, la fabricación, envasado y transporte. Así pues, el Consumo Sostenible hace referencia a un tipo de consumo que no daña al medio ambiente ni a la sociedad. Asimismo, la definición de Consumo Sostenible es cercana a la de Comercio Justo, es decir, tiene que respetar los derechos humanos, infantiles y las culturas indígenas.
Las influencias culturales, sociales y económicas han originado cambios en los estilos de vida y hábitos de consumo. En este sentido es importante la educación de los consumidores a través del consumo responsable, es decir, educar para colaborar haciendo un uso razonable de los servicios y una buena gestión de los desperdicios para el reciclaje. La educación en el consumo responsable tiene como objetivo proporcionar a los consumidores los conocimientos, habilidades y aptitudes necesarias para actuar de forma responsable.

El término Consumo Sostenible tiene su origen en el de Desarrollo Sostenible. El Principio 8 del Informe de la Conferencia de las Naciones Unidas, resultado de la Cumbre de la Tierra en Río de Janeiro (1992), recoge el lazo entre el desarrollo y el Consumo Sostenible: “Para lograr un desarrollo sostenible y una mayor calidad de vida para sus pueblos, los estados deberán reducir y eliminar los patrones insostenibles de producción y consumo y promover políticas demográficas apropiadas”. De aquí la definición de Consumo Sostenible como el “desarrollo que cubre las necesidades del presente sin comprometer la posibilidad de las futuras generaciones para satisfacer las suyas” (definición que tiene su origen en el Informe Brundlant). 

En España organismos como Facua-Consumidores en Acción y la Organización de Consumidores y Usuarios (OCU) han llevado a cabo campañas con el fin de concienciar a los ciudadanos las consecuencias que sus hábitos pueden tener en el futuro. Estas campañas tienen como objetivo promover cambios en nuestros hábitos así como también el proponer criterios éticos y de sostenibilidad en nuestras compras y actitudes.[cita requerida]

Asimismo cada vez son más las jornadas agroecológicas que tienen lugar, el objetivo de las cuales es la de proporcionar información sobre temas de agroecología y consumo responsable. En ellas también se dan a conocer las distintas entidades que operan en el marco de la economía solidaria y en el respeto al medio ambiente.[cita requerida]



El término judaísmo se refiere a la religión, tradición y cultura del pueblo judío. Históricamente, es la más antigua de las tres religiones abrahámicas,[5]​ grupo que tiene como base e incluye el cristianismo y el islam, originadas en Medio Oriente y tiene la tradición espiritual identificada con Abraham. Cuenta con el menor número de fieles entre ellas.

Aunque no existe un cuerpo único que sistematice y fije el contenido dogmático del judaísmo, su práctica se basa en las enseñanzas de la Torá, también llamada Pentateuco, compuesto por cinco libros. A su vez, la Torá o el Pentateuco es uno de los tres libros que conforman el Tanaj (o Antiguo Testamento), a los que se atribuye inspiración divina.

En la práctica religiosa ortodoxa, la tradición oral también desempeña un papel importante. Según las creencias, fue entregada a Moisés junto con la Torá y conservada desde su época y la de los profetas. La tradición oral rige la interpretación del texto bíblico, la codificación y el comentario. Esta tradición oral fue transcrita, dando nacimiento a la Mishná, que posteriormente sería la base del Talmud y de un enorme cuerpo exegético, desarrollado hasta el día de hoy por los estudiosos. El compendio de las leyes extraídas de estos textos forma la ley judía o Halajá.

El rasgo principal de la fe judía es la creencia en un Dios omnisciente, omnipotente, personal y providente, que habría creado el universo y elegido al pueblo judío para revelarle la ley contenida en los Diez Mandamientos y las prescripciones rituales de los libros tercero y cuarto de la Torá. Consecuentemente, las normas derivadas de tales textos y de la tradición oral constituyen la guía de vida de los judíos, aunque su observancia varía mucho de unos grupos a otros.

Otra de las características del judaísmo, que lo diferencia de las otras religiones monoteístas, radica en que se considera no solo como una religión, sino también como una tradición, una cultura y una nación.[6]​[7]​ Las otras religiones trascienden varias naciones y culturas, mientras que el judaísmo considera la religión y la cultura concebidas para un pueblo específico. El judaísmo no exige de los no judíos unirse al pueblo judío ni adoptar su religión, aunque los conversos son reconocidos como judíos en todo el sentido de la palabra.

Según la Tanaj, la tradición se remonta a Abraham, llamado el primer hebreo (del hebreo עִבְרִי, ivrí: «el que viene del otro lado»), por haber venido a la tierra de Canaán desde Mesopotamia siguiendo el llamado de Dios (Génesis), hace unos 4000 años. Abraham es considerado patriarca por los tres principales credos monoteístas, por lo que a estos se los conoce también con el nombre de religiones abrahámicas.

En la Biblia, los judíos son denominados «hijos de Israel» (Éxodo; nótese la extensión en el significado entre el versículo 1 y el 7); y, más adelante, fueron llamados «el pueblo de Israel» o «israelitas». El nombre de Israel le fue otorgado al patriarca Jacob, nieto de Abraham, por el ángel con el que se trabó en lucha, quien al bendecirlo lo llamó Israel (יִשְׂרָאֵל, del hebreo: «uno que ha luchado con Dios», Génesis). El término «judío» aparece solo con posterioridad (Ester 2:5), y proviene del reino de Judá (del hebreo יְהוּדָה, Yehudá, hijo de Jacob); reino que estaba formado por dos de las doce tribus del pueblo de Israel, las únicas remanentes luego de la escisión entre este reino y el de Israel y de la destrucción del último tras el exilio de las diez tribus que lo formaban a manos de Asiria, en el año 722 a. C.: «Yahveh, por tanto, se airó en gran manera contra Israel, y los quitó de delante de su rostro; y no quedó sino solo la tribu de Judá».(2Reyes 17:18)

La identidad judía no depende en primer lugar de la aceptación de creencias o del seguimiento de un modelo de vida determinado. Es tema de debate entre los religiosos, los filósofos y los sociólogos judíos sobre quién es considerado judío. Dentro de la religión judía, existen tres ramas que la conforman y cada una de ellas tiene una versión propia de quien es reconocido como judío.

En primer lugar, el judaísmo ortodoxo defiende que la ley judía (halajá) establece que aquel que ha nacido de madre judía o ha realizado un proceso de conversión (guiur) conducido por un rabino, una comunidad judía (la sinagoga) y finalizado ante un beit din (tribunal judío) ortodoxo, es judío por definición.

En segundo lugar, el judaísmo conservador defiende los mismos puntos, con la particularidad de que los procesos de conversión aceptados son los realizados por la ortodoxia (proceso anteriormente citado) o por los beit din propios del judaísmo conservador.

En tercer lugar, los reformistas creen que son judíos aquellas personas que han nacido de padres judíos o se han convertido ante un beit din ortodoxo, conservador o ante un rabino reformista (cabe mencionar que cada rabino reformista tiene libertad para decidir cuando un prosélito pasa a ser judío). A este punto cabe añadir que los rabinos reformistas estadounidenses establecieron que los hijos de padre judío podían ser considerados como tales si recibían algún tipo de educación judía. Esto se debe a que un 57 % de los hombres judíos decidían casarse con mujeres gentiles.

Los judíos caraítas, citando prácticas del Tanaj, consideran judío a todo aquel que nazca de padre judío.

Por lo tanto, ser judío es una cuestión de ascendencia biológica o adopción espiritual, por medio de hacerse prosélito, descendientes biológicos o espiritualmente de los patriarcas Abraham, Isaac y Jacob. Según la halajá, una persona judía puede ser cristiana o musulmana sin perder su condición formal de judío, pero perdiendo los derechos religiosos y comunitarios como por ejemplo, el derecho a la sepultura en un cementerio judaico.

A pesar de todo esto, convertirse al judaísmo desde otra confesión (o ninguna) es posible, pues en el Talmud se menciona lo siguiente: «Los rabinos dicen: "Si alguien llega y quiere ser un converso, ellos le dicen: '¿Por qué quieres ser un converso? ¿Acaso no sabes que los judíos están hostigados, acosados, perseguidos y acorralados, y que numerosos problemas los aquejan?' Si contesta: 'Lo sé, y no soy digno', entonces lo reciben sin que sea necesario argumentar nada más"».

Sin embargo, en la práctica será una tarea ardua y compleja, ya que la Torá debe ser seguida por toda la comunidad. Hubo una época en la que el cristianismo consideró una grave ofensa la conversión de sus fieles al judaísmo, y se defendían aludiendo a esta obligación argumentando que por ello no hay ningún tipo de provecho al convertirse al judaísmo ni motivo para fomentar la conversión.

Este punto es uno de los que más diferencia al judaísmo del cristianismo o del Islam, pues a estas dos últimas religiones monoteístas cualquiera puede pertenecer con tan solo que profese y respete sus creencias.

Estos son algunos de los principios sobre los que se basa la religión judía o que la caracterizan.

La comunidad judía de Israel fue dominada por varios antiguos imperios. Los asirios fueron seguidos por los babilonios y luego por los persas hasta la conquista por parte de los griegos. Es en esta época (hacia el 170 a. C.) cuando estalla una revolución encabezada por Judas El Macabeo ("martillo", hasmoneo) que logra colocar a todo el territorio del antiguo Israel nuevamente bajo dominio judío. El Reino Hasmoneo de Judá pasó por último a manos del Imperio romano.

Israel fue conquistado por el rey asirio Sargón II, al final del siglo VIII aC. El reino de Judea pudo continuar durante un siglo y medio, hasta que en el año 586 aC fue conquistado por los babilonios, comandados por Nabucodonosor II. En ese año se destruyó el primer templo, lugar central de la actividad religiosa judía de la época. Muchos de los judíos fueron desterrados de Israel y fueron llevados como esclavos a Babilonia (actual Irak), lo cual constituye la primera diáspora judía. Durante el exilio en Babilonia, los judíos escriben lo que se conoce como el "Talmud de Babilonia" (Talmud Bavli), mientras que los judíos todavía establecidos en Judea escriben el "Talmud de Jerusalén". Estos dos manuscritos representan las primeras manifestaciones de la Torá en forma escrita, y el Talmud de Babilonia es el utilizado actualmente por las comunidades judías. La subsecuente conquista de Babilonia a manos de los persas permitió a muchos judíos regresar a su tierra natal luego de 70 años en el exilio babilónico. Se construyó un nuevo Segundo Templo y se restablecieron las antiguas prácticas.

Es en el año 70 dC, cuando estalla una nueva rebelión y es destruido el Segundo Templo. Muchos habitantes judíos son vendidos como esclavos y esparcidos por los confines del Imperio romano, proceso que se conoce como la "diáspora". La historia de Masada demuestra el arrojo de los soldados judíos de la época. Numerosas comunidades judías florecieron en el Imperio sasánida y en el Imperio romano.

En la temprana Edad Media el reino Jázaro (en la estepa del Volga) adoptó el judaísmo como su religión oficial, pero aún se discute el alcance de esta conversión entre los pueblos sujetos al khan Jázaro.

La hegemonía del cristianismo en Europa significó numerosas persecuciones contra el pueblo judío, las cuales derivaron en frecuentes y reiteradas expulsiones. Muchas comunidades tuvieron que vivir en barrios segregados llamados guetos, pero también es cierto que en otros períodos gozaron de mayor tolerancia, sin ser nunca aceptados del todo.

Durante el Medievo, por más que se buscasen mercaderes de profesión, no se hallaba ninguno o más bien se hallaban únicamente judíos. Sólo ellos, a partir de la época carolingia, practicaban con regularidad el comercio, hasta tal punto que en el idioma de aquel tiempo, las palabras judaeus y mercator eran casi sinónimos. Unos cuantos se establecieron en el sur de Francia, pero la mayoría venía de los países musulmanes del Mediterráneo, desde donde se trasladaron, pasando por España, al occidente y Norte de Europa. Todos ellos eran radhanitas, perpetuos comerciantes viajeros, merced a los cuales se mantuvo el contacto superficial con las religiones orientales.

El comercio al que se dedicaron fue exclusivamente de especias y telas preciosas, que transportaban trabajosamente desde Siria, Egipto y Bizancio hasta el Imperio carolingio. Los mercaderes judíos se dirigían a una clientela muy reducida. Las utilidades que realizaron debieron ser muy importantes, no obstante se debe considerar que su papel económico no llegó a ser trascendental.

En el mundo musulmán, a pesar de algunos episodios de persecución y matanzas (sobre todo en el primer siglo de expansión del islam), los judíos fueron tolerados por ser uno de los "Pueblos del Libro" –a cambio del pago de importantes tributos y de numerosas restricciones–, llegando a ocupar en algunos casos altos puestos en la administración califal tanto en Damasco como en Bagdad y en Córdoba. Sin embargo, que fueran tolerados no les libró nunca de su condición legal de dhimmies, lo cual los condenaba a numerosas discriminaciones y a una situación de sumisión.

Los judíos españoles, conocidos como sefardíes, fueron obligados a convertirse al cristianismo o ser expulsados en 1492 de los reinos de Castilla y Aragón mediante el Edicto de Granada. Muchos encontraron refugio en el Imperio otomano; incluso hoy en día viven en ciudades como Estambul o Esmirna judíos sefardíes que conservan el español medieval como su lengua.

No existió otro Estado judío en Israel hasta 1948, cuando fue declarada finalmente su independencia.

La historia judía se remonta a las viejas tradiciones bíblicas. Cuando el arca de Noé encalló en el monte Ararat, los hijos de Noé (Sem, Cam y Jafet) dieron origen, respectivamente, a los semitas del Próximo Oriente, a los camitas de África y a los jafetitas del resto del mundo.

Abraham, padre de los judíos, al recibir de Yahvéh la orden de asentarse en la tierra de Canaán, se puso en camino inmediatamente, partiendo de su patria, Ur, de los caldeos (Mesopotamia). Abraham, su hijo Isaac y su nieto Jacob fueron pastores nómadas.

Sus descendientes se vieron empujados por el hambre a la tierra de Gosén, en el delta del río Nilo. Pero el faraón de Egipto, viendo que aumentaban imparablemente y se hacían poderosos, los redujo a la esclavitud. Con Moisés ungido como líder y legislador, el pueblo elegido por Dios se dirigió hacia Canaán, la tierra prometida.

La dramática marcha desde Egipto a través del mar Rojo y la peregrinación de 40 años por el desierto son hitos importantes en la historia del pueblo israelita. Los judíos, una vez conquistada la ciudad de Jericó, se establecieron en la zona agrícola de Canaán, tierra de la cual en la Biblia se dice que «manaba la leche y la miel».

Una vez establecidos en Israel, la tierra fue dividida entre las doce tribus: Aser, Neftalí, Manasés, Zabulón, Isacar, Gad, Efraín, Dan, Benjamín, Rubén, Judá y Simeón.
Con el tiempo se pasó de una teocracia a una forma de gobierno monárquica, y fueron los reyes más famosos de la época Saúl, David y su hijo Salomón, con su capital en Jerusalén. Después del reino de Salomón, la nación se dividió en dos reinos: el reino de Israel en el norte y el reino de Judea en el sur, donde los judíos toman su nombre.

En la Edad Media surgen dos obras consideradas el centro de la literatura halájica:

Cabe destacar también la importancia del libro fundamental de la Cábala judía:

Según el profesor Sergio Della Pérgola, experto en demografía del pueblo judío de la Universidad Hebrea de Jerusalén, en 2001 vivían en el mundo 13 200 000 judíos, de los cuales 4,9 millones residían en Israel (aproximadamente un 37 % del total), mientras que los restantes 8,3 millones lo hacían en la diáspora, el nombre dado por los judíos a la comunidad judía fuera de Israel.

La mayor concentración de población judía se encontraba en Israel. El mayor núcleo urbano del mundo judío era el área metropolitana de Gush Dan —o el Gran Tel Aviv—, con 2,5 millones, a la que seguían Nueva York, con 1,9 millones; Haifa, con 655 000; Los Ángeles, con 621 000, Jerusalén, con 570 000 y el sudeste de Florida con 514 000 judíos (datos de 2001).

En 2010, según The Jewish Population of the World, cuya fuente es el American Jewish Year Book y el North American Jewish Databanka de la Universidad de Connecticut, la cifra era de 13 430 000 judíos en el mundo. En América residían alrededor de 6 039 600 (5 275 000 en Estados Unidos), en Asia alrededor de 5 741 500 (5 703 700 en Israel), en Europa aproximadamente 1 500 000, en Oceanía 115 100 y en África 76 200. Son cifras que cambian permanentemente.[9]​

Desde el año 70 de nuestra era, fecha en la que el Imperio romano destruyó el Segundo Templo de Jerusalén, la sinagoga pasó a ser el lugar de preferencia para el culto, aunque el judaísmo no emite una preferencia sobre un lugar específico para dicha actividad. En hebreo, la sinagoga se llama Bet Haknéset (בית הכנסת) o "lugar de reunión".

Los varones, al entrar a la sinagoga, generalmente se ponen una kipá o yarmulke sobre su cabeza. También se acostumbra utilizar espacios normalmente destinados al estudio para la oración. A los miembros del clero judío se les llama rabinos o dayanim.

El judaísmo no es una religión monolítica ni presenta una absoluta cohesión ni unidad. Los judíos reformistas, ortodoxos y masortíes mantienen unos con otros relaciones, no siempre cordiales, pero están organizados en grupos completamente autónomos. Las diferencias entre los judíos ortodoxos y no ortodoxos, o practicantes y no practicantes según los ortodoxos, se considera una amenaza a largo plazo a la estabilidad del estado de Israel, donde la mayoría es no practicante pero el poder político y religioso está en manos de los ortodoxos.[cita requerida]

Esas tres ramas principales del judaísmo se vinculan a través de la tradición rabínica de la Edad Media y del Talmud, aunque la importancia que prestan a dicha tradición varía de uno a otro. Los tres grupos provienen del tronco común de los fariseos, quienes al principio de la era cristiana representaban la tendencia más numerosa en el seno del judaísmo. Aún hoy en día, existen algunos samaritanos y caraítas, disidentes desde el punto de vista de la ortodoxia rabínica, en Medio Oriente.[10]​

También conocido como haredí (los que tiemblan ante Dios), presenta dos diferencias doctrinales con el ortodoxo, una práctica especialmente devota, y su distanciamiento del sionismo. Tiene dos grandes subdivisiones:

El judaísmo jasídico es un movimiento ultraortodoxo. El jasidismo fue creado en Polonia a principios del siglo XVIII. Su fundador fue el rabino Israel ben Eliezer, también conocido como el "Baal Shem Tov". Los seguidores del jasidismo desearon crear un judaísmo más alegre y menos académico. Actualmente están divididos en múltiples tendencias.
El subsubgrupo de los Chabad-lubavitchers se distingue por sus esfuerzos para atraer a los judíos, sobre todos a los no practicantes, a la variedad del judaísmo que para ellos es la única auténtica, y por su expansión geográfica para conseguir este fin. Tienen representación en más de 1000 ciudades en 80 países, y constituyen la organización judía más extendida en el mundo.

También ultraortodoxos, los mitnagdím (del hebreo מתנגדים, oponentes), por el contrario, rechazan algunas posturas del jasidismo, como el estudio intensivo de la parte oculta de la Torá. Es una corriente más unificada.

Los ultraortodoxos, por su casi inexistente matrimonio fuera de su misma variedad del judaísmo, y por sus grandes familias, están en auge demográfico.

El judaísmo reformista (Hebreo: יהדות רפורמית) es una de las grandes ramas de la religión judía (Judaísmo Rabínico) en la actualidad, de origen ashkenazí, junto con el judaísmo ortodoxo y el judaísmo conservador o masortí'. El judaísmo reformista (también llamado progresista o progresivo) defiende la autonomía individual en lo relativo a la interpretación de los preceptos religiosos (Hebreo: מצוות mitzvot).[11]​

No pretenden ser dogmáticos:[12]​

El judaísmo ortodoxo es una de las ramas de la religión judía en la actualidad, junto con el judaísmo reformista y el judaísmo conservador o masortí. Se distingue de ellas por su adhesión rigurosa a la Halajá. Carece de una autoridad doctrinal central permitiendo cierta variación en la práctica. Afirma que la festividad de pésaj, el shabat (sábado) y todos los preceptos de la Torá (tanto la escrita como la que llaman parte "torá oral"), fueron entregadas por Dios mismo a Moisés hace más de 3.323 años en el Monte Sinaí. Creen que Moisés a su vez enseñó estas leyes a todo el pueblo israelita, que como una sola entidad aceptó cumplirlas antes de saber en qué consisten o el porqué de cada una de ellas, con una disposición única de entidad indivisible. De acuerdo a su actitud hacia la cultura contemporánea, el judaísmo ortodoxo se divide informalmente en judaísmo ortodoxo moderno, que busca adecuar hasta algún punto sus prácticas y estudios a la situación social contemporánea, aunque es firme con respecto a la halajá, el sionismo religioso que liga el judaísmo ortodoxo con el sionismo y el judaísmo haredí, que rechaza toda innovación que sus líderes consideren contraria al espíritu de la Torá.

El judaísmo ortodoxo nació como respuesta adversa al crecimiento del judaísmo reformista en la Alemania del siglo XIX. Este se guía principalmente por la Halajá o ley judía especificada en el Talmud y codificada en el Shulján Aruj. Estos a su vez se basan en la Torá. Considera que las leyes fueron entregadas no solamente a esta generación, sino también dirigidas a todos sus descendientes, y contienen en sí todas las facetas que se puedan pensar que requieran su aplicación.

En el judaísmo ortodoxo, no se acepta como judío a aquel que se haya convertido al judaísmo por otras reglas que las de la Halajá. El rabino ortodoxo es el único que puede celebrar un matrimonio -religioso- en Israel (y no casará a judío con no judío).

El judaísmo ortodoxo basa sus creencias en los trece principios de fe de Moisés Maimónides. Sus principios son:

También conocido como judaísmo masortí o tradicionalista (del hebreo masóret, מסורת "tradición"). Este movimiento se formó en los Estados Unidos a través de la fusión de dos grupos distintos: los reformistas y los ortodoxos. Enfatizan que los judíos constituyen una nación (Am Israel), pero que esta no puede identificarse, en su totalidad, con el estado de Israel.

Los conservadores no siguen la ley judía en su totalidad, sino que se inclinan hacia interpretaciones más abiertas al mundo moderno, no siempre basada en la opinión mayoritaria de los sabios (talmidim o jajamim).

Los judíos seculares son aquellos que pertenecen al pueblo judío por ascendencia familiar, en concordancia con las leyes del judaísmo, sin embargo se esfuerzan poco o nada por practicar las leyes judías. La mayoría de los judíos seculares son indiferentes al judaísmo, el cual forma parte relativamente pequeña de su identidad. Esto último los diferencia de los judíos humanistas seculares. En hebreo, principalmente en Israel, a los judíos seculares se los conoce como jilonim (en hebreo, חילונים‎), en singular jiloní (חילוני).

El judaísmo humanista secular es una corriente que ve al hombre como centro del mundo y de la vida judía, a diferencia de las otras corrientes que subrayan la centralidad de Dios. Para los judíos humanistas seculares la religión y sus leyes no necesariamente deben regir el comportamiento del individuo. Esta corriente destaca los valores humanistas universales, que se basan históricamente en las fuentes judías. Los distintos libros del judaísmo son remarcados como fuentes de inspiración para los conceptos de libertad, justicia, justicia social, solidaridad, respeto y ayuda al prójimo, tolerancia y demás.

Esta corriente, al igual que la reforma, es uno de los intentos de adaptar el judaísmo y compatibilizarlo con las distintas posibilidades de identidades seculares y nacionalistas, que surgen como consecuencia de la Revolución francesa.

El caraísmo es una corriente religiosa del judaísmo, conocida por ese nombre, que proviene del término hebreo קראית (Qaraim: "lectores") y, que también es designada como Bené mikrá, que significa "seguidores de la Escritura", que reconocen la Tanaj como única máxima autoridad, en oposición a los Bene mishnah, seguidores de la tradición. Proclama el derecho de todo judío a estudiar las Escrituras Hebreas de un modo libre, sin tener en cuenta la interpretación rabínica ni el Talmud; debido al énfasis que le daban a las Escrituras, se les llamó desde el siglo VIII "Qara'ìm".[15]​

El judaísmo nazareno es un movimiento judío, cuya base de creencias se encuentra en el Tanaj, además de tener en cuenta tanto los escritos de los primeros discípulos de Yeshua de Nazaret (sin considerarlos evangelios) como textos de fuentes judías, como el Talmud y los manuscritos de Qumram. El término no debería ser confundido con los nazarenos, sino que es una denominación en el judaísmo de los discípulos de Yeshua, que eran judíos (ortodoxos según la aceptación de la época), y que en el cristianismo llegarían a llamarse apóstoles. 

Lo más notable en la corriente nazarena (como también la mesiánica) es su creencia que Yeshua de Nazaret es el mashiaj elegido por Hashem, apartándola del grueso del judaísmo que ve imposible la llegada del mesías como hecho consumado. Sin embargo, los judíos nazarenos no creen en conceptos trinitarios, ni en la divinización del mesías, ni en el nacimiento virginal u otros dogmas que forman la base dogmática del cristianismo.[16]​[17]​[18]​



Nikola Tesla (en serbio, Никола Тесла; Smiljan, Imperio austríaco, actual Croacia; 10 de julio de 1856-Nueva York, 7 de enero de 1943) fue un inventor, ingeniero eléctrico y mecánico serbio nacionalizado estadounidense,[1]​[2]​[3]​ célebre por sus contribuciones al diseño del sistema moderno de suministro de electricidad de corriente alterna (CA).[4]​

Tesla, que nació y se crio en el Imperio austríaco, estudió ingeniería y física en la década de 1870 sin obtener un título, aunque adquirió experiencia práctica a principios de la década de 1880 trabajando en telefonía en la empresa Continental Edison en la nueva industria de la energía eléctrica. En 1884 emigró a Estados Unidos, donde adquirió la doble nacionalidad. Trabajó durante un corto tiempo en Edison Machine Works en Nueva York antes de emprender el camino por su cuenta. Con la ayuda de socios para financiar y comercializar sus ideas, Tesla fundó laboratorios y empresas en Nueva York para desarrollar dispositivos eléctricos y mecánicos. Su motor asíncrono de corriente alterna (CA) y las patentes relacionadas con el sistema polifásico, licenciadas por Westinghouse Electric en 1888, le reportaron grandes sumas de dinero y además se convirtieron en la piedra angular del sistema polifásico finalmente comercializado por esta empresa.

En sus intentos por desarrollar inventos que pudiera patentar y comercializar, Tesla realizó experimentos con osciladores/generadores mecánicos, tubos de descarga eléctrica y las primeras imágenes de rayos X. También construyó uno de los primeros barcos con control remoto inalámbrico. Adquirió fama como inventor, mostrando en su laboratorio los logros a numerosas personalidades y patrocinadores adinerados, además de sobresalir por su talento para el espectáculo en conferencias públicas. A lo largo de la década de 1890, Tesla siguió investigando sobre iluminación inalámbrica y la distribución inalámbrica de energía eléctrica por todo el mundo a través de sus experimentos con energía de alta tensión y alta frecuencia en Nueva York y Colorado Springs. En 1893 anunció la posibilidad de establecer comunicación inalámbrica con sus dispositivos y trató de ponerlo en práctica en su proyecto inconcluso de la Wardenclyffe Tower, un transmisor de potencia y comunicación inalámbrica intercontinental, pero se quedó sin fondos antes de poder completarlo.[5]​

Después, Tesla experimentó con otras invenciones en las décadas de 1910 y 1920 con diverso éxito. Tras gastar la mayor parte de su dinero, vivió en varios hoteles de Nueva York, en los que dejó facturas sin abonar. Murió en esa ciudad en enero de 1943.[6]​ El trabajo de Tesla cayó en un relativo olvido después de su muerte, pero en 1960 la unidad de inducción electromagnética en el Sistema Internacional de Unidades fue nombrada tesla en su honor.[7]​ Desde la década de 1990 hay un claro resurgimiento del reconocimiento de sus aportaciones a la ciencia.[8]​

Nikola Tesla, de etnia serbia,[9]​[10]​ nació en el pueblo de Smiljan (actualmente en Croacia), en el entonces Imperio austrohúngaro, y tiempo después se nacionalizaría estadounidense.[11]​

Tras su demostración de la comunicación inalámbrica por medio de ondas de radio en 1894[12]​ y después de su victoria en la guerra de las corrientes, se le reconoció ampliamente como uno de los más grandes ingenieros eléctricos de los Estados Unidos.[13]​ Durante este periodo la fama de Tesla rivalizaba con la de cualquier inventor o científico de la historia o la cultura popular,[14]​ pero debido a su personalidad excéntrica y a sus afirmaciones increíbles —a veces totalmente inverosímiles, y en ocasiones, falsas— acerca del posible desarrollo de innovaciones científicas y tecnológicas, Tesla terminó relegado al ostracismo y considerado un científico loco.[15]​ Nunca prestó especial atención a sus finanzas y se dice que murió empobrecido a los 86 años.[16]​

Además de su trabajo en electromagnetismo e ingeniería electromecánica, el trabajo de Tesla más tarde sirvió en diferente medida al desarrollo de la robótica, el control remoto, el radar, las ciencias de la computación, la balística, la física nuclear[17]​ y la física teórica. Llevó adelante estudios que permitirían desarrollar la radio, pero nunca desarrolló este concepto debido a que no entendía del todo la física inherente a este fenómeno. Posteriormente, cuando Guillermo Marconi reclamó los derechos de uso de la radio en plena Segunda Guerra Mundial, la Corte Suprema de los Estados Unidos rechazó la solicitud, incluyendo en su decisión la restauración de ciertas patentes previas a la de Marconi, entre ellas algunas de Tesla.[18]​

La unidad de medida del campo magnético (B) del Sistema Internacional de Unidades (también denominado densidad de flujo magnético o inducción magnética), el tesla (T), fue llamado así en su honor en la Conferencia General de Pesas y Medidas de París en 1960.[19]​

Su personalidad, su carácter excéntrico y la historia de su experimento sobre transmisión inalámbrica, son utilizados por aficionados a las teorías conspirativas para justificar varias pseudociencias, atribuyéndole inventos, hechos y/o investigaciones que no se corresponden con la realidad.[20]​

Nikola Tesla era hijo de padres serbios.[21]​ Nació en el pueblo de Smiljan, en el Imperio austrohúngaro, cerca de la ciudad de Gospić, perteneciente al territorio de la actual Croacia. Su certificado de bautismo afirma que nació el 28 de junio de 1856 del calendario juliano, correspondiente al 10 de julio del calendario gregoriano en uso actualmente. Su padre fue Milutin Tesla, un sacerdote de la iglesia ortodoxa serbia en la jurisdicción de Sremski Karlovci, y su madre, Đuka Mandić, un ama de casa de ascendencia serbia,[21]​ que dedicaba parte de su tiempo como científica autodidacta al desarrollo de pequeños aparatos caseros.[1]​[22]​

Se cree que su origen paterno proviene de alguno de los clanes serbios del valle del río Tara, o bien del noble herzegovino Pavle Orlović.[23]​ Su madre, Đuka, provenía de una familia ortodoxa domiciliada en Lika y Banija, pero con profundos orígenes en Kosovo.[24]​ Era competente fabricando herramientas artesanales caseras y había aprendido de memoria numerosos poemas épicos serbios, pero nunca aprendió a leer.[25]​

La familia se trasladó a Gospić en 1862. Tesla asistió al gymnasium de Karlovac, donde completó el plan de estudios de cuatro años en tres.[26]​

Más tarde comenzó los estudios de ingeniería eléctrica en la Universidad de Graz, en la ciudad del mismo nombre, en 1875. Mientras estuvo allí, estudió los usos de la corriente alterna. Algunas fuentes afirman que se licenció por la Universidad de Graz,[27]​[28]​[29]​ aunque la universidad afirma que no recibió ningún grado y que no continuó más allá del segundo semestre del tercer año, durante el cual dejó de asistir a las clases.[30]​[31]​[32]​[33]​

Respecto a su época en Graz, Tesla afirmaba que "trabajaba desde las 3 a.m. a las 11 p.m., incluso domingos y días festivos".[12]​ Después de la muerte de su padre en 1879, Tesla encontró un paquete de cartas de sus profesores a su padre, advirtiéndole de que, a menos que lo sacaran de la escuela, su hijo moriría por exceso de trabajo. Al final de su segundo año perdió su beca y se volvió adicto al juego.[12]​ Durante su tercer año perdió su asignación y el dinero de su matrícula, aunque más adelante se recuperó de sus pérdidas iniciales y devolvió el saldo a su familia. Afirmaba que "pudo dominar [su] pasión en ese momento", pero más tarde en los EE. UU. fue nuevamente conocido por jugar al billar.[34]​

En diciembre de 1878 abandonó Graz y dejó de relacionarse con sus familiares. Sus amigos pensaban que se había ahogado en el río Mura. Se dirigió a Maribor (hoy Eslovenia), donde obtuvo su primer empleo como ayudante de ingeniería, trabajo que desempeñó durante un año. Durante este periodo sufrió una crisis nerviosa. Posteriormente fue persuadido por su padre para continuar sus estudios en la Universidad Carolina en Praga, a la que asistió durante el verano de 1880. Allí fue influido por Ernst Mach. Sin embargo, después de que su padre falleciera dejó la Universidad, completando solamente un curso.[35]​

Tesla pasaba el tiempo leyendo muchas obras y memorizando libros completos, ya que supuestamente poseía una memoria fotográfica.[36]​ En su autobiografía relató que en ciertas ocasiones experimentó determinados momentos de inspiración. Durante su infancia sufrió varios episodios de una enfermedad muy peculiar, que le provocaba que cegadores haces de luz apareciesen ante sus ojos, a menudo acompañados de alucinaciones. Normalmente las visiones estaban asociadas a una palabra o idea que le rondaba la cabeza. Otras veces, estas le daban la solución a problemas que se le habían planteado. Simplemente con escuchar el nombre de un objeto era capaz de visualizarlo de forma muy realista. Actualmente la sinestesia presenta síntomas similares. Tesla podía visualizar una invención en su cerebro con precisión extrema, incluyendo todas las dimensiones, antes de iniciar la etapa de construcción; una técnica algunas veces conocida como pensamiento visual. No solía dibujar esquemas; en lugar de eso concebía todas las ideas solo con la mente. También en ocasiones tenía reminiscencias de hechos que le habían sucedido previamente en su vida, fenómeno este que se inició durante su infancia.[36]​

En 1880 se trasladó a Budapest para trabajar bajo las órdenes de Tivadar Puskás en una compañía de telégrafos,[37]​ la compañía nacional de teléfonos. Allí conoció a Nebojša Petrović, un joven inventor serbio que vivía en Austria. A pesar de que su encuentro fue breve, trabajaron juntos en un proyecto usando turbinas gemelas para generar energía continua. Para cuando se produjo la apertura de la central telefónica en 1881 en Budapest, Tesla se había convertido en el jefe de electricistas de la compañía, y fue más tarde ingeniero del primer sistema telefónico del país. También desarrolló un dispositivo que, de acuerdo con ciertas fuentes, era un repetidor telefónico o amplificador, pero que, según otros, pudo haber sido el primer altavoz.[38]​

En 1882 se trasladó a París, Francia, para trabajar como ingeniero en la Continental Edison Company (una de las compañías de Thomas Alva Edison), diseñando mejoras para el equipo eléctrico traído del otro lado del océano gracias a las ideas de Edison. Según su biografía, en el mismo año concibió el motor de inducción e inició el desarrollo de varios dispositivos que usaban el campo magnético rotativo, por los cuales recibió patentes en 1888.

Poco después, Tesla despertó de un sueño en el cual su madre había muerto, «y yo supe que eso había sucedido».[40]​ Tras esto cayó enfermo. Permaneció dos o tres semanas recuperándose en Gospić y en el pueblo de Tomingaj, cerca de Gračac, el lugar de nacimiento de su madre.

En junio de 1884 llegó por primera vez a los Estados Unidos, a la ciudad de Nueva York,[41]​ con poco más que una carta de recomendación de Charles Batchelor, un antiguo empleador. En la carta de recomendación a Thomas Edison, Batchelor escribió: «conozco a dos grandes hombres, usted es uno de ellos; el otro es este joven».[42]​ Edison contrató a Tesla para trabajar en su Edison Machine Works. Empezó a trabajar para Edison como un simple ingeniero eléctrico, resolviendo algunos de los problemas de la compañía.

La compañía de Edison había instalado varias dinamos en el SS Oregon, en aquel momento uno de los transatlánticos más rápidos y el primer barco en contar con electricidad a bordo,[43]​ empleada para la iluminación de la nave.[44]​ En 1884 las dinamos se dañaron, lo que retrasó la salida del buque de Nueva York.[45]​ Tesla se presentó voluntario para realizar la reparación, y estuvo trabajando toda la noche para lograr hacer funcionar de nuevo las dinamos,[45]​ gracias a lo cual recibió las felicitaciones de Edison a la mañana siguiente.[43]​

La carrera de Tesla progresó rápidamente. Se le ofreció incluso la tarea de rediseñar completamente los generadores de corriente continua de la compañía de Edison.[46]​ Tesla afirmaba que le ofrecieron 50.000 dólares(~ 1,1 millones en 2007, ajustado por la inflación)[47]​ por rediseñar los ineficientes motores y generadores de Edison, mejorando tanto su servicio como su economía.[36]​ En 1885, cuando Tesla preguntó acerca de su remuneración, Edison replicó: "Tesla, usted no entiende nuestro humor estadounidense", rompiendo así su palabra.[48]​[49]​ Con un sueldo de solo 18 dólares a la semana, tendría que haber trabajado 53 años para reunir el dinero que le fue prometido; la oferta era igual al capital inicial de la compañía. Renunció a su empleo de inmediato cuando se le denegó aumentar su salario a 25 dólares semanales.[50]​

Así pues, poco después, necesitado de trabajo, se encontró a sí mismo cavando zanjas para la compañía de Edison por un corto periodo de tiempo, que aprovechó para concentrarse en su sistema polifásico de corriente alterna.[36]​

En 1886, Tesla fundó su propia compañía, la Tesla Electric Light & Manufacturing. Los primeros inversores no estuvieron de acuerdo con sus planes para el desarrollo de un motor de corriente alterna y finalmente lo relevaron de su puesto en la compañía. Trabajó como obrero en Nueva York de 1886 a 1887 para mantenerse y reunir capital para su próximo proyecto. En 1887 construyó un motor de inducción sin escobillas, alimentado con corriente alterna,[51]​ que presentó en el American Institute of Electrical Engineers (Instituto Americano de Ingenieros Eléctricos), actualmente IEEE (Instituto de Ingenieros Eléctricos y Electrónicos) en 1888. Sin embargo, Galileo Ferraris había desarrollado el mismo diseño varios meses antes de manera independiente. En el mismo año desarrolló el principio de su bobina de Tesla, y comenzó a trabajar con George Westinghouse en la Westinghouse Electric & Manufacturing Company's en los laboratorios de Pittsburgh. Westinghouse escuchó sus ideas sobre sistemas polifásicos, que podrían permitir la trasmisión de corriente alterna a larga distancia.[52]​

Conflicto comercialLa demostración de Tesla de su motor de inducción y el posterior otorgamiento de la patente por parte de Westinghouse, ambos en 1888, se produjeron en un momento de competencia extrema entre las compañías eléctricas.[53]​[54]​ Las tres grandes empresas, Westinghouse, Edison y Thompson-Houston, intentaban crecer en una economía capitalista de negocios intensivos, mientras que financieramente se socavaban unas a otras. Incluso hubo una campaña de propaganda, denominada "guerra de las corrientes", con Edison Electric tratando de afirmar que su sistema de corriente continua era mejor y más seguro que el sistema de Westinghouse.[55]​[56]​ Competir en este mercado significaba que Westinghouse no dispondría inmediatamente de los recursos en efectivo o de ingeniería para desarrollar el motor de Tesla y el sistema polifásico relacionado.[57]​

Dos años después de firmar el contrato de Tesla, Westinghouse Electric estaba en problemas. El casi colapso de Baring Brothers en Londres desencadenó el pánico financiero de 1890, lo que provocó que los inversores solicitaran sus préstamos a Westinghouse.[58]​ La repentina escasez de efectivo obligó a la compañía a refinanciar sus deudas. Los nuevos prestamistas exigieron que Westinghouse recortara lo que parecía un gasto excesivo en la adquisición de otras compañías, investigación y patentes, incluidos los derechos acordados por el motor de Tesla.[59]​[60]​ En ese punto, el motor de inducción de Tesla no había tenido éxito y estaba estancado su desarrollo.[58]​[57]​ Westinghouse estaba pagando un canon[61]​ garantizado de 15 000 dólares por año, aunque los ejemplos operativos del motor todavía no eran habituales, al igual que los sistemas de alimentación polifásicos necesarios para alimentarlos.[58]​[62]​

A principios de 1891, George Westinghouse explicó sus dificultades financieras a Tesla en términos contundentes, diciéndole que, si no cumplía con las demandas de sus prestamistas, ya no tendría el control de Westinghouse Electric y Tesla tendría que "tratar con los banqueros" para intentar cobrar sus futuros derechos.[63]​ Las ventajas de que Westinghouse continuara defendiendo su motor probablemente parecieron obvias a Tesla y aceptó liberar a la empresa de la cláusula de pago del canon del contrato.[63]​[64]​ Seis años después, Westinghouse compraría la patente de Tesla por un pago de 216.000 dólares como parte de un acuerdo de intercambio de patentes firmado con General Electric (una compañía creada a partir de la fusión de Edison y Thompson-Houston en 1892).[65]​[66]​[67]​

El dinero que Tesla obtuvo de la licencia de sus patentes de corriente alterna lo hizo económicamente independiente y le proporcionó el tiempo y los fondos necesarios para perseguir sus propios intereses.[68]​ En 1889, Tesla se mudó de la tienda de Liberty Street que Peck y Brown habían alquilado y durante los siguientes doce años trabajó en una serie de talleres/laboratorios en Manhattan, como un laboratorio en el 175 de Grand Street (1889-1892), el cuarto piso del 33-35 South de la Quinta Avenida (1892-1895), y el sexto y séptimo pisos del 46-48 East de Houston Street (1895-1902).[69]​[70]​ Tesla y su personal contratado realizarían parte de su trabajo más importante en estos talleres.

Ciudadano estadounidense

El 30 de julio de 1891, Tesla se convirtió en ciudadano de los Estados Unidos a la edad de 35 años. Instaló su laboratorio en la Quinta Avenida con 35 Sur, en la ciudad de Nueva York, en ese mismo año. Posteriormente lo trasladó a la calle Houston con 46 Este. En este lugar, mientras realizaba experimentos sobre resonancia mecánica con osciladores electromecánicos, generó resonancia en algunos edificios vecinos y, aunque debido a las frecuencias utilizadas no afectó al suyo, sí generó quejas ante la policía: como la velocidad del resonador creció, y siendo consciente del peligro, se vio obligado a terminar el experimento utilizando un martillo, justo en el momento en que llegaron los agentes.[71]​

También hizo funcionar lámparas eléctricas en dos lugares de Nueva York, proporcionando evidencia para el potencial de la trasmisión inalámbrica de energía.[72]​

Algunas de sus amistades más cercanas eran artistas. Se hizo amigo de Robert Underwood Johnson, editor del Century Magazine, quien adaptó algunos poemas serbios de Jovan Jovanović Zmaj (que Tesla tradujo). También en esta época, Tesla fue influido por la filosofía védica (es decir, la doctrina hinduista) según los preceptos de Swami Vivekananda; en tal medida que después de su exposición a estas enseñanzas, empezó a usar palabras en sánscrito para nombrar algunos de sus conceptos fundamentales referentes a la materia y la energía.[73]​

A los 36 años le fueron otorgadas las primeras patentes relacionadas con la alimentación polifásica y continuó con sus investigaciones sobre los principios del campo magnético rotativo. De 1892 a 1894 sirvió como vicepresidente del Instituto Americano de Ingenieros Eléctricos (American Institute of Electrical Engineers), el precursor, junto con el Institute of Radio Engineers, del actual IEEE. De 1893 a 1895, investigó la corriente alterna de alta frecuencia, generando una corriente alterna de un millón de voltios usando una bobina de Tesla cónica e investigó el efecto pelicular en conductores,[74]​ diseñó circuitos LC,[75]​ inventó una máquina para inducir el sueño,[76]​ lámparas de descarga inalámbricas,[77]​ y transmisión de energía electromagnética, construyendo el primer radiotransmisor.[78]​ En San Luis, Misuri, hizo una demostración sobre radiocomunicación en 1893.

En la Exposición Mundial Colombina de Chicago de 1893, hubo por primera vez un edificio dedicado a exposiciones eléctricas. En este evento Tesla y George Westinghouse presentaron a los visitantes la alimentación mediante corriente alterna que fue usada para iluminar la exposición. Además se exhibieron las lámparas fluorescentes y bombillas de Tesla de un solo nodo.[79]​

También explicó los principios del campo magnético rotativo y del motor de inducción, demostrando cómo mantener verticalmente un huevo de cobre mediante su dispositivo conocido como "Huevo de Colón de Tesla".[80]​

Desarrolló el llamado generador de Tesla en 1895, en conjunto con sus inventos sobre la licuefacción del aire.[81]​ Sabía, por los descubrimientos de Kelvin, que el aire en estado de licuefacción absorbía más calor del requerido teóricamente, cuando retornaba a su estado gaseoso y era usado para mover algún dispositivo.[82]​ Justo antes de finalizar su trabajo y patentar cualquier aplicación, ocurrió un incendio en su laboratorio, que destruyó todo su equipo, modelos e invenciones. Poco después, Carl von Linde, en Alemania, presentó una patente de la aplicación de este mismo proceso.[83]​

En el verano de 1889, Tesla viajó a la Exposición Universal en París, donde se enteró de los experimentos de Heinrich Rudolf Hertz (realizados en 1886-1888) que demostraron la existencia de radiación electromagnética, incluidas las ondas de radio.[84]​ Encontró este nuevo descubrimiento "refrescante" y decidió explorarlo más a fondo. Al repetir, y luego expandir, estos experimentos, intentó alimentar una bobina de Ruhmkorff con un alternador de alta velocidad, que había estado desarrollando como parte de un sistema de lámpara de arco mejorado, pero descubrió que la corriente de alta frecuencia sobrecalentaba el núcleo de hierro y fundía el aislamiento entre los devanados principales y secundarios en la bobina. Para solucionar este problema, se le ocurrió su bobina de Tesla con un espacio de aire en lugar de material aislante entre los devanados primarios y secundarios, y un núcleo de hierro que se podía mover a diferentes posiciones dentro o fuera de la bobina.[85]​

Después de 1890, Tesla experimentó con la transmisión de potencia mediante acoplamiento inductivo y capacitivo, utilizando altos voltajes de corriente alterna generados con su bobina Tesla.[86]​ Intentó desarrollar un sistema de iluminación inalámbrico basado en acoplamiento inductivo y capacitivo y realizó una serie de demostraciones públicas donde encendió tubos de Geissler e incluso bombillas incandescentes en un escenario.[87]​ Pasó la mayor parte de la década trabajando en variaciones de esta nueva forma de iluminación con la ayuda de varios inversores, pero ninguna de las empresas logró sacar un producto comercial de sus hallazgos.[88]​

En 1893 en St. Louis, Misuri, ante el Instituto Franklin de Filadelfia (Pensilvania) y ante la National Electric Light Association, Tesla dijo a los espectadores que estaba seguro de que un sistema como el suyo podría eventualmente conducir "señales inteligibles o incluso energía eléctrica a cualquier distancia sin el uso de cables" al conducirlas a través de la Tierra.[89]​[90]​ Pensaba que solo era cuestión de tiempo que el hombre pudiese adaptar las máquinas al engranaje de la naturaleza, declarando: «Antes de que pasen muchas generaciones, nuestras máquinas serán impulsadas por energía obtenida en cualquier punto del universo».[91]​

Tratando de encontrar una forma mejor de generar corriente alterna, Tesla desarrolló un alternador accionado con vapor. Lo patentó en 1893 y lo presentó en la Exposición Mundial Colombina de Chicago de ese año. El vapor era forzado hacia el oscilador y se precipitaría a través de una serie de válvulas, empujando un pistón unido a una armadura hacia arriba y hacia abajo. La armadura magnética vibraba hacia arriba y hacia abajo a alta velocidad, produciendo un campo magnético alterno. Este campo inducía a su vez una corriente eléctrica alterna en las bobinas de alambre adyacentes. Eliminó las partes complicadas de una máquina/generadora de vapor, pero nunca se contempló como una solución de ingeniería factible para generar electricidad.[92]​[93]​

A principios de 1893, el ingeniero Benjamin G. Lamme de Westinghouse había progresado mucho desarrollando una versión eficiente del motor de inducción de Tesla, y Westinghouse Electric comenzó a calificar su sistema bifasico completo como el "Sistema Tesla Polifase". Creían que las patentes de Tesla les daban la prioridad sobre otros sistemas de corriente alterna.[94]​

Westinghouse Electric le pidió a Tesla que participara en el Exposición Mundial Colombina de Chicago de 1893, donde la compañía tenía un gran espacio en un edificio dedicado a exhibiciones eléctricas. Westinghouse Electric ganó la licitación para iluminar la Exposición con corriente alterna  y fue un evento clave en la historia de esta forma de electricidad, ya que la compañía demostró al público estadounidense la seguridad, fiabilidad y eficiencia de un sistema de corriente alterna completamente integrado.[95]​[96]​[97]​ Tesla mostró una serie de efectos eléctricos relacionados con la corriente alterna, así como su sistema de iluminación inalámbrico, utilizando una demostración que había realizado anteriormente en toda América y Europa; [98]​ incluyendo el uso de alto voltaje, y una corriente alterna de alta frecuencia para encender un lámpara de descarga inalámbricamente.[99]​

Un observador anotó:

Tesla también explicó los principios del campo magnético rotativo en un motor de inducción al demostrar cómo hacer que un huevo de cobre se coloque de punta, usando un dispositivo que él construyó conocido como el Huevo de Colón,[101]​ e introdujo su nuevo generador de corriente alterna con un oscilador alimentado a vapor.

En 1893, Edward Dean Adams, que encabezaba la compañía que explotaría el salto hidroeléctrico adyacente a las Cataratas del Niágara, solicitó la opinión de Tesla sobre qué sistema sería mejor para transmitir la energía generada en las cataratas. Durante varios años, hubo una serie de propuestas y concursos abiertos sobre la mejor manera de usar la energía generada por las cataratas. Entre los sistemas propuestos por varias empresas de EE. UU. y Europa se encontraban las corrientes alternas bifásicas y trifásicas, las corrientes continuas de alta tensión y sistemas de aire comprimido. Adams se dirigió a Tesla para obtener información sobre el estado actual de todos los sistemas de la competencia, y Tesla le aconsejó que un sistema de dos fases sería el más fiable, y que había un sistema de Westinghouse para encender bombillas incandescentes usando corriente alterna de dos fases. La compañía adjudicó un contrato a Westinghouse Electric para construir un sistema de generación de corriente alterna de dos fases en las Cataratas del Niágara, basado en el asesoramiento de Tesla y la demostración de Westinghouse en la Exposición Colombina de que podrían construir un sistema de corriente alterna completo. Al mismo tiempo, se otorgó un contrato adicional a General Electric para construir el sistema de distribución de la corriente generada.[102]​

Algunas personas creen falsamente que en las cataratas del Niágara se construyó la primera central hidroeléctrica gracias a los desarrollos de Tesla en 1893, consiguiendo en 1896 transmitir electricidad a la ciudad de Búfalo (Nueva York). Las primeras centrales hidroeléctricas se desarrollaron primero en Europa en 1878-1885. Tras 1885 Westinghouse contrató, entre otros, a William Stanley, Oliver B. Shallenberger y Benjamin Lamme, para construir sistemas de potencia de corriente alterna en todo EE. UU. Tesla no se unió a Westinghouse hasta 1888.[103]​

En 1895, Edward Dean Adams, impresionado con lo que vio cuando recorrió el laboratorio de Tesla, aceptó ayudar a fundar la empresa Nikola Tesla, creada para financiar, desarrollar y comercializar una variedad de patentes e invenciones anteriores de Tesla, así como otras nuevas. Alfred Brown firmó, trayendo patentes desarrolladas bajo Peck and Brown. El consejo de la empresa se completó con William Birch Rankine y Charles F. Coaney.[104]​ Encontraron pocos inversores, dado que a mediados de la década de 1890 se vivía un momento difícil desde el punto de vista financiero, y las patentes inalámbricas de iluminación y osciladores que se establecieron en el mercado nunca se materializaron. La compañía manejaría las patentes de Tesla en las décadas siguientes.

En la madrugada del 13 de marzo de 1895, el edificio de la Quinta Avenida Sur que albergaba el laboratorio de Tesla se incendió. El fuego comenzó en el sótano del inmueble y fue tan intenso que el laboratorio de Tesla situado en el cuarto piso se quemó y colapsó en el segundo piso. El incendio no solo retrasó los proyectos en curso de Tesla, sino que también destruyó una colección de notas tempranas y material de investigación, modelos y piezas de demostración, incluidas muchas que habían sido expuestas en la Exposición Colombina de 1893. Tesla dijo al The New York Times: "Estoy demasiado apenado para hablar. ¿Qué puedo decir?"[105]​ Después del incendio, Tesla se mudó al 46-48 de East Houston Street y reconstruyó su laboratorio en los pisos 6 y 7.

En 1894, Tesla empezó a investigar los que después se llamaron rayos X. En el incendio de su laboratorio en 1895 se perdió todo su trabajo, según afirmó el propio Tesla. Mientras tanto, en noviembre de ese mismo año, el científico alemán Wilhelm Röntgen concluía su extensa y sistemática investigación de los rayos X, publicando sus conclusiones en 1895. La primera publicación de Tesla sobre los "rayos de Rontgen" data de 1895.[108]​

Según el propio Tesla narra, usó su propio tubo de vacío (similar a su patente Patente USPTO n.º 514170: «#514,170»). Este dispositivo difería de otros tubos de rayos X por el hecho de no tener electrodo receptor. El término moderno para el fenómeno producido por este artefacto es Bremsstrahlung (o radiación de frenado).

En sus primeras investigaciones Tesla diseñó algunos experimentos para producir rayos X. Afirmó que con estos circuitos, «el instrumento podrá generar rayos de Roentgen de mayor potencia que la obtenida con aparatos ordinarios».[109]​

También mencionó los peligros de trabajar con sus circuitos y con los rayos X producidos por sus dispositivos de un solo nodo. De muchas de sus notas en las investigaciones preliminares de este fenómeno, se deduce que atribuyó el daño de la piel a varias causas. Inicialmente creyó que el daño no podría ser causado por los rayos de Roentgen, sino por el ozono generado al contacto con la piel y en parte también al ácido nitroso. Pensaba que se trataba de ondas longitudinales, como las producidas por las ondas en plasmas.[110]​[111]​

En 1898, Tesla mostró en público un barco que controlaba usando un radiocontrol basado en un cohesor -que denominó "telautomaton"- durante una exposición eléctrica en el Madison Square Garden.[112]​ La multitud que presenció la demostración hizo afirmaciones escandalosas sobre el funcionamiento del barco, tales como magia, telepatía o que estaba siendo pilotado por un mono entrenado oculto en su interior.[113]​ Tesla intentó vender su idea al ejército de los EE. UU. como un tipo de torpedo controlado por radio, pero la marina mostró poco interés.[114]​ El radiocontrol remoto siguió siendo una novedad hasta la Segunda Guerra Mundial, cuando varios países lo usaron en sus programas militares.[115]​ Tesla aprovechó la oportunidad para demostrar aún más la "Teleautomática" en una conferencia pronunciada en una reunión del Club Comercial de Chicago, mientras viajaba a Colorado Springs, el 13 de mayo de 1899.[105]​

Desde la década de 1890 hasta 1906, Tesla invirtió gran parte de su tiempo y fortuna en una serie de proyectos para desarrollar la transmisión inalámbrica de energía. Fue una expansión de su idea de usar bobinas para transmitir la potencia que había estado demostrando en la iluminación inalámbrica. Vio este procedimiento no solo como una forma de transmitir grandes cantidades de energía en toda la Tierra, sino también, como había señalado en sus conferencias anteriores, una forma de transmitir comunicaciones en todo el mundo.

En el momento en que Tesla estaba formulando sus ideas, no había una forma factible de transmitir de forma inalámbrica señales de comunicación a largas distancias, y mucho menos grandes cantidades de energía. Había estudiado las ondas de radio desde el principio y llegó a la conclusión de que parte del estudio existente sobre ellas, realizado por Hertz, era incorrecto.[116]​[117]​[118]​ Además, esta nueva forma de radiación era ampliamente considerada en ese momento como un fenómeno de corta distancia que parecía extinguirse en menos de una milla.[119]​ Tesla notó que, incluso si las teorías sobre ondas de radio eran verdaderas, no tenían ningún valor para los propósitos previstos, ya que esta forma de "luz invisible" disminuiría a distancia como cualquier otra radiación y la haría viajar en línea recta hacia el espacio, "perdiéndose irremediablemente".[120]​

A mediados de la década de 1890, Tesla estaba trabajando en la idea de que podría conducir electricidad a larga distancia a través de la Tierra o la atmósfera, y comenzó a desarrollar experimentos para probar esta idea, incluyendo la instalación de un gran transformador de resonancia basado en una bobina de Tesla ubicado en su laboratorio de East Houston Street.[121]​[122]​[123]​ Parece que tomó prestada la idea común en aquel momento de que la atmósfera de la Tierra era conductiva,[124]​[125]​ y propuso un sistema compuesto por globos suspendidos, transmisores y receptores, electrodos en el aire por encima de 9.000 m de altitud, donde pensó que la presión más baja le permitiría enviar altos voltajes (millones de voltios) a largas distancias.

Su «sistema mundial para la transmisión de energía eléctrica sin cables» basado en la conductividad eléctrica de la Tierra, funcionaría mediante la transmisión de energía por varios medios naturales y el uso subsiguiente de la corriente trasmitida entre los dos puntos para alimentar dispositivos eléctricos.[126]​

Tesla afirmó haber demostrado la transmisión inalámbrica de energía a principios de 1891. Sin embargo nunca pudo llevarla a la práctica de una forma eficiente.[127]​

En 1899, Tesla se traslada a un laboratorio en Colorado Springs, Estados Unidos, para iniciar sus experimentos con alta tensión y mediciones de campo eléctrico. Los objetivos trazados por Tesla en este laboratorio eran: desarrollar un transmisor de gran potencia, perfeccionar los medios para individualizar y aislar la potencia transmitida y determinar las leyes de propagación de las corrientes sobre la Tierra y su atmósfera.[128]​ Durante los ocho meses que estuvo en Colorado Springs, Tesla escribió notas con una detallada descripción de sus investigaciones día a día. Allí dedicó la mitad de su tiempo a medir y probar su enorme bobina Tesla y otro tanto a desarrollar receptores de pequeñas señales y a medir la capacidad de una antena vertical. También realizó observaciones sobre bolas de fuego, que afirmaba haber producido. Un día, notó un comportamiento inusual de un instrumento que registraba tormentas, un cohesor rotativo. Se trataba de un instrumento que realizaba registros cuando una tormenta se aproximaba y se alejaba de su laboratorio. Concluyó que se trataba de la existencia de ondas estacionarias, que podían ser creadas por su oscilador. Con equipos sensibles pudo realizar mediciones de rayos que caían a gran distancia de su laboratorio, observando que las ondas de las descargas crecían hasta un pico y luego decrecían antes de repetir el ciclo total. Tesla sugirió que esto se debía al hecho de que la Tierra y la atmósfera poseían electricidad, lo que hacía que el planeta se comportara como un conductor de dimensiones ilimitadas, en el que era posible hacer transmisión de mensajes telegráficos sin hilos, y todavía más; transmitir potencia eléctrica a cualquier distancia terrestre, casi sin pérdidas, por medio de sus conocimientos de resonancia. Tesla había descubierto que podía producir un anillo alrededor de la Tierra como una campana, con descargas cada dos horas, y también que podía hacerlo resonar eléctricamente. Encontró que la resonancia del planeta era del orden de los 10 Hz, un valor realmente exacto para su época, ya que hoy en día se sabe que es de 8 Hz. Después de que descubriera cómo crear ondas eléctricas permanentes para transmitir potencia eléctrica alrededor del mundo, el científico alemán W. O. Schumann postuló que la Tierra conductiva y la ionosfera forman una guía de onda esférica, a través de la cual se pueden propagar ondas electromagnéticas de muy baja frecuencia (conocidas como ELF por sus siglas en inglés), generadas por la actividad de los rayos a escala mundial, con valores cercanos a los 8 Hz, fenómeno que se conoce como la resonancia Schumann. Tesla realizó trabajos mucho más avanzados que los otros pioneros de la transmisión sin hilos, Hertz y Marconi, quienes usaron altas frecuencias que no resonaban con la Tierra, a diferencia de las ondas de radio de altas longitudes de onda empleadas por Tesla, que tenían la ventaja de ser recibidas en sitios remotos de la Tierra, o en las profundidades del mar, para mantener la comunicación entre naves de superficie y submarinos.[129]​

En su laboratorio de Colorado Springs, observó señales inusuales que más tarde creyó podrían ser evidencia de comunicaciones de radio extraterrestre provenientes de Venus o Marte.[130]​ Notó que eran señales repetitivas, pero con una naturaleza distinta a las observadas en tormentas y ruido terrestre. Tesla mencionó que sus invenciones podrían ser usadas para hablar con otros planetas. Y afirmó que inventó el "Teslascopio" para ese propósito. Actualmente se debate sobre el tipo de señales que Tesla pudo recibir, que podrían ser resultado de la radiación natural extraterrestre,[131]​ aunque de todas formas, ha quedado para la historia de la ciencia como el precursor de la radioastronomía.

El 7 de enero de 1900, Tesla dejó Colorado Springs, no sin antes trasladarse durante ciertos períodos de tiempo a la localidad cercana de Cripple Creek, en donde realizaba experimentos colocando bombillas sobre el terreno y, comentaban asombrados sus vecinos, estas se encendían solas. El laboratorio fue demolido y su contenido vendido para pagar las deudas. El conjunto de los experimentos allí preparados por Tesla para el establecimiento de la transmisión de telecomunicaciones inalámbricas trasatlánticas fue conocido como Wardenclyffe.

Se dice que Nikola Tesla no hacía planos, sino que lo memorizaba todo.[36]​
[132]​ Buena parte de la etapa final de su vida la vivió absorto con el proceso judicial que entabló en lo relativo a la invención de la radio, que se disputaba con Marconi, pues Tesla había inventado un dispositivo similar al menos 15 años antes que él. En la década de 1960, el Tribunal Supremo de los Estados Unidos dictaminó que la patente relativa a la radio era legítimamente propiedad de Tesla, reconociéndolo de forma legal como inventor de esta, si bien esto no trascendió a la opinión pública, que todavía considera a Marconi como su inventor.[133]​

Algunos de sus estudios nadie podía descifrarlos debido a su enorme capacidad inductiva. Para la mayoría de sus proyectos ideaba los documentos de cabeza, le bastaba con tener la imagen de dicho objeto sin saber cómo funcionaba, simplemente lo elaboraba sin saber que podía suponer un gran avance para la humanidad.[36]​ Fue un lector minucioso de la teoría física de Ruđer Bošković.[134]​

Se especula que ideó un sistema de transmisión de electricidad inalámbrico, de tal suerte que la energía podría ser llevada de un lugar a otro mediante ondas de naturaleza no hertzianas.[135]​ Dicho sistema se basaría en la capacidad de la ionosfera para conducir electricidad, la potencia se transmitiría a una frecuencia de 6 Hz con una enorme torre llamada Wardenclyffe Tower, para valerse de la resonancia Schumann como medio de transporte.

Hoy en día se sabe que esta frecuencia es de 7,83 Hz y no de 6 , aunque realmente varía desde 7,83 Hz a 12 Hz, según la actividad solar y el estado de la ionosfera. Si bien se ha creído que el fracaso del proyecto se dio por problemas financieros, otras versiones aseguran que en realidad las ideas de Tesla para la torre (planeaba utilizar la Tierra como conductora de la electricidad a todo el globo) no funcionaron y que volvió a pedirle dinero a Morgan, pero este, decepcionado ante los resultados, se negó. Algunos expertos de la actualidad han intentado estudiar cómo se suponía que funcionara la Torre Wardenclyffe, pero generalmente terminan con más preguntas que respuestas.[cita requerida] No está del todo claro qué método pretendía utilizar Tesla para transmitir electricidad y muchos creen que tal vez ni siquiera él mismo lo tenía definido.

Tesla recorrió Nueva York tratando de encontrar inversores para lo que pensó que sería un sistema viable de transmisión inalámbrica, invitándolos a cenar en el Palm Garden del Waldorf Astoria (el hotel donde vivía en ese momento), The Players Club y el restaurante Delmonico.[136]​ En marzo de 1901, obtuvo 150.000 dólares (equivalentes a unos 4,4 millones de dólares de 2018) de J. P. Morgan a cambio de una participación del 51% sobre cualquier patente inalámbrica generada, y comenzó a planear la instalación de la Torre Wardenclyffe para ser construida en Shoreham (Nueva York), unos 160 km al este de la ciudad, en la costa norte de Long Island.[137]​

En julio de 1901, había expandido sus planes para construir un transmisor más potente para pasar por delante del sistema de radio de Marconi, que Tesla pensó que era una copia de su propio sistema.[138]​ Se acercó a Morgan para pedirle más dinero y poder construir el sistema más grande, pero Morgan se negó a proporcionar más fondos.[139]​

En diciembre de 1901, Marconi transmitió con éxito la letra S de Inglaterra a Newfoundland, derrotando a Tesla en la carrera por ser el primero en completar dicha transmisión. Un mes después del éxito de Marconi, Tesla intentó hacer que Morgan respaldara un plan aún más grande para transmitir mensajes y energía mediante el control de "vibraciones en todo el mundo".[138]​ Durante los cinco años siguientes, Tesla escribió más de 50 cartas a Morgan, implorando y exigiendo fondos adicionales para completar la construcción de Wardenclyffe. Continuó el proyecto durante otros nueve meses en 1902. La torre se erigió en su totalidad, hasta alcanzar los 57 m de altura.[140]​ En junio de 1902, trasladó sus operaciones desde el laboratorio de Houston Street a Wardenclyffe.[137]​

Los inversores de Wall Street estaban poniendo su dinero en el sistema de Marconi, y algunos medios de prensa comenzaron a volverse contra el proyecto de Tesla, alegando que era un engaño.[141]​ El proyecto se detuvo en 1905, y en 1906, los problemas financieros y otros eventos pudieron llevar a Tesla a sufrir un ataque de nervios.[142]​ Tuvo que hipotecar la propiedad Wardenclyffe para cubrir sus deudas en el Waldorf Astoria, que finalmente ascendieron a 20.000 dólares (aproximadamente medio millón de dólares de 2018).[143]​ Perdió la propiedad por ejecución hipotecaria en 1915, y en 1917, la Torre fue demolida por el nuevo propietario para hacer del suelo un activo inmobiliario más viable.

Después de que Wardenclyffe cerró, Tesla continuó escribiendo a Morgan. Tras la muerte de "el gran hombre", escribió al hijo de Morgan, Jack, tratando de obtener más fondos para el proyecto. En 1906, abrió oficinas en el 165 de Broadway en Manhattan, tratando de recaudar más fondos desarrollando y comercializando sus patentes. Más adelante pasó a tener oficinas en la Metropolitan Life Tower de 1910 a 1914; a estar alquilado durante unos meses en el Woolworth Building (de donde tuvo que mudarse porque no podía pagar el alquiler); y finalmente a las oficinas de 8 West 40th Street, donde permaneció de 1915 a 1925. Cuando se mudó a esta última oficina, estaba en bancarrota: la mayoría de sus patentes se habían agotado y estaba teniendo problemas con los nuevos inventos que trataba de desarrollar.[144]​

En su 50 cumpleaños, en 1906, Tesla demostró una turbina sin álabes de 200 CV de potencia y capaz de girar a 16.000 rpm. Entre 1910 y 1911 se probaron varios de sus motores de turbina sin paletas (de entre 100 y 5000 CV) en la Central eléctrica de Waterside de Nueva York.[145]​ Colaboró en este desarrollo con varias compañías, incluyendo el período 1919-1922 trabajando con la Allis-Chalmers en Milwaukee.[146]​[147]​ Pasó la mayor parte de su tiempo tratando de perfeccionar la turbina Tesla con Hans Dahlstrand, el ingeniero jefe de la empresa, pero las dificultades de ingeniería hicieron que nunca se convirtiera en un dispositivo práctico.[148]​ Tesla licenció la idea a una empresa de instrumentos de precisión y encontró uso en forma de exactos velocímetros y de otros instrumentos.[149]​

Cuando estalló la Primera Guerra Mundial, los británicos interceptaron el cable telegráfico transatlántico que unía los EE. UU. con Alemania para controlar el flujo de información entre los dos países. También trataron de cortar la comunicación inalámbrica alemana desde y hacia los Estados Unidos, haciendo que la empresa estadounidense Marconi demandase a la compañía de radio alemana Telefunken por infracción de patente.[150]​ Telefunken convocó a los físicos Jonathan Zenneck y Carl Ferdinand Braun para su defensa, y contrató a Tesla como testigo durante dos años por 1000 dólares al mes. El caso se estancó y luego quedó sin efecto cuando los EE. UU. entraron en guerra contra Alemania en 1917.[150]​[151]​

En 1915, Tesla intentó demandar a la Compañía Marconi por infringir sus patentes de sintonización inalámbrica. La patente de radio inicial de Marconi había sido adjudicada en los EE. UU. en 1897, pero su solicitud de patente de 1900, que cubría mejoras a la transmisión de radio, había sido rechazada varias veces antes de su aprobación final en 1904, con base en que infringía otras patentes existentes, incluidas dos patentes de ajuste inalámbrico de Tesla de 1897.[152]​[117]​[153]​ El caso de 1915 de Tesla quedó en nada,[154]​ pero en un caso relacionado, donde la compañía Marconi intentó demandar al gobierno de EE. UU. por infracciones de patentes durante la Primera Guerra Mundial, una decisión de la Corte Suprema de los Estados Unidos de 1943 restauró las patentes anteriores de Oliver Joseph Lodge, John Stone y Tesla.[155]​ El tribunal declaró que su decisión no tenía relación con la reclamación de Marconi como el primero en lograr la transmisión de radio, solo que dado que la demanda de Marconi sobre ciertas mejoras patentadas era cuestionable, la compañía no podía reclamar una infracción sobre esas mismas patentes.[156]​[117]​

El 6 de noviembre de 1915, un informe de la agencia de noticias Reuters de Londres le otorgó el Premio Nobel de Física de 1915 a Thomas Edison y a Nikola Tesla; sin embargo, el 15 de noviembre, un despacho de Reuters desde Estocolmo declaró que el premio de ese año estaba siendo otorgado a Sir William Henry Bragg y a su hijo William Lawrence Bragg "por sus servicios en el análisis de la estructura cristalina por medio de rayos X".[157]​[158]​[159]​ Hubo rumores infundados en aquel momento de que tanto Tesla como Edison habían rechazado el premio. [157]​ La Fundación Nobel señaló que: "Cualquier rumor de que una persona no ha recibido un Premio Nobel porque ha hecho conocer su intención de rechazar la recompensa es ridículo"; un destinatario podría rechazar un Premio Nobel solo después de que se le anuncie como ganador.[157]​

Ha habido afirmaciones posteriores de los biógrafos de Tesla de que Edison y Tesla fueron los destinatarios originales y que ninguno recibió el premio debido a su animosidad entre ellos; que cada uno trató de minimizar los logros y el derecho a ganar el premio del otro; que ambos rechazaron aceptar el premio si el otro lo recibía primero; que ambos rechazaron cualquier posibilidad de compartirlo; e incluso que un acaudalado Edison se negó a que Tesla recibiera el premio en metálico de 20.000 dólares.[157]​[160]​

En los años posteriores a estos rumores, ni Tesla ni Edison ganaron el premio (aunque Edison recibió una de las 38 posibles candidaturas en 1915 y Tesla recibió una de las 38 candidaturas posibles en 1937).[161]​

El 7 de enero de 1943, a la edad de 86 años, Tesla murió solo en la habitación 3327 del Wyndham New Yorker Hotel. Su cuerpo fue encontrado por una doncella que entró en la habitación de Tesla, ignorando el cartel de "no molestar" que el propio Tesla había colocado en su puerta dos días antes. Un médico forense examinó el cuerpo y dictaminó que la causa de la muerte había sido una trombosis coronaria.[105]​

Dos días después, el FBI ordenó que la Custodia de Propiedades Extranjeras se apropiara de las pertenencias del difunto,[105]​ aunque Tesla era ciudadano estadounidense.[105]​ John G. Trump, profesor del M.I.T. y un conocido ingeniero eléctrico que prestaba servicios como ayudante técnico del National Defense Research Committee, fue llamado para analizar los artículos de Tesla, que estaban en custodia.[105]​ Después de una investigación de tres días, el informe de Trump concluyó que no había nada que pudiera constituir un riesgo en manos hostiles, concluyendo que:

En una caja que supuestamente contenía una parte del "rayo de la muerte" de Tesla, Trump encontró una caja de décadas (una sencilla caja conmutadora de resistencias eléctricas)[172]​ de 45 años de antigüedad.

El 10 de enero de 1943, el alcalde de la ciudad de Nueva York, Fiorello La Guardia, leyó en directo un panegírico escrito por el autor esloveno Louis Adamic en la emisora de radio WNYC, mientras que las piezas de violín "Ave María" y la canción popular serbia "Tamo daleko" se escuchaban en segundo plano.[105]​ El 12 de enero, dos mil personas asistieron a un funeral de estado en la Catedral de San Juan el Divino. Después del funeral, el cuerpo de Tesla fue llevado al Cementerio Ferncliff en Ardsley, Nueva York, donde fue incinerado. Al día siguiente, un segundo servicio fue conducido por destacados sacerdotes en la Capilla de la Trinidad (hoy Catedral serbia ortodoxa de San Java) en la ciudad de Nueva York.[105]​

En 1952, la insistencia del sobrino de Tesla, Sava Kosanović, consiguió que todas la propiedades de Tesla fueran enviadas a Belgrado en 80 baúles marcados con las letras NT.[105]​ En 1957, la secretaria de Kosanović, Charlotte Muzar, transportó las cenizas de Tesla de Estados Unidos a Belgrado.[105]​ Las cenizas se muestran en una esfera dorada sobre un pedestal de mármol en el Museo Nikola Tesla.[173]​

Desde 1900 vivía en el Hotel Waldorf Astoria de Nueva York, acumulando una gran factura.[174]​ En 1922, se trasladó al Hotel St. Regis, y seguiría un patrón a partir de entonces de mudarse a un nuevo hotel cada pocos años, dejando cuentas impagadas.[175]​[176]​

Paseaba hasta un parque todos los días para alimentar a las palomas. Se dedicó a alimentarlas en la ventana de la habitación de su hotel, atrayendo a los pichones heridos para curarlos.[176]​[177]​[178]​  Afirmaba que era visitado diariamente por una paloma blanca que había recogido cuando estaba herida. Se gastó más de 2000 dólares, incluyendo la construcción de un dispositivo que sostuvo al animal cómodamente para que sus huesos pudieran sanar, curando sus alas y patas rotas.[179]​ Tesla declaró:

Las facturas impagadas de Tesla y las quejas sobre el desastre de su alimentación a las palomas lo obligaron a abandonar el St. Regis en 1923, el Hotel Pennsylvania en 1930 y el Hotel Governor Clinton en 1934.[176]​ En un momento dado, también ocupó habitaciones en el edificio JPMorgan Chase Tower de Nueva York.[181]​

En 1934, se mudó al Hotel Wyndham New Yorker, y la Westinghouse Electric & Manufacturing Company comenzó a pagarle 125 dólares por mes, además de abonar su alquiler, gastos que la compañía pagaría durante el resto de la vida de Tesla. El relato de cómo se llegó a esta situación varían. Distintas fuentes dicen que Westinghouse estaba preocupado (o había sido advertido) por la posible mala publicidad en torno a las condiciones empobrecidas bajo las cuales vivía su ex inventor estrella.[182]​[183]​[184]​[185]​ El pago ha sido descrito como una "tarifa de consultoría" para evitar la aversión de Tesla a aceptar caridad, o como algún tipo de compensación no especificada.[184]​

En 1931, Kenneth Swezey, un joven escritor que había estado asociado con Tesla por un tiempo, organizó una celebración para el 75° cumpleaños del inventor. Tesla recibió cartas de felicitación de más de 70 pioneros de la ciencia y la ingeniería, incluido Albert Einstein,[186]​ y también apareció en la portada de Time magazine.[187]​ La leyenda de la portada "Todo el mundo es su central eléctrica" destacó su contribución a la generación de energía eléctrica. La fiesta fue tan celebrada, que Tesla la convirtió en un evento anual, una ocasión en la que sacaba una gran cantidad de comida y bebida (con platos de su propia creación) e invitaba a la prensa a ver sus inventos y escuchar historias sobre hazañas pasadas, opiniones sobre eventos actuales, o algunas veces anuncios extraños o desconcertantes.[188]​[189]​

Con ocasión de su cumpleaños en 1932, Tesla afirmó que había inventado un motor que funcionaría gracias a la radiación cósmica.[189]​ En 1933, a la edad de 77 años, dijo a los periodistas que, después de treinta y cinco años de trabajo, estaba a punto de producir pruebas de una nueva forma de energía. Afirmó que era una teoría de la energía que estaba "violentamente opuesta" a la física de Einstein, y podría ser aprovechada con un aparato que sería barato de ejecutar y duraría 500 años. También dijo a los periodistas que estaba trabajando en una forma de transmitir longitudes de onda de radio privadas individualizadas, investigando en avances de metalurgia y desarrollando una forma de fotografiar la retina para registrar el pensamiento.[190]​

En la fiesta de 1934, Tesla dijo a los periodistas que había diseñado una superarma que, según él, terminaría con todas las guerras.[191]​[192]​ La llamaría "teleforce", pero usualmente se la mencionaba como su rayo de la muerte.[193]​ La describió como un arma defensiva que se pondría en la frontera de un país para ser utilizada contra el ataque de la infantería o de aeronaves. Tesla nunca reveló durante su vida los planes detallados de cómo el arma podía funcionar, pero en 1984, salieron a la luz en el archivo del Museo Nikola Tesla de Belgrado.[194]​ El tratado, titulado "El nuevo arte de proyectar energía concentrada no dispersiva a través de los medios naturales", describe un tubo de vacío abierto en un lado con un sellado de gas que permite que las partículas salgan, un método para cargar lingotes de tungsteno o de mercurio a millones de voltios, y dirigir la corriente resultante mediante repulsión electrostática.[195]​[189]​ Tesla intentó interesar al Departamento de Defensa de los Estados Unidos,[196]​ el Reino Unido, la Unión Soviética y Yugoslavia en el dispositivo.[197]​

En 1935, en su 79 fiesta de cumpleaños, Tesla cubrió muchos temas. Afirmó haber descubierto el rayo cósmico en 1896 e inventó una forma de producir corriente directa por inducción, y realizó muchas afirmaciones sobre su oscilador mecánico.[198]​ Describiendo el dispositivo (que esperaba que le haría ganar 100 millones de dólares en dos años), dijo a los periodistas que una versión de su oscilador había causado un terremoto en su laboratorio de 46 East Houston Street y en las calles vecinas en el centro de la ciudad de Nueva York en 1898.[198]​ Siguió diciendo a los periodistas que su oscilador podría destruir el Empire State Building con 5 libras de presión de aire.[199]​ También explicó una nueva técnica que desarrolló usando sus osciladores, que llamó "telegeodinámica", utilizándola para transmitir vibraciones al suelo que, según afirmó, funcionarían a cualquier distancia para ser utilizadas para la comunicación o para localizar depósitos minerales subterráneos.[200]​

Tesla trabajaba todos los días desde las 9:00 a.m. hasta las 6:00 p.m. o más tarde, con una cena exactamente a las 8:10 p.m., en el restaurante Delmonico's y más adelante en el Waldorf Astoria. Pedía su cena por teléfono al maitre, quien también debía ser el único que la sirviera. "La comida debía estar lista a las ocho en punto ... Cenaba solo, excepto en las raras ocasiones en que daba una cena a un grupo para cumplir con sus obligaciones sociales. A continuación, Tesla reanudaba su trabajo, a menudo hasta las 3:00 a.m."[201]​

Para hacer ejercicio, caminaba entre 13 y 16 km cada día. Todas las noches flexionaba los dedos de sus pies cien veces, afirmando que estimulaba sus células cerebrales.[202]​

En una entrevista con el editor de periódicos Arthur Brisbane, dijo que no creía en la telepatía, y comentó que: "Supongamos que pienso en asesinarte", dijo, "en un segundo lo sabrías. ¿No es maravilloso? ¿Por qué proceso realizaría la mente todo esto?" En la misma entrevista, afirmó que creía que todas las leyes fundamentales podrían reducirse a una.[203]​

Tesla se hizo vegetariano en sus últimos años, viviendo de alimentarse tan solo de leche, pan, miel y jugos de vegetales.[192]​[204]​

Tesla medía 1,88 m de altura y pesaba 64 kg, con casi ninguna variación de peso desde 1888 hasta alrededor de 1926. Su aspecto fue descrito por el editor periodístico Arthur Brisbane como "casi el hombre más alto, casi el más delgado y sin duda, el más serio que va a Delmonico regularmente".[203]​[205]​ Era una figura elegante y estilizada en la ciudad de Nueva York, meticuloso en su aseo y vestimenta, y ordenado en sus actividades cotidianas, un aspecto que mantuvo para promover sus relaciones comerciales.[206]​ También se ha dicho que tenía los ojos claros, "manos muy grandes" y "enormes pulgares".[203]​

Tesla leyó muchas obras, memorizó libros completos y supuestamente tenía una memoria eidética.[207]​ Era polígloto, dominando ocho idiomas: serbocroata, checo, inglés, francés, alemán, húngaro, italiano y latín.[208]​ En su autobiografía relató que experimentaba ciertos momentos concretos de inspiración. Durante sus primeros años, padeció repetidamente episodios en los que estuvo enfermo. Sufría una aflicción peculiar, consistente en que destellos cegadores de luz aparecían ante sus ojos, a menudo acompañados de visiones. [207]​ A menudo, las visiones estaban relacionadas con una palabra o una idea que podría haber encontrado; en otras ocasiones le proporcionarían la solución a un problema particular que se había planteado. Con solo escuchar el nombre de un elemento, podría visualizarlo con detalles realistas. [207]​ Tesla visualizaría una invención en su mente con extrema precisión, incluidas todas sus dimensiones, antes de pasar a la etapa de construcción, una técnica a veces conocida como dibujo mental. Por lo general, no hacía dibujos a mano, sino que trabajaba de memoria. Desde su infancia, tuvo frecuentes ráfagas de recuerdos relativas a eventos que habían sucedido previamente en su vida.[207]​

Tesla afirmó que nunca dormía más de dos horas por noche.[209]​ Sin embargo, admitió haber estado "dormitando" de vez en cuando "para recargar sus baterías".[202]​ Durante su segundo año de estudio en Graz, desarrolló una afición apasionada por competir al billar, al ajedrez y a las cartas, a veces pasando más de 48 horas seguidas en una mesa de juego.[210]​ En una ocasión trabajó durante 84 horas sin descanso en su laboratorio.[211]​ Kenneth Swezey, un periodista del que Tesla se hizo amigo, confirmó que el inventor rara vez dormía. Swezey recordaba una madrugada, cuando Tesla lo llamó a las 3 a. m.: "Estaba durmiendo en mi habitación como un muerto ... De repente, el timbre del teléfono me despertó ... [Tesla] habló animadamente, con pausas, [como solía hacer] ... solucionó un problema, comparó una teoría con otra, lo comentó, y cuando sintió que había llegado a la solución, de repente colgó el teléfono."[202]​

Tesla nunca se casó, explicando que su castidad fue muy útil para sus capacidades científicas.[207]​ Una vez dijo que en su juventud sentía que una mujer nunca podría ser lo suficientemente digna para él, considerando que las mujeres se creen superiores en todos los sentidos. Su opinión se había visto influida en los últimos años cuando contempló cómo las mujeres estaban tratando de superar a los hombres y hacerse más dominantes. Esta "nueva mujer" suscitó mucha indignación por parte de Tesla, que sentía que las mujeres estaban perdiendo su feminidad al tratar de alcanzar el poder. En una entrevista con el Galveston Daily News del 10 de agosto de 1924, declaró: "En lugar de la suave voz de mi adoración reverente, ha llegado la mujer que piensa que su principal éxito en la vida reside en hacerse a sí misma tanto como sea posible, adoptando la vestimenta, la voz y las acciones del hombre, en los deportes y en logros de todo tipo ... La tendencia de las mujeres a apartar al hombre, suplantar el antiguo espíritu de cooperación con él en todos los asuntos de la vida, es muy decepcionante para mí".[212]​ Aunque le dijo a un periodista en años posteriores que a veces sentía el no haberse casado, y que había hecho un sacrificio demasiado grande por su trabajo.[179]​ Eligió prescindir de perseguir o de entablar ninguna relación conocida, encontrando todo el estímulo que necesitaba en su trabajo.

Tesla era asocial y propenso a aislarse con su trabajo.[213]​[214]​[112]​[215]​ Sin embargo, cuando se involucró en la vida social, muchas personas hablaron muy positivamente y con admiración de él. Robert Underwood Johnson lo describió como poseedor de una "distinguida dulzura, sinceridad, modestia, refinamiento, generosidad y fuerza". [179]​ Su secretaria, Dorothy Skerrit, escribió: "Su sonrisa genial y su nobleza siempre denotaron las características caballerescas que estaban tan arraigadas en su alma".[216]​ El amigo de Tesla, Julian Hawthorne, dejó escrito que: "Rara vez uno conoce a un científico o a un ingeniero que también sea un poeta, un filósofo, un conocedor de la música refinada, un lingüista y un entendido degustador de la comida y la bebida".[217]​

Era un buen amigo de F. Marion Crawford, Robert Underwood Johnson,[218]​ Stanford White,[219]​ Fritz Lowenstein, George Scherff y Kenneth Swezey.[220]​[221]​[222]​

Siendo ya un hombre maduro, se hizo amigo íntimo de Mark Twain; pasaron mucho tiempo juntos en su laboratorio y en otros lugares.[218]​ Twain describió notablemente la invención del motor asíncrono de Tesla como "la patente más valiosa desde el teléfono".[223]​ A finales de la década de 1920, se hizo amigo de George Sylvester Viereck, poeta, escritor, místico y posteriormente, propagandista del nazismo. De vez en cuando, asistía a cenas organizadas por Viereck y su esposa.[224]​[225]​

A veces, Tesla podía ser duro y expresar su disgusto por las personas con sobrepeso, como cuando despidió a una secretaria por su peso.[226]​ Rápidamente criticaba la indumentaria de sus empleados; y en varias ocasiones, ordenó a un subordinado que se fuera a casa y cambiara su vestimenta.[207]​

Relación con Edison

Tesla había sido empleado de la Compañía Edison entre 1883 y 1885, primero en París y a continuación en Nueva York. Conocía personalmente al propio Edison, pero su prometedora carrera en la empresa se vio truncada porque no recibió el salario que pensaba que se le había prometido. Sintiéndose engañado, comenzó a trabajar por su cuenta.

Tres años después, en 1888, Tesla había pasado a colaborar con Westinghouse, y en 1893 inició en Chicago sus demostraciones públicas para evidenciar la superioridad de la corriente alterna sobre la corriente continua de Edison, entablándose entonces lo que se conoce como la "guerra de las corrientes".[227]​

Edison trató de combatir la teoría de Tesla mediante una campaña para fomentar ante el público el peligro que corrían al utilizar la corriente alterna. El ingeniero Harold P. Brown, que había iniciado una cruzada contra los peligros de la corriente alterna, fue financiado por Edison para investigar la electrocución, contribuyendo al desarrollo de la silla eléctrica y consiguiendo por medios subrepticios que fuera alimentada con corriente alterna.[228]​

Respecto a su rival, Edison llegó a decir que: "Tesla es un sujeto que siempre está a punto de hacer algo", en una despectiva alusión a las habituales y altisonantes declaraciones a la prensa de su competidor.[229]​ Por su parte, Tesla llegó a criticar abiertamente el desaliñado modo de vida de Edison, afirmando que: "Tal era su desidia que, de no haber contraído matrimonio con una mujer de sobresaliente inteligencia, que puso todo su empeño en sacarlo a flote, habría muerto hace muchos años".[230]​ 

Cuando Thomas Alva Edison murió, en 1931, Tesla aportó la única opinión negativa a The New York Times, enterrando bajo un denso manto la vida de Edison:

No estuvo de acuerdo con la teoría de que los átomos se componen de partículas subatómicas más pequeñas, indicando que no existía un electrón que creara una carga eléctrica. Creía que si existían electrones, eran un cuarto estado de la materia o "subátomo" que solo podía existir en un vacío experimental y que no tenían nada que ver con la electricidad.[232]​[233]​ Opinaba que los átomos son inmutables: no podían cambiar de estado o dividirse de ninguna manera. Creía en el concepto dominante en el siglo XIX de un "éter" omnipresente que transmitía la energía eléctrica.[234]​

Generalmente era antagónico con las teorías sobre la conversión de la materia en energía.[235]​ También criticaba la teoría de la relatividad de Einstein, diciendo:

Afirmó haber desarrollado su propio principio físico con respecto a la materia y la energía, en el que comenzó a trabajar en 1892,[216]​ y en 1937, a los 81 años, afirmó en una carta que había completado una "teoría dinámica de la gravedad" que [haría] poner fin a las especulaciones ociosas y a las concepciones falsas, como la del espacio curvo. Señaló que la teoría estaba resuelta en todos los detalles y que esperaba poder dársela pronto al mundo.[237]​ Esta elucidación de su teoría nunca ha sido encontrada en sus escritos.[238]​

Además de sus dotes como científico tecnológico, Tesla es ampliamente considerado por sus biógrafos desde el punto de vista filosófico como un humanista.[239]​[240]​[241]​ Esto no impidió que, como muchas otras personas notables de su época, se convirtiera en un defensor de una versión de selección artificial impuesta en forma de eugenesia.

Expresó la creencia de que la "piedad" humana había interferido con el "funcionamiento despiadado de la naturaleza". Aunque su argumentación no dependía del concepto de una "raza elegida" o la superioridad inherente de una persona sobre otra, abogó por la eugenesia. En una entrevista de 1937, declaró:

En 1926, Tesla hizo un comentario sobre los males de la sumisión social de las mujeres y de su lucha para obtener la igualdad de género, e indicó que el futuro de la humanidad estaría a cargo de "abejas reinas". Pensaba que las mujeres se convertirían en el sexo dominante en el futuro.[243]​

Hizo predicciones sobre temas relevantes en el entorno inmediato a la Primera Guerra Mundial, en un artículo titulado "Ciencia y Descubrimiento son las grandes Fuerzas que conducirán a la Consumación de la Guerra" (20 de diciembre de 1914).[244]​ Pensaba que la Sociedad de las Naciones no era el remedio adecuado para los problemas de su tiempo.[245]​

Tesla fue educado en la religión ortodoxa. Durante su vida adulta no se consideró a sí mismo como un "creyente en el sentido ortodoxo", oponiéndose al fanatismo religioso, y opinaba que "el budismo y el cristianismo son las religiones más grandes tanto al número de creyentes como en importancia".[246]​ También dijo que "Para mí, el universo es simplemente una gran máquina que nunca comenzó a existir y nunca terminará" y "lo que llamamos 'alma' o 'espíritu' no es más que la suma de los funcionamientos del cuerpo. Cuando este funcionamiento cesa, el 'alma' o el 'espíritu' también cesa".[246]​

En su artículo, "El problema de incrementar la energía humana", publicado en 1900, Tesla escribió:

Entre los más destacables inventos y descubrimientos que han llegado al conocimiento del público en general, se pueden destacar:

Para un listado más amplio de Patentes ver Anexo:Patentes de Tesla

Un juglar[a]​ —antiguamente también joglar[1]​— era un artista ambulante en la Edad Media. A cambio de dinero o comida, ofrecía su espectáculo callejero en las plazas públicas, y en ocasiones era contratado para participar como atracción y entretenimiento en las fiestas y los banquetes de reyes y nobles.[2]​[3]​

De las tres acepciones que el DRAE consigna de «juglar», la principal (recogida también en el diccionario de teatro de Gómez García) insiste en su condición de artista de atracciones: «En la Edad Media, persona que ante el pueblo o los nobles y los reyes recitaba, cantaba o bailaba o hacía juegos, yendo de unos lugares a otros». También señalan los académicos la sinonimia entre «juglar» y «pícaro», además de la antigua relación con «trovador» o «poeta».

Por su parte, el semiólogo francés Patrice Pavis acoge el sentido de equivalencia que se les da a «juglar» y «malabarista» en muchos países europeos: bateleur, en francés; juggler, en inglés, y gaukler, en alemán. De ahí que «juglar» sea, en el medio histórico-medieval, un término genérico en el que se incluyen farsantes, charlatanes, saltimbanquis, feriantes, acróbatas e, incluso, barberos, dentistas y amaestradores de animales.

«Joglar» y «juglar» derivan de la palabra latina iocularis que se puede traducir como gracioso, divertido o entretenido. A su vez, iocularis procede del vocablo iocus, de la que deriva la palabra «juego»; una de las acepciones de «juegos» en el DRAE es: «fiestas y espectáculos públicos que se celebraban antiguamente».

Según Jacques Le Goff, se calificaba como joculator a todos aquellos que se consideraban peligrosos o convenía separar de la sociedad. Un joculator era, pues, un indeseable, un rebelde...[4]​

Ramón Menéndez Pidal concluye que el término joglaría significaba, en un principio, diversión o espectáculo que proporciona el juglar, evolucionando posteriormente su significado al de burla o chanza.

Fray Liciniano Sáez sostiene que la voz joglar no solo corresponde a truhan bufón, cantor de coplas por las calles y comediantes, sino que también comprende a los poetas, a los que cantaban en las iglesias y palacios de grandes señores, a los compositores de danzas, juegos y toda especie de diversiones y alegrías, a los tañedores de instrumentos; en definitiva, a todos los que causaban alegría.

El vocablo «juglar» engloba, durante buena parte de la Edad Media, a todos los profesionales de las artes escénicas, independientemente de su calidad artística, remuneración y público al que entretenía. Así mismo, era habitual que los juglares dominasen más de una disciplina artística y las combinasen en sus espectáculos. Así lo atestiguan textos de la época, es el caso del cantar de gesta Daurel et Beton, en el que el juglar Daurel tañe arpa y vihuela, canta cantares de gesta y lais, sabe trovar y ejerce de saltimbanqui. 

Estudiosos como Edmond Faral sostienen la teoría de los juglares como artistas indiferenciados y multidisciplinares desde sus orígenes, frente a las teorías que admiten la existencia de una élite juglaresca, descendiente de bardos o cantores germanos, de origen distinto al de los otros juglares. Al margen del origen, sí surgieron distintos estatus juglarescos en función de sus habilidades artísticas, el grado de refinamiento de sus espectáculos y el público al que deleitaban. Poco tenían en común el nivel de vida y el repertorio de un juglar tabernario con el de un juglar al servicio de un rey.

Los juglares se fueron especializando con el paso de los siglos, surgiendo diversos tipos y categorías con nombres propios específicos. Así hubo tipos de juglares bien vistos y valorados por sus artes (es el caso de los juglares épicos o los líricos), en cambio otros fueron desdeñados y reprobados por los moralistas (por ejemplo: los goliardos o los cazurros). El mismo Alfonso X el Sabio, en la Séptima Partida, trató de infames "a los que son juglares e los remedadores e los facedores de los zaharrones que públicamente andan por el pueblo o cantan o facen juegos por precio, esto es, porque se envilecen ante otros por aquel precio que les dan. Más los que tañeren estrumentos o cantasen por facer solaz a sí mesmos, o por facer placer a sus amigos o dar solaz a los reyes o a los otros señores, non serían por ende enfamados (infames)". Asimismo, por la Ley 3º, título XIV, Partida 4º, se prohibía a las "personas ilustres que tuvieran por barraganas a juglaresas ni sus hijas".[5]​

Los primeros juglares aparecieron en torno al s. VII d. C. divirtiendo, tanto a clases altas en sus palacios como a las bajas en las plazas y pueblos. Ramón Menéndez Pidal razona que en los textos se referían a ellos con figuras procedentes del teatro romano como mimos, histriones y timélicos,[6]​ sin quedar claro si sus espectáculos eran iguales a los de la época romana u otros distintos más parecidos a los espectáculos juglarescos de la Plena Edad Media. Los juglares continuaron las artes escénicas de sus predecesores durante la Alta Edad Media, con la gran diferencia de que componen y actúan en las nuevas lenguas romances locales que van sustituyendo al latín.

Poco a poco las distintas élites artísticas, que actuaban en cortes o estaban al servicio de la nobleza, fueron adoptando otras denominaciones para diferenciarse de artistas de inferior calidad o categoría que actuaban para público más humilde. Así, por ejemplo, a partir del siglo XIII comenzó a llamarse ministriles a los músicos cortesanos y juglares al resto de músicos, creando así dos tipos de músicos en función del público para el que actuaban. 

Por otra parte, los juglares van abandonando mayoritariamente su faceta creativa, tanto en el ámbito musical como en el literario, para ponerse al servicio de las composiciones de clérigos y trovadores, pasando de ser compositores de sus propias obras a convertirse en meros intérpretes y difundidores de obras ajenas al mester de juglaría.  

Según avanza la Baja Edad Media, el prestigio de la figura del juglar va disminuyendo hasta quedar relegado solo para ser llamados así a los truhanes y mendigos que se ganaban la vida más por medio de la picaresca que por sus habilidades artísticas. Este punto de vista sobre la juglaría fue compartido por muchos intelectuales. Por ejemplo, Marcelino Menéndez Pelayo la definió así:

La juglaría desapareció progresivamente prolongando su decadencia hasta finales del XV o las primeras décadas del siglo XVI, exceptuando al ciego juglar, que pervivió hasta bien entrado el siglo XX recitando romances y otros cantares o coplas acompañados con zanfona o rabel.

El juglar podía incluir en sus espectáculos desde música y literatura hasta acrobacias, juegos o simple charlatanería. Según la actividad en la que se centraran, podía distinguirse varios tipos de juglares:

Otra posible clasificación sería la división en juglares de voz, que combinaban música y voz en sus actuaciones, y juglares de instrumentos, que prescindían de la voz. Entre estos últimos, y según el instrumento utilizado, se podían encontrar violeros, cedreros, cítolas, tromperos y tamborreros, entre otros.

De modo muy esquemático, suele asociarse al trovador con el autor (creador), y al juglar con el actor (intérprete).[7]​ Ambos se sintetizarían en la cultura musical del siglo XX con la imagen del cantautor.

Así pues, detrás del juglar —o en su origen— ha de situarse la personalidad del trovador occitano y catalán, o el «minnesänger» alemán.[nota 1]​ La conclusión más definidora de la diferencia entre juglar y trovador, parece evidente: el primero necesita un público y pertenece al ámbito teatral del espectáculo, el segundo (el trovador en su concepto de origen) no necesita público y se encuadra en el ámbito literario.[8]​

Medievalistas como Ramón Menéndez Pidal, defienden que los juglares son los primeros poetas en lengua romance y los trovadores aparecen posteriormente por imitación del juglar. En el siglo XII, los nobles comienzan a cultivar la poesía lírica en lengua vulgar, surgiendo así los trovadores como poetas refinados para las clases más cultas. 

La diferenciación entre juglar y trovador era difusa, pues trovar aludía al acto de la invención o creación artística, cosa que también hacían algunos juglares. Así mismo, ni todos los trovadores eran de origen noble, ni todos los juglares eran más pobres o menos distinguidos. Incluso se dieron casos de trovadores que interpretaban sus propias piezas y cobraban por ello, algo propio de los juglares. 

Generalmente, el trovador solía ser más instruido y de mayor posición económica que el juglar; incluso podían tener juglares a su servicio que interpretaban su obra o servían de acompañamiento musical. Por ejemplo, los trovadores occitanos solían mencionar el nombre del juglar al que encomendaban interpretar una composición ante el destinatario de la misma.

Progresivamente los juglares líricos dejaron de componer para pasar a ser meros intérpretes que solicitaban repertorio a trovadores, dando lugar al concepto de la juglaría como simple propagadora de la lírica trovadoresca.  


Cabe destacar que con los clérigos nace la literatura escrita en lenguas romances en torno al siglo XI, pero la literatura oral ya contaba con siglos de existencia gracias a la juglaría. Los juglares se basaban en la oralidad, no en la escritura, por tanto, una parte de su obra se perdió cuando cayó en el olvido y otra parte se conservó gracias a copias manuscritas por clérigos o las prosificaciones de cantares y romances en las distintas crónicas.

A pesar de que clérigos y juglares parecen totalmente contrapuestos, existieron clérigos ajuglarados, tipos de juglares que provenían de la clerecía (los goliardos o los sopistas) y juglares devotos que cantaban las vidas de los santos. 

Se conocen casos de clérigos que pasaron a ser juglares (Peire Rogier) y viceversa (el juglar del siglo XIV Martín Vaasquez). Esto se debe a que no todos los juglares eran analfabetos y a que los escolares tenían formación musical, como así lo indica el autor del libro de Alexandre <<todos los estrumentos que usan los joglares, otros de maor preçio que usan escolares>>   

Los clérigos, en su faceta de moralistas y adoctrinadores, acaban conjugando su formación culta con la necesidad de ser comprendidos por una población que, mayoritariamente, apenas comprendía el latín. Para ello utilizan a la juglaría como divulgadores de obras clericales o como amenizadores de actos religiosos. No obstante, no solo se valieron de juglares para predicar, sino que hubo clérigos que utilizaron artes juglarescas para ello.[nota 2]​ 

Si bien había clérigos a favor de la juglaría, obispos que tenían juglares a sueldo y moralistas que apreciaban algunos tipos de juglaría. Con el tiempo, se impuso la corriente que veía a los juglares y a las juglaresas como: pecaminosos, una distracción reprobable o una fuente de relajación moral, tanto para la iglesia como para el pueblo. Así surgen quejas, como la de Don Juan Manuel,[nota 3]​ por actos profanos juglarescos en vigilias como cantares y bailes. En el siglo XIII comienzan a dictarse prohibiciones contra la juglaría.[nota 4]​

Con respecto a la poesía, conviene aclarar que el mester de clerecía engloba a hombres instruidos, no solo a sacerdotes. Ambas escuelas poéticas no eran opuestas, como se creyó durante el romanticismo, sino que los juglares pudieron formarse técnicamente en la escuela de los clérigos, aunque su poesía era muy distinta. Para los clérigos, la poesía asonante y asimétrica del mester de juglaría era muy rudimentaria, ellos desarrollan una forma poética más elaborada, compleja y sin fallos métricos: la poesía en cuaderna vía de versos alejandrinos y rima consonante.

Los clérigos no solo prosificaron, tanto en latín como en lengua romance, la poesía narrativa juglaresca. También escribieron poemas épicos basándose en ella, como es el caso del Poema de Fernán González, o reelaboraciones de los mismos, como las Mocedades de Rodrigo.

Mención aparte merece el Arcipreste de Hita, que no solo escribió el Libro de buen amor para ser utilizado por los juglares, también escribió muchas cantigas para toda clase de juglares, de las que solo se conservan dos cantos de escolar, que son simples peticiones de limosna.

Al sector más cultivado de la juglares se le debe la conservación de un rico tesoro, transmitido en forma de tradición oral,[9]​ y que puede abarcar, en un sentido muy amplio, desde la poesía épica medieval hasta la poesía cortesana prerenacentista.[nota 5]​

Como herencia de la Europa medieval, el juglar aparecería como punto fijo (o parte del paisaje) en el «Pont Neuf» de París o en la Plaza de San Marcos veneciana. Muchas veces, su espectáculo (una auténtica actuación física y lúdica) fue gratuito, como reclamo o anuncio callejero para invitaciones de aristócratas o grandes señores, entre los que a menudo se encontraban los eclesiásticos más ricos y poderosos. Así, fueron mencionados en la Estoria de España, a finales del siglo X, 'invitados' a las bodas de las hijas del Cid, y en diversos pasajes de otras grandes Crónicas europeas y muy variadas obras anónimas de la alta y baja Edad Media.[10]​ 

levantóse la dama ricamente adornada,
tomó una viola buena y bien templada,
y salió al mercado a tocar por soldada.

Comenzó unos ritmos y unos sones tales
que gran dulzor traían y eran naturales;
henchíanse de hombres aprisa los portales,
no caben en las plazas, súbense a los poyales.

Cuando con su viola hubo bien agradado,
a gusto de los pueblos bastante hubo cantado,
tornóles a decir un romance rimado,

A pesar de haber antecedentes de poetas y cantores épicos clásicos como los aedas y rapsodas griegos, el juglar épico surge, o cuanto menos está influenciado, por los cantores de origen bárbaro como los bardos, los escaldos o los scopas. Progresivamente van sustituyendo a estas figuras y surgiendo con fuerza en la Europa occidental y meridional. La poesía épica juglaresca deriva de la poesía épica que cultivaron pueblos germánicos como: Godos, Francos y Anglosajones. Esta poesía primitiva se transmitía oralmente y ocasionalmente por escrito. Los primeros juglares épicos o juglares primitivos componían poemas cortos, de unos 500 o 600 versos, muy diferentes a los romances posteriores, centrados en la narración de hechos.

De los poemas narrativos de los juglares primitivos nacen los cantares de gesta, mucho más extensos (podían sobrepasar los 10000 versos), con más elementos líricos y de fantasía. Esta extensión se debe a que solían estar compuestos por varios juglares de diversas épocas, ya que se iban refundiendo cantares más antiguos y añadiendo contenido. Estos poemas eran recitados por los juglares de gesta, de los cuales poco se sabe debido a que, a diferencia de los juglares líricos, sus obras se centran en la narración y apenas se mencionan a sí mismos.


En la península ibérica, los juglares de gesta llegan a su momento culminante en la segunda mitad del siglo XIII. La poesía narrativa de los juglares convive con la de los clérigos, siendo muy variado el repertorio de ambos: vidas de santos, hechos de la edad clásica, de grandes caballeros de la Edad media, poemas morales y devotos, acontecimientos históricos, sátiras, etc. Como en otros lugares, se llegó a tener en muy alta estima a los juglares de gesta por su contribución a difundir la historia y como elemento fundamental para fomentar y preservar el sentimiento nacional. Su poesía narrativa era muy apreciada en todos los estamentos de la sociedad.

En la segunda mitad del siglo XIV, ya solo persiste la poesía narrativa de los juglares de gesta, debido al auge de los relatos en prosa. Por ese tiempo en Francia, los cantares de gesta van quedando relegados a los juglares ciegos. En España, los juglares de gesta perviven algo más, hasta el siglo XV, pero ya solo tienen éxito entre el público burgués y el más bajo.

Los cantares de gesta se dejaron de componer y cantar en la segunda mitad del siglo XV, sin embargo, no fue el fin de la juglaría épica. Antes del siglo XV surgen los romances como partes o episodios de los cantares de gesta, siendo difundidos por juglares de segundo orden en ciudades y aldeas.[nota 6]​ Estos romances van tomando forma propia, siendo evolucionados lírica y formalmente en cada refundición, hasta quedar plasmados por escrito en el romancero viejo, ya como versiones muy posteriores y alteradas de los originales.   

Estos romances eran recitados por los juglares de romances, los cuales convivieron con los juglares de gesta, hasta que éstos desaparecieron o se reciclaron como juglares de romances. Ramón Menéndez Pidal sostiene que existían dos tipos de romances cultivados por los juglares: el romance popular épico-lírico que conocemos y un tipo de romance juglaresco más extenso, parecido a los antiguos poemas épicos de los juglares primitivos.


Los juglares de romances fueron reemplazados por el propio pueblo a medida que comenzó a recitar y transmitir los romances de generación en generación. El juglar épico desaparece porque ya no es necesario, pero su obra pervive en la memoria popular, dando lugar a otros géneros posteriores como el romancero nuevo o los libros de caballerías.
Dentro de las obras Juglares española se destacan los siguientes temas, de tradición castellana:

Para captar la atención de su público, los juglares utilizarán, por influencia de las chansons de geste francesas, una serie de recursos basados en el carácter oral de las representaciones de la épica: 

La relación de la juglaría con la música abarca dos vertientes; por un lado, están los juglares líricos que componían cantigas o interpretaban las de los trovadores y por otro lado los juglares músicos que se ganaban la vida amenizando eventos.

En la música medieval europea predominaba la música sacra o del entorno de la iglesia (como la escuela de Notre Dame), o cuanto menos es de la que más producción se conserva. Sin embargo, además de la música de canto llano religiosa existe una música profana[12]​ desarrollada por juglares y trovadores, también de escritura monódica y vocal que llevaba acompañamiento instrumental y compás definido.

La juglaría es la transmisora generacional de la música popular no litúrgica, incorporando las novedades que van surgiendo en la música religiosa y en la cortesana. Los juglares también contribuyeron a propagar el Ars antiqua y el Ars nova,[nota 7]​ así como la música instrumental y los propios instrumentos musicales, la mayoría ajenos a la música sacra y en buena parte de procedencia oriental.

La música juglaresca no se limitaba solo al uso de las siete notas naturales, también utilizaban semitonos propios de la denominada música ficta o cromática. Así mismo, era habitual el uso de instrumentos polifónicos.

Los juglares líricos y los instrumentistas no eran propios solo de una zona o una religión, sino que existieron juglares cristianos, mozárabes, sarracenos y judíos.[13]​

Los juglares líricos son compositores e intérpretes; quedando, en muchos casos, a servicio de trovadores para interpretar y difundir las composiciones de estos últimos. Por tanto, el juglar lírico sabe cantar y tocar instrumentos.[nota 8]​ Muestra de la relación entre juglares y trovadores, así como de la capacidad para componer de los juglares, es el subgénero lírico llamado tensón.

Ramón Menéndez Pidal establece tres tipos de juglares líricos: los que son autores de sus propias canciones, los que cantan obras anónimas, ya compuestas, que van modificando a su gusto y los que cantan la obra de un trovador, lo cual les obliga a respetar la composición original y ceñirse a ella.   

Los juglares líricos que componen tienden a querer salir de la clase juglaresca para alcanzar la categoría de trovador, pudiendo llegar a quedarse en una categoría intermedia denominada segrel.

En la península ibérica, la lírica de influencia provenzal se cultiva en el este y la lírica gallego-portuguesa en el oeste y el centro peninsular, en el que convive con la lírica castellana hasta el siglo XV. Además, en el sur peninsular se cultiva la lírica mozárabe, como por ejemplo las jarchas o los cantares a lo arábico. Tampoco conviene olvidar la música sefardita cultivada en las juderías de toda la península.

Desde mediados del siglo XIV, el juglar cortesano abandona su faceta de compositor de poesía lírica y el canto, quedando su papel reducido al de músico, tomando el nombre de ministril. Aun así su papel como transmisor y difundidor musical sigue siendo relevante.[nota 9]​

En un escalafón inferior al juglar lírico se encuentra el juglar instrumentista, el cual se ganaba vida tocando uno o varios instrumentos, ya fuese en las cortes de los nobles, en fiestas populares, religiosas o celebraciones.

Los juglares de instrumentos estuvieron a sueldo de: casas reales, casas nobles, prelados y ciudades, llegando a tener buenas cantidades de ingresos por medio de ropas, alimentos, etc. Los cuales vendían para obtener dinero. Dichas ganancias solían ir en función del tipo de instrumento que tocasen, ya que había categorías según el mismo, siendo la mayor la de los juglares que tocaban instrumento de cuerda y de menor categoría los juglares de instrumentos de viento y los de percusión.[nota 10]​ La clase más ínfima de juglar músico era el tamborero.[14]​ En las fiestas y eventos, los juglares no se mezclaban como en una orquesta, sino que cada tipo solía tener asignado un lugar, en función de la cantidad de nivel sonoro que se requiriese; si el nivel era bajo, los juglares de instrumentos más sonoros ocupaban las zonas más alejadas, si por el contrario, la celebración requería mucho estruendo tomaban protagonismo en cantidad y cercanía.[nota 11]​

La clasificación más corriente de este tipo de juglares se basa en el instrumento en el que están especializados. Los más destacados, en los poemas de clerecía, son los que tocan la vihuela (violeros), seguidos de cedreros y cítolas. También son comunes: roteros, organistas, tromperos, tamboreros, etc.

Algunas hipótesis relacionan la actividad juglaresca con la aparición del teatro profano medieval. El juglar, hombre-espectáculo, mimo cómico-dramático, recitador, cantor y músico, acróbata y domador, era, como el bululú descrito por Rojas Villandrando, un bufón con pretensiones artísticas (en palabras de Quevedo: un "bufo farandulero miserable").

No obstante, su dignidad escénica, como actor callejero con recursos, quedó ya escrita en el siglo XI, en el epitafio del juglar Vitalis: 


En la juglaría se refugiaron los distintos tipos de actores procedentes del teatro latino, junto con los de procedencia bárbara, cada uno con sus técnicas y cualidades, ambos se mezclan en el polifacético oficio de juglar. Hay rasgos de teatralidad o, cuanto menos, interpretativos en los juegos de escarnio, en la lírica (donde hay indicios de que las cantigas pudieron ser representadas ante público),[16]​ en los espectáculos de los zaharrones y en el de los juglares de gesta.[nota 12]​

A pesar de la escasa información sobre el origen de teatro medieval, existe un cierto consenso en creer que las primeras obras teatrales medievales son de temática religiosa y ligadas a las prácticas litúrgicas. Los clérigos copian la teatralidad juglaresca, dignificando el oficio de actor.[nota 13]​ Es probable que la iglesia contase con juglares[nota 14]​ como actores para la representación de autos o para los tropos (Hay juglares representados en un tropario de la abadía de San Marcial de Limoges).

Tal y como se deduce en las Siete Partidas de Alfonso X,[nota 15]​ existía un teatro laico de temática profana del que apenas queda rastro, sobre cuyo origen hay diversas teorías.[17]​ Parte del mismo estaría en manos de los juglares histriones, o bien por ser compuesto por ellos o por ser los actores del mismo. El teatro juglaresco sería bien distinto al teatro convencional, enfocado a entretener un público que, en vez de ir a presenciar el desarrollo de una trama o intriga, fuese a ver escenas sueltas dentro de un espectáculo más amplio y variado. 

El juglar, a diferencia del actor convencional, ni se ciñe a un texto, (lo usa como guía, junto a su capacidad de improvisación para adaptarse al público), ni abandona su personalismo al interpretar (no se convierte en el personaje que interpreta, sino que este se expresa por medio del juglar). El arte juglaresco no está centrado en la parte interpretativa, sino en la comunicativa; el juglar se parecería más a un showman, un monologuista o al actor del teatro de calle actual que a un actor convencional de cine o teatro.[18]​

Con su inclusión en el teatro religioso y el cortesano, el juglar se va transformando en actor, abandonando su faceta de narrador y performance, dando lugar al actor medieval y al renacentista especializado en la interpretación de personajes. De los juglares histriónicos derivan figuras como el arlequín o los cómicos de la legua.  

Bajo el término juglar, o como un tipo de juglaría, sobrevivieron una serie de artes escénicas y circenses procedentes del teatro y el circo romano. Así pasaron a llamarse juglares a los: volatineros, saltimbanquis, prestidigitadores y domadores.[nota 16]​

En el s. V d. C., el oficio callejero del bufón se mezcló con el de los juglares; como tales, les correspondió el mérito de mantener y propagar a lo largo de toda la Edad Media el arte del títere y la marioneta.[19]​

Las juglaresas o juglaras, soldaderas, cantaderas y danzaderas contribuyeron a preservar el arte del canto y del baile. Sucesoras de las danzadoras griegas, las bailadoras gaditanas (puellae Gaditanae) en tiempos del imperio romano, las ballimatias de la época visigoda[nota 17]​ y las cantoras árabes. Cabe destacar el éxito del canto de las juglaresas andaluzas en los siglos X y XI[20]​ o la cantidad de miniados medievales en las que aparecen juglaresas y soldaderas; por ejemplo, en 12 de los 16 que hay en Cancionero de Ajuda (en 9 de ellas bailando o cantando). Este tipo de artes fueron reprobadas y condenadas por los moralistas, que las relacionaron directamente con la mala vida, la lujuria e incluso la prostitución, sirva como ejemplo el Libro de las confesiones de Martín Pérez, escrito en torno al año 1316.[nota 18]​      

La faceta polifacética que aglutinó el oficio de juglar se refleja en los consejos que el juglar Giraut de Calansón le da al juglar Fadet para gustar en la corte de Pedro II de Aragón:


El rescate que intelectuales y dramaturgos europeos y americanos del siglo XX hicieron del teatro popular, llevó a la recuperación de prácticas y recursos escénicos ancestrales, y la búsqueda de públicos marginales (y marginados) en entornos ajenos a los circuitos teatrales. Los nuevos juglares, auténticos reyes del teatro de calle, pusieron de nuevo en juego un teatro no literario, satírico-político muy a menudo, y siempre divertido y popular.[21]​ Los Tabarin y Montdor que montaban sus «fantasías tabarínicas» entre 1619 y 1625, se habían reencarnado en los Dario Fo y los Enrique Buenaventura de la segunda mitad del siglo XX.

En España comienza a recuperarse la figura y el oficio de juglar, al albor de las recreaciones de mercados y acontecimientos medievales,[22]​ surgidos a partir de la última década del siglo XX. Posteriormente, en el siglo XXI, aparecen eventos específicos sobre juglaría como, por ejemplo, el Encuentro de juglares de Sahagún[23]​ o el Encuentro internacional de juglares, trovadores y cuenteros en Melque,[24]​ que van dando visibilidad a juglares modernos,[25]​ tanto a los que adoptan alguna característica juglaresca (como en su día hizo la histórica compañía Els Joglars), como a los más convencionales, cuyo repertorio se basa en la oralidad, el folclore y la literatura. Así, las características y cualidades de juglares modernos, como el leonés Crispín d’Olot, poco diferirían de las de sus antecesores de los siglos XII al XVI.

El peculiar marco socio-político en buena parte de Iberoamérica generó, ya en pleno siglo XX, diversos modelos que podrían considerarse continuadores o reflejo de la juglaresca medieval europea y el folk-singer de América del Norte. Modernos trovadores (así conocidos en varios países del Nuevo Mundo) y troveros, con el título de payadores en el Cono Sur, pueden ser considerados, en diferentes niveles de compromiso y personalidad: Atahualpa Yupanqui, Víctor Jara, Pablo Milanés, Jacinto Palacios o Jorge Cafrune, por citar tan solo a los más conocidos en el plano internacional.[26]​
También pueden ser considerada dentro de la juglaría de instrumentos a la agrupación callejera chilena, integrada por un organillero y dos chinchineros o percusionistas bailarines. Esta tradicional tripleta es, por lo demás, extensión de la tradición organillera alemana, desaparecida ya en la primera mitad del siglo XX.[27]​

Herrero Massari, José Manuel (2015) Juglares y trovadores, Ediciones Akal.

Menéndez Pidal, Ramón. Poesía juglaresca y juglares. Orígenes de las literaturas románicas. Ed: Espasa-Calpe 9ª edición 1991

Milá y Fontanals, Manuel. De la poesía heroico-popular castellana : estudio precedido de una oración acerca de la literatura española (1874)

Gómez Muntané, María del Carmen. La música medieval en España. Reichenberger, 2001.

La estética (del griego αἰσθητική [aesthetic], ‘sensación’, ‘percepción’, y este de[aísthesis], ‘sensación’, ‘sensibilidad’, e -ικά [-icá], ‘relativo a’) es la rama de la filosofía que estudia la esencia y la percepción de la belleza y el arte.[1]​[2]​

Algunos autores definen la estética de manera más amplia, como el estudio de las experiencias estéticas y los juicios estéticos en general, y no solo los relativos a la belleza.[3]​ Cuando juzgamos algo como «bello», «feo», «sublime» o «elegante» (por dar algunos ejemplos), estamos haciendo juicios estéticos, que a su vez expresan experiencias estéticas.[3]​ La estética es el dominio de la filosofía, estudiando el arte y cualidades como la belleza; asimismo es el estudio de estas experiencias y juicios que suceden día a día en las actividades que realizamos, produciendo sensaciones y emociones ya sean positivas o negativas en nuestra persona. La estética busca el por qué de algunas cuestiones, por ejemplo, por qué algún objeto, pintura o escultura no resulta atractivo para los espectadores; por lo tanto el arte lleva relación con la estética ya que busca generar sensaciones a través de una expresión.

En otra acepción, la estética es el estudio de la percepción en general, sea sensorial o entendida de manera más amplia. Estos campos de investigación pueden coincidir, aunque no necesariamente es lo mismo.

La estética estudia las más amplias y vastas historias del conocimiento isabelino, así como las diferentes formas del arte. La estética, así definida, es el campo de la filosofía que estudia el arte y sus cualidades, tales como la belleza, lo eminente, lo feo o la disonancia. Es la rama de la filosofía que estudia el origen del sentimiento puro y su manifestación, que es el arte, se puede decir que es la ciencia cuyo objeto primordial es la reflexión sobre los problemas del arte, la estética analiza filosóficamente los valores que en ella están contenidos.

Desde que en 1750 (en su primera edición) y 1758 (segunda edición publicada) Alexander Gottlieb Baumgarten usara la palabra «estética» como ‘ciencia de lo bello, misma a la que se agrega un estudio de la esencia del arte, de las relaciones de ésta con la belleza y los demás valores’. Algunos autores han pretendido sustituirla por otra denominación: «calología», que atendiendo a su etimología significa ciencia de lo bello (kalos, ‘bello’).

Al ser la estética también una reflexión filosófica sobre el arte, uno de sus problemas será el valor que se contiene en el arte; y aunque un variado número de ciencias puedan ocuparse de la obra de arte, solo la estética analiza filosóficamente los valores que en ella están contenidos. Por otro lado, filósofos como Mario Bunge consideran que la estética no es una disciplina.[4]​ Además Elena Oliveras, formada tanto en el campo filosófico como en el artístico, define el concepto de estética como la marca de Modernidad de su momento de la historia donde se realiza su nacimiento, donde se inaugura el principio de subjetividad.

La historia de la estética es una disciplina de las ciencias sociales que estudia la evolución de las ideas estéticas a lo largo del tiempo.[5]​ La estética es la rama de la filosofía que se encarga de estudiar la manera en que el ser humano interpreta los estímulos sensoriales que recibe del mundo circundante, dando lugar al conocimiento sensible, adquirido a través de los sentidos.[6]​ Entre los diversos objetos de estudio de la estética figuran la belleza o los juicios de gusto, así como las distintas maneras de interpretarlos por parte del ser humano. Por tanto, la estética está íntimamente ligada al arte y al estudio de la historia del arte, analizando los diversos estilos y periodos artísticos conforme a los diversos componentes estéticos que en ellos se encuentran. A menudo se suele denominar la estética como una «filosofía del arte».[7]​

El término estética proviene del griego αἴσθησις (aísthêsis), «sensación». Fue introducido por el filósofo alemán Alexander Gottlieb Baumgarten en su obra Reflexiones filosóficas acerca de la poesía (1735), y más tarde en su Aesthetica (1750).[8]​ Así pues, la historia de la estética, rigurosamente hablando, comenzaría con Baumgarten en el siglo XVIII, sobre todo con la sistematización de esta disciplina realizada por Immanuel Kant. Sin embargo, el concepto es aplicable a los estudios sobre el tema efectuados por los filósofos anteriores, especialmente desde la Grecia clásica. Cabe señalar, por ejemplo, que los antiguos griegos tenían un vocablo equiparable al actual concepto de estética, que era φιλοκαλία (filocalía), «amor a la belleza».[9]​ Se podría decir que en Grecia nació la estética como concepto, mientras que con Baumgarten se convirtió en una rama de la filosofía.[10]​

Los seres humanos han mantenido y mantienen diversas relaciones con el mundo. Diversas son también en ellas su actitud hacia la realidad, las necesidades que trata de satisfacer y el modo de satisfacerlas. Entre esas relaciones figuran:

Muchos pensadores se han interesado por el arte y su significado:[12]​

Hay dos maneras de lo bello:

Lo estético: no se funda en conceptos, no se puede medir: «No puede haber ninguna regla de gusto objetiva que determine por conceptos lo que sea bello, puesto que todo juicio de esta fuente es estético, es decir, que su motivo determinante es el sentimiento del sujeto y no un concepto del objeto».
No hay ciencia sino crítica de lo bello. La sensación sensorial es incomunicable. La comunicación viene de lo común (u ordinario) a todos.

Diferentes autores se refieren a la metodología de estudio del arte y la belleza. A continuación autores y obras contemporáneas (con excepción de Aristóteles) que estudian la estética y el arte, y una pincelada de su ideología:

Dados modelos neurofisiológicos de la discriminación de estímulos aferentes, se procede a confeccionar un modelo cerebral hipotético denominado «centro de sensación estética». Se desarrolla una analítica matemática al respecto, y se observan múltiples resultados experimentales de laboratorio que son confirmatorios.

El arte del siglo XX supone una reacción contra el concepto tradicional de belleza. Algunos teóricos (Hal Foster[13]​) llegan incluso a describir el arte moderno como «antiestético».

Evoluciones como la aparición de la fotografía, capaz de reproducir con fidelidad absoluta su modelo, o los medios mecánicos de reproducción de las obras, que las introducen en el conjunto de los bienes de consumo de nuestra sociedad, suponen a principios del siglo XX una verdadera convulsión para la teoría y la práctica artísticas. Así no solo el campo de estudio de la Estética sino el propio campo de trabajo del arte se orienta hacia una profundísima corriente autorreflexiva que ha marcado todo el arte del siglo veinte: «¿qué es el arte?», «¿Quién define qué es arte?». El dadaísmo utilizaba el collage para mostrar su naturaleza fragmentada; Joseph Beuys (y en general toda la corriente povera europea) usaba materiales como troncos, huesos y palos para su obra, elementos tradicionalmente «feos»; los minimalistas utilizarían acero para resaltar lo industrial del arte, mientras Andy Warhol lo intentaría mediante la serigrafía. Algunos incluso se desharían completamente de la obra final para centrarse únicamente en el proceso en sí. En los años 1960 Nam June Paik y Wolf Vostell empiezan a utilizar televisores o monitores de video para crear sus obras.

Esta reflexión estética y su aplicación en las obras de arte aparece con el prerromanticismo del siglo XVIII y se acentúa con el romanticismo del XIX. Edgar Allan Poe demuestra cómo el principal objetivo del arte es provocar una reacción emocional en el receptor. Lo verdaderamente importante no es lo que siente el autor, sino lo que este hace sentir al receptor de su obra, que debe ser condicionado de manera que su imaginación sea la que construya el mensaje que transmite la obra, sin necesidad de que el autor lo exprese directamente, si es que realmente la obra tiene un solo significado o solo el objetivo de que el receptor imagine, no solo poemas de ambientación siniestra, sino también escenas grotescas, desde crímenes sádicos al terror más consternador. El arte contemporáneo no buscó principalmente la belleza serena o pintoresca, sino también lo repulsivo o melancólico, y provocar ansiedad u otras sensaciones intensas, como en El Grito de Edvard Munch y en movimientos como el expresionismo y el surrealismo. Se rechaza el arte vacío, que no busque una emoción en el receptor, ya sea una reflexión o un sentimiento, incluidos la angustia o el temor.

Otro modo de entender la antiestética es el rechazo de la estética establecida, entendiendo esta como la moda o la imagen personal.

La inteligencia artificial (IA) es, en informática, la inteligencia expresada por máquinas, sus procesadores y sus softwares, que serían los análogos al cuerpo, el cerebro y la mente, respectivamente, a diferencia de la inteligencia natural demostrada por humanos y ciertos animales con cerebros complejos. [1]​ Se considera que el origen de la IA se remonta a los intentos del hombre desde la antigüedad por incrementar sus potencialidades físicas e intelectuales, creando artefactos con automatismos y simulando la forma y las habilidades de los seres humanos.[2]​ En ciencias de la computación, una máquina «inteligente» ideal es un agente flexible que percibe su entorno y lleva a cabo acciones que maximicen sus posibilidades de éxito en algún objetivo o tarea.[3]​

Coloquialmente, el término   inteligencia artificial se aplica cuando una máquina imita las funciones «cognitivas» que los humanos asocian con otras mentes humanas, como por ejemplo: «percibir», «razonar», «aprender» y «resolver problemas».[4]​ Andreas Kaplan y Michael Haenlein definen la inteligencia artificial como «la capacidad de un sistema para interpretar correctamente datos externos, para aprender de dichos datos y emplear esos conocimientos para lograr tareas y metas concretas a través de la adaptación flexible».[5]​ A medida que las máquinas se vuelven cada vez más capaces, tecnología que alguna vez se pensó que requería de inteligencia se elimina de la definición. Por ejemplo, el reconocimiento óptico de caracteres ya no se percibe como un ejemplo de la «inteligencia artificial» habiéndose convertido en una tecnología común.[6]​ Avances tecnológicos todavía clasificados como inteligencia artificial son los sistemas de conducción autónomos o los capaces de jugar ajedrez o Go.[7]​

La inteligencia artificial es una nueva forma de resolver problemas dentro de los cuales se incluyen los sistemas expertos, el manejo y control de robots y los procesadores, que intenta integrar el conocimiento en tales sistemas, en otras palabras, un sistema inteligente capaz de escribir su propio programa. Un sistema experto definido como una estructura de programación capaz de almacenar y utilizar un conocimiento sobre un área determinada que se traduce en su capacidad de aprendizaje. [8]​ De igual manera se puede considerar a la IA como la capacidad de las máquinas para usar algoritmos, aprender de los datos y utilizar lo aprendido en la toma de decisiones tal y como lo haría un ser humano,[9]​ además uno de los enfoques principales de la inteligencia artificial es el aprendizaje automático, de tal forma que los ordenadores o las máquinas tienen la capacidad de aprender sin estar programados para ello.[9]​ 

Según Takeyas (2007) la IA es una rama de las ciencias computacionales encargada de estudiar modelos de cómputo capaces de realizar actividades propias de los seres humanos con base en dos de sus características primordiales: el razonamiento y la conducta.[10]​

En 1956, John McCarthy acuñó la expresión «inteligencia artificial», y la definió como «la ciencia e ingenio de hacer máquinas inteligentes, especialmente programas de cómputo inteligentes».[11]​

También existen distintos tipos de percepciones y acciones, que pueden ser obtenidas y producidas, respectivamente, por sensores físicos y sensores mecánicos en máquinas, pulsos eléctricos u ópticos en computadoras, tanto como por entradas y salidas de bits de un software y su entorno software. 

Varios ejemplos se encuentran en el área de control de sistemas, planificación automática, la capacidad de responder a diagnósticos y a consultas de los consumidores, reconocimiento de escritura, reconocimiento del habla y reconocimiento de patrones. Los sistemas de IA actualmente son parte de la rutina en campos como economía, medicina, ingeniería, el transporte, las comunicaciones y la milicia, y se ha usado en gran variedad de programas informáticos, juegos de estrategia, como ajedrez de computador, y otros videojuegos.

Stuart J. Russell y Peter Norvig diferencian varios tipos de inteligencia artificial:[12]​

La IA se divide en dos escuelas de pensamiento:

Se conoce también como IA simbólico-deductiva. Está basada en el análisis formal y estadístico del comportamiento humano ante diferentes problemas:

La inteligencia computacional (también conocida como IA subsimbólica-inductiva) implica desarrollo o aprendizaje interactivo (por ejemplo, modificaciones interactivas de los parámetros en sistemas de conexiones). El aprendizaje se realiza basándose en datos empíricos.

La inteligencia computacional tiene una doble finalidad. Por un lado, su objetivo científico es comprender los principios que posibilitan el comportamiento inteligente (ya sea en sistemas naturales o artificiales) y, por otro, su objetivo tecnológico consiste en especificar los métodos para diseñar sistemas inteligentes.[19]​

El acelerado desarrollo tecnológico y científico de la inteligencia artificial que se ha producido en el siglo XXI supone también un importante impacto en otros campos. En la economía mundial durante la segunda revolución industrial se vivió un fenómeno conocido como desempleo tecnológico, que se refiere a cuando la automatización industrial de los procesos de producción a gran escala reemplaza la mano de obra humana. Con la inteligencia artificial podría darse un fenómeno parecido, especialmente en los procesos en los que interviene la inteligencia humana, tal como se ilustraba en el cuento ¡Cómo se divertían! de Isaac Asimov, en el que su autor vislumbra algunos de los efectos que tendría la interacción de máquinas inteligentes especializadas en pedagogía infantil, en lugar de profesores humanos, con los niños en etapa escolar. 

Otras obras de ciencia ficción también exploran algunas cuestiones éticas y filosóficas con respecto a la Inteligencia artificial fuerte, como las películas Yo, robot o A.I. Inteligencia Artificial, en los que se tratan temas tales como la autoconsciencia o el origen de una conciencia emergente de los robots inteligentes o sistemas computacionales, o si éstos podrían considerarse sujetos de derecho debido a sus características casi humanas relacionadas con la sintiencia, como el poder ser capaces de sentir dolor y emociones o hasta qué punto obedecerían al objetivo de su programación, y en caso de no ser así, si podrían ejercer libre albedrío. Esto último es el tema central de la famosa saga de Terminator, en la que las máquinas superan a la humanidad y deciden aniquilarla, historia que según varios especialistas, podría no limitarse a la ciencia ficción y ser una posibilidad real en una sociedad posthumana que dependiese de la tecnología y las máquinas totalmente.[27]​[28]​

El Derecho[29]​ desempeña un papel fundamental en el uso y desarrollo de la IA. Las leyes establecen reglas y normas de comportamiento vinculantes para asegurar el bienestar social y proteger los derechos individuales, y pueden ayudarnos a obtener los beneficios de esta tecnología mientras minimizamos sus riesgos, que son significativos. De momento no hay normas jurídicas que regulen directamente la IA. Pero con fecha 21 de abril de 2021, la Comisión Europea ha presentado una propuesta de Reglamento europeo para la regulación armonizada de la inteligencia artificial (IA) en la UE. Su título exacto es Propuesta de Reglamento del Parlamento Europeo y del Consejo por el que se establecen normas armonizadas en materia de inteligencia artificial –Ley de Inteligencia Artificial– y se modifican otros actos legislativos de la Unión. 

Los primeros investigadores desarrollaron algoritmos que imitaban el razonamiento paso a paso que los humanos usan cuando resuelven acertijos o hacen deducciones lógicas.[30]​ A finales de los años 80 y 90, la investigación de la inteligencia artificial había desarrollado métodos para tratar con información incierta o incompleta, empleando conceptos de probabilidad y economía.[31]​

Estos algoritmos demostraron ser insuficientes para resolver grandes problemas de razonamiento porque experimentaron una «explosión combinatoria»: se volvieron exponencialmente más lentos a medida que los problemas crecían.[32]​ De esta manera, se concluyó que los seres humanos rara vez usan la deducción paso a paso que la investigación temprana de la inteligencia artificial seguía; en cambio, resuelven la mayoría de sus problemas utilizando juicios rápidos e intuitivos.[33]​

La representación del conocimiento[34]​ y la ingeniería del conocimiento[35]​ son fundamentales para la investigación clásica de la inteligencia artificial. Algunos «sistemas expertos» intentan recopilar el conocimiento que poseen los expertos en algún ámbito concreto. Además, otros proyectos tratan de reunir el «conocimiento de sentido común» conocido por una persona promedio en una base de datos que contiene un amplio conocimiento sobre el mundo.

Entre los temas que contendría una base de conocimiento de sentido común están: objetos, propiedades, categorías y relaciones entre objetos,[36]​ situaciones, eventos, estados y tiempo[37]​ causas y efectos;Poole, Mackworth y Goebel, 1998, pp. 335–337 y el conocimiento sobre el conocimiento (lo que sabemos sobre lo que saben otras personas)[38]​ entre otros.

Otro objetivo de la inteligencia artificial consiste en poder establecer metas y alcanzarlas.[39]​ Para ello necesitan una forma de visualizar el futuro, una representación del estado del mundo y poder hacer predicciones sobre cómo sus acciones lo cambiarán, con tal de poder tomar decisiones que maximicen la utilidad (o el «valor») de las opciones disponibles.Russell y Norvig, 2003, pp. 600–604

En los problemas clásicos de planificación, el agente puede asumir que es el único sistema que actúa en el mundo, lo que le permite estar seguro de las consecuencias de sus acciones.[40]​ Sin embargo, si el agente no es el único actor, entonces se requiere que este pueda razonar bajo incertidumbre. Esto requiere un agente que no solo pueda evaluar su entorno y hacer predicciones, sino también evaluar sus predicciones y adaptarse en función de su evaluación.Russell y Norvig, 2003, pp. 430–449 La planificación de múltiples agentes utiliza la cooperación y la competencia de muchos sistemas para lograr un objetivo determinado. El comportamiento emergente como este es utilizado por algoritmos evolutivos e inteligencia de enjambre.Russell y Norvig, 2003, pp. 449–455

El aprendizaje automático es un concepto fundamental de la investigación de la inteligencia artificial desde el inicio del campo; consiste en el estudio de algoritmos informáticos que mejoran automáticamente a través de la experiencia.[41]​

El aprendizaje no supervisado es la capacidad de encontrar patrones en un flujo de entrada, sin que sea necesario que un humano etiquete las entradas primero. El aprendizaje supervisado incluye clasificación y regresión numérica, lo que requiere que un humano etiquete primero los datos de entrada. La clasificación se usa para determinar a qué categoría pertenece algo y ocurre después de que un programa observe varios ejemplos de entradas de varias categorías. La regresión es el intento de producir una función que describa la relación entre entradas y salidas y predice cómo deben cambiar las salidas a medida que cambian las entradas.[41]​ Tanto los clasificadores como los aprendices de regresión intentan aprender una función desconocida; por ejemplo, un clasificador de spam puede verse como el aprendizaje de una función que asigna el texto de un correo electrónico a una de dos categorías, «spam» o «no spam». La teoría del aprendizaje computacional puede evaluar a los estudiantes por complejidad computacional, complejidad de la muestra (cuántos datos se requieren) o por otras nociones de optimización.[42]​

El procesamiento del lenguaje natural[43]​ permite a las máquinas leer y comprender el lenguaje humano. Un sistema de procesamiento de lenguaje natural suficientemente eficaz permitiría interfaces de usuario de lenguaje natural y la adquisición de conocimiento directamente de fuentes escritas por humanos, como los textos de noticias. Algunas aplicaciones sencillas del procesamiento del lenguaje natural incluyen la recuperación de información, la minería de textos, la respuesta a preguntas y la traducción automática.[44]​ Muchos enfoques utilizan las frecuencias de palabras para construir representaciones sintácticas de texto. Las estrategias de búsqueda de «detección de palabras clave» son populares y escalables, pero poco óptimas; una consulta de búsqueda para «perro» solo puede coincidir con documentos que contengan la palabra literal «perro» y perder un documento con la palabra «caniche». Los enfoques estadísticos de procesamiento de lenguaje pueden combinar todas estas estrategias, así como otras, y a menudo logran una precisión aceptable a nivel de página o párrafo. Más allá del procesamiento de la semántica, el objetivo final de este es incorporar una comprensión completa del razonamiento de sentido común.[45]​ En 2019, las arquitecturas de aprendizaje profundo basadas en transformadores podían generar texto coherente.[46]​

La percepción de la máquina[47]​ es la capacidad de utilizar la entrada de sensores (como cámaras de espectro visible o infrarrojo, micrófonos, señales inalámbricas y lidar, sonar, radar y sensores táctiles) para deducir aspectos del mundo. Las aplicaciones incluyen reconocimiento de voz,[48]​ reconocimiento facial y reconocimiento de objetos.Russell y Norvig, 2003, pp. 885–892 La visión artificial es la capacidad de analizar la información visual, que suele ser ambigua; un peatón gigante de cincuenta metros de altura muy lejos puede producir los mismos píxeles que un peatón de tamaño normal cercano, lo que requiere que la inteligencia artificial juzgue la probabilidad relativa y la razonabilidad de las diferentes interpretaciones, por ejemplo, utilizando su «modelo de objeto» para evaluar que los peatones de cincuenta metros no existen.[49]​

Las principales críticas a la inteligencia artificial tienen que ver con su capacidad de imitar por completo a un ser humano. Sin embargo, hay expertos[cita requerida] en el tema que indican que ningún humano individual tiene capacidad para resolver todo tipo de problemas, y autores como Howard Gardner han teorizado sobre la solución.

En los humanos, la capacidad de resolver problemas tiene dos aspectos: los aspectos innatos y los aspectos aprendidos. Los aspectos innatos permiten, por ejemplo, almacenar y recuperar información en la memoria, mientras que en los aspectos aprendidos reside el saber resolver un problema matemático mediante el algoritmo adecuado. Del mismo modo que un humano debe disponer de herramientas que le permitan solucionar ciertos problemas, los sistemas artificiales deben ser programados de modo tal que puedan llegar a resolverlos.

Muchas personas consideran que el Prueba de Turing ha sido superado, citando conversaciones en que al dialogar con un programa de inteligencia artificial para chat, no saben que hablan con un programa. Sin embargo, esta situación no es equivalente a una prueba de Turing, que requiere que el participante se encuentre sobre aviso de la posibilidad de hablar con una máquina.

Otros experimentos mentales como la habitación china, de John Searle, han mostrado cómo una máquina podría simular pensamiento sin realmente poseerlo, pasando la prueba de Turing sin siquiera entender lo que hace, tan solo reaccionando de una forma concreta a determinados estímulos (en el sentido más amplio de la palabra). Esto demostraría que la máquina en realidad no está pensando, ya que actuar de acuerdo con un programa preestablecido sería suficiente. Si para Turing el hecho de engañar a un ser humano que intenta evitar que le engañen es muestra de una mente inteligente, Searle considera posible lograr dicho efecto mediante reglas definidas a priori.

Uno de los mayores problemas en sistemas de inteligencia artificial es la comunicación con el usuario. Este obstáculo es debido a la ambigüedad del lenguaje, y se remonta a los inicios de los primeros sistemas operativos informáticos. La capacidad de los humanos para comunicarse entre sí implica el conocimiento del lenguaje que utiliza el interlocutor. Para que un humano pueda comunicarse con un sistema inteligente hay dos opciones: o bien que el humano aprenda el lenguaje del sistema como si aprendiese a hablar cualquier otro idioma distinto al nativo, o bien que el sistema tenga la capacidad de interpretar el mensaje del usuario en la lengua que el usuario utiliza. También hay desperfectos en las instalaciones de los mismos.

Un humano, durante toda su vida, aprende el vocabulario de su lengua nativa o materna, siendo capaz de interpretar los mensajes (a pesar de la polisemia de las palabras) utilizando el contexto para resolver ambigüedades. Sin embargo, debe conocer los distintos significados para poder interpretar, y es por esto que lenguajes especializados y técnicos son conocidos solamente por expertos en las respectivas disciplinas. Un sistema de inteligencia artificial se enfrenta con el mismo problema, la polisemia del lenguaje humano, su sintaxis poco estructurada, y los dialectos entre grupos.

Los desarrollos en inteligencia artificial son mayores en los campos disciplinares en los que existe mayor consenso entre especialistas. Un sistema experto es más probable que sea programado en física o en medicina que en sociología o en psicología. Esto se debe al problema del consenso entre especialistas en la definición de los conceptos involucrados y en los procedimientos y técnicas a utilizar. Por ejemplo, en física hay acuerdo sobre el concepto de velocidad y cómo calcularla. Sin embargo, en psicología se discuten los conceptos, la etiología, la psicopatología, y cómo proceder ante cierto diagnóstico. Esto dificulta la creación de sistemas inteligentes porque siempre habrá desacuerdo sobre la forma en que debería actuar el sistema para diferentes situaciones. A pesar de esto, hay grandes avances en el diseño de sistemas expertos para el diagnóstico y toma de decisiones en el ámbito médico y psiquiátrico (Adaraga Morales, Zaccagnini Sancho, 1994).

Al desarrollar un robot con inteligencia artificial se debe tener cuidado con la autonomía,[50]​ hay que tener cuidado en no vincular el hecho de que el robot interaccione con seres humanos a su grado de autonomía. Si la relación de los humanos con el robot es de tipo maestro esclavo, y el papel de los humanos es dar órdenes y el del robot obedecerlas, entonces sí cabe hablar de una limitación de la autonomía del robot. Pero si la interacción de los humanos con el robot es de igual a igual, entonces su presencia no tiene por qué estar asociada a restricciones para que el robot pueda tomar sus propias decisiones.
[51]​
Con el desarrollo de la tecnología de inteligencia artificial, muchas compañías de software como el aprendizaje profundo y el procesamiento del lenguaje natural han comenzado a producirse y la cantidad de películas sobre inteligencia artificial ha aumentado.
Stephen Hawking advirtió sobre los peligros de la inteligencia artificial y lo consideró una amenaza para la supervivencia de la humanidad.[52]​

Las técnicas desarrolladas en el campo de la inteligencia artificial son numerosas y ubicuas. Comúnmente cuando un problema es resuelto mediante inteligencia artificial la solución es incorporada en ámbitos de la industria y de la vida[53]​ diaria de los usuarios de programas de computadora, pero la percepción popular se olvida de los orígenes de estas tecnologías que dejan de ser percibidas como inteligencia artificial. A este fenómeno se le conoce como el efecto IA.[54]​

Al hablar acerca de la propiedad intelectual atribuida a creaciones de la inteligencia artificial se forma un debate fuerte alrededor de si una máquina puede tener derechos de autor. Según la Organización Mundial de la Propiedad Intelectual (OMPI), cualquier creación de la mente puede ser parte de la propiedad intelectual, pero no especifica si la mente debe ser humana o puede ser una máquina, dejando la creatividad artificial en la incertidumbre.

Alrededor del mundo han comenzado a surgir distintas legislaciones con el fin de manejar la inteligencia artificial, tanto su uso como creación. Los legisladores y miembros del gobierno han comenzado a pensar acerca de esta tecnología, enfatizando el riesgo y los desafíos complejos de esta. Observando el trabajo creado por una máquina, las leyes cuestionan la posibilidad de otorgarle propiedad intelectual a una máquina, abriendo una discusión respecto a la legislación relacionada con IA.

El 5 de febrero de 2020, la Oficina del Derecho de Autor de los Estados Unidos y la OMPI asistieron a un simposio donde observaron de manera profunda cómo la comunidad creativa utiliza la inteligencia artificial (IA) para crear trabajo original. Se discutieron las relaciones entre la inteligencia artificial y el derecho de autor, qué nivel de involucramiento es suficiente para que el trabajo resultante sea válido para protección de derechos de autor; los desafíos y consideraciones de usar inputs con derechos de autor para entrenar una máquina; y el futuro de la inteligencia artificial y sus políticas de derecho de autor.[55]​[56]​

El director general de la OMPI, Francis Gurry, presentó su preocupación ante la falta de atención que hay frente a los derechos de propiedad intelectual, pues la gente suele dirigir su interés hacia temas de ciberseguridad, privacidad e integridad de datos al hablar de la inteligencia artificial. Así mismo, Gurry cuestionó si el crecimiento y la sostenibilidad de la tecnología IA nos guiaría a desarrollar dos sistemas para manejar derechos de autor- uno para creaciones humanas y otro para creaciones de máquinas.[57]​

Aún hay una falta de claridad en el entendimiento alrededor de  la inteligencia artificial. Los desarrollos tecnológicos avanzan a paso rápido, aumentando su complejidad en políticas, legalidades y problemas éticos que se merecen la atención global. Antes de encontrar una manera de trabajar con los derechos de autor, es necesario entenderlo correctamente, pues aún no se sabe cómo juzgar la originalidad de un trabajo que nace de una composición de una serie de fragmentos de otros trabajos.

La asignación de derechos de autor alrededor de la inteligencia artificial aún no ha sido regulada por la falta de conocimientos y definiciones. Aún hay incertidumbre sobre si, y hasta que punto, la inteligencia artificial es capaz de producir contenido de manera autónoma y sin ningún humano involucrado, algo que podría influenciar si sus resultados pueden ser protegidos por derechos de autor. 

El sistema general de derechos de autor aún debe adaptarse al contexto digital de inteligencia artificial, pues están centrados en la creatividad humana. Los derechos de autor no están diseñados para manejar cualquier problema en las políticas relacionado con la creación y el uso de propiedad intelectual, y puede llegar a ser dañino estirar excesivamente los derechos de autor para resolver problemas periféricos dado que:

«Usar los derechos de autor para gobernar la inteligencia artificial es poco inteligente y contradictorio con la función primordial de los derechos de autor de ofrecer un espacio habilitado para que la creatividad florezca»[58]​

La conversación acerca de la propiedad intelectual tendrá que continuar hasta asegurarse de que la innovación sea protegida pero también tenga espacio para florecer.

El cristianismo (del latín christianismus, y este del griego χριστιανισμός)[2]​ es una religión abrahámica monoteísta basada en la vida y enseñanzas de Jesús de Nazaret.  Es la religión más extensa del mundo con un número estimado de 2400 millones de seguidores, siendo el catolicismo la confesión cristiana con más fieles (1360 millones).[3]​[1]​[4]​[5]​[6]​[7]​

Es una religión diversa tanto cultural como doctrinalmente. Sus principales ramas son el catolicismo, el protestantismo y la ortodoxia. Sus adherentes, llamados cristianos, comparten la creencia de que Jesús de Nazaret es el Hijo de Dios y el Mesías (en griego, Cristo) profetizado en el Antiguo Testamento y que sufrió, fue crucificado, descendió al infierno y resucitó de entre los muertos para la salvación de la humanidad. 

Surgió del judaísmo[8]​[9]​[10]​ a mediados del siglo I d. C.[11]​[12]​ en la provincia romana de Judea. En sus primeras décadas, el cristianismo era considerado por algunos como una doctrina sectaria de las tradiciones judías ortodoxas.[13]​ Los primeros líderes de las comunidades cristianas fueron los apóstoles y sus sucesores los padres apostólicos. Este cristianismo primitivo se extendió, pese a ser una religión minoritaria y perseguida, por Judea, Siria, Europa, Anatolia, Mesopotamia, Transcaucasia, Egipto y Etiopía. Durante estos primeros siglos, los Padres de la Iglesia gradualmente consolidaron las doctrinas del cristianismo y elaboraron el canon del Nuevo Testamento.[14]​

Algunos de los escritos sagrados cristianos son compartidos con el judaísmo. El Tanaj constituye, junto con la Biblia griega —más antigua que el Tanaj en su forma actual—, la base y la fuente para el Antiguo Testamento de las diferentes biblias cristianas. Por este motivo, el cristianismo es considerado una religión abrahámica, junto con el judaísmo y con el islam.

En el año 301, el reino de Armenia, bajo el reinado de Tiridates III, se convirtió en el primer estado en oficializar el cristianismo.[15]​ En el año 311, se decretó el Edicto de Tolerancia de Nicomedia, que da fin a la persecución contra los cristianos en el Imperio romano. Dos años después, en el año 313, los emperadores Licinio y Constantino I reconocieron la libertad de cultos y legalizaron el cristianismo mediante el Edicto de Milán; luego de lo cual se formuló el credo niceno. En el año 380, el emperador Teodosio I convirtió al cristianismo en la religión oficial del Imperio romano,[16]​[17]​[18]​ convirtiendo al Imperio en un estado confesional y teocrático. Desde entonces, el cristianismo ha sido, en sus diferentes ramas, la religión dominante en el continente europeo y ha influido de manera significativa en la cultura occidental y en muchas otras.

La Iglesia de los primeros concilios ecuménicos se conoce frecuentemente como la «Gran Iglesia», porque la Iglesia católica, la Iglesia ortodoxa y las Iglesias ortodoxas orientales estaban en plena comunión.[19]​ Debido a disputas cristológicas, la Iglesia del Oriente se separó luego del Concilio de Éfeso (431) y las Iglesias ortodoxas orientales se separaron tras el Concilio de Calcedonia (451). La Iglesia católica y la Iglesia ortodoxa se separaron en el "gran cisma" de 1054, en parte por diferencias acerca de la autoridad del papa de Roma. El protestantismo, aunque es en realidad un conjunto de denominaciones, aparece por primera vez durante la Reforma protestante del siglo XVI, por lo que percibían como importantes desviaciones teológicas y eclesiológicas de la Iglesia católica.[20]​ Como respuesta a la Reforma protestante, la Iglesia católica celebró el Concilio de Trento (1545-1563) e impulsó la Contrarreforma. Con el descubrimiento de América y la expansión europea el cristianismo se extendió por América y otras partes del mundo.

La palabra «cristianismo» proviene del griego χριστιανισμός, christianismós, y esta a su vez de χριστιανός, christianós, ‘cristiano’, la cual a su vez procede del nombre propio Χριστός, Christós, ‘Cristo’, traducción del hebreo Mesías, que significa ‘ungido’. El origen del término se indica en el libro de Hechos de los Apóstoles:


El cristianismo tiene su origen histórico en el judaísmo del Segundo Templo de comienzos de la era actual. Si bien Jesús de Nazaret se autoidentificó siempre como un judío devoto, en su doctrina y sus enseñanzas, Él mismo se identificó como el camino al Padre Celestial:[21]​


En los evangelios hay amplia evidencia de que Jesucristo aseguró ser el único camino a Dios, lo cual sería enseñado así mismo por sus primeros seguidores, incluyendo a los apóstoles Simón Pedro y Pablo de Tarso.[22]​[23]​

La tradición cristiana sitúa la pasión, muerte y resurrección de Cristo en el año 33; no obstante, algunos estudios del siglo XX no toman ese año como fecha incontrovertible para la muerte de Jesús de Nazaret. Hay quienes, al indagar en las fechas, sugieren que pudo haber un desfase de 4 a 8 años entre el inicio del cómputo de la era cristiana y la fecha precisa del nacimiento de Jesús de Nazaret.[24]​ En adición a esto, no hay clara certeza ni consenso entre estos autores de que este haya muerto a la edad de 33 años, tal como algunos textos bíblicos parecen mostrar.[Nota 3]​

No se conoce con precisión el número de seguidores que pudo alcanzar el cristianismo en vida de Jesús de Nazaret, ni cuántos seguían dentro de la comunidad cristiana por él fundada tras su muerte, ajusticiado por las autoridades seculares. Pocos años después de su muerte, Pablo de Tarso, un judío que —en el decir de los Hechos de los Apóstoles— poseía la ciudadanía romana, tuvo un papel destacado predicando y poniendo en contacto a diversos grupos cristianos del Oriente Próximo.[26]​ El carácter misionero de Pablo de Tarso y otras figuras del cristianismo primitivo influyó de forma decisiva en toda la historia posterior del cristianismo.[27]​

Al final del siglo I, ya se habían constituido las cuatro corrientes básicas del cristianismo primitivo que terminaron por integrar el canon bíblico, y que podrían esquematizarse escriturísticamente en: (1) el cristianismo paulino, integrado por el corpus de cartas escritas por Pablo de Tarso y su escuela;[28]​ (2) el judeocristianismo, representado por los escritos derivados de las posturas de Santiago el Justo y de Simón Pedro; (3) el complejo cristianismo sinóptico (que abarca desde el judeocristianismo del Evangelio de Mateo hasta el pagano-cristianismo del Evangelio de Lucas y de los Hechos de los Apóstoles), y (4) el cristianismo joánico.[29]​

La tarea de estos primeros cristianos llevó a la formación de comunidades cristianas en numerosos lugares del Imperio Romano, especialmente en su parte oriental. El sociólogo Rodney Stark, quien estudió diversas fuentes históricas para su libro El auge del cristianismo, concluyó que hacia el año 300 d. C., el cristianismo estaba difundido tanto entre las clases populares como en un número de personas ricas e influyentes de la sociedad romana, y se aventuró a situar la cifra de cristianos entre el 10 y el 25 % de la población del Imperio.[30]​ Con el edicto de tolerancia del emperador Constantino I el Grande, el cristianismo se convirtió en religión legal y progresivamente en la religión favorecida por el estado. Sin embargo no fue hasta el Edicto de Tesalónica, promulgado por el emperador Teodosio, que el cristianismo se convierte en la religión oficial. En las ciudades el número de cristianos siempre había sido mayor, y hacia el siglo V la población no cristiana del imperio se concentraba masivamente en zonas rurales (pagi), por lo que la religión olímpica acabó llamándose paganismo por ser importante solo en esas zonas.[30]​

Una vez convertida en religión mayoritaria del Imperio, el cristianismo se expandió a toda Europa. Los pueblos germánicos se fueron cristianizando progresivamente entre los siglos IV y IX. Cirilo y Metodio predicaron a los eslavos en el siglo X. El cristianismo había llegado a las islas británicas en el siglo V, cuando Patricio de Irlanda estaba activo en la región. A partir del siglo VII las potencias cristianas de Europa rivalizaron con las potencias islámicas. En el sur y centro de Europa, con la excepción de las zonas bajo administración musulmana, el cristianismo fue la principal religión desde antes del siglo IXI hasta la actualidad. La expansión al norte de Europa y Europa oriental fue más tardía, pero también en esas regiones desde hace siglos el cristianismo ha sido históricamente la religión mayoritaria. Con la expansión europea en América hubo un esfuerzo deliberado por imponer ya sea pacíficamente, ya sea mediante el uso de la fuerza, el cristianismo a las poblaciones de origen americano. Desde el siglo XVI los portugueses hicieron esfuerzos también por llevar el cristianismo a ciertas áreas de África y Asia, que estaban bajo su dominio. El auge del colonialismo europeo en África, Asia y Oceanía aumentó el número de cristianos en todo el mundo.

Según un estudio de 2005, habría en el mundo más de 2100 millones de cristianos,[32]​ o cerca de un tercio de la población mundial, siendo la religión con más seguidores del mundo. Otro estudio, publicado en 2011, habla de 2180 millones de cristianos en el mundo.[33]​

Existe un núcleo más o menos compartido de creencias y doctrinas entre los diferentes grupos cristianos, si bien algunas de esas doctrinas no son aceptadas por todos. En ese núcleo se encuentra:

En cambio, en otras creencias y doctrinas los cristianos difieren entre ellos, por ejemplo, sobre cuál es el criterio válido para aceptar una creencia. Para los católicos y ortodoxos, sus respectivas Iglesias están instituidas o tuteladas de algún modo por Dios para servir de guía a los cristianos. Para los protestantes la principal fuente de conocimiento es la Biblia y la gracia divina que Dios concede a ciertos hombres. En general todos los cristianos reconocen que las acciones que Dios quiere que sean llevadas a cabo están inspiradas por el Espíritu Santo. Los escritos sagrados, entre los que destaca la Biblia, son la principal fuente doctrinal válida de muchas denominaciones, en particular las de corriente protestante.

Otra fuente doctrinal importante es la tradición apostólica (especialmente para la Iglesia católica y la Iglesia ortodoxa), los concilios y los credos, aunque no poseen necesariamente la unicidad de criterios para su aceptación, ya que pueden ser asumidos total o parcialmente, o rechazados totalmente, dependiendo de la denominación. Algunas tradiciones cristianas, tales como los bautistas y las Iglesias de Cristo, aceptan estas creencias, pero no el credo mismo, debido a que los credos son considerados en estos grupos como no pertenecientes a las escrituras. Todo lo anterior sucede también con otros escritos aunque no poseen tanta aceptación como la Biblia. Sin embargo, el catolicismo argumenta que fue gracias a su tradición apostólica que tuvo los criterios para seleccionar los documentos válidos que constituyen el Nuevo Testamento y determinar los apócrifos, durante el año 397 en el concilio de Cartago. Además la imprenta solo se inventó en el siglo XV en Alemania, por tanto los creyentes no contaban con la Biblia para sustentar su doctrina; había muy pocas biblias, pues durante el medioevo los textos eran copiadas manualmente por los monjes.

Ya desde los primeros tiempos de difusión de las enseñanzas de Cristo y de las diferentes escuelas que formaron los discípulos suyos al final de su vida y sus ministerios históricos, biográficos y humanos,[Nota 4]​ surgieron diferencias muy significativas respecto del papel e importancia de Cristo, de su misión redentora, de su naturaleza y de su glorificación, y de muy numerosas cuestiones doctrinales referentes a su predicación y enseñanzas, la selección de textos que pudieron haberlas descrito de forma más correcta —el Nuevo Testamento, los llamados Logia (dichos o palabras) de Jesús, o bien, los evangelios y escritos gnósticos y apócrifos—, y la interpretación —textual o contextual— de los cuerpos de textos sagrados.

De hecho, de los doce que, según el testimonio de dos de los llamados Evangelios canónicos, habrían sido investidos como apóstoles de forma original, solo cinco de ellos dejaron documentos que fueron admitidos en el Canon del Nuevo Testamento, el resto de los doce —incluyendo a Judas Iscariote—, y algunos de los cinco ya antes mencionados, pasaron a la historia como autores de documentos gnósticos, que, al paso de los siglos, dejaron de ser vistos como textos sagrados, llegando a ser tenidos por apócrifos.

Debe tenerse en cuenta que el nombre de cristianos ha sido compartido a través de los siglos, y no siempre de formas muy armónicas, por grupos numerosos de creyentes, cada cual, a su vez llegó a desconocer como cristianos a grupos con posturas dogmáticas concretas distintas de las propias. Dicho de otra forma, cristianos es el nombre común de grupos tan distintos entre sí como los católicos, marcionitas, arrianos, nestorianos, coptos, jacobitas, ortodoxos, cátaros o albigenses, anglicanos, protestantes, mormones, veterocatólicos y otros tipos de grupos que reflejan posturas dogmáticas concretas más disímiles.

La Iglesia católica adoptó ese nombre luego que los discípulos liderados por Pedro siguieron las instrucciones de Jesús cuando resucitó: “Vayan y lleven las buenas nuevas a todas las naciones”, es decir: catolisis según como se narró en griego en los evangelios. O sea que “católico” es un adjetivo que corresponde al sustantivo “cristianos”. Se acostumbraba así llamarles católicos por su trabajo evangelizador en viajes misioneros de país en país.

Los evangélicos (protestantes) aparecieron con los reformistas quince siglos después y en los últimos tiempos se han denominado más como 'cristianos'. A través de los siglos, todos estos distintos grupos confesionales, o al menos doctrinales, reivindican a Cristo como su Maestro, Líder, Rey, Señor o Dios, y algunos, así mismo, como su Redentor o Salvador, acogiendo con gusto todas sus enseñanzas —o cuerpos doctrinales que en su nombre les fueron entregadas—, y dando testimonio de estos hechos de múltiples maneras, que incluyen el dejarse privar de la existencia antes que renegar de su adhesión a él, o bien, de los valores, ideas o creencias de alguna u otra forma vinculadas a él.

Aunque existen enormes diferencias en las creencias entre unos cristianos y otros, la mayoría de las cuales basadas en diferentes interpretaciones de los mensajes bíblicos, aun así es posible plantear afirmaciones generales que describen las doctrinas de una gran mayoría, entre las que destacan: la pasión, muerte y resurrección de Cristo,

No todos los cristianos han aceptado completamente estos estatutos de fe. De hecho, la mayor parte de los credos apuntan a diferenciar ciertas creencias de otros cristianos primitivos, los cuales son tomados usualmente como heréticos, ya que representan una divergencia consciente de la corriente principal del cristianismo. La mayoría de las disputas se centran en la divinidad de Jesús, la Trinidad, o ambos. Ejemplos de esto incluyen a los grupos ebionitas, los cuales niegan la divinidad de Jesús; los no trinitarios o unitarios, que rechazan el dogma de la Trinidad; los grupos docetistas, que niegan que Cristo haya sido humano; o los arrianos, quienes consideran que el Hijo de Dios es una criatura creada por Dios, pero no Dios mismo, entre otros.

Existe dentro del cristianismo una agrupación de libros que se conoce como Biblia, que contiene texto sagrado para su consideración y obediencia.[34]​ Las distintas denominaciones cristianas varían en cuanto a la forma de traducción e interpretación de dichas escrituras.

Virtualmente todas las Iglesias cristianas aceptan la autoridad de la Biblia, la cual incluye el Antiguo Testamento y el Nuevo Testamento, si bien el canon bíblico, o libros que se incluyen, difiere entre las diferentes denominaciones, como es el caso del Antiguo Testamento.

Las distintas Iglesias cristianas ortodoxas, así como diversas Iglesias orientales de dogma nestoriano y eutiquiano, y la Iglesia católica, incluyen en sus Biblias otros libros llamados los deuterocanónicos, que las comunidades cristianas primitivas habían recibido en la Biblia Septuaginta, bastante más extensa que el Tanaj judío hebreo-arameo, de las comunidades israelitas de habla griega de todo el Mundo Clásico.

Debido a la ignorancia de algunos de los Padres de la Iglesia de esta transferencia cultural, la Iglesia occidental mantuvo una postura bastante reservada hacia estos escritos, los deuterocanónicos, durante algunos siglos (III al V). Pero ratificó su pertenencia al canon de la Biblia en los Concilios II de Roma (382), III de Hipona (393), III de Cartago (397) y IV de Cartago (419).

Durante la Reforma protestante del siglo XVI, Lutero decidió que no eran inspirados, y retomó el Tanaj como su fundamento para el canon del Antiguo Testamento. En medio del debate suscitado, la Iglesia occidental ratificó la decisión de recibirlos como parte del canon durante los trabajos del Concilio de Trento (1546).

Las distintas Iglesias cristianas ortodoxas, nestorianas de Oriente y eutiquianas de África, reivindican posturas bastante más eclécticas, pues asumen posturas de Padres de la Iglesia junto a las decisiones conciliares tempranas de la Iglesia católica. A causa de lo cual, el canon de sus Biblias es bastante más amplio que el canon de la Iglesia católica, e incluye el Salmo 151, la Oración de Manasés, el Libro III de Esdras y el Libro III de los Macabeos (además de estos, el Libro IV de Esdras y el Libro IV de los Macabeos figuran, así mismo, en muchas importantes versiones y ediciones de la Biblia).

Cada grupo cristiano suele llamar apócrifos a todos los escritos no incluidos en su versión del canon, si bien las diferentes confesiones dentro del cristianismo coinciden en el uso de este término para hacer referencia a los textos excluidos del canon de las Biblias cristianas ortodoxas. Solamente la Iglesia latina, y algunos protestantes respetuosos, llaman deuterocanónicos a los libros católicos ausentes de las Biblias protestantes. Las Iglesias de Oriente rechazan de manera terminante el uso occidental de distinguir los libros propios del canon amplio, de los protocanónicos comunes a todos los cristianos.

Otros, como los Testigos de Jehová, han producido sus propias traducciones de la Biblia asegurando que se trata de una versión fidedigna y leal con los idiomas originales.

Algunos grupos cristianos también han generado escrituras adicionales y son consideradas como escritura “inspirada”. Ejemplos muy conocidos incluyen los escritos de Ellen G. White, teóloga y doctora de la Iglesia Adventista del Séptimo Día; el Libro de Mormón, adscrito a Jesucristo como otro Testamento, Doctrina y Convenios, y La Perla de Gran Precio, empleados por La Iglesia de Jesucristo de los Santos de los Últimos Días (conocidos popularmente como Iglesia mormona); o las escrituras de Mary Baker Eddy, teórica y fundadora de la Ciencia Cristiana.

Esta elevación de otras escrituras al mismo nivel de las escrituras aceptadas es la mayor causa de disputas entre estos grupos y las principales corrientes cristianas. Se podría esperar que los luteranos y los calvinistas considerasen las interpretaciones de Lutero y Calvino, respectivamente, con similar reverencia, pero no es así; de hecho la mayoría de los teólogos católicos y protestantes están de acuerdo en que no son de ninguna forma “inspirados”.

El grado de sacralidad de los textos bíblicos varía según las distintas denominaciones. En el catolicismo y la Iglesia ortodoxa, el texto suele ser considerado per se digno de algún grado de culto, y es llevado en procesión y colocado en altares o lugares dignificados. En el protestantismo, el texto carece de este tipo de valoración y solo es tomado en cuenta, en forma independiente al libro físico, el contenido de las escrituras y su interpretación; sin embargo, son denominados como 'fundamentales' debido a concentrarse y referirse a la Biblia como si esta fuera Dios mismo.

Entre las distintas denominaciones cristianas no existe consenso en la interpretación de la Biblia, lo cual ha sido la principal causa de las divisiones históricas y presentes en la doctrina y práctica cristiana. La posición más extrema en cuanto a la literalidad y conservacionismo del contenido de la Biblia cristiana se ha denominado “fundamentalismo cristiano” y se asocia principalmente al protestantismo. Esto tiene relación a uno de los principios de la Reforma, que es la sola scriptura, de acuerdo a lo cual, se ve a la Biblia como la única y final fuente de fe y doctrinas y asume que cualquier creyente cristiano es capaz de interpretarla.

Católicos, ortodoxos y algunos anglicanos consideran a la Biblia como una fase formativa de la tradición de la iglesia, la cual ha sido continuada mediante decisiones de los concilios ecuménicos, las escrituras de los Padres de la Iglesia y, en el caso del catolicismo, por declaraciones papales.

Una de las causas de las diferencias en las interpretaciones radica en la precisión con la que se han traducido los textos de los originales y se ha transmitido su sentido, con las consideraciones etimológicas y lingüísticas que corresponden.

Debido a esto, existen en el mundo numerosas traducciones de la Biblia, cuyo sentido, muchas veces, carece de la fiabilidad requerida y varía su sentido, hasta el punto de generar controversias doctrinales o de aplicabilidad entre quienes las interpretan.

Las visiones de los cristianos de la vida después de la muerte generalmente involucran el Cielo (también llamado Paraíso) y el Infierno. El catolicismo, desde los primeros siglos, cree en un lugar intermedio llamado Purgatorio. A excepción de este último (cuyos habitantes entrarán finalmente al Cielo, después de una “purificación”), la permanencia en estas regiones es usualmente asumida como eterna. Hay, sin embargo, algunos debates en este último punto, por ejemplo entre los ortodoxos.

Muchos cristianos interpretan la “salvación” como la posibilidad de entrar al Cielo como don de Dios (y escapar del infierno) después de la muerte. La pregunta de “quién es salvo” ha sido considerada como un misterio por muchos teólogos, aunque los protestantes lo consideran como un tema de aceptación de Jesús como único Señor y Salvador, rasgo que es solo la expresión de un hecho consumado para los predestinacionistas, como los calvinistas. La creencia de que todos serán o pueden ser salvos se conoce como universalismo que deriva de la idea de Apocatástasis aceptada entre otros por los ortodoxos griegos.

Generalmente no está claro cómo la vida después de la muerte se ajusta con la doctrina de la Resurrección General, en cuestiones como, por ejemplo, si la vida eterna comienza inmediatamente después de la muerte, o al final del tiempo; y si esta vida después de la muerte involucrará la resurrección de un cuerpo físico o en una forma espiritual glorificada. La mayoría de los cristianos aseguran que un alma sin conciencia sobrevive a la muerte física del cuerpo, aunque otros rechazan esto diciendo que solamente los buenos serán físicamente “resucitados”, mientras que los otros permanecerán en la tumba.

En cambio, algunos grupos, como los Adventistas del Séptimo Día y los denominados Testigos de Jehová, aseguran que los muertos están inconscientes e impotentes en sus sepulcros, que no existe nada que sobreviva a la muerte del cuerpo físico, y que en la resurrección Dios devolverá la vida a quienes Él tenga en su memoria, tanto personas justas como injustas. Por lo tanto, lo que creen los Testigos es que la resurrección significará una reconstrucción completa de los seres humanos fallecidos que están durmiendo en el sueño de la muerte.

Algunas denominaciones cristianas, tratadas como apóstatas por las más numerosas o representativas corrientes existentes dentro del cristianismo, han promovido la creencia en la reencarnación (principalmente el Nuevo Pensamiento e iglesias de la Nueva Era) o espíritus (muchas iglesias espiritistas se identifican a sí mismas como cristianas). Estos grupos normalmente aseguran que tales doctrinas se pueden encontrar en la Biblia o en la tradición cristiana primitiva.

El Credo Niceno afirma que este mundo algún día llegará a su fin, cuando Cristo vuelva (véase Segunda Venida) para juzgar a los vivos y a los muertos e inaugurar un cielo nuevo y una tierra nueva. Además de esta importante doctrina, los cristianos mantienen diferentes opiniones del tiempo, significado y naturaleza de los eventos que preceden el retorno de Cristo. Varias interpretaciones escatológicas, como el Futurismo, añaden detalles como el reinado del Anticristo, el Armagedón, el Rapto y el Milenio. Aunque son de mucha importancia para ciertos grupos, la mayoría de los cristianos y de las denominaciones no dan un gran énfasis a las enseñanzas escatológicas, y se enfocan en el evangelio y las enseñanzas de Cristo. Algunos cristianos esperan que estos eventos ocurran en un futuro muy distante, mientras otros lo interpretan de manera simbólica.

Otros insisten en que el Juicio Final es inminente, siguiendo una antigua línea de pensamiento, el cual posiblemente se extiende a Jesús mismo. Aunque Jesús no dijo el “día o la hora”, algunos han intentado predecir el fin del mundo en el año 1000 (la “Larga Noche de Terror”), 1666, 1844 (la Gran Decepción de la historia del movimiento millerita), 2000 y 2001 por nombrar algunos episodios históricos. Tales expectativas son fácil blanco para el humor (por ejemplo, El cuento del molinero de los Cuentos de Canterbury). Aun así, los principales grupos cristianos todavía afirman que, algún día, el Juicio Final vendrá, y muchos no estarán preparados.

Algunos grupos sostienen que todos estos eventos ya están ocurriendo. Los Testigos de Jehová afirman que “los últimos días” referidos en la Biblia comenzaron en 1914, y que Cristo se encuentra gobernando de manera “invisible” desde esta fecha. La Iglesia de la Unificación enseña que Cristo ha retornado en la persona de su fundador, Sun Myung Moon.

En líneas generales, Jesucristo es para los cristianos el Hijo de Dios, por lo que sus prácticas se orientan hacia su relación con Dios, de la cual se desprenden sus actividades típicas.[35]​

Dentro de las prácticas protestantes reformadas, ortodoxas y católicas, estas destacan especialmente:

Las diversas denominaciones surgidas tras la Reforma protestante reconocen mucho menos de siete de estos en número variable; en general, los protestantes reconocen la naturaleza sacramental del bautismo y la Santa Cena (Eucaristía) y los de línea calvinista reconocen la profesión de fe equivalente a la Confirmación de los católicos, pero solo cuando la persona ya es totalmente consciente de su salvación.

La mayoría de las iglesias cristianas evangélicas adhiriéndose a la doctrina de la Iglesia de creyentes, usan el término "ordenanzas" para referirse al bautismo del creyente y la comunión. [36]​

El bautismo es un rito usual por medio del cual se hace una iniciación al cristianismo.[37]​ Involucra la inmersión en agua. El bautismo proviene de la práctica judía de la inmersión (mikve) para propósitos de un ritual de purificación. La práctica cristiana es derivada del llamado de Juan el Bautista al arrepentimiento (metanoia). Se puede aplicar tanto a niños como a “creyentes adultos” (el cual puede incluir jóvenes adolescentes).[38]​

La Iglesia católica, Iglesia ortodoxa, Iglesias protestantes reformadas, bautizan bebés por efusión.[39]​

En las iglesias evangélicas adhiriéndose a la doctrina de la iglesia de creyentes, el bautismo del creyente está reservado para los creyentes adultos por inmersión en agua, después de un nuevo nacimiento.[40]​ Para los bebés, hay una ceremonia llamada presentación de niño.[41]​

Los cristianos se reúnen principalmente los domingos para un servicio.[42]​

En el libro Primera Apología de Justino Mártir (capítulo LXVII) se describe un oficio del siglo XVII, cuya estructura se puede identificar igualmente en la mayoría de las Iglesias de hoy, que incluye los siguientes componentes:

Existe un alto número de variaciones o excepciones; en algunas ocasiones, rituales como bautismos o bodas se incorporan al servicio. En muchas iglesias de hoy, los niños y los jóvenes son excusados de ir al servicio principal para ir a la Escuela Dominical. Muchas denominaciones se desvían del patrón general en una forma más fundamental. Por ejemplo, los Adventistas del Séptimo Día se reúnen en sábado (el Sabbath judío), no como el resto de las ramas del cristianismo, que lo hacen en domingo. Pentecostales y carismáticos aseguran moverse espontáneamente en el Espíritu Santo, en vez de seguir un orden formal de servicio. En las reuniones de los cuáqueros, los participantes se sientan silenciosamente hasta que son movidos por el Espíritu Santo para hablar.

En algunas denominaciones (principalmente las litúrgicas), el servicio es dirigido por un sacerdote. En otros (principalmente entre protestantes), hay un ministro, predicador o pastor. Otros grupos pueden tener déficit de líderes formales, ya sea por principio o por necesidad local. Además, hay servicios “mayores” de iglesia, caracterizados por una gran solemnidad y rituales, y servicios “menores” en donde prevalece una atmósfera más informal, incluso si el servicio en cuestión es de naturaleza litúrgica.

En Iglesias ortodoxas, la congregación tradicionalmente se mantiene a través de la liturgia. Los católicos y muchas Iglesias protestantes siguen algo predeterminado, en donde los participantes se ponen de pie para cantar, se arrodillan para orar y se sientan para escuchar (por ejemplo, en el sermón). Otros son menos programados, y pueden ser muy animados y espontáneos. De ordinario se incorpora música, y a menudo interviene un coro o un órgano. Algunas iglesias usan solo música a capella, ya sea como regla (muchas Iglesias de Cristo objetan el uso de instrumentos musicales en la adoración) o por tradición (como en la ortodoxa). Una tendencia reciente es el crecimiento de la “adoración integrada”, la cual combina la liturgia con la espontaneidad. Este orden en la adoración es a menudo un resultado de la influencia de la renovación carismática dentro de las iglesias que son tradicionalmente litúrgicas.

Al contrario que en otras religiones, el cristianismo no ha desarrollado un código legislativo religioso, probablemente debido a que el Imperio romano ya poseía un código penal funcional, haciendo innecesario para las autoridades cristianas el duplicar varias de sus prohibiciones.

Existe una gran tradición dentro del cristianismo al decir que Cristo excede las leyes del judaísmo; que el amor (a Dios y al prójimo) es el “Gran Mandamiento”, desde el cual todas las otras leyes morales son obtenidas; que ningún ser humano puede esperar evitar el pecado completamente; que una persona no debe juzgar a otros (teniendo únicamente Dios ese privilegio), entre otras.

Aun así, el Nuevo Testamento también contiene importantes guías morales para los cristianos. Jesús en el Sermón de la montaña le pide a sus seguidores, entre otras cosas, el amar a sus enemigos, ser perseverantes, misericordiosos y humildes; en Marcos 10:21 le pide a un “joven hombre rico” que venda sus posesiones y dé el dinero a los pobres. Sin embargo, el pedido de Jesús en este caso no fue un enfoque en vivir una vida sin riqueza alguna, sino más bien desenmascarando la idolatría en el corazón de la mayoría de los ricos.

Algunos cristianos dicen que estas directivas son extraordinariamente difíciles, bordeando lo impracticable. Al mismo tiempo, la mayoría de los cristianos admiran a aquellos cuyas vidas parecen personificar estos principios, como Francisco de Asís, Albert Schweitzer o la Madre Teresa.

Algunos juicios morales de Jesús son más abordables, pero todavía no son de práctica general entre todos los cristianos. En el Sermón del Monte él habla en contra del divorcio (un tema controvertido en muchas denominaciones cristianas), y contra el juramento (una prohibición enfatizada principalmente por los cuáqueros).

Todas las versiones y variaciones conocidas del cristianismo practican la oración. Las oraciones cristianas pueden ser formulistas, improvisadas o inspiradas por el Espíritu Santo. Las oraciones normalmente se agrupan en categorías: de acción de gracia, adoración, petición, intercesión y comunión. Las oraciones cristianas pueden ser dirigidas a Dios Padre, a Cristo o a un santo (en el caso de los católicos y ortodoxos). Los católicos han desarrollado una práctica devocional de orar el rosario. Entre las oraciones formulistas, el Padre Nuestro y los Salmos, y en círculos católicos el Ave María son las más comunes.

La pregunta sobre la eficacia de la oración está llena de diferendos teológicos. Algunas iglesias enseñan que la oración es capaz de alterar el ambiente físico, tomando en cuenta cosas como la sanidad espiritual. Ejemplos de este tipo de iglesias incluyen la Ciencia Cristiana, así como varias iglesias del Nuevo Pensamiento.

Al final de cada oración, normalmente se dice amén (‘así sea’).

Las prácticas penitenciales ya estaban presentes en el cristianismo primitivo. Una práctica cristiana de origen antiguo, inspirada probablemente en la tradición judía, y practicada por Jesús, es el ayuno. Además de ser mencionado en distintos pasajes neotestamentarios, la Didaché señala cómo la oración debía combinarse con ayunos, que se prescribían «el cuarto y el día de la preparación» (miércoles y viernes) (Did 8).[43]​ Existen además evidencias históricas de la existencia del ayuno como práctica preparatoria de la Pascua desde fines del siglo II y principios del siglo III, lo que derivaría en el siglo IV en la conformación del tiempo litúrgico conocido hoy en varias denominaciones cristianas como Cuaresma.[44]​

El catolicismo distingue entre «ayuno», que consiste en privarse —con mayor o menor estrictez— de todo alimento y bebida, y «abstinencia», que involucra la renuncia voluntaria a la ingesta de ciertos alimentos, usualmente cárnicos. En el catolicismo, el ayuno se practica particularmente en dos jornadas de significación penitencial por excelencia: el Miércoles de Ceniza y el Viernes Santo. En la actualidad, la Iglesia católica ha ido sustituyendo esta práctica por un ayuno que implica el privarse de algo deseable, como ofrenda a Dios.[45]​ En Iglesias evangélicas y en otras denominaciones, el ayuno se practica frecuentemente como privación total de alimentos durante un lapso de tiempo, ingiriendo solamente agua.[46]​

La mayoría de las denominaciones cristianas presentan calendarios litúrgicos con distintos tiempos y festividades que, si bien no son siempre coincidentes, presentan muchos aspectos en común. Aunque las fechas de las celebraciones varían en mayor o menor grado entre las diferentes Iglesias cristianas, la secuencia y lógica utilizadas para su planificación son en esencia las mismas. Incluso algunas comunidades cristianas que no siguen una tradición litúrgica celebran la Navidad y la Pascua, y las que objetan el reconocimiento de festividades especiales reconocen, no obstante, que los eventos que se celebran en ellas (la encarnación, la resurrección de Cristo) en verdad ocurrieron, aunque no necesariamente en esa fecha. La Comunión anglicana y numerosas Iglesias protestantes siguen en la liturgia un esquema de lecturas bíblicas muy similar al de la Iglesia católica postconciliar, ya que el actual Revised Common Lectionary (primera edición de 1992)[47]​ es el resultado de una serie de obras litúrgicas previas inspiradas en el Ordo Lectionum Missae (1969), fruto del Concilio Vaticano II. Las diferencias suelen ser menores y propias de las Iglesias particulares, tales como la de la Iglesia de Inglaterra en su Common Worship Lectionary.[48]​ Una de las diferencias más marcadas entre los calendarios litúrgicos radica en el grado de participación que se otorga a las festividades asociadas a los santos. Las Iglesias católica, ortodoxa y anglicana presentan calendarios litúrgicos con una participación importante de celebraciones en honor de María (madre de Jesús) y de otros santos, lo que no se verifica en igual medida en los calendarios de las comunidades protestantes.

Uno de los símbolos cristianos originarios fue el del pez o Ichthys (del griego, en letras mayúsculas, IXΘΥΣ).[50]​[51]​ Este vocablo conformaba un acrónimo: «Ἰησοῦς Χριστός, Θεοῦ Υἱός, Σωτήρ» (Iēsoûs Christós, Theoû Hyiós, Sōtḗr) que, traducido al español, significa «Jesús Cristo, Hijo de Dios, Salvador». Junto con el símbolo del ancla,[49]​ el pez fue uno de los más empleados por los cristianos primitivos.

El símbolo más conocido del cristianismo es sin duda la cruz, por ser en ella donde murió Jesús según los Evangelios y sobre la que existen una gran variedad de formas. Varias denominaciones tienden a favorecer cruces distintivas: el crucifijo para los católicos —dentro del cual diversas órdenes religiosas también incluyen variantes para identificarse, como la Tau franciscana o la Cruz de Calatrava de los Dominicos—, la cruz ortodoxa para los ortodoxos, una cruz sin adornos para los protestantes. Sin embargo, no es una regla utilizar una u otra cruz.
Constantino I el Grande usó también el Crismón para identificarse con el cristianismo, el cual está formado por las primeras dos letras griegas del nombre “Cristo”.

Crucifijo

Cruz ortodoxa

Crismón

A través de su historia, el cristianismo ha pasado por numerosas divisiones generando diversos grupos con creencias y tradiciones propias que varían de acuerdo a la cultura y el lugar. Estas amplias divisiones, a su vez, no son homogéneas. Por el contrario, algunas ramas poseen amplios desacuerdos y en otros casos la división omite simpatías existentes. Desde la Reforma, el cristianismo se representa normalmente como dividido en tres ramas principales: católico, ortodoxo y protestante;[52]​ pero históricamente existen muchas más:[53]​[54]​[55]​[56]​

La Iglesia católica[57]​ (en latín, Ecclesia Catholica y en griego, Καθολικὴ Ἐκκλησία) es la Iglesia cristiana más numerosa.[58]​ Está compuesta por 24 Iglesias sui iuris: la Iglesia latina y 23 Iglesias orientales,[59]​[60]​[61]​ que se encuentran en completa comunión con el papa y que en conjunto reúnen a más de 1360 millones de fieles en el mundo.[62]​[63]​

La Iglesia ortodoxa, formalmente llamada Iglesia católica apostólica ortodoxa (en griego, Ορθόδοξη Καθολική και Αποστολική Εκκλησία, romanizado: Orthódoxi Katholikí kai Apostolikí Ekklisía),[69]​ es una comunión cristiana, cuya antigüedad, tradicionalmente, se remonta a Jesús y a los doce apóstoles, a través de una sucesión apostólica nunca interrumpida. Cuenta con entre 225 y 300 millones de fieles en todo el mundo.[70]​[71]​

La Iglesia ortodoxa se considera la heredera de todas las comunidades cristianas de la mitad oriental del Mediterráneo,[72]​ reclamo no aceptado por las Iglesias ortodoxas orientales. Su doctrina se estableció en una serie de concilios, de los cuales los más importantes son los siete "concilios ecuménicos", que tuvieron lugar entre los siglos IV y VIII. Tras varios desencuentros y conflictos, la Iglesia ortodoxa y la Iglesia católica se separaron en el llamado "Cisma de Oriente y Occidente", el 16 de julio de 1054. El cristianismo ortodoxo se difundió por Europa Oriental gracias al prestigio del Imperio bizantino y a la labor de numerosos grupos misioneros.

Con el nombre de Iglesias ortodoxas orientales se agrupa a todas aquellas jurisdicciones cristianas que rechazan la Cristología emanada del Concilio Ecuménico de Calcedonia el año 451. Se autodenominan ortodoxas a pesar de no ser consideradas ortodoxas por otras Iglesias que aceptan la fórmula de fe católica y apostólica.

La Iglesia ortodoxa copta de Alejandría se remonta al siglo I y ha mantenido muchas tradiciones muy cercanas a la Iglesia anterior a los primeros cismas importantes. De ella surgió la Iglesia ortodoxa de Etiopía, que se remonta al siglo IV, cuando el cristianismo fue adoptado como religión estatal del reino de Aksum, aunque fue apenas en 1959 que la Iglesia copta le reconoció la autocefalia. Finalmente, la historia de la Iglesia ortodoxa de Eritrea se confunde con la de la Iglesia etíope hasta tiempos recientes, cuando Eritrea se independizó de Etiopía y se produjo la separación de las respectivas jerarquías eclesiásticas en 1993.

Se estima que en total los fieles de estas iglesias ascienden a unos 65 millones de personas, repartidas entre Egipto (unos 10 millones), Etiopía (50 millones), Eritrea (3 millones), Sudán y Sudán del Sur (unos 500 000).

El origen de la Iglesia ortodoxa siria se remonta a la comunidad cristiana establecida en Antioquía en el siglo I, cuyos miembros fueron los primeros en llamarse cristianos (Hechos 11:26). Actualmente tiene aproximadamente 4 millones de miembros.[74]​

De otra parte, la Iglesia ortodoxa malankara es uno de los varios grupos denominados cristianos de Santo Tomás, quienes creen, según la tradición, que el apóstol llegó a la India en el año 52, estableciendo la Iglesia y sufriendo el martirio en el año 72 en Mylapore (actualmente la ciudad de Chennai) en el sur de la India. Esta Iglesia estaba en comunión y dependía de la Iglesia del Oriente hasta que en 1665 aceptó la cristología de la Iglesia ortodoxa siriana, directamente contraria a la de la Iglesia del Oriente, y adoptó el rito siríaco occidental.

Desde 1975, cuando el patriarca sirio depuso y excomulgó al catolicós Basilio Augen y le nombró un rival, la Iglesia ortodoxa de India se encuentra dividida entre los que mantienen la fidelidad al patriarca sirio —miembros de la Iglesia sirio-ortodoxa de la India— y los que siguieron reconociendo al catolicós depuesto y sus sucesores —miembros de la Iglesia ortodoxa malankara—. Según el censo del Kerala del 2011, la Iglesia malankara tenía 493 858 fieles y la Iglesia sirio-ortodoxa o jacobita tenía unos 482 762.

La Iglesia apostólica armenia es la Iglesia nacional más antigua del mundo.[75]​[76]​[77]​[78]​ Se encuentra dividida en cuatro centros jurisdiccionales: Patriarcado armenio de Jerusalén, Patriarcado armenio de Constantinopla, Patriarcado armenio de Sis (Líbano) y la Sede Madre de la Santa Echmiadzin, sede del patriarca supremo y catolicós de todos los armenios, primado de la Iglesia apostólica armenia. Actualmente tiene unos 10 millones de miembros.

Se define como Iglesias derivadas de la reforma protestante a todos los grupos cristianos derivados de la llamada Iglesia católica por la Reforma protestante del siglo XVI, que incluye numerosas denominaciones y doctrinas como el anglicanismo, luteranismo, anabaptismo y calvinismo, entre otras.,[82]​ Sus cultos adquirieron diferentes modalidades, aunque en general comparten la centralidad de la Biblia y la importancia de la predicación. Los sacramentos reconocidos suelen ser solo dos: bautismo y Santa Cena, aunque con interpretaciones diversas según las distintas denominaciones.

La llamada Comunión anglicana, compuesta de diversas iglesias que se reconocen como derivadas de la Reforma anglicana, como la Iglesia de Inglaterra o la Iglesia episcopaliana, entre otras, representa en muchos aspectos una forma intermedia de organización eclesiástica entre la Iglesia católica y las confesiones protestantes dominantes en Europa Central y del Norte; y, por ende, a menudo se clasifica de forma separada.

A pesar de los matices según los movimientos evangélicos, existe un conjunto de creencias similares para las denominaciones adhiriéndose a la doctrina de la iglesia de creyentes (Iglesias bautistas, pentecostalismo, movimiento carismático y cristianismo no denominacional). [83]​[84]​[85]​[86]​ [87]​

Las Iglesias bautistas son un movimiento evangélico que surge de un avivamiento iniciado por el pastor inglés John Smyth en Holanda en 1609.[88]​La primera iglesia bautista fue fundada en Inglaterra en Spitalfields, al este de Londres en 1612. En 2010, el bautismo tendría 100 millones de creyentes.[89]​ La Alianza Mundial Bautista, la denominación bautista más grande del mundo, tendría 169.000 iglesias y 47.000.000 de miembros bautizados en 2020.[90]​

El adventismo es una rama del cristianismo protestante que remonta su origen al Movimiento Millerita ocurrido en Estados Unidos a mediados del xix y que se caracteriza por su énfasis en la creencia del regreso personal, visible y glorioso de Cristo —es decir, la Segunda Venida— es inminente. La Iglesia Adventista del Séptimo Día se ha convertido en la organización con mayor extensión y membresía, con congregaciones en más de 210 países y una membresía que supera los 22 millones, seguida por la Iglesia de Dios (Séptimo Día), con una presencia en 45 países del mundo.

El Pentecostalismo es un movimiento evangélico que tiene su origen en varios avivamientos que tuvieron lugar en el siglo XIX y el siglo XX.[91]​ El más importante fue el reavivamiento de la Calle Azusa en Los Ángeles en 1906, con William J. Seymour, donde un grupo de creyentes señala haber experimentado un bautismo del Espíritu Santo con glosolalia.

Esta iglesia le da mucha importancia a los «dones del Espíritu» tales como la sanación espiritual, profecía, exorcismo, hablar en lenguas.[92]​ En 2011, el pentecostalismo llegaría a 279 millones de personas.[93]​ Los Asambleas de Dios, la denominación pentecostal más grande del mundo, tendría 69,200,000 miembros en 2018.[94]​

Se consideran restauracionistas aquellas iglesias y corrientes cristianas que se dedican a recuperar la esencia de la fe cristiana basada en las enseñanzas cristocéntricas que se habrían perdido o tergiversado con el devenir histórico. Su objeto es restaurar a la Iglesia siguiendo el modelo de la Iglesia primitiva o de los primeros discípulos. Se basan en la lectura e interpretación directa de la Biblia y no por los dogmas establecidos por la Iglesia católica. En cuanto a su denominación, se consideran a sí mismos como totalmente separados del protestantismo en el cual a menudo se les incluye,[cita requerida] incluso muchos ya no se identifican con ningún credo en particular sino que simplemente se llaman “cristianos” (ver Cristianismo no denominacional).[cita requerida]

También perteneciente al grupo del neopentecostalismo, la Iglesia de Dios Ministerial de Jesucristo Internacional es fundada en 1972 en Colombia, y promueve la abundante ministración de los dones espirituales.

Ramificación principal del protestantismo a través de los siglos

Otras corrientes cristianas

Algunas ramificaciones del cristianismo histórico llegaron a incluir en el pasado a los cristianos gnósticos de los primeros siglos de la Era Cristiana, los cuales propugnaban un plan de salvación completamente diferente del de la redención por la pasión y muerte del Maestro,[95]​ a los cristianos arrianos del siglo IV, los cuales impugnaban el concepto trinitario, a los cátaros o albigenses medievales, los cuales rechazaban, en su totalidad, el Antiguo Testamento, y las comunidades de cristianos valdenses que en la Alta Edad Media solían refugiarse en los valles de los Alpes de las persecuciones por parte del papado. Estos últimos, junto a los husitas en Bohemia, se consideran precursores de la Reforma protestante del siglo XVI, ya que son anteriores a ella.

Algunas confesiones minoritarias se han escindido de la Iglesia católica, rompiendo así la comunión con Roma, pero se siguen definiendo como «católicos», como los veterocatólicos, la Iglesia católica apostólica brasileña o los católicos sedevacantistas, entre otros.

Los Testigos de Jehová si bien consideran a Jesús de Nazaret como Hijo de Dios y un ser divino con una existencia prehumana, al no aceptar el Credo Nicenoconstantinopolitano, no lo ven como la Segunda Persona de la Trinidad. Utilizan principalmente su propia versión específica de la Biblia, aunque en ocasiones usan otras traducciones. Los adeptos a esta religión suman más de 8.6 millones, y son conocidos por su amplia actividad evangelizadora, distribuyendo publicaciones en unos mil idiomas. [cita requerida]

Engloba a algunos credos que tienen por característica común el reconocer a una persona plenamente histórica como una representación, presencia, reencarnación o resurrección de Jesús de Nazaret o de un nuevo apóstol de este. Estos grupos giran en torno a una persona y la interpretación que esta dé de la Biblia o un texto que se crea equivalente por este grupo. Como ejemplos de estos tenemos a la Federación de Familias para la Paz y Unificación Mundial, llamada de forma común Secta Moon en honor a su fundador el coreano Sun Myung Moon, La Iglesia de Jesucristo de los Santos de los Últimos Días fundada por Joseph Smith y que se basa en el Libro de Mormón y la Iglesia Adventista del Séptimo Día, fundada por Ellen G. White junto con otras cuatro personas (incluyendo su esposo James White), aceptándola como profetiza y a sus libros como si fueran tan inspirados como la Biblia.

Existen otras denominaciones e iglesias que se consideran como cristianas pero que se automarginan de la clasificación descrita, por lo que generalmente no son aceptadas como tales por las Iglesias apostólicas.[cita requerida] Estas incluyen a las Iglesias indígenas africanas con cerca de 110 millones de miembros[32]​ (las estimaciones varían significativamente).

Por último, habría que agregar a esta lista a algunos grupos y movimientos del llamado judaísmo mesiánico, que, si bien se autoproclaman judíos, reconocen a Jesús como Mesías. Históricamente han existido también grupos cristianos que han restaurado costumbres religiosas propias del judaísmo (a veces llamados judaizantes), como los sabatarianos en Europa Central y los subbotniks en Rusia.

La Iglesia católica y las Iglesias orientales (tanto en comunión con Roma como autocéfalas) son gobernadas por una jerarquía: los obispos dirigen regiones locales (llamadas diócesis) y nombran sacerdotes para administrar congregaciones individuales. En la Iglesia católica, la autoridad suprema la posee el obispo de Roma, quien es llamado “el papa” (del latín “Petri Apostoli Potestatem Accipiens”, que significa ‘El que recibe la potestad en nombre de Pedro’). Es elegido por un Colegio cardenalicio y normalmente sirve de por vida.

Las Iglesias ortodoxas y orientales pueden ser descritas como redes de iglesias en las cuales los obispos están “en comunión” unos con otros. No tienen una personalidad similar al papa, aunque los Patriarcas presiden sobre ciertas partes de la Iglesia. Las Iglesias anglicanas también son episcopales (“dirigidas por obispos”) en su gobierno.

Los Creyentes Antiguos se levantaron cuando algunos creyentes ortodoxos rusos se rebelaron contra sus obispos por el tema de las “reformas” del patriarca Nikón. Aunque su motivación original era prevenir los cambios en su religión, finalmente se encontraron en la posición de tener que funcionar sin obispos o sacerdotes (ya que estos últimos son ordenados por los obispos). Algunos eliminaron el rol sacerdotal, mientras que otros buscaron reclutar nuevos sacerdotes entre los ortodoxos.

La mayoría de las Iglesias protestantes carecen del orden jerárquico que caracteriza a las denominaciones litúrgicas. El rol de “predicador” o “ministro” es a menudo tratado como un trabajo ordinario, en el cual muchas iglesias creen que puede ser asumido por cualquier creyente con el suficiente conocimiento de Cristo. Otros especifican que el líder de la congregación debe haber asistido a un seminario educativo relacionado o tener la sensación de haber sido “llamado” (similar a la vocación) por Dios en ese rol.

La Iglesia de Jesucristo de los Santos de los Últimos Días es dirigida por una jerarquía consistente en un profeta y doce apóstoles. Aseguran que es la misma estructura que se encontraba en la Iglesia primitiva. Su dirección es implementada en todo el mundo en congregaciones locales por presidentes y obispos locales. No hay un clero pagado y los hombres son ordenados al sacerdocio para mantener los “decretos sacerdotales”.

Un tema teológico importante es “¿qué es la Iglesia?” La mayoría de los cristianos aceptan que existe solo una sola Iglesia (a la que los credos clásicos se refieren), la cual se identifica con “el cuerpo de Cristo”. Los católicos y los ortodoxos consideran que la Iglesia es simultáneamente una realidad espiritual (Cuerpo místico) y también una comunidad existente y visible (institución). Los católicos identifican esta Iglesia como la que subsiste en la Iglesia católica, mientras que los ortodoxos consideran que su rama constituye la “Iglesia verdadera”. Los protestantes tienden a ver a “la Iglesia” como una entidad invisible que se puede distinguir de la unión de todos los creyentes “verdaderos” (que toman a Jesucristo como su Señor) existentes dentro de varias denominaciones cristianas. Algunos grupos (Testigos de Jehová) aseguran que solo ellos son la Iglesia verdadera. Tanto en la Iglesia católica, como entre diversas comunidades protestantes, existen algunas corrientes ecuménicas que tienden a universalizar el concepto de Iglesia.

Debido a su historia cambiante y a las numerosas denominaciones, es difícil entender el nivel actual de las relaciones del cristianismo con otras religiones. Esto varía de región en región, y de denominación en denominación. La siguiente sinopsis refleja parte de estas:

El cristianismo y la religión olímpica grecorromana son representadas popularmente como antagónicas, donde cada una persigue y destruye a la otra, pero esta es una simplificación muy grande. Incluso el emperador pagano y anticristiano Juliano el Apóstata (361-363) admitió que “Estos galileanos sin dioses [los cristianos] alimentan no solo su propia pobreza, sino nuestra falta de cuidado propio”.[96]​ Sin embargo, como apuntan Karlheinz Deschner y tantos otros, Juliano fue un emperador más bondadoso y permisivo que cualquiera de sus antecesores o sucesores cristianos.[97]​

Los Padres de la Iglesia tuvieron diversas actitudes hacia la enseñanza pagana, desde el rechazo vocalizado, hasta el reconocimiento de la inspiración parcial de filósofos como Platón, cuya imagen se encuentra entre los santos en algunas iglesias y paredes de monasterios.

En el pasado, a los cristianos a menudo se les enseñaba que los judíos habían matado a Cristo. Esta muerte generaba una culpa colectiva atribuida a la totalidad de los judíos, una interpretación que la mayoría de las denominaciones ahora rechaza.

Los judíos fueron víctimas de masacres, marginaciones, destierros y expropiaciones a manos de la Iglesia o de los príncipes cristianos.

La prédica antisemita ha sido una constante histórica por autoridades cristianas. Por ejemplo, en la parte católica, Vicente Ferrer (siglo XIV) predicaba:  “los judíos son animales con rabo y menstrúan como las mujeres”.[98]​Por el lado protestante, parte de la prédica de Martín Lutero (siglo XVI) era de tono claramente antisemita: “Mi consejo es: primero que sus sinagogas sean quemadas hasta los cimientos, y que todo aquel que sea capaz esparza azufre y brea; mejor sería que alguien arrojara sobre ellas fuego del infierno”, escribe en “Sobre los judíos y sus mentiras” (1543).
El antisemitismo tiene una larga historia en el cristianismo, y sin duda está lejos de declinar (por ejemplo, en la Rusia contemporánea). No obstante, desde el Holocausto, muchas conversaciones han apuntado a la reconciliación cristiano-judía y las relaciones han mejorado de manera importante. Hoy en día, muchos evangélicos conservadores aceptan el sionismo cristiano.

Sin embargo, no se puede afirmar que el cristianismo sea “antisemita”, sino más bien algunos cristianos. Muchas corrientes cristianas defienden el trato de los judíos como hermanos a partir de las palabras de Jesús: “Padre, perdónalos porque no saben lo que hacen” (Lc. 23,34), mejorándose así las relaciones entre judaísmo y cristianismo.

El fenómeno del judaísmo mesiánico se ha transformado en algo que debilita las relaciones cristiano-judías. Los judíos mesiánicos, que generalmente buscan combinar la identidad judía con el reconocimiento de Jesús, son rechazados por grupos de la corriente principal judía, quienes descartan al judaísmo mesiánico casi tanto como el cristianismo con connotación judía.

Seguidores del islam se han referido históricamente a los judíos, a los cristianos y a ellos mismos como la Gente del Libro debido a que todos basan su religión en libros que tienen un origen divino. Los cristianos, sin embargo, no reconocen el Corán como un libro genuino de revelación divina, ni aceptan que Mahoma fuera un profeta genuino.

Los musulmanes, por su parte, creen que parte de los Evangelios, la Torá y los libros proféticos de los judíos han sido olvidados, malinterpretados y distorsionados por sus seguidores. Basados en esa perspectiva, los musulmanes ven el Corán como la corrección a los errores del cristianismo. Por ejemplo, los musulmanes rechazan la creencia en la Trinidad, y otras expresiones de divinidad de Jesús, como incompatibles con el monoteísmo.

Las dos creencias han experimentado a menudo controversias y conflictos (un ejemplo son las Cruzadas) aunque también han existido relaciones de bien mutuo. Las escrituras del teólogo Tomás de Aquino suelen citar aquellas del filósofo judío Moisés Maimonides, así como las del pensador musulmán Averroes ('Ibn-Rushd).

El 6 de mayo de 2001, el papa Juan Pablo II, el primer papa en orar en una mezquita, entregó un escrito en la mezquita de Omayyad en Damasco, diciendo: «Es importante que los musulmanes y los cristianos continúen explorando las preguntas filosóficas y teológicas en conjunto, para poder obtener un conocimiento más objetivo y comprensivo de cada creencia religiosa del otro. El mejor entendimiento mutuo seguramente llevará, a nivel práctico, a una nueva forma de presentar nuestras dos religiones no en oposición, como ha sucedido a menudo en el pasado, sino en asociación para el bien de la familia humana».

Las relaciones cristiano-hindúes han tenido destinos encontrados. Por una parte, la tendencia natural del hinduismo ha sido el reconocer las bases divinas de muchas otras religiones y reverenciar a sus fundadores y santos practicantes. Por otra parte, las percepciones de un proselitismo agresivo por parte del cristianismo han generado un despliegue de violencia anticristiana, a menudo alimentada por los partidos políticos nacionalistas hindúes. En países occidentales, el Vedānta ha influenciado a algunos pensadores cristianos, mientras que los movimientos antisectistas han reaccionado en contra de actividades de gurús inmigrantes y sus seguidores.

El budismo y el protestantismo se vieron en conflicto político en el siglo XIX en Sri Lanka, con la final ofuscación del cristianismo; y en el Tíbet alrededor de 1904 (la expedición Younghusband) con el mismo resultado. Varios acontecimientos han originado ciertas tensiones en la teología budista y la meditación de varias generaciones de buscadores espirituales occidentales (incluyendo las religiones católicas), al punto de que el budismo se ha convertido en un “competidor” menor del cristianismo en su “hogar”. Sin embargo, las relaciones son en general buenas, excepto quizás en Corea del Sur y Vietnam. La república rusa de Kalmykia reconoce al budismo tibetano y a la ortodoxia rusa como sus religiones oficiales.

Grupos occidentales esotéricos y mágicos se han levantado a menudo para protestar contra el cristianismo. Algunos de estos, como la teosofía o la cientología, han producido polémicas hostiles en contra del cristianismo.

Al discutir la persecución, se debe distinguir con cuidado entre:

Durante siglos la historia del cristianismo ha estado ligada a la historia social de Europa occidental (y de varias otras culturas y regiones). En resumen, podemos notar la expansión inicial del cristianismo a través de la cuenca del Mediterráneo, su legalización bajo Constantino I el Grande (siglo IV) y el establecimiento como religión oficial del Imperio romano bajo Teodosio I el Grande; el desarrollo de antiguas comunidades minoritarias en Persia, India y China; la conversión de varios reinos europeos; el Gran Cisma donde se separó el cristianismo ortodoxo de oriente del catolicismo (fechado convencionalmente en 1054); la pérdida del norte de África y el Medio Oriente a manos del islam; la Reforma Protestante con la publicación por Martín Lutero de sus 95 tesis en 1517); la expansión del cristianismo en las Américas, Oceanía, Filipinas y Corea del Sur; la división del protestantismo en denominaciones, destacando últimamente el rápido crecimiento del pentecostalismo y los evangélicos; y los debates modernos de la ciencia, criticismo bíblico y el feminismo.

Para ver las contribuciones del cristianismo a la humanidad y a la cultura mundial, véase en filosofía cristiana, arte cristiano, literatura cristiana, música cristiana, arquitectura cristiana.

Antes del Edicto de Milán (313), el cristianismo primitivo era un movimiento ilegal, el cual muchos consideraban antisocial y ateo debido a que se comportaba como una secta subversiva contra el imperio. Eran muy comunes las rebeliones y las revueltas por parte de cristianos en el antiguo imperio, convirtiéndose en una amenaza para la sociedad. Según Tertuliano, “Los cristianos tienen la culpa de todo desastre público y toda desgracia que sobreviene al pueblo. Si el Tíber sube hasta los muros, si el Nilo no sube e inunda los campos, si el cielo retiene la lluvia, si hay un terremoto o hambre o plaga, enseguida surge el clamor: «¡Los cristianos a los leones!»”.[99]​ Un dibujo encontrado en Roma en el que un hombre con la cabeza de un asno cuelga de una cruz, corrobora la idea que tenían los paganos con respecto al cristianismo.[100]​ Muchos cristianos primitivos murieron en el martirio, algunas veces en la arena, después de rehusar renunciar a su fe.

Además de los motivos religiosos, también existen motivos políticos. Muchos emperadores se deificaban a sí mismos y exigían a los súbditos de su imperio el que adoraran sus estatuas colocadas en las plazas de las ciudades; igualmente exigían se les dirigiera como hijos de dioses y señor de señores. Los cristianos se negaban a realizar estos actos, debido a que para ellos era herético decirle hijo de Dios a otro que no fuera Jesucristo, lo mismo que señor de señores, al igual que la adoración de estatuas. Por ello, los cristianos solían ser vistos como renegados políticos que iban contra el statu quo establecido, lo que propiciaba también sus persecuciones.

De acuerdo a los datos aportados por el historiador Edward Gibbon en la parte VIII del capítulo XVI de su “Decadencia y Caída del Imperio romano” se presenta el cálculo de un máximo de 2000 víctimas cristianas durante la Gran Persecución (303-313 E.C.) y un estimado total de 4000. Kenneth Humphreys afirma en un cuadro detallado que las persecuciones llevadas a cabo por el poder romano[101]​ se produjeron en períodos intermitentes y muy restringidos.

Una vez legalizado el cristianismo con el Edicto de Milán, los cristianos, alentados primero por los privilegios que les garantizó Constantino I y luego por la declaración del cristianismo como religión exclusiva del Imperio romano que promulgó Teodosio en el 380 d. C. mediante el Edicto de Tesalónica, expandieron la nueva religión por el mundo pagano.[Nota 6]​

Los cristianos han perpetrado asimismo numerosas y sangrientas persecuciones. En tiempos antiguos, las turbas cristianas solían hostigar a los paganos y destruían sus templos, incluso con apoyo del poder civil.[102]​

Los cristianos no solo han perseguido a seguidores de otras religiones, sino también a otros cristianos. Bizancio suprimió las iglesias no calcedonias; los ejércitos de las Cruzadas saquearon Bizancio; protestantes y católicos pelearon en la Guerra de los Treinta Años. También se pueden mencionar la caza de brujas al principio de la Europa moderna.[103]​[104]​

Antonio Socci estimó que alrededor de 70 000 000 de cristianos fueron asesinados por su fe en dos milenios, de los cuales 45 500 000 (es decir, 65 % del total) fueron muertos en el siglo XX por su condición de cristianos.[105]​ Solo en España, durante la Guerra civil de 1936-1939, fueron asesinados más de 6800 eclesiásticos.[106]​ Los datos de Socci, sin embargo, han sido cuestionados por distintos estudiosos quienes apuntan a que las cifras dadas por Socci son un cálculo personal, no diferencia entre perseguidos por razones políticas y aquellos por razones religiosas y utilizan fuentes como la World Christian Encyclopedia.[107]​

Igualmente ha habido quejas sobre discriminación en diferentes contextos, tanto por parte de cristianos como en contra de cristianos.[cita requerida] Como ejemplos actuales se pueden nombrar: las restricciones gubernamentales griegas y rusas para las actividades religiosas no ortodoxas; la violencia antiaborto en Estados Unidos y la “problemática” de entrada a Irlanda del Norte, respectivamente.[cita requerida]

Según un informe publicado por Ayuda a la Iglesia Necesitada, alrededor de 350 millones de cristianos sufren persecución o discriminación religiosa en el mundo.[108]​[109]​

A lo largo de la historia, muchas personas y sociedades han realizado críticas al cristianismo, a las iglesias cristianas y a los propios cristianos. Algunas críticas van especialmente dirigidas a los creyentes, a las enseñanzas o a la interpretación de las Escrituras. La respuesta de estas críticas por parte de los cristianos se denomina apologética cristiana.

Error en la cita: Existen etiquetas <ref> para un grupo llamado «nota», pero no se encontró la etiqueta <references group="nota"/> correspondiente.





Estados Unidos[nota 2]​ (EE. UU.; en inglés, United States o US),[nota 3]​ cuyo nombre oficial es Estados Unidos de América (EUA; en inglés, United States of America o USA),[12]​ es un país soberano constituido en una república federal constitucional compuesta por cincuenta estados y un distrito federal. La mayor parte del país se ubica en el medio de América del Norte —donde se encuentran sus 48 estados contiguos y Washington D. C., el distrito federal—, entre los océanos Pacífico y Atlántico. Limita con Canadá al norte y con México al sur. El estado de Alaska está en el noroeste del continente, limita con Canadá al este, separado de Rusia al oeste por el estrecho de Bering. El estado de Hawái es un archipiélago polinesio en medio del océano Pacífico y es el único de sus estados que no se encuentra en América. El país posee en el mar Caribe y en el Pacífico varios territorios no incorporados.

Con 9,83 millones de kilómetros cuadrados,[5]​ y con más de 331 millones de habitantes, el país está en el cuarto puesto por superficie total, en el quinto por superficie contigua y en el tercer lugar por población. Es una de las naciones con más diversidad de etnias y culturas, producto de la inmigración a gran escala.[13]​

Es la economía nacional más grande del mundo en términos nominales, con un PIB estimado en 22,6 billones de dólares (una cuarta parte del PIB global nominal) y una quinta parte del PIB global en paridad de poder adquisitivo.[8]​ El país es la principal fuerza capitalista del planeta, además de ser líder en la investigación científica y la innovación tecnológica desde el siglo XIX y, desde comienzos del siglo XX, el principal país industrial. En PIB PPA, Estados Unidos es la segunda economía más grande, por detrás de la China.[14]​

El territorio continental estadounidense estuvo habitando por diversos grupos indígenas durante miles de años. Esta población aborigen fue reducida por las enfermedades y la guerra después del primer contacto con los europeos. Estados Unidos fue fundado por trece colonias británicas, a lo largo de la costa atlántica. El 4 de julio de 1776, emitieron la Declaración de Independencia, que proclamó su derecho a la libre autodeterminación y el establecimiento de una unión cooperativa. Los estados rebeldes derrotaron al Imperio británico en la guerra de independencia, el primer conflicto bélico colonial exitoso de carácter independentista.[15]​ La Constitución de los Estados Unidos fue adoptada el 17 de septiembre de 1787; su ratificación al año siguiente hizo a los estados parte de una sola república con un gobierno central fuerte. La Carta de Derechos, que comprende diez enmiendas constitucionales que garantizan muchos derechos civiles fundamentales y las libertades, fue ratificada en 1791.

En el siglo XIX, los Estados Unidos adquirieron territorios de Francia, España, Reino Unido, México, Rusia y Japón, además de anexionarse las repúblicas de Florida, Texas, California y Hawái. En la década de 1860, las disputas entre el sur agrario y conservador y el norte industrial y progresista sobre los derechos de los estados y la abolición de la esclavitud provocaron la Guerra de Secesión. La victoria del norte evitó una división permanente del país y condujo al final de la esclavitud legal. Para la década de 1890, la economía nacional era la más grande del mundo[16]​ y la guerra hispano-estadounidense y la Primera Guerra Mundial confirmaron su estatus como una potencia militar. Después de la Segunda Guerra Mundial, surgió como el primer país con armas nucleares y miembro permanente del Consejo de Seguridad de las Naciones Unidas.

Durante la Guerra Fría, Estados Unidos luchó en la Guerra de Corea y la Guerra de Vietnam, pero evitó el conflicto militar directo con la Unión Soviética. Las dos superpotencias compitieron en la Carrera Espacial, que culminó en el vuelo espacial de 1969 que llevó a los humanos a la Luna por primera vez. El final de la Guerra Fría y la disolución de la URSS la dejaron como la única superpotencia internacional. En el siglo XXI, los atentados del 11-S dieron lugar a la denominada guerra contra el terrorismo, y el ascenso de China y el regreso de Rusia conducen una Nueva Guerra Fría, mientras que eventos como la crisis económica de 2008, la elección de Donald Trump como presidente o las protestas por la muerte de George Floyd provocaron tensiones sociales internas.

El país representa dos quintas partes del gasto militar mundial y es una fuerza económica, política y cultural, líder en el mundo.[17]​[18]​

En 1507, el cartógrafo alemán Martin Waldseemüller elaboró un planisferio en el que llamó a las tierras del hemisferio occidental «América», en honor al explorador y cartógrafo italiano Américo Vespucio.[19]​ Las antiguas colonias británicas utilizaron por primera vez el nombre del país moderno en la Declaración de Independencia, la «unánime declaración de los trece Estados Unidos de la América» adoptada por los «representantes de los Estados Unidos de América», 4 de julio de 1776.[20]​ El nombre actual se determinó el 15 de noviembre de 1777, cuando el Segundo Congreso Continental aprobó los Artículos de la Confederación, que estipulan, «El nombre de esta Confederación será “Los Estados Unidos de América”». «Columbia», un nombre una vez popular para los Estados Unidos, se deriva del nombre de Cristóbal Colón y aún permanece en el nombre del distrito de Columbia. Ocasionalmente se le llama, de forma incorrecta, «Estados Unidos de Norteamérica», derivando en una confusión en su gentilicio.

La manera estándar para referirse a un ciudadano de los Estados Unidos son los términos estadounidense (o estadunidense en Honduras y México).[12]​ También se utilizan gringo y yanqui, que pueden o no tener un matiz despectivo, según el uso, el contexto y el tono de la voz.[21]​ A veces se utiliza «norteamericano» como sinónimo de «estadounidense», pero se debe tener presente que la región de Norteamérica está formada por Canadá, Estados Unidos y la mayor parte de México, hasta el istmo de Tehuantepec. Debe evitarse el gentilicio «americano» para referirse exclusivamente a los naturales de los Estados Unidos[12]​ debido a que esta palabra engloba a todos los habitantes del continente americano. Por esta razón, el gentilicio recomendado es la palabra «estadounidense». Para escribir abreviadamente el nombre de este país suelen emplearse, de manera correcta, la abreviatura «EE. UU.» (Estados Unidos),[22]​ o la sigla «EUA» (Estados Unidos de América). Aunque frecuente, en español es incorrecto emplear la sigla inglesa «USA».[12]​

Comúnmente se piensa que los pueblos indígenas de los Estados Unidos continentales, incluyendo a los nativos de Alaska, emigraron desde Asia entre 12 000 y 40 000 años atrás.[23]​ Algunos, tales como la cultura misisipiana, desarrollaron una agricultura avanzada, grandes obras arquitectónicas y sociedades con un orden jerárquico. Después de que los europeos comenzaran a asentarse en América, millones de indígenas americanos murieron debido a las epidemias de enfermedades traídas desde Europa, como la viruela.[24]​

En 1492, el explorador Cristóbal Colón, patrocinado por la Corona Española, llegó desde Europa hasta varias islas del Caribe, realizando el primer contacto con los pueblos indígenas. El 2 de abril de 1513, el conquistador español Juan Ponce de León desembarcó en lo que llamó La Florida, siendo la primera llegada europea documentada en el territorio estadounidense. Los asentamientos españoles en la región fueron seguidos por otros en el actual suroeste de Estados Unidos. Los comerciantes de pieles franceses se establecieron en Nueva Francia, alrededor de la zona de los Grandes Lagos; finalmente Francia reclamaría gran parte del interior de Estados Unidos, hasta la costa del golfo de México. Los primeros asentamientos ingleses exitosos fueron la colonia de Virginia en Jamestown en 1607 y la colonia de Plymouth fundada por peregrinos en 1620. En 1628, el establecimiento de la provincia de la bahía de Massachusetts dio lugar a una nueva ola de inmigración: para 1634, Nueva Inglaterra estaba habitada por cerca de 10 000 puritanos. Entre la década de 1610 y la guerra de independencia, cerca de 50 000 convictos fueron enviados desde el Viejo Continente hacia las colonias.[25]​ Desde 1614, los neerlandeses se establecieron a lo largo del río Hudson inferior, fundando Nueva Ámsterdam en la isla de Manhattan.

En 1674, los Países Bajos cedieron su territorio a Inglaterra y la provincia de los Nuevos Países Bajos fue renombrada con el nombre de Nueva York, convertida en la ciudad más importante de los Estados Unidos desde mediados del siglo XIX. Muchos inmigrantes recién llegados, especialmente en el sur, fueron contratados como criados, de tal modo que cerca de dos tercios de todos los inmigrantes que llegaron a Virginia entre 1630 y 1680 trabajaban como sirvientes.[26]​ Para finales de ese siglo, los esclavos africanos se convirtieron en la principal fuente de mano de obra en condiciones de servidumbre. Con la división de las Carolinas en 1729 y la colonización de Georgia en 1732, se establecieron las Trece Colonias británicas, que finalmente se convertirían en los Estados Unidos de América. Todas contaban con un gobierno local electo, apegado al republicanismo, además de que se legalizó el comercio de esclavos. Con altas tasas de nacimiento, bajas tasas de mortalidad y la constante inmigración, la población colonial creció rápidamente. El movimiento cristiano revivalista de las décadas de 1730 y 1740, conocido como «el Gran Despertar», alimentó el interés en temas como la religión y la libertad de culto. En la Guerra Franco-India, las fuerzas británicas le arrebataron Canadá a Francia, pero la población de habla francesa permaneció políticamente aislada de las colonias del sur. Sin contar a los nativos americanos (popularmente conocidos como «indios») que finalmente fueron desplazados en contra de su voluntad, en 1770 las Trece colonias tenían una población de 2,6 millones de habitantes, alrededor de una tercera parte de la del Reino Unido, aunque casi uno de cada cinco estadounidenses era un esclavo negro.[27]​ Sin embargo, los colonos estadounidenses no tenían ninguna representación en el Parlamento del Reino Unido.

Las tensiones entre los colonos y los británicos durante las décadas de 1760 y 1770 condujeron a la Guerra de Independencia, que se extendió desde 1775 hasta 1781. El 14 de junio de 1775, el Congreso Continental, reunido en Filadelfia, estableció un Ejército Continental bajo el mando de George Washington. Proclamando que «todos los hombres nacen iguales» y dotados de «ciertos derechos inalienables», el Congreso aprobó la Declaración de Independencia, redactada en gran parte por Thomas Jefferson, el 4 de julio de 1776.[28]​ Anualmente, en esta fecha se celebra el Día de la Independencia de los Estados Unidos. Así, Estados Unidos se convirtió en la primera nación americana en declarar su independencia. En 1777, los artículos de la Confederación establecieron un débil gobierno confederal, que operó hasta 1789.

Después de la derrota británica por las fuerzas estadounidenses, asistidas por franceses y españoles, el Reino Unido reconoció su independencia y soberanía sobre el territorio al este del río Misisipi. Una convención constitucional fue organizada en 1787 por aquellos que deseaban establecer un gobierno nacional fuerte. La Constitución de los Estados Unidos fue ratificada en 1788 y un año más tarde, George Washington se convirtió en el primer presidente. La Carta de Derechos, que prohibía la restricción federal de los derechos humanos y garantizaba una serie de medidas para su protección jurídica, fue adoptada en 1791.[29]​

Con la nueva autonomía, las actitudes hacia la esclavitud fueron cambiando; una cláusula en la Constitución protegió el comercio de esclavos hasta 1808. Los estados del norte abolieron la esclavitud entre 1780 y 1804, dejando a los estados esclavistas del sur como defensores de la "institución peculiar". El "Segundo Gran Despertar", que comenzó alrededor de 1800, convirtió a las Iglesias evangélicas en una de las principales fuerzas detrás de varios de los movimientos reformistas de la época, incluyendo el abolicionismo.[30]​

En 1803, la compra de la Luisiana a Francia durante el mandato del presidente Thomas Jefferson, casi duplicó el tamaño de la nación,[31]​ al mismo tiempo que la guerra anglo-estadounidense de 1812 fortaleció aún más el nacionalismo entre la población. Derrotado por el Reino Unido y tras su segunda y definitiva abdicación al trono francés en 1815, Napoleón Bonaparte intentó escapar a Estados Unidos, entonces rival del Reino Unido, pero fue capturado por las fuerzas británicas y deportado a Santa Helena. En 1819, una serie de incursiones militares en Florida obligó a España a ceder este y otros territorios de la costa del golfo.[31]​ El sendero de lágrimas en la década de 1830 ejemplifica la política de Remoción India que despojó a varios pueblos indígenas de sus tierras. Estados Unidos se anexó la República de Texas en 1845, época durante la cual el concepto del Destino Manifiesto se popularizó.[32]​ En 1846, la firma del Tratado de Oregón con el Reino Unido, le otorgó al país los actuales territorios del noroeste.[31]​ Dos años más tarde, la victoria en la guerra contra México dio lugar a la cesión de California y la mayor parte del suroeste actual.[31]​ La fiebre del oro de 1848 y 1849 estimuló aún más la migración hacia el oeste y los nuevos ferrocarriles facilitaron la reubicación de los colonos y el aumento de los conflictos con los nativos americanos. Durante medio siglo, hasta 40 millones de bisontes americanos fueron sacrificados por sus pieles y carne para facilitar la propagación de los ferrocarriles. La pérdida de los búfalos, una fuente principal de alimento para los indígenas de las llanuras, fue un golpe mortal para muchas culturas nativas.[33]​

Las tensiones entre estados pro-esclavistas y los abolicionistas, junto al aumento de los desacuerdos en la relación entre el gobierno federal y estatal, provocaron conflictos violentos por causa de la expansión de la esclavitud hacia los nuevos territorios. Abraham Lincoln, candidato del Partido Republicano y un gran abolicionista, fue elegido presidente en 1860. Antes de que tomase posesión de su cargo, seis estados esclavistas declararon su secesión de la Unión, formando los Estados Confederados de América, si bien otros estados también esclavistas permanecieron leales a la Unión.[34]​ El gobierno federal declaró que la secesión era ilegal y pronto se produjo el ataque por parte de los secesionistas a Fort Sumter, iniciándose así la guerra civil estadounidense el 12 de abril de 1861.[35]​

Tras la victoria de la Unión el 9 de abril de 1865, se añadieron tres enmiendas a la constitución para garantizar la libertad de los casi 4 millones de afroamericanos que habían sido esclavos, convirtiéndolos en ciudadanos y dándoles el derecho de voto.[36]​ La guerra y su resolución dio lugar a un aumento sustancial de las competencias del gobierno federal.[37]​

Después del asesinato de Abraham Lincoln, tuvo lugar la época conocida como la Reconstrucción, en la cual se desarrollaron políticas encaminadas a la reintegración y la reconstrucción de los estados sureños garantizando al mismo tiempo los derechos de los nuevos esclavos liberados. Las controvertidas elecciones presidenciales de 1876 se resolvieron mediante el Compromiso de 1877, por el cual los demócratas sureños reconocieron como presidente a Rutherford B. Hayes a cambio de que este retirara las tropas que aún permanecían desplegadas en Luisiana, Carolina del Sur y Florida. A partir de 1876 empiezan a aplicarse las llamadas leyes de Jim Crow, una política de apartheid que perduraría hasta 1965.[38]​ La Masacre de Wounded Knee en 1890 fue el último conflicto armado en la larga guerra con los pueblos originarios.[39]​

En el norte, la urbanización sin precedentes y una afluencia de inmigrantes aceleró la industrialización del país. La ola de la inmigración, que duró hasta 1929, proporcionó mano de obra para los negocios, transformado a su vez la cultura. La alta protección arancelaria, la creación de infraestructuras nacionales y los nuevos reglamentos bancarios alentaron el crecimiento industrial. En 1867, se produce la compra de Alaska a Rusia, completando la expansión continental del país.[31]​ La masacre de Wounded Knee en 1890 fue el último gran conflicto armado contra los nativos indios americanos. En 1893, la monarquía indígena del Reino de Hawái fue derrocada en un golpe de estado liderado por ciudadanos estadounidenses; el archipiélago fue anexado al país en 1898.[31]​ La victoria en la Guerra hispano-estadounidense ese mismo año, demostró que Estados Unidos era una potencia mundial y dio lugar a la anexión de Puerto Rico y las Filipinas.[40]​ Filipinas accedió a la independencia en 1946, mientras que Puerto Rico continúa siendo un Estado libre asociado.

Al estallar la Primera Guerra Mundial en Europa en 1914, Estados Unidos se declaró neutral. Posteriormente, los estadounidenses se solidarizaron con los británicos y franceses, a pesar de que muchos ciudadanos, sobre todo los originarios de Irlanda y Alemania, se opusieron a la intervención.[41]​ En 1917 se sumaron a los Aliados, contribuyendo a la derrota de las Potencias Centrales. Reacio a participar en asuntos europeos, el Senado no ratificó el Tratado de Versalles (1919), que estableció la Sociedad de Naciones, aplicando una política de unilateralismo, que rayaba en el aislacionismo.[42]​ En 1920, el movimiento de los derechos de la mujer ganó la aprobación de una enmienda constitucional para otorgar a las mujeres el sufragio.[29]​

Durante la mayor parte de la década de 1920, el país gozó de un período de prosperidad, disminuyendo el desequilibrio de la balanza de pagos mientras crecían las ganancias de las granjas industriales. Este período, conocido como los felices años veinte, culminó en la crisis de 1929 que desencadenó la Gran Depresión. Después de su elección como presidente en 1932, Franklin D. Roosevelt respondió con el New Deal (nuevo trato), una serie de políticas que aumentaron la intervención del gobierno en la economía.[43]​ De 1920 a 1933 se estableció una ley seca conocida como La prohibición.[44]​ La Dust Bowl (cuenca de polvo) de mediados de la década de 1930 dejó varias comunidades de agricultores empobrecidos y estimuló una nueva ola de migración hacia la costa occidental.[45]​

Estados Unidos, oficialmente neutral durante las primeras etapas de la Segunda Guerra Mundial, inició el suministro de provisiones a los Aliados en marzo de 1941, a través del Programa de Préstamo y Arriendo. El 7 de diciembre de 1941, el país se unió a la lucha de los Aliados contra las Potencias del Eje, después del ataque japonés a Pearl Harbor. La Segunda Guerra Mundial impulsó la economía mediante el suministro de capital de inversión y puestos de trabajo, haciendo que muchas mujeres entraran en el mercado laboral. De los principales combatientes, Estados Unidos fue la única nación que se enriqueció a causa de la guerra.[46]​ Las conferencias en Bretton Woods y Yalta crearon un nuevo sistema de organización internacional que colocó al país y a la Unión Soviética en el centro de los asuntos mundiales. En 1945, cuando llegó el fin de la Segunda Guerra Mundial en Europa, una conferencia internacional celebrada en San Francisco redactó la Carta de las Naciones Unidas, que entró en vigor después de la guerra.[47]​ Después de haber desarrollado la primera arma nuclear, el gobierno decidió utilizarla en las ciudades japonesas de Hiroshima y Nagasaki en agosto de ese mismo año. Japón se rindió el 2 de septiembre, poniendo fin a la guerra.[48]​

En la Guerra Fría, Estados Unidos y la Unión Soviética compitieron tras la Segunda Guerra Mundial, dominando los asuntos militares de Europa a través de la OTAN y del Pacto de Varsovia. El primero promovió la democracia liberal y el capitalismo, mientras que el segundo extendía el comunismo y una economía planificada por el gobierno. Ambos apoyaron varias dictaduras y participaron en guerras subsidiarias. Entre 1950 y 1953, las tropas estadounidenses combatieron a las fuerzas comunistas chinas en la guerra de Corea.[49]​ Desde la ruptura con la URSS y el inicio de la Guerra Fría y hasta 1957, dentro de Estados Unidos se desarrolló el macarthismo, también llamado Segundo Temor rojo, en el que el Estado desató una ola de represión política y una campaña de miedo contra aquellas personas comunistas o simplemente sospechosas de serlo, que algunos autores señalan como propio de un Estado totalitario. Cientos de personas fueron detenidas, incluidas decenas de celebridades, y entre 10 000 y 12 000 personas perdieron sus puestos de trabajo.[50]​ La persecución finalizó cuando los tribunales la declararon inconstitucional.[51]​

En 1961, el lanzamiento soviético de la primera nave espacial tripulada provocó que el presidente John F. Kennedy propusiera al país ser los primeros en enviar "un hombre a la Luna", hecho logrado en 1969.[52]​ Kennedy también enfrentó un tenso conflicto nuclear con las fuerzas soviéticas en Cuba, al tiempo que la economía crecía y se expandía de manera constante. Un creciente movimiento por los derechos civiles, representado y liderado por afroamericanos como Rosa Parks, Martin Luther King y James Bevel, utilizó la no violencia para hacer frente a la segregación y la discriminación.[53]​ Después del asesinato de Kennedy en 1963, la Ley de Derechos Civiles de 1964 y la Ley de Derechos Electorales de 1965 se aprobaron durante el mandato del presidente Lyndon B. Johnson. Johnson y su sucesor, Richard Nixon, llevaron una guerra civil subsidiaria en el sudeste asiático a la infructuosa guerra de Vietnam.[49]​ Un movimiento contracultural generalizado creció, impulsado por la oposición a la guerra, el nacionalismo negro y la revolución sexual. También surgió una nueva ola de movimientos feministas, liderados por Betty Friedan, Gloria Steinem y otras mujeres que buscaban la equidad política, social y económica.

En 1974, como resultado del escándalo Watergate, Nixon se convirtió en el primer presidente en renunciar, para evitar ser destituido por cargos como obstrucción a la justicia y abuso de poder; fue sucedido por el Vicepresidente Gerald Ford.[54]​ La administración de Jimmy Carter en la segunda mitad de la década de 1970 (1977-1981) estuvo marcada por la estanflación y la crisis de los rehenes en Irán. La elección de Ronald Reagan como presidente en 1980 anunció un cambio en la política estadounidense, que se reflejó en reformas importantes en los impuestos y gastos fiscales. Su segundo mandato trajo consigo el escándalo Irán-Contra y el significativo progreso diplomático con la Unión Soviética. El posterior colapso soviético terminó la Guerra Fría.

Bajo el mandato del presidente George H. W. Bush, el país tomó un papel de liderazgo hegemónico mundial, como en la guerra del Golfo (1991). La expansión económica más larga en la historia moderna de Estados Unidos, desde marzo de 1991 hasta marzo de 2001, abarcó la administración de Bill Clinton y la burbuja punto com.[55]​ Una demanda civil y un escándalo sexual llevó al impeachment de Clinton en 1998, aunque logró terminar su periodo. Las elecciones presidenciales de 2000, de las más competidas en la historia estadounidense, fueron resueltas por una decisión de la Corte Suprema. George W. Bush, hijo de George H. W. Bush, se convirtió en presidente a pesar de que obtuvo menos votos que su rival, Al Gore.[56]​ Esta situación, que ha ocurrido tres veces en la historia, se debe al sistema de voto estadounidense (colegiado).

El 11 de septiembre de 2001, los terroristas del grupo Al-Qaeda atacaron las Torres Gemelas del World Trade Center de la ciudad de Nueva York (que fueron destruidas) y El Pentágono cerca de Washington D. C., en una serie de atentados que acabó con la vida de casi 3000 personas. En respuesta, la administración de Bush lanzó la "guerra contra el terrorismo". A finales de 2001, las fuerzas estadounidenses invadieron Afganistán, derrocaron al gobierno talibán y destruyeron los campos de entrenamiento de Al-Qaeda. Los insurgentes talibanes continúan luchando una guerra de guerrillas. En 2002 Bush comenzó a presionar para que se llevara a cabo un cambio de régimen en Irak.[57]​[58]​ Con la falta de apoyo de la OTAN y sin un mandato explícito de la ONU para una intervención militar, Bush organizó la coalición de la voluntad; las fuerzas de la coalición rápidamente invadieron Irak en 2003 y derrocaron al dictador Saddam Hussein. Al año siguiente, Bush fue reelegido como el presidente más votado en unas elecciones.

En 2005, el huracán Katrina, que terminaría siendo el desastre natural más mortífero en la historia nacional, causó una destrucción severa a lo largo de la costa del Golfo: la ciudad de Nueva Orleans quedó devastada,[59]​ con 1833 muertos.

El 4 de noviembre de 2008, en medio de una recesión económica mundial, Barack Obama fue elegido presidente, habiendo sido el primer afroamericano en ocupar el cargo. En mayo de 2011, las fuerzas especiales estadounidenses lograron matar a Osama bin Laden, escondido en Pakistán. Al año siguiente, Barack Obama fue reelegido. Bajo su segundo mandato impartió la guerra contra el Estado Islámico y se restablecieron las relaciones diplomáticas con Cuba.

El 8 de noviembre de 2016, el magnate Donald Trump por el Partido Republicano venció a la ex primera dama Hillary Clinton para la presidencia en unas elecciones insólitas[60]​ y cuyos planes han sido descritos por analistas políticos como populistas, proteccionistas y nacionalistas, jurando el cargo el 20 de enero de 2017.

Las masacres en Orlando del 12 de junio de 2016 en la discoteca gay Pulse (51 muertos)[61]​ y en Las Vegas el 1 de octubre de 2017 (60) figuran como las mayores matanzas del país desde los atentados del 11-S.

El 7 de noviembre de 2020, Joe Biden gana las elecciones presidenciales, venciendo a Donald Trump quien no consiguió la reelección presidencial. Es la segunda vez que un presidente estadounidense no alcanza la reelección después de George H. W. Bush. Biden juró al cargo el 20 de enero de 2021. El 6 de enero de 2021, los partidarios del presidente saliente Trump irrumpieron en el Capitolio de los Estados Unidos en un esfuerzo infructuoso por interrumpir el recuento de votos del Colegio Electoral presidencial.[62]​

Estados Unidos es la federación más antigua del mundo. Es una república constitucional, democrática y representativa, "en la que el mandato de la mayoría es regulado por los derechos de las minorías, protegidos por la ley".[63]​ El gobierno está regulado por un sistema de controles y equilibrios, definidos por la Constitución, que sirve como el documento legal supremo del país.[64]​ En el sistema federalista estadounidense, los ciudadanos están generalmente sujetos a tres niveles de gobierno: federal, estatal y local; los deberes del gobierno local comúnmente se dividen entre los gobiernos de los condados y municipios. En casi todos los casos, los funcionarios del poder ejecutivo y legislativo son elegidos por sufragio directo de los ciudadanos del distrito.

El gobierno federal se divide en tres ramas de poder:[64]​

La Cámara de Representantes tiene 435 miembros electos, cada uno representando a un distrito del Congreso para un mandato de dos años.[64]​ Los lugares dentro de la cámara se distribuyen entre los estados según su población cada diez años. Según el censo de 2000, siete estados tienen el mínimo de un representante, mientras que California, el estado más poblado, tiene cincuenta y tres. El Senado tiene 100 miembros, ya que cada estado cuenta con dos senadores, elegidos para un término de seis años; un tercio de los escaños en el Senado son electos cada dos años.[64]​ La Corte Suprema, liderada por el jefe de justicia, tiene nueve miembros, que sirven de manera permanente.[64]​

El presidente cumple mandato por un término de cuatro años y podrá ser reelegido para el cargo no más de una vez. El presidente no es elegido por sufragio directo, sino por un sistema indirecto de colegios electorales, en el que los votos determinantes son prorrateados por estado.[64]​ Un estado solo puede brindar determinada cantidad de votos según el número de congresistas que tenga dentro del poder legislativo: senadores (dos por cada estado) y representantes (que varía según la población de cada estado); dando un total de 538 miembros. El sistema bipartidista del país permite que un candidato a la presidencia, ya sea republicano o demócrata, solo necesite 270 votos para asegurar la victoria.[65]​

Los gobiernos de los cincuenta estados están estructurados de manera más o menos similar, aunque Nebraska es el único que tiene una legislatura unicameral.[66]​ El gobernador (jefe ejecutivo) de cada estado es elegido por sufragio directo. Algunos jueces de estado y funcionarios de gabinete son designados por los gobernadores de los respectivos estados, mientras que otros son elegidos por voto popular.

Todas las leyes y los procedimientos gubernamentales están sujetas a revisión judicial, y se anula cualquier ley que esté en contra de la Constitución. El texto original de la Constitución establece la estructura y responsabilidades del gobierno federal y su relación con los gobiernos estados.[64]​ El Artículo I protege el derecho al "gran recurso" de habeas corpus[29]​ y el Artículo III garantiza el derecho a un juicio con jurado en todos los casos penales.[29]​ Las enmiendas a la Constitución requieren la aprobación de tres cuartas partes de los estados. La Constitución ha sido enmendada veintisiete veces; las primeras diez enmiendas, que componen la Carta de Derechos y la decimocuarta enmienda forman la base central de las garantías individuales.[29]​

Estados Unidos ejerce una influencia económica, política y militar a nivel mundial. Es un miembro permanente del Consejo de Seguridad de Naciones Unidas, además de que la Sede de la Organización de las Naciones Unidas se encuentra en la ciudad de Nueva York. También es miembro del G8, el G-20 y la Organización para la Cooperación y el Desarrollo Económico. La inmensa mayoría de los países tienen una embajada o un consulado en Washington D. C. u otra ciudad importante del país. A su vez, casi todos los países del mundo cuentan con una misión diplomática estadounidense.[67]​ Sin embargo, Corea del Norte, Bután, Sudán y la República de China (Taiwán) no tienen relaciones diplomáticas formales con la nación.

También goza de fuertes lazos con el Reino Unido, Canadá, Australia, Nueva Zelanda, Japón, Corea del Sur e Israel. Trabaja en estrecha colaboración con sus colegas de la Organización del Tratado del Atlántico Norte sobre cuestiones militares y de seguridad, y con sus vecinos a través de la Organización de los Estados Americanos y de tratados internacionales como el acuerdo trilateral del Tratado de Libre Comercio de América del Norte con Canadá y México. En 2012, Estados Unidos gastó un neto de 30 460 millones de dólares en ayuda oficial al desarrollo, la mayor cantidad en el mundo, aunque en términos de porcentaje del Producto Interior Bruto (PIB), su contribución de 0,19 % ocupó uno de los últimos lugares entre las veintitrés naciones donantes. En contraste, las empresas privadas estadounidenses son relativamente más generosas.[68]​[69]​

El presidente ostenta el título de comandante en jefe de las fuerzas armadas de la nación y nombra a sus líderes: el Secretario de Defensa y la Junta de Jefes de Estado Mayor. El Departamento de Defensa administra las fuerzas armadas, incluyendo el Ejército, la Armada, el Cuerpo de Marines y la Fuerza Aérea. La Guardia Costera es administrada por el Departamento de Seguridad Nacional en tiempos de paz y por el Departamento de la Armada en tiempo de guerra. En 2008, las fuerzas armadas contaban con 1,4 millones de miembros activos. Las reservas y la Guardia Nacional elevan el número total de tropas a 2,3 millones. El Departamento de Defensa también emplea a aproximadamente 700 000 civiles, sin incluir a los contratistas.[70]​

El servicio militar es voluntario, aunque en tiempos de guerra el Estado puede imponer el reclutamiento a través de un sistema de servicio selectivo, el llamado «Selective Service System».[71]​ Como la gran potencia militar del mundo, las fuerzas armadas de Estados Unidos poseen las más avanzadas máquinas de guerra. Cuenta con miles de aviones, cientos de embarcaciones, carros de combate, misiles intercontinentales (ICBM), miles de ojivas nucleares y tiene diez portaaviones activos en la Armada; además, posee fuerzas aeronavales de carácter permanente en las costas del océano Atlántico —Fleet Forces Command, Pacífico —Flota del Pacífico— y el Golfo Pérsico —Quinta Flota.

La instalación progresiva de las fuerzas armadas estadounidenses a gran escala empezó con la división y ocupación de Alemania en 1945, se amplió con la guerra fría en sus países de influencia, y continúa vigente.

El ejército opera alrededor de 870 bases e instalaciones en el extranjero[72]​ y mantiene guarniciones de más de 100 militares activos en 28 países distintos.[73]​ El alcance de esta presencia militar global ha llevado a algunos autores a describir al país como si mantuviera un «imperio de bases».[74]​[75]​

Los gastos militares en 2020, fueron de 740 500 millones de dólares,[76]​ aproximadamente el 37,5 % del gasto militar mundial[77]​ y más altos que los gastos juntos de los siguientes catorce países con los ejércitos más grandes. El gasto per cápita fue de 1967 dólares, alrededor de nueve veces el promedio mundial; ocupando el 4,4 % del PIB[78]​ y el 18 % del presupuesto federal.[79]​ El presupuesto base del Departamento de Defensa para 2010 —fijado en 533 800 millones de dólares— aumentó un 4 % en 2009 y un 80 % más que en 2001; se destinarán 130 000 millones de dólares adicionales para las campañas militares en Irak y Afganistán.[80]​ En mayo de 2010 había 94 000 soldados estadounidenses desplegados en Afganistán y 92 000 en Irak.[81]​ Para junio de 2010, el ejército estadounidense había sufrido 4400 bajas durante la guerra en Irak[82]​ y 1087 durante la guerra en Afganistán.[83]​

Estados Unidos cuenta, según las informaciones disponibles, con los mayores servicios de inteligencia del mundo, que se agrupan y coordinan alrededor de la llamada Comunidad de Inteligencia —United States Intelligence Community (IC). Diecisiete agencias federales de inteligencia forman esta comunidad; estas agencias generalmente están subordinadas al Departamento de Defensa o al Departamento de Justicia, mientras que otras tienen la consideración de agencia independiente federal, por lo que solo deben rendir cuentas al Director Nacional de Inteligencia y al Presidente de los Estados Unidos. Las agencias de la comunidad —tanto militares como civiles— se dedican a actividades de inteligencia relacionadas con asuntos exteriores y la seguridad nacional y cuentan con cientos de miles de empleados —107 035 a mediados de 2013, último dato conocido—[84]​ y unos presupuestos confidenciales que solo en 2013 sumaron 52 600 millones de dólares.[84]​ Una de las agencias que la componen es la célebre Agencia Central de Inteligencia (CIA).[85]​

En 2010, según informaciones de The Washington Post, 1271 organizaciones gubernamentales, 1931 empresas privadas y 854 000 personas con acceso a espacios e información de alto secreto se dedicaban en Estados Unidos a labores de contraterrorismo, inteligencia y seguridad nacional.[86]​

En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Estados Unidos de América ha firmado o ratificado:

Estados Unidos es una unión federal de cincuenta estados. Los trece estados originales fueron los sucesores de las Trece colonias que se rebelaron contra el Imperio británico. Poco después de la independencia, se crearon tres nuevos estados a partir de otros ya existentes: Kentucky de Virginia; Tennessee de Carolina del Norte y Maine de Massachusetts. La mayoría de los otros estados fueron creados a partir de territorios obtenidos a través de la guerra o la compra por parte del gobierno. Vermont, Texas y Hawái son la excepción: cada uno de ellos fue una república independiente antes de integrarse a la Unión. Durante la guerra de Secesión, Virginia Occidental se separó de Virginia. El estado más reciente es Hawái, que logró el reconocimiento como estado el 21 de agosto de 1959. Los estados no tienen el derecho de separarse de la Unión.

Los estados componen gran parte del territorio estadounidense; las otras dos áreas que se consideran parte integrante del país es el distrito de Columbia, el distrito federal donde se encuentra la capital Washington D. C., y el Atolón Palmyra, un territorio deshabitado pero incorporado, ubicado en el océano Pacífico. Estados Unidos también posee cinco grandes territorios de ultramar: Puerto Rico y las Islas Vírgenes de los Estados Unidos en el Caribe y Samoa Americana, Guam y las Islas Marianas del Norte en el Pacífico. Aquellos que nacieron en esos territorios (excepto en la Samoa Americana) poseen la ciudadanía estadounidense. Los ciudadanos estadounidenses que residen en los territorios tienen muchos de los derechos y responsabilidades de los ciudadanos que residen en los estados; sin embargo, generalmente están exentos del pago de impuestos federales, no pueden votar en las elecciones presidenciales y solo tienen representación en calidad de observadores en el Congreso.[93]​



La superficie de los Estados Unidos continentales es de aproximadamente 7 700 000 km² (kilómetros cuadrados). Alaska, que está separada de los Estados Unidos continentales por Canadá, es el estado más grande del país, con 1 500 000 km². Hawái, ocupa un archipiélago ubicado en el Pacífico central, al suroeste de América del Norte, que abarca poco más de 16 000 km².[94]​ Después de Rusia y Canadá, es el tercer o cuarto país más grande del mundo por área total (tierra y agua), clasificado justo por encima o por debajo de China. La lista varía dependiendo de si se consideran los territorios en disputa entre China y la India y de cómo se calcula el tamaño total de los Estados Unidos: The World Factbook de la CIA considera 9 833 517 km²,[5]​ la División de Estadísticas de las Naciones Unidas calcula 9 629 091 km²,[95]​ y la Enciclopedia Británica estipula 9 522 055 km².[96]​ Incluyendo solo la superficie de la tierra, Estados Unidos es tercero en tamaño detrás de Rusia y Canadá, justo por delante de China.[97]​

El territorio nacional cuenta con múltiples formas de relieve y accidentes geográficos. A medida que se avanza tierra adentro, la llanura costera del litoral Atlántico da lugar al bosque caducifolio y a la meseta del Piedmont. Los Apalaches separan la costa oriental de los Grandes Lagos de las praderas del Medio Oeste. El río Misisipi-Misuri, el cuarto sistema fluvial más largo del mundo, corre de norte a sur a través del centro del país. La pradera llana y fértil de las Grandes Llanuras se extiende hacia el oeste, hasta que es interrumpida por una región de tierras altas en el sureste. Las Montañas Rocosas, en el borde occidental de las Grandes Llanuras, atraviesan de norte a sur todo el país, llegando a altitudes superiores a los 4300 m s. n. m. (metros sobre el nivel del mar) en Colorado. Más hacia el oeste se encuentra la Gran Cuenca y los desiertos, tales como el desierto de Mojave, de Sonora y de Chihuahua. Las montañas de la Sierra Nevada y la cordillera de las Cascadas se encuentran cerca de la costa del Pacífico. Con sus 6194 m s. n. m., el monte McKinley en Alaska es el punto más alto del país. Los volcanes activos son comunes a lo largo de Alaska y las islas Aleutianas, además de que Hawái consta de solo islas volcánicas. El supervolcán ubicado debajo del parque nacional Yellowstone en las montañas Rocosas, es la forma volcánica más grande del continente.[98]​

Por su gran tamaño y variedad geográfica, el país cuenta con la mayoría de los tipos de clima. Al este del meridiano 100, el clima varía de continental húmedo en el norte a húmedo subtropical en el sur. El extremo sur de la Florida y las islas de Hawái tienen un clima tropical. Las Grandes Llanuras al oeste del meridiano 100 son semiáridas, mientras que gran parte de las montañas occidentales poseen un clima alpino. El clima es árido en la Gran Cuenca y en los desiertos del suroeste, es mediterráneo en la costa de California y oceánico en la costa sur de Alaska, Oregón y Washington. La mayor parte del territorio alaskeño tiene un clima subártico o polar. Los fenómenos meteorológicos extremos no son raros —los estados ribereños del golfo de México son propensos a huracanes y la mayoría de los tornados del mundo se desarrollan dentro del país, principalmente en la zona de Tornado Alley, en el Medio Oeste—.[99]​

Clima oceanico CfbSeattle

Clima subtropical húmedoOrlandoFlorida

Clima ecuatorial lluviosoJupiter

Clima de tundraParque nacional y reserva Denali

Clima continental templadoNueva York

Clima mediterráneo típico CsaCalifornia

Clima arido subtropicalLas Vegas

Bosque templado caducifolioVirginia

Desierto de DunasDesierto de Mojave

Tropical Monzónico AmMiami

Estados Unidos es considerado un país megadiverso: unas 17 000 especies de plantas vasculares viven en los Estados Unidos contiguos y Alaska y más de 1800 especies de plantas con flores se pueden encontrar tan solo en Hawái, pocas de las cuales crecen en el continente.[100]​ El país es hogar de más de 400 especies de mamíferos, 750 especies de aves y 500 especies de reptiles y anfibios.[101]​ Aquí también se han descubierto más de 91 000 diferentes clases de insectos.[102]​

La Ley de Especies en Peligro aprobada en 1973 protege a las especies amenazadas y en peligro de extinción y sus hábitats, que son supervisados por el Servicio de Pesca y Vida Silvestre de los Estados Unidos. En total, el gobierno federal posee el 28,8 % de la superficie total del país.[103]​ La mayor parte de este porcentaje está conformado por los cincuenta y ocho parques nacionales y cientos de otras áreas naturales protegidas administradas por las autoridades federales y estatales.[104]​ Del resto de las tierras del gobierno, algunas son alquiladas para la extracción de petróleo y gas natural, para la minería, agricultura o ganadería; solo el 2,4 % se utiliza para fines militares.[103]​

La economía de los Estados Unidos es una economía mixta capitalista, que se caracteriza por los abundantes recursos naturales, una infraestructura desarrollada y una alta productividad.[105]​ De acuerdo al Fondo Monetario Internacional, su PIB de 15,7 billones USD constituye el 24 % del Producto Mundial Bruto y cerca del 21 % del mismo en términos de paridad de poder adquisitivo (PPA).[8]​ Este es el PIB más grande en el mundo, aunque en 2008 era un 5 % menor que el PIB (PPA) de la Unión Europea. El país tiene el undécimo PIB per cápita nominal y el sexto PIB (PPA) per cápita más altos del mundo.[8]​ Además, el país está en segundo lugar del Índice de Competitividad Global de 2010.[106]​

Estados Unidos es el importador de bienes más grande a nivel internacional y el tercero en términos de exportaciones, aunque las exportaciones per cápita son relativamente bajas para ser un país desarrollado. En 2008, el total de la balanza comercial estadounidense era de 696 000 millones de dólares.[107]​ En 2009, los automóviles constituyeron los principales productos exportados e importados.[108]​ Canadá, China, México, Alemania y Japón son sus principales socios comerciales.[109]​ Ese último es el que tiene la mayor deuda pública con Estados Unidos, ya que a principios de 2010 superó la deuda de China con 34 200 millones de dólares.[110]​

En 2010, el sector privado constituía un estimado del 55,3 % de la economía de Estados Unidos, las actividades del gobierno federal sumaban el 24,1 % y la actividad de los gobiernos estatales y locales ocupaban el restante 20,6 %.[113]​ Pese a que la economía estadounidense es posindustrial, ya que el sector servicios contribuye con el 67,8 % del PIB, la nación continúa siendo una potencia industrial.[114]​ En el campo de negocios, la actividad líder por sus ingresos es el comercio al por mayor y al por menor; por ingresos netos es la industria,[115]​ siendo la industria química la más importante.[116]​ Estados Unidos es el tercer productor de petróleo más importante en el mundo, así como el mayor importador de este producto.[117]​[118]​[119]​ También es el productor número uno de energía eléctrica y de energía nuclear, así como gas natural licuado, azufre, fosfatos y sal. Mientras que la agricultura representa menos del 1 % del PIB,[114]​ el país es el mayor productor de maíz[120]​ y soya.[121]​ Toda esta producción contribuye a que la bolsa de Nueva York sea la más grande del mundo.[122]​ A su vez, las empresas estadounidenses de Coca-Cola, McDonalds y Microsoft son las marcas más reconocidas en el mundo.[123]​

En el tercer trimestre de 2009, la fuerza de trabajo estadounidense era de 154,4 millones de personas. De estos empleados, un 81 % poseía un empleo en el sector de los servicios. Con 22,4 millones de personas, el gobierno es el principal campo de empleo.[124]​ Aproximadamente el 12 % de los trabajadores están sindicalizados, en comparación con el 30 % de Europa occidental.[125]​ El Banco Mundial clasifica a los Estados Unidos en primer lugar en la facilidad de contratación y liquidación de los trabajadores.[126]​ Entre 1973 y 2003, el año laboral para un estadounidense promedio creció 199 horas.[127]​ En parte como consecuencia, el país sostiene la máxima productividad de mano de obra en el mundo. En 2008, también llegó al primer puesto en productividad por hora, superando a Noruega, Francia, Bélgica y Luxemburgo, que habían superado a los Estados Unidos la mayor parte de la década anterior.[128]​ Comparado con Europa, los impuestos corporativos y de propiedad son más altos, mientras que los impuestos al consumidor son más bajos.[129]​

Al ser un país desarrollado, Estados Unidos cuenta con una infraestructura de transportes avanzada: 6 465 799 km (kilómetros) de carreteras, 226 427 km de vías férreas, 15 095 aeropuertos y 41 009 km de vías fluviales.[131]​ La mayor parte de sus habitantes utilizan el automóvil como su principal medio de transporte. En 2003, había 759 automóviles por cada 1000 personas, en comparación con los 472 por cada 1000 habitantes de la Unión Europea.[132]​ Más del 40 % de los vehículos personales son camionetas, todoterrenos o camiones ligeros.[133]​ El adulto estadounidense promedio (incluyendo a conductores y no conductores) pasa diariamente 55 minutos en un automóvil, viajando una distancia de 47 km.[134]​

La industria aérea civil es propiedad privada, mientras que la mayoría de aeropuertos son de propiedad pública. Las tres aerolíneas más grandes en el mundo son de capital estadounidense: Southwest Airlines, American Airlines y Delta Air Lines.[135]​ De los treinta aeropuertos con mayor tránsito de pasajeros en el mundo, dieciséis están en el país, siendo el más concurrido de todos el Aeropuerto Internacional Hartsfield-Jackson en Atlanta.[136]​ Mientras que el transporte de mercancías por ferrocarril es muy importante, relativamente pocas personas utilizan este medio de transporte para viajar, dentro o entre las zonas urbanas.[137]​ Solamente el 9 % de las personas utilizan el transporte público para acudir al trabajo, un nivel muy bajo comparado con el 38,8 % de Europa.[138]​ También el uso de la bicicleta es mínimo, muy por debajo de los niveles europeos.[139]​

El consumo energético total del país es de 3873 billones kWh anuales, lo que equivale a un consumo per cápita de 7,8 toneladas de petróleo al año.[131]​ En 2005, un 40 % de esta energía provenía del petróleo, 23 % del carbón y 22 % del gas natural; el resto provenía de centrales nucleares y fuentes de energía renovable.[140]​ Estados Unidos es el mayor consumidor de petróleo y de gas natural: anualmente utiliza 19,5 millones de barriles de petróleo y 627 200 millones de metros cúbicos de gas natural.[118]​[141]​ Por otro lado, en el país se encuentran el 27 % de las reservas mundiales de carbón.[142]​ Por décadas, la energía nuclear ha jugado un papel limitado en la producción de energía, en comparación con la mayoría de los países desarrollados, debido en parte a la reacción pública después del accidente de Three Mile Island. Sin embargo, en 2007 el gobierno recibió múltiples peticiones para la construcción de nuevas centrales nucleares, lo que podría significar una disminución considerable en el consumo de combustibles fósiles[143]​ y un cambio en la política energética.

En el último censo decenal del país (2020), la cifra asciende a 331 449 281 de habitantes en abril de 2020 según la Oficina Nacional del Censo.[144]​ Esta cifra incluye un estimado de 11,2 millones de inmigrantes ilegales,[145]​ pero excluye la población de cinco territorios no incorporados (Puerto Rico, Islas Vírgenes de los Estados Unidos, Guam, Samoa americana e Islas Marianas del Norte). Esto la convierte en la tercera nación más poblada en el mundo, después de China y la India. Además, Estados Unidos es la única nación industrializada donde se prevé un aumento significativo en la población.[146]​ Con una tasa de natalidad de 13,82 bebés por cada 1000 habitantes (30 % por debajo de la media mundial), su tasa de crecimiento demográfico es de 0,98 %, significativamente más alto que los de Europa occidental, Japón y Corea del Sur.[147]​ En el año fiscal de 2009, 1,1 millones de inmigrantes obtuvieron la residencia legal.[148]​ México ha sido el principal país de origen de los nuevos residentes durante más de dos décadas; desde 1998, China, India y Filipinas también se han destacado en este sentido cada año.[149]​

Estados Unidos tiene una población muy diversa: treinta y un diferentes grupos étnicos cuentan con más de un millón de representantes.[150]​ Los blancos estadounidenses son el grupo étnico más grande; los germano-estadounidenses, los hiberno-estadounidenses y los angloamericanos constituyen tres de los cuatro grupos étnicos más numerosos del país.[150]​ Los afroamericanos son la «minoría» racial más importante y el tercer grupo étnico más grande.[150]​[151]​ Los asiáticos son la segunda "minoría" racial con mayor presencia; dentro de este grupo destacan los grupos de origen chino y filipino.[150]​ En 2008, la población incluía un estimado de 6 millones de personas con ascendencia indígena, ya sea de un pueblo amerindio (1,8 millones), alaskeño (3,1 millones), hawaiano (500 000) o de una isla del Pacífico (600 000).[151]​

El crecimiento de la población de origen latinoamericano es una importante tendencia demográfica. De acuerdo a la Oficina Nacional del Censo, los 46,9 millones de descendientes de latinos o hispanos[151]​ son un grupo heterogéneo que comparten una distinta «etnicidad», así 64 % de los hispanos son de ascendencia mexicana.[152]​ Entre 2000 y 2008, la población hispana aumentó 32 %, mientras que la población no hispana aumentó solo un 4,3 %.[151]​ Gran parte de este crecimiento es debido a la inmigración, por ejemplo, en 2007 el 12,6 % de la población estadounidense había nacido en el extranjero, de los cuales, el 54 % nacieron en América Latina.[153]​ La tasa de fecundidad también es un factor: la mujer hispana promedio da a luz a tres niños, mientras que las mujeres negras tienen 2,2 y las mujeres blancas 1,8.[146]​ Las minorías (definidas por la Oficina del Censo como todos aquellos que no son hispanos o blancos) constituyen el 34 % de la población y se prevé que constituirán la mayoría para el 2042.[154]​

Alrededor del 82 % de los estadounidenses viven en zonas urbanas (tal como las define la Oficina del Censo, estas áreas incluyen los suburbios);[131]​ cerca de la mitad residen en ciudades con una población superior a 50 000 habitantes.[155]​ En 2008, 273 localidades contaban con más de 100 000 personas, nueve ciudades tenían más de un millón de residentes y cuatro ciudades globales tenían más de 2 millones de residentes (Nueva York, Los Ángeles, Chicago y Houston).[156]​ Existen cincuenta y dos áreas metropolitanas con más de un millón de habitantes.[157]​ De las cincuenta áreas metropolitanas de más rápido crecimiento demográfico, cuarenta y siete se encuentran en el oeste y en el sur.[158]​ Las áreas metropolitanas de Dallas, Houston, Atlanta y Phoenix aumentaron su población en más de un millón de personas entre 2000 y 2008.[157]​

La educación pública estadounidense es operada por los gobiernos estatales y locales, regulados por el Departamento de Educación de los Estados Unidos. Es obligatorio que los niños asistan a la escuela desde los seis o siete años (por lo general, al jardín de niños o al primer grado de educación primaria) hasta que cumplen los dieciocho años (generalmente hasta cursar el duodécimo grado, el final de la escuela secundaria); algunos estados permiten a los estudiantes abandonar la escuela a los dieciséis o diecisiete años.[159]​ Aproximadamente el 12 % de los niños están inscritos en escuelas privadas, mientras que el 2 % recibe educación en el hogar.[160]​ Existen múltiples instituciones privadas y públicas de educación superior, así como de colleges comunitarios con las políticas de admisión abierta. De las personas mayores de veinticinco años, el 84,6 % se graduó de la escuela secundaria, un 52,6 % asistió a algún college, el 27,2 % obtuvo una licenciatura y el 9,6 % obtuvo un título de posgrado.[161]​ La tasa de alfabetización es de aproximadamente un 99 %.[131]​ La ONU le asigna al país un índice de educación de 0,97, el 12.º más alto en el mundo.[162]​ De acuerdo a la Unesco, Estados Unidos es el segundo país con más instituciones de educación superior en el mundo, con un total de 5758 y un promedio de más de 15 por cada estado.[163]​ El país también cuenta con el mayor número de estudiantes universitarios en el mundo, ascendiendo a 14 261 778, es decir, casi el 4,75 % de la población total.[164]​ Finalmente, aquí se encuentran algunas de las universidades más prestigiosas y de mayor fama en todo el mundo. Harvard, Yale, Columbia, Stanford y el Instituto Tecnológico de Massachusetts son consideradas como las mejores universidades por varias publicaciones.[165]​[166]​[167]​

El inglés es el idioma nacional de facto. Aunque no existe ningún idioma oficial a nivel federal, algunas leyes —como los Requisitos para la Naturalización— colocan al inglés como idioma obligatorio. En 2006, cerca de 224 millones, o sea, el 80 % de la población mayor de cinco años, hablaba únicamente el inglés en casa. El español, hablado por el 12 % de la población, es el segundo idioma más hablado, y el que más comúnmente se aprende como segunda lengua.[168]​[169]​ Algunas personas abogan por convertir el inglés en el idioma oficial, como lo es en al menos 28 estados.[170]​ El hawaiano y el inglés son los idiomas oficiales de Hawái.[171]​ Aunque carecen de un idioma oficial, Nuevo México tiene leyes que alientan el uso del inglés y el español, de la misma forma que Luisiana lo hace con el inglés y el francés cajún.[172]​ En otros estados, como en California, la publicación de ciertos documentos oficiales en español es obligatoria.[173]​[174]​ Los territorios insulares garantizan el reconocimiento oficial de los idiomas nativos, junto con el inglés: el samoano y el chamorro son reconocidos por Samoa Americana y Guam, respectivamente; el carolinio y el chamorro son reconocidos por las Islas Marianas del Norte, y el español es un idioma oficial de Puerto Rico.

Además de las lenguas mencionadas, se siguen hablando un número elevado de lenguas nativas en numerosas reservas por todo el país y pequeñas localidades. Cada una de estas lenguas cuenta con unos pocos de miles de hablantes, y la más importante demográficamente es el navajo, con casi 200 000 hablantes nativos.

Estados Unidos es oficialmente un Estado laico; la Primera Enmienda garantiza el libre ejercicio de la religión y prohíbe el establecimiento de cualquier gobierno religioso.[29]​ En un estudio de 2002, el 59 % de los estadounidenses aseguró que la religión desempeñaba un "papel muy importante en sus vidas", una cifra más elevada que la de cualquier otra nación desarrollada.[175]​ De acuerdo con una encuesta de 2007, un 78,4 % de los adultos se identificaron como cristianos,[176]​ registrándose una disminución desde 1990, cuando eran el 86,4 %.[177]​ Las denominaciones protestantes representaban el 51,3 %, mientras que la Iglesia católica con el 23,9 %, era la corriente religiosa más grande. El estudio clasifica a los evangélicos blancos, 26,3 % de la población, como la cohorte religiosa más grande del país;[176]​ otro estudio estima que los evangélicos de todas las razas conforman entre el 30 y 35 % de la población.[178]​ En 1990, el total de adeptos a religiones no cristianas eran el 3,3 %, para 2007 había crecido hasta un 4,7 %.[177]​ Las principales religiones no cristianas eran el judaísmo (1,7 %), el budismo (0,7 %), el islam (0,6 %), el hinduismo (0,4 %) y el unitarismo universalista (0,3 %).[176]​ La encuesta también informó que el 16,1 % de los estadounidenses se describían a sí mismos como agnósticos, ateos o simplemente sin ninguna religión.[176]​[177]​

En 2006, la esperanza de vida era de 77,8 años,[179]​ siendo un año más corta que el promedio de Europa occidental, y mucho más corta que la de países como Noruega, Suiza y Canadá.[180]​ Durante las últimas dos décadas, el nivel de esperanza de vida cayó del 11.º lugar mundial, hasta el 42.º.[181]​ La mortalidad infantil es de 6,37 fallecimientos por cada 1000 nacidos vivos.[182]​ Aproximadamente un tercio de la población adulta es obesa y otro tercio tiene sobrepeso;[183]​ el índice de obesidad, uno de los más altos del mundo, se ha duplicado en los últimos veinticinco años.[184]​ La alta incidencia de diabetes tipo 2 relacionada con la obesidad se considera una epidemia por algunos profesionales de la salud.[185]​ El índice de embarazos en la adolescencia asciende a 79,8 por cada 1000 mujeres, cuatro veces el índice de Francia y cinco veces el de Alemania.[186]​ El aborto, legal en algunos casos, es un tema muy controvertido: muchos estados prohibieron la financiación pública del procedimiento, restringieron el aborto cuando el feto ya está desarrollado, requieren notificación parental para las menores y un período de espera. La tasa de aborto está disminuyendo, ya que existen 241 por cada 1000 nacimientos, aunque sigue siendo superior al de la mayoría de las naciones occidentales.[187]​

El gasto en el sistema de atención a la salud sobrepasa al de cualquier otra nación, tanto en términos de gasto per cápita como en porcentaje del PIB.[188]​ En 2000, la Organización Mundial de la Salud colocó el sistema de salud estadounidense en primer lugar en la capacidad de respuesta, pero 37.º en rendimiento global. Estados Unidos es líder en innovación médica. En 2004, este sector invirtió tres veces más que cualquier país de Europa en la investigación biomédica.[189]​

Estados Unidos es sede de muchos de los mejores centros médicos del mundo. Gran parte de las instalaciones médicas son de propiedad privada que cuentan con algunos subsidios del gobierno local. A pesar de ser asociaciones sin fines de lucro, muchos de los hospitales más importantes del país están afiliados a grandes corporaciones o escuelas de medicina, que han hecho posible que anualmente estos centros alberguen a cerca del 70 % de todos los pacientes médicos del país.[190]​ El Hospital Johns Hopkins, la Clínica Mayo, el Hospital General de Massachusetts y la Clínica Cleveland se encuentran entre los mejores hospitales del país y del mundo.[191]​

A diferencia de otros países desarrollados, la cobertura del sistema de salud no es universal. En 2004, los seguros médicos pagaron el 36 % de los gastos en materia de salud y los gobiernos federales y estatales otorgaron el 44 %.[192]​ En 2005, 46,6 millones de estadounidenses, 15,9 % de la población, no estaban asegurados, 5,4 millones más que en 2001. La principal causa de este aumento es la caída en el número de empleos donde se garantiza el seguro médico.[193]​ El tema de los estadounidenses no asegurados es una cuestión política importante.[194]​[195]​ Un estudio de 2009 estimó que la falta de seguros está asociada con casi 45 000 muertes al año.[196]​ En 2006, Massachusetts se convirtió en el primer estado en implementar la asistencia sanitaria universal.[197]​ La ley federal aprobada a principios de 2010 creará un sistema de salud casi universal para todo el país en 2014.[cita requerida]

Nueva York
Los Ángeles
Chicago

Houston
Phoenix
Filadelfia

La responsabilidad legal de aplicar y hacer cumplir las leyes en Estados Unidos recae principalmente en los cuerpos de policía local, en los alguaciles —llamados sheriff o marshal, dependiendo del estado— y sus departamentos y en la policía estatal. A nivel federal existen varias agencias federales, como la Oficina Federal de Investigación (FBI) y el Cuerpo de Alguaciles de Estados Unidos, especializados en determinadas funciones.[199]​ A nivel federal y en casi todos los estados impera el derecho consuetudinario. En el país existen dos tipos de tribunales, los tribunales estatales, encargados de estudiar y juzgar delitos ordinarios, y los tribunales federales, encargados de juzgar ciertos delitos designados y resolver apelaciones procedentes de sentencias de tribunales estatales. Los acuerdos entre las partes es la forma más común de terminar los juicios, pues en la gran mayoría de casos penales se alcanza un acuerdo con el fiscal antes de celebrarse un juicio con jurado.[200]​[201]​

En 2012, hubo 4,7 homicidios por cada 100 000 habitantes en los Estados Unidos, lo que representa un descenso del 54 % desde el pico de 10,2 alcanzado en 1980,[202]​[203]​[204]​ sin embargo en 2015 se produjo un severo repunte del número de homicidios, con una tasa de 4,9 por cada 100 000 personas, lo que equivale en cifras a 15 696 homicidios, 1532 más que en 2014 (un incremento del 10,8 %), el mayor repunte anual desde 1971.[205]​ Entre 1980 y 2008 los hombres cometieron el 77 % de los homicidios y el 90 % de los crímenes. La población negra se vio involucrada en el 52,5 % de los homicidios, una proporción ocho veces mayor que la población blanca (incluyendo a la mayoría de hispanos como «blancos») y fueron víctimas en una proporción seis veces mayor que los blancos. La mayoría de homicidios fueron entre personas de la misma etnia, ya que un 93 % de los negros fueron asesinados por negros y un 84 % de los blancos por blancos.[206]​ De los países desarrollados, Estados Unidos es, con diferencia, el país que tiene un mayor índice de delitos con violencia, homicidios y violencia armada.[207]​ Un análisis de la Organización Mundial de la Salud de 2003 reveló que Estados Unidos «tenía una tasa de homicidios 6,9 veces superior a las tasas registradas en el resto de los países de ingresos altos, tasa impulsada al alza debido a que su tasa de homicidios por arma de fuego era 19,5 veces mayor».[208]​ El derecho a comprar y portar un arma de fuego, consagrado en la Segunda Enmienda a la Constitución, sigue siendo objeto de debate político y social.

La pena de muerte sigue siendo un tipo de condena legalmente, y sigue contemplada para ciertos delitos federales y militares en 32 de los 50 estados del país.[209]​ Entre 1967 y 1977 no se realizó ninguna ejecución, debido, en parte, a un fallo de la Corte Suprema de los Estados Unidos que prohibía la imposición arbitraria de la pena capital. En 1976, el mismo tribunal dictaminó que, en circunstancias adecuadas, la pena de muerte podía aplicarse sin que ello violara la legalidad constitucional. Desde esta decisión, Estados Unidos ha ejecutado a más de 1300 reos, la mayoría de ellos en Texas, Virginia y Oklahoma.[210]​ Por otro lado, son varios los estados que han ido aboliendo por ley o simplemente dejado de aplicar la pena de muerte. En 2010, Estados Unidos fue el quinto país en número de ejecuciones, detrás de tan solo China, Irán, Corea del Norte y Yemen.[211]​

Estados Unidos es el país con una mayor tasa de encarcelamiento documentado del mundo y tiene la mayor población carcelaria del mundo.[212]​[213]​ A inicios de 2008, más de 2,3 millones de personas estaban encarceladas, más de uno de cada 100 adultos del país.[214]​ Entre presos y personas en libertad condicional, el sistema penitenciario supervisaba a uno de cada 31 adultos del país en 2010,[215]​ cifra que para 2012 se había rebajado a 1 de cada 35 adultos (la tasa más baja desde 1997), haciendo un total de 6 937 600 personas.[216]​ La población carcelaria se ha cuadruplicado desde 1980.[217]​ El coste de mantener a los presos y las instalaciones, sumado a la construcción de nuevas prisiones debido a la masificación, asciende a más de 60 000 millones de dólares anuales; esto provoca que algunos estados del país tengan que destinar al sistema penitenciario un 10 % de sus ingresos, en el caso de California, estado a la cabeza en número de prisioneros, en el año 2010 dedicó más dinero a esta faceta que, por ejemplo, a educación.[215]​ Los presos más comunes son los varones afroamericanos, con una tasa de encarcelados tres veces mayor que los hispanos y seis veces mayor que la de los varones blancos.[218]​ La alta tasa de presos puede explicarse en parte por las duras leyes promulgadas durante las últimas décadas para combatir el tráfico, posesión y consumo de drogas.[219]​ En 2008 Luisiana fue el estado con mayor tasa de encarcelados y Maine el de menor tasa.[220]​

El sistema penitenciario de Estados Unidos es duramente criticado tanto por analistas como por algunas organizaciones. Además de la privatización progresiva del sistema penitenciario desde la década de 1980, en 2014 la ONG Human Rights Watch y otras organizaciones denunciaron que más de 80 000 presos se encuentran en celdas de aislamiento minúsculas, que las ONG califican como cámaras de tortura.[221]​ En 2012, The New York Times denunció que unos 60 000 indocumentados recluidos en centros de detención son utilizados como mano de obra barata por el gobierno o empresas privadas, con salarios que pueden llegar a ser de 13 centavos la hora; en algunos casos ni siquiera se les retribuye.[222]​ Algunos estados también emplean a miles de reclusos de sus prisiones en fábricas de algunas empresas a cambio de la reducción de su condena o el salario mínimo. Esto supone un ahorro para las empresas de millones de dólares al año en salarios, aunque su legalidad está cuestionada.[223]​ Algunas de las empresas involucradas serían, directamente o a través de subcontrataciones, Microsoft, Boeing, Starbucks o Victoria’s Secret, entre otras.[223]​

Estados Unidos es una nación multicultural, hogar de una amplia variedad de grupos étnicos, tradiciones y valores.[13]​[224]​ Aparte de las ahora pequeñas poblaciones de nativos americanos y hawaianos, casi todos los estadounidenses o sus antepasados emigraron durante los últimos cinco siglos.[225]​ La cultura común para la mayoría de los estadounidenses es una cultura occidental, que en gran parte proviene de las tradiciones de los inmigrantes europeos con influencias de muchas otras fuentes, tales como las tradiciones traídas por los esclavos de África.[13]​[226]​[227]​ Recientemente, los inmigrantes de Asia y Latinoamérica han añadido más elementos a esta mezcla cultural que ha sido descrita como «un crisol de razas homogeneizados y en una ensaladera heterogénea», en donde los inmigrantes y sus descendientes retienen, comparten y absorben varias características culturales distintivas.[13]​

De acuerdo con un análisis de dimensiones culturales elaborado por Geert Hofstede, Estados Unidos tiene uno de los índices de individualismo más alto que cualquier otro país estudiado.[228]​ Mientras que en la cultura popular se considera al país como una sociedad sin clases sociales,[229]​ varios estudiosos identifican diferencias significativas que pueden considerarse como clases sociales, que afectan a la socialización, el idioma y los valores.[230]​[231]​ La clase media trabajadora estadounidense ha sido la iniciadora de muchas de las tendencias sociales contemporáneas como el feminismo moderno, el ecologismo y el multiculturalismo.[232]​ A diferencia de otras culturas, la mayoría de las mujeres trabajan fuera del hogar y cuenta con alguna licenciatura.[233]​ En 2007, 58 % de las estadounidenses mayores de 18 años se habían casado, 6 % estaban viudas, 10 % se divorciaron y el 25 % nunca se había casado.[234]​

El mismo estudio de Hofstede también reveló que el punto de vista social de los estadounidenses, sus expectativas culturales y la imagen que tienen de sí mismos están asociadas con sus empleos y ocupaciones en un grado inusitadamente estrecho.[235]​ Así también se tiene la tendencia a valorar más los logros socioeconómicos, de tal modo que el hecho de ser una persona ordinaria o promedio es visto como un atributo positivo en la mayoría de los casos.[236]​ Pese a que el llamado "sueño americano" y la percepción de que los estadounidenses disfrutan de una alta movilidad social desempeñan un papel clave en la atracción de los inmigrantes, algunos analistas encuentran que Estados Unidos tiene una menor movilidad social que los países de Europa y Canadá.[237]​[238]​

Desde la colonización europea, el arte estadunidense se mantuvo apegado a las corrientes artísticas del Viejo Continente. A mediados del siglo XIX, surgió la Escuela del río Hudson, uno de los primeros movimientos artísticos desarrollado en el país, aunque todavía se encontraba muy influenciado por la tradición europea del naturalismo. En 1913, el Armory Show de la ciudad de Nueva York, una exhibición de obras del arte moderno europeo, causó un gran impacto en el público y transformó la escena artística estadunidense.[239]​ Georgia O'Keeffe, Marsden Hartley y otros artistas experimentaron con nuevos estilos, mostrando una sensibilidad y técnica sin precedentes en los artistas europeos. Los movimientos artísticos más importantes como el expresionismo abstracto de Jackson Pollock y Willem de Kooning y el pop art de Andy Warhol y Roy Lichtenstein se desarrollaron en gran medida en los Estados Unidos. De igual forma, la corriente del modernismo y el posmodernismo llevaron a la fama a varios arquitectos estadounidenses como Frank Lloyd Wright, Philip Johnson y Frank Gehry.

Uno de los más grandes promotores del teatro en Estados Unidos fue el empresario P. T. Barnum, quien se introdujo en el negocio del entretenimiento del bajo Manhattan en 1841. A finales de la década de 1870, el equipo Harrigan and Hart produjo una serie de comedias musicales muy exitosas en Nueva York. En el siglo XX, el formato para los musicales modernos emergió de Broadway; las canciones compuestas por personajes como Irving Berlin, Cole Porter y Stephen Sondheim se convirtieron en estándares para la música pop. El dramaturgo Eugene O'Neill ganó el Premio Nobel de Literatura en 1936; otros escritores estadounidenses aclamados incluyen a ganadores de un Premio Pulitzer, como Tennessee Williams, Edward Albee y August Wilson.

Coreógrafos como Isadora Duncan y Martha Graham ayudaron al desarrollo de la danza moderna, mientras que George Balanchine y Jerome Robbins fueron dos de las grandes figuras del ballet del siglo XX. Los estadounidenses han jugado un papel importante en el medio artístico de la fotografía, destacando entre ellos Alfred Stieglitz, Edward Steichen y Ansel Adams.

La primera función de cine realizada en la historia se realizó en 1894 en la ciudad de Nueva York, utilizando el quinetoscopio de Thomas Edison. El año siguiente fue presentada la primera proyección de una película con fines comerciales, también en Nueva York, y durante las décadas siguientes Estados Unidos estuvo a la vanguardia del desarrollo del cine sonoro. Desde principios del siglo XX, la industria del cine estadunidense se ha centrado en gran medida alrededor de las producciones de Hollywood, California. El director D. W. Griffith fue fundamental para el desarrollo de la estructura básica de una película y Citizen Kane (1941) de Orson Welles es citada frecuentemente como la mejor película de todos los tiempos.[240]​[241]​ Otros directores fundamentales de esta nacionalidad en la historia del cine: John Ford, Howard Hawks, Elia Kazan o Michael Curtiz, y, entre los más recientes, Stanley Kubrick, Francis Ford Coppola o Steven Spielberg. Actores estadounidenses como Humphrey Bogart, John Wayne o Marilyn Monroe se han convertido en figuras icónicas, mientras que el productor y empresario Walt Disney fue uno de los líderes en la animación y el merchandising de sus películas. Los estudios de cine más importantes de Hollywood han producido las películas comercialmente más exitosas en la historia, tales como Star Wars (1977), Titanic (1997) o Avatar (2009). Para la década de 2020, los productos de Hollywood continuarán dominando la industria cinematográfica a nivel mundial.[242]​

Con la historieta, la producción estadounidense ha perdido el liderazgo que ostentaba a nivel mundial desde principios del siglo XX, destacando varias tiras de prensa como Flash Gordon, The Phantom o El Príncipe Valiente. Luego de la Segunda Guerra Mundial, el cómic se convirtió en un medio minoritario que recientemente vive el fenómeno de la novela gráfica, en el que destacan autores como Charles Burns, Joe Sacco o Chris Ware. No obstante, las tiras cómicas y las historietas también son parte del legado artístico del país. Por ejemplo, Superman, uno de los superhéroes más conocidos dentro de este medio, se ha convertido en todo un ícono nacional.[243]​

Estados Unidos tiene la mayor cantidad de televidentes en el mundo,[244]​ a la vez que el promedio de tiempo dedicado a ella sigue en aumento, llegando a cinco horas al día en 2006.[245]​ Las cuatro cadenas de televisión más importantes son todas entidades comerciales. Los estadounidenses también escuchan programas de radio, en gran medida comercializados, en promedio durante dos y media horas al día.[246]​ Los sitios web y motores de búsqueda más populares de la red son inventiva estadounidense, incluidos Google, Facebook, YouTube, Wikipedia, Blogger, eBay y Yahoo!.[247]​

En cuanto a la música, la obra de Charles Ives de la década de 1910, en gran parte ignorada durante su desarrollo, lo llevó a convertirse en el primer compositor de música clásica estadounidense exitoso; otros pioneros como Henry Cowell y John Cage crearon un enfoque estadounidense dentro de las composiciones clásicas. Aaron Copland y George Gershwin desarrollaron una mezcla única de música clásica con música popular.

Los estilos rítmicos y líricos de la música afroamericana han influido profundamente en la música de Estados Unidos en general, distinguiéndola de las tradiciones europeas. Elementos de la música folclórica, como el blues y lo que ahora se conoce como old music fueron adaptados y transformados en géneros populares para una audiencia global. El jazz fue desarrollado por artistas innovadores tales como Louis Armstrong y Duke Ellington a principios del siglo XX. El country surgió en la década de 1920 y el rhythm and blues en la década de 1940. A mediados de la siguiente década, Elvis Presley y Chuck Berry estuvieron entre los pioneros del rock and roll. En la década de 1960, el surgimiento de la carrera de Bob Dylan ayudó a revivir la influencia de la música folclórica, para convertirse en uno de los compositores más célebres del país; al mismo tiempo, James Brown lideró el desarrollo del funk. Las más recientes creaciones estadounidenses incluyen el hip hop y la música house. Finalmente, las estrellas pop tales como Elvis Presley, Michael Jackson, Madonna o Mariah Carey se han convertido en iconos reconocidos a nivel mundial.[248]​

Durante el siglo XVIII y principios del siglo XIX, la literatura estadunidense tomaba la mayoría de sus influencias de Europa. A mediados del siglo XIX, escritores como Nathaniel Hawthorne, Edgar Allan Poe y Henry David Thoreau establecieron una literatura propia del país. Mark Twain y el poeta Walt Whitman fueron dos de las grandes figuras de la segunda mitad del siglo; Emily Dickinson, virtualmente desconocida durante su vida, también es considerada como una de las poetisas más importantes de la literatura estadunidense.[249]​ Una obra que captura aspectos fundamentales de la vida en el país en sus personajes —tales como Moby-Dick (1851) de Herman Melville, Las aventuras de Huckleberry Finn (1885) de Mark Twain y El gran Gatsby (1925) de F. Scott Fitzgerald— puede ser considerada como la «gran novela estadounidense».

Once ciudadanos estadounidenses han ganado el Premio Nobel de Literatura, siendo el más reciente Toni Morrison en 1993. William Faulkner y Ernest Hemingway, ganadores en los años 1949 y 1954, respectivamente, a menudo son catalogados entre escritores más influyentes del siglo XX.[250]​ Varios géneros literarios populares como los del viejo oeste y la novela negra se desarrollaron en el país. Los escritores de la generación beat (Allen Ginsberg, Jack Kerouac, William S. Burroughs…) abrieron el campo a nuevas formas literarias, así como a autores posmodernistas como John Barth, Thomas Pynchon y Don DeLillo. Merece la pena mencionar asimismo a otros poetas y escritores tan importantes como Ezra Pound, Wallace Stevens, Gore Vidal, Truman Capote, el nacionalizado Vladimir Nabokov, Raymond Carver, y, dentro de la literatura fantástica, son de renombre internacional H. P. Lovecraft, Philip K. Dick, Ray Bradbury, Stephen King, etc.

En el campo de la filosofía, los trascendentalistas, liderados por Henry Thoreau y Ralph Waldo Emerson, establecieron el primer movimiento filosófico importante del país. Después de la Guerra de Secesión, Charles Sanders Peirce, William James y John Dewey fueron los líderes del desarrollo del pragmatismo. En el siglo XX, las obras de W. V. O. Quine, Richard Rorty y Noam Chomsky, trajeron la filosofía analítica a las academias estadounidenses. En décadas más recientes, John Rawls y Robert Nozick fueron dos de los líderes más importantes del resurgimiento de la filosofía política.

La gastronomía de Estados Unidos es similar a la de otros países occidentales, con el trigo siendo el cereal más utilizado. La cocina tradicional estadunidense utiliza ingredientes como el pavo, carne de ciervo, papas, camotes, maíz, calabazas, miel de maple y otros elementos indígenas utilizados por los amerindios y los primeros colonizadores europeos. Parrilladas de puerco y res, tortas de cangrejo, papas chips y las galletas con chispas de chocolate son algunos de los platos hechos al «estilo estadounidense». La soul food, la cocina tradicional de los esclavos africanos, es aún popular en el sur y entre los afroamericanos de otras partes del país. Las gastronomías sincréticas, como la cocina criolla de Luisiana, la cajún y la Tex-mex, tienen gran importancia regional.

Platos característicos como la tarta de manzana, el pollo frito, la pizza, la hamburguesa y el perrito caliente provienen de recetas introducidas por los inmigrantes. Las papas fritas, los platillos mexicanos como los burritos y tacos y los platillos con pastas adaptados de recetas italianas también son ampliamente consumidos.[251]​ En el consumo de bebidas, los estadounidenses prefieren el café en vez del té. La publicidad de las industrias estadounidenses ha hecho que el jugo de naranja y la leche sean las bebidas típicas de un desayuno.[252]​[253]​ El consumo frecuente de comida rápida está asociado con lo que los médicos llaman «epidemia de obesidad». Las gaseosas son ampliamente populares: el azúcar contenido en ellas aporta el 9 % de la ingesta calórica promedio.[254]​[255]​

Estados Unidos es pionero en la investigación científica y la innovación tecnológica desde el siglo XIX. En 1876, Alexander Graham Bell recibió la primera patente para un estadounidense por el teléfono.[256]​ El laboratorio de Thomas Edison desarrolló el fonógrafo, la primera lámpara incandescente y el primer proyector de películas. Nikola Tesla fue precursor en experimentar con la corriente alterna, el motor de corriente alterna y la radio. En el siglo XX, las compañías de automóviles de Ransom Eli Olds y Henry Ford promovieron la producción en cadena. En 1903, los hermanos Wright realizaron el primer vuelo propulsado en su aeronave Wright Flyer.[257]​

El ascenso del nazismo en la década de 1930 llevó a muchos científicos europeos, incluyendo a Albert Einstein y Enrico Fermi, a emigrar al país. Durante la Segunda Guerra Mundial, el proyecto Manhattan ya había desarrollado las primeras armas nucleares, anunciando el inicio de la era nuclear. La carrera espacial también produjo avances rápidos en la construcción y el desarrollo de cohetes, la ciencia de materiales y la informática. El país fue responsable del desarrollo de la ARPANET y su sucesor, Internet. Hoy en día, la mayor parte de los ingresos para la investigación y desarrollo, un 64 %, provienen del sector privado.[258]​ El país es líder mundial en publicaciones de investigación científica y en el factor de impacto.[259]​ Los estadounidenses poseen bienes de consumo tecnológicamente avanzados[260]​[261]​[262]​ y casi la mitad de los hogares tienen acceso a Internet de banda ancha.[263]​ También es el principal desarrollador y cultivador de organismos genéticamente modificados; más de la mitad de las tierras con cultivos biotecnológicos del mundo se encuentran en Estados Unidos.[264]​

Desde finales del siglo XIX, el béisbol se considera como el deporte nacional, mientras que el fútbol americano, el baloncesto y el hockey sobre hielo son los tres otros grandes deportes de equipo profesionales. Las ligas universitarias también atraen a grandes audiencias. El fútbol americano es el deporte más popular en Estados Unidos.[266]​[267]​ El boxeo y las carreras de caballos fueron en su momento los deportes individuales más vistos, pero han sido eclipsados por el golf y el automovilismo, particularmente por los campeonatos Stock Cars como la más importante Copa NASCAR y otras categorías nacionales y regionales de NASCAR y ARCA. Otros campeonatos de monoplazas reseñables son la IndyCar, anteriormente la desaparecida Champ Car World Series y las carreras de resistencia como la United SportsCar Championship. Una de las principales carreras de Estados Unidos son las 500 Millas de Indianapolis de la IndyCar, que cuenta con prestigio internacional. Las carreras de resistencia como las 24 Horas de Daytona, 12 Horas de Sebring y la Petit Le Mans son de igual importancia en el automovilismo mundial. Las 500 millas de Daytona de la copa NASCAR es la carrera más importante de los Stock Cars del mundo. El fútbol, aunque no es uno de los principales deportes a nivel profesional en el país, tiene gran presencia y nivel de aficionados. El tenis y otros deportes de exterior también son populares junto al Abierto de Estados Unidos, uno de los campeonatos de tenis más vistos del mundo.

Si bien la mayoría de los deportes importantes de los Estados Unidos han evolucionado de prácticas europeas, el baloncesto, el voleibol, la animación y el snowboarding son invenciones locales. El lacrosse y el surf surgieron de los amerindios y de los nativos de Hawái. El Comité Olímpico de los Estados Unidos (USOC) fue anfitrión de los Juegos Olímpicos de Verano en cuatro ocasiones: San Luis 1904, Los Ángeles 1932, Los Ángeles 1984 y Atlanta 1996, y de los Juegos de Invierno en cuatro ocasiones: Lake Placid 1932, Squaw Valley 1960, Lake Placid 1980 y Salt Lake City 2002.[268]​ Estados Unidos organizara los juegos de verano de 2028 en Los Ángeles. Estados Unidos ha ganado 2802 medallas en los Juegos Olímpicos de Verano, más que ningún otro país y 282 medallas en los Juegos Olímpicos de Invierno.[269]​

Países transcontinentales: Armenia Azerbaiyán Chipre Egipto Georgia Indonesia Kazajistán Rusia Turquía

Asia es el continente más grande y poblado de la Tierra. Con cerca de 45 millones de km², supone el 8,7 % del total de la superficie terrestre y el 30 % de las tierras emergidas[1]​  y, con alrededor de 4600 millones de habitantes, el 60 % de la población mundial.[2]​ Se extiende sobre la mitad oriental del hemisferio Norte, desde el océano Glacial Ártico, al norte, hasta el océano Índico, al sur. Limita, al oeste, con los montes Urales, y al este, con el océano Pacífico. El continente fue el hogar de muchas de las primeras civilizaciones del mundo tales como; mesopotámica, india, china, entre otras.

En la división convencional de continentes, de origen europeo, Asia y Europa aparecen como dos entidades diferentes por razones culturales e históricas. En términos geográficos, forman en realidad un único continente, llamado Eurasia. Además, África está unida a Eurasia por el istmo de Suez por lo que también se puede considerar toda la extensión conjunta de Europa, Asia y África como un único supercontinente, ocasionalmente denominado Eurafrasia, o Afro-eurasia.

El vocablo «Asia», como topónimo, corresponde a la civilización griega, para la cual Asia eran las tierras al este del mar Egeo, sometidas al Imperio persa, sin otro límite que la India y el mar Eritreo (mar Rojo y océano Índico).[3]​[4]​ 

El  nombre se origina en 𐀀𐀯𐀹𐀊, (a-si-wi-ja) Aswia, atestiguado como gentilicio en griego micénico,[5]​ una entidad política de Anatolia conocida en los documentos hititas como  𒀸𒋗𒉿 Assuwa derrotada en el siglo XV a. C., pero cuyo nombre subsistió para la región más occidental de Anatolia (por ejemplo Assos).[6]​ Se ha sugerido que Assuwa contiene la raíz egea asis «barroso»[7]​ o bien sea un derivado del acadio 𒀀𒍮 aṣû, «salir fuera», bien en el sentido de rebelarse,[8]​ dado que se la menciona como una alianza «contra» el Imperio hitita, o bien en el sentido de «amanecer», si bien este último es problemático al encontrarse al oeste respecto de Hattusas. El mismo vocablo acadio Asu, por otra parte, podría ser el origen del nombre, sin hacer referencia a  Assuwa con el sentido de Oriente, así como Europa significaría Occidente.[9]​ La mitología griega consideraba que el nombre derivaba de Asia, oceánida y esposa de Prometeo.[10]​

El concepto europeo de tres continentes (Asia, Europa y África) se remonta a la Antigüedad clásica, y se difundió durante la Edad Media debido a los estudios del erudito del siglo VII Isidoro de Sevilla (véase Mapa de T en O). La delimitación entre Asia y África (en el suroeste) forma el istmo de Suez y el mar Rojo. Los límites entre Asia y Europa, convencionalmente, se considera que discurren entre los Dardanelos, el mar de Mármara, el Bósforo, el mar Negro, el Cáucaso, el mar Caspio, el río Ural y los montes Urales hasta el mar de Kara (Rusia).[11]​

En el Lejano Oriente de Asia, Siberia está separada de Norteamérica por el estrecho de Bering. Asia está rodeada por el sur por el océano Índico (específicamente, de oeste a este, el golfo de Adén, el mar Arábigo y la bahía de Bengala), al este por las aguas del océano Pacífico (incluyendo, en contra las manecillas del reloj, el mar de China, el mar de China Oriental, el mar Amarillo, el mar de Japón, el mar de Ojotsk y el mar de Bering) y por el norte por el océano Ártico. Australia (u Oceanía) permanece en el sudeste, el límite geológico y ecológico entre Asia y Oceanía en el sector que separa a la Austronesia de la Australasia es la Línea de Wallace aunque histórica y culturalmente el límite con Oceanía está dado más frecuentemente por la Línea de Weber (tras ser una colonia neerlandesa, a mediados del recién pasado s XX el Estado de núcleo asiático llamado Indonesia heredó territorios de Oceanía como las islas Aru y el oeste de la isla de Nueva Guinea por lo que actualmente los neófitos suelen confundirse con los límites del Sudeste Asiático y la Oceanía).

Algunos geógrafos no consideran que Asia y Europa sean continentes separados,[12]​ ya que no existe una separación física lógica entre ambas.[13]​ Por ejemplo, Barry Cunliffe, el profesor emérito de arqueología europea de Oxford, sostiene que Europa ha sido geográficamente y culturalmente la "excrecencia occidental del continente asiático".[14]​ Geográficamente, Asia es la mayor parte oriental del continente denominado Eurasia con Europa constituyendo una península noroccidental de la masa continental denominada Eurafrasia: geológicamente, Asia, Europa y África constituyen un territorio único y continuo (salvo el artificial canal de Suez) y comparten una plataforma continental común. La mayor parte de Europa y Asia se asientan en la placa tectónica euroasiática, que bordea a la placa arábiga y la placa India por el sur; la parte más nororiental de Siberia (al este de los Montes Cherski) forma parte de la placa Norteamericana.

En geografía, existen dos escuelas de pensamiento principales. Una escuela obedece a las convenciones históricas y trata Europa y Asia como continentes diferentes, categorizando subregiones dentro de ellos para un análisis más detallado. La otra escuela compara la palabra "continente" con una región geográfica cuando se refiere a Europa, y utiliza el término "región" para describir a Asia en términos de la fisiografía. Dado que, en términos lingüísticos, "continente" implica una masa distinta, cada vez es más común sustituir el término "región" por el de "continente" para evitar el problema de la desambiguación.

Dado el alcance y diversidad de la masa de tierra, a veces no está siquiera claro en qué consiste "Asia" exactamente. Algunas definiciones excluyen Turquía, Oriente Medio, Asia Central y Rusia, y solo consideran que componen Asia el Lejano Oriente, el Sureste Asiático y el subcontinente indio,[15]​[16]​ especialmente en Estados Unidos después de la II Guerra Mundial.[17]​ El término a veces se reduce de forma más estrecha para referirse a la región Asia-Pacífico, que no incluye Oriente Medio, el Sureste Asiático ni Rusia,[18]​ pero sí incluye las islas del océano Pacífico y puede incluir partes de Australasia u Oceanía, a pesar de que los isleños del Pacífico no son considerados asiáticos.[19]​

El relieve de Asia está constituido por una dilatada meseta, en cuyos bordes se levanten fuertes cordilleras y a cuyo pie septentrional se extienden las llanuras que alcanzan el océano Ártico, en tanto que la vertiente sur se disponen, como en Europa, tres dilatadas penínsulas. En el núcleo central de todo el sistema orográfico es la meseta de Pamir o apodada junto a la del Tíbet como «El Techo del Mundo» (3600 m s. n. m. de altura media), de la que en direcciones a amplias cordilleras. Al sureste del «nudo orográfico» del Pamir se extiende otra meseta excelsa, que es la del Tíbet (la meseta más extensa y elevada actual del planeta Tierra), de mayor extensión y altura (5000 m) que el propio Pamir.

Entre las cadenas del sudoeste del Pamir y del Tíbet destacan las siguientes: del este de la meseta nace la cadena de los Kuen-Lun que, con los Tian Shan, encierra la depresión del Tarim; más al sur, las formaciones de Hedin (7000 m) y del Karakorum (8611 m); en los bordes del suroeste y sur del Tíbet se levantan las alturas más importantes de la Tierra en el Himalaya (Everest, 8848 m; Kanchenjunga, 8585 m). La zona montañosa central es, pues, una extensa región de mesetas de gran altura en cuyos bordes se eleva la crestería montañosa. De este modo las mesetas quedan confinadas, a manera de depresiones, en el cerco de sus montañas limitantes, por lo que son desérticas o subdesérticas (faja de desiertos: desierto de Arabia, desierto de Persia, Turquestán o Turán, desierto de Gobi, etc).

Por la vertiente septentrional corren hacia el océano Glacial, entre otros, los ríos Obi (4100 km) Yenisei (4750 km) y Lena (4270 km). Es muy interesante el régimen peculiar de estos ríos siberianos: como sus fuentes están a millares de kilómetros, más al sur que su desembocadura, al sobrevenir aflujos de agua en el curso alto, todavía permanece helado el tramo inferior, lo que es causa de grandes inundaciones. En el Pacífico desemboca el Amur (en el golfo de Ojotsk), procedentes de los Kuen-lun, vierten sus aguas en el golfo de Pechili y en el mar de la China Oriental, respectivamente; el Sikiang y el Song-ka, que desaguan por Cantón y Hanoi.
De la vertiente meridional del Himalaya descienden hasta el golfo de Bengala y el mar Arábigo, dependencias del océano Índico, el Ganges y el Indo: ambos de una longitud aproximada de unos 3000 km que construyen amplios deltas. En Asia occidental, los ríos más notables son el Éufrates (2700 km) y el Tigris (2 000 km) que tienen su arranque en la meseta de Armenia,[20]​ y fundidos en un solo río al final de su curso, desaguan en el golfo Pérsico, al que poco a poco van colmando con sus aluviones. En las estepas centrales son numerosas las cuencas interiores, cerradas, que desembocan en lagos. Así por ejemplo ocurre con los ríos Sir Daria y Amu Daria, que vierten en el mar de Aral, que, a pesar de su nombre, es un lago.

Las circunstancias históricas y humanas han marcado la verdadera división entre el Asia Occidental desde la India hacia el oeste y el Asia Oriental conocida como el Lejano Oriente, desde la India hacia el este.
Geográficamente, gran parte de Rusia pertenece al norte de Asia (el resto está en Europa).[21]​ La mayor parte de Turquía está en Asia (el resto en Europa). Una pequeña parte de Egipto, la península del Sinaí, pertenece a Asia Occidental (la mayor parte está en África).

Subregiones geográficas de Asia:

El continente asiático está plenamente integrado en las organizaciones internacionales. Todos los países asiáticos salvo Taiwán, Palestina y algunos con un reconocimiendo muy limitado (Abjasia, Osetia del Sur, Artsaj),  pertenecen a la ONU, contando entre sus fundadores con la República de China (sustituida por la RP China en 1971) o Arabia Saudí. Recientemente, la Organización de las Naciones Unidas aceptó al Estado de Palestina como Estado observador (2012). Previamente, en 1974, le había otorgado la condición de observadora a la Organización para la Liberación de Palestina como representante del pueblo palestino.

A nivel económico los estados asiáticos forman parte del Fondo Monetario Internacional (salvo Corea del Norte), aunque algunos no cumplen con el artículo VIII como son Siria, Irak, Turkmenistán, Afganistán, Bután, Birmania y Laos. En el caso de la Organización Mundial del Comercio se integran todos salvo Turkmenistán y Corea del Norte, además de países como Irán, Uzbekistán, Iraq, Siria, Bután o Timor Oriental que tienen el estatus de observadores. Dentro del continente tiene un papel destacado la OPEP, pues de los trece países miembros seis son asiáticos, concretamente del Golfo Pérsico, como Arabia Saudí, Kuwait, Iraq, Irán, Emiratos Árabes Unidos y Catar.

En materia de justicia y seguridad todos los países asiáticos están integrados en la INTERPOL, salvo Corea del Norte. Por otro lado en el caso de la Corte Penal Internacional veintiún países no han firmado ni ratificado el Estatuto de Roma. Mientras que son diez los firmantes que aún no lo han ratificado. En el resto de países se acepta la jurisdicción del Corte Penal Internacional para juzgar casos de crímenes contra la humanidad.

En caso del Movimiento de Países No Alineados (1961) son 38 naciones asiáticas las que forman parte de esta organización en favor de la neutralidad.

Existen otras organizaciones de tipo transcontinental como la OCDE en 1961 (países americanos, europeos y asiáticos) a la que se adhirieron países como Japón (1964), Corea del Sur (1996) o Israel (2010) o el Foro de Cooperación Económica Asia-Pacífico (1989) que aglutina a 12 países asiáticos (incluidas las dos Chinas) más Rusia, Estados Unidos, Canadá, México, Perú y Chile por América; y Nueva Zelanda, Australia y Papúa Nueva Guinea por Oceanía. Así mismo, en este ámbito transcontinental destaca el Plan Colombo (1950) para fomentar el desarrollo económico y social de 27 países (22 asiáticos). Finalmente la Asociación ribereña del Océano Índico para la cooperación regional (1997) que engloba países asiáticos, africanos más Australia.

A nivel regional no existen ninguna organización al mismo nivel que la Unión Europea, pero si muchos bloques regionales que buscan favorecer la integración de sus miembros sobre todo a nivel económico. En el año 2002 se fundó Diálogo para la Cooperación Asiática (ACD) con el objetivo de coordinar a nivel continental las distintas organizaciones políticas y económicas de Asia.

Desde 2005 se vienen realizando cumbres de jefes de Estado dentro de la Cumbre de Asia Oriental (más Estados Unidos y Rusia) con el objetivo de fomentar las relaciones de paz y la estabilidad de las regiones asiáticas. Existen otras dos organizaciones que buscan la cooperación pacífica, una entre continentes, como Cumbre Asia-Europa (ASEM) y otra a nivel regional como Secretaría para la Cooperación Trilateral (2011) entre Japón, la República Popular de China y Corea del Sur.

Destacan dos instituciones financieras internacionales que compiten entre sí. Por un lado el Banco Asiático de Desarrollo (1966), con 48 países regionales y 19 no regionales, donde Estados Unidos y Japón son los máximos accionistas y otro el Banco Asiático de Inversión en Infraestructura, a iniciativa de la República Popular de China en 2014, que engloba a 31 estados asiáticos y algunos europeos como España, Alemania o Francia.

La organización regional más importante y más antigua es la Asociación de Naciones del Sudeste Asiático (1967) que tiene como objetivos acelerar el crecimiento económico, fomentar la paz y la estabilidad regional. También destaca la Asociación Sudasiática para la Cooperación Regional (1985) entre cuyos miembros destacan India o Pakistán. Además de esta dos organizaciones, dentro de la ACD está también el Consejo de Cooperación para los Estados Árabes del Golfo (1981) y la Unión Económica Euroasiática (2015) entre Rusia y algunas de las repúblicas exsoviéticas de Asia Central.

Existen otras organismos de tipo regional como la Organización de Cooperación de Shanghái (1996) de tipo militar y que incluye a potencias nucleares como la República Popular de China y Rusia, y en 2017 se unieron India y Pakistán. La Organización de Cooperación Económica (1985) de países de Asia Central o Consejo de Cooperación de los Estados de Habla Túrquica (2009).

Rusia tienen una fuerte influencia en países asiáticos de la antigua órbita soviética, de ello se desprende la creación de la Comunidad de Estados Independientes (1992) o la alianza de tipo militar de la Organización del Tratado de la Seguridad Colectiva (1994).

En la región también destaca la Bay of Bengal Initiative for Multi-Sectoral Technical and Economic Cooperation (BIMSTEC) que engloba países del Golfo de Bengala, la Comisión del Río Mekong en la península de Indochina o la Cooperación Mekong-Ganges.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 junio de 2016.
Recoge los estados europeos con una independencia de facto como naciones con un limitado reconocimiento (o irreconocimiento) internacional, y ninguno es miembro de la Organización de las Naciones Unidas. El contencioso entre RP China y la República de China se remonta al final de la guerra civil china en 1949, los aliados occidentales se negaron a reconocer al gobierno comunista hasta 1971 cuando la República Popular de China ocupó su asiento en la ONU. La República de China esta reconocida por 21 estados.
Así mismo, el conflicto árabe-israelí, iniciado en 1948, dio como resultado un conflicto por la región de Palestina entre Israel y la Autoridad palestina. Palestina esta reconocida por 136 países y es miembro observador de la ONU.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 junio de 2016.
Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 junio de 2016.
Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 junio de 2016.
Mientras que África es considerada generalmente como el lugar de nacimiento de la humanidad, se cree que Asia fue la cuna de la civilización, aunque esta en realidad no fue única y uniforme: la gran extensión del continente asiático hizo casi inevitable que surgieran varias culturas de manera independiente. 

Los principales reinos antiguos en la interacción cultural como el gran emperador de Ciro II el Grande, que unificó a los pueblos de origen iraní en el Reino de Persia. Después la creación del Imperio persa aqueménida (550 a. C.-330 a. C. aproximadamente), que extendió la cultura persa desde el mar Mediterráneo hasta el río Indo. La expansión india, la difusión de la civilización china, el dominio musulmán (conocido como el auge y expansión del islam) y mongol, la influencia china y mongola. Además los primeros inicios del dominio colonial de las grandes potencias europeas, como la exploración y colonización realizadas primeramente por España y Portugal, como la llegada de Fernando de Magallanes en 1521 quien descubrió la parte oriental del continente asiático seguido después por Juan Sebastián Elcano, y más tarde colonizadas por Francia, Gran Bretaña, Países Bajos y Rusia.

En la Edad Moderna, Asia constituyó el continente económicamente más productivo. Hacia 1500, Oriente Medio, India y China concentraban cerca del 60 % de la producción mundial, y poco antes de 1800 el 80 % de la misma. Durante el siglo XVIII, los textiles de India se exportaban extensivamente a Francia e Inglaterra. Y gran cantidad de productos industriales chinos estaban presentes tanto en la América colonial desde el siglo XVII como en Europa. Se estima que un 75 % de la plata extraída por los españoles en América acabó en China a cambio de la compra de productos manufacturados en China.[26]​[27]​ La Revolución Industrial europea alteró este equilibrio, y mediante conquista militar gran parte de Asia pasó a estar controlada por potencias europeas.

Tras llegar a la independencia de los países colonizados de sus antiguas metropólis mediante la adopción de una respuesta al imperialismo propia en cada país, Asia vería el mantenimiento de esa independencia, la aparición de conflictos, la intensificación del nacionalismo, la confrontación ideológica y la expansión económica.

Un modelo común para la colonización de Asia, asume que esta se dio a través de varios eventos migratorios desde África y Oriente Medio. Una primera migración temprana a través de la costa sur del continente dio origen a ciertas poblaciones actuales de Oceanía como los onge de las islas Andamán, mientras que una migración posterior hacia el Norte dio origen a europeos orientales y asiáticos.

El hallazgo del Hombre de Ust’-Ishim en Siberia y el estudio de su genoma han permitido conocer nuevos datos sobre los procesos migratorios que se produjeron hacia este continente. El hecho de que el individuo de Ust'-Ishim no esté más estrechamente relacionado con los onge de las islas Andamán (descendientes de la primera migración costera) que con los actuales asiáticos orientales o nativos americanos (descendientes de la migración hacia el Norte), muestra que al menos otro grupo, al cual pertenecieron los ancestros del individuo de Ust’-Ishim, colonizaron Asia antes de 45 000 BP.

Además, el individuo probablemente vivió durante un periodo cálido (Interestadio de Groenlandia 12), que ha sido propuesto como tiempo de expansión por Eurasia.
Este individuo podría entonces representar una radiación humana temprana hacia Europa y Asia Central que pudo haber fracasado en cuanto a la presencia de descendientes en las poblaciones actuales.[28]​

La población del continente asiático se estima en más de 4,463 millones de habitantes, concentrándose en la costa del mar de la China y en la península indostánica, en regiones que llegan a alcanzar densidades de mil habitantes por kilómetro cuadrado. Aunque en los últimos lustros su crecimiento ha disminuido, todavía en países del Medio Oriente se mantienen importantes tasas de crecimiento demográfico; en general cerca del 70% de los nacimientos en el mundo se producen en Asia de manera que el envejecimiento no es tan progresivo. Además, cuenta con una alta proporción poblacional cuya edad es inferior a los 30 años y en la que los ancianos son un porcentaje relativamente minoritario.

La población asiática presenta diferentes características, no es homogénea entre sí; algunos rasgos comunes en ciertas zonas de Asia son los ojos rasgados (en el Lejano Oriente desde Siberia, China, Vietnam, la península de Corea, las islas de Taiwán, Japón, el archipiélago malayo y filipino). Dentro de ese grupo étnico se destacan los nativos malayos, de piel morena u oscura que se dividen en diferentes ramas, ya que de ellos descienden los tagalos, visayos, iloacanes, polinesios, melanesios, micronesios, etc. Las etnias de fenotipo caucásico comprenden el Medio y Próximo Oriente. Entre ellas se destacan los árabes, armenios, judíos, persas, asirios, turcos, rusos, etc. Existen también algunas etnias no originarias de Asia, provenientes de Europa y África, producto de la migración.

En la mayor parte de los países del continente asiático tanto del Lejano Oriente como Oriente Medio, predominan los hombres, excepto en Vietnam, Laos, Camboya, Tailandia, Birmania, Líbano, Rusia, Kazajistán, Armenia, Sri Lanka entre otros, donde predominan las mujeres. En Israel y Japón, los hombres menores de 65 años son la mayoría de la población, no así en Filipinas, donde las mujeres entre 15 y 65 años (y de ahí en adelante) son mayoría. En Rusia el alto número de mujeres que tiene se debe a las guerras que atravesó el país, lo que ha hecho que los varones hayan ido disminuyendo como en otros países de Europa Oriental, así como otros casos también en Europa y Asia.

Los más de 4200 millones de habitantes, suponen casi el 61 % de la población mundial. Es, además, el continente más densamente poblado.

Las lenguas que se hablan en el continente asiático son numerosas, a continuación destacamos las siguientes:

Entre ellas la más hablada es el chino o mandarín, seguidas del hindi, el chino cantonés, el urdu, el árabe, el tamil entre otras destacamos como lenguas oficiales y nativas; el japonés, el bahasa indonesio, el coreano, turco, hebreo, persa, birmano, tailandés, tagalo, mongol, armenio, tibetano, tetun, entre otros.

Debido a la colonización europea en diferentes países asiáticos, algunos idiomas europeos son también de uso habitual. Por ejemplo el inglés es utilizada como lengua oficial, asociada y comercial en la India, Malasia, Filipinas, Singapur, Birmania, Sri Lanka, Pakistán, Líbano, Bangladés, Israel, Maldivas, Brunéi, Emiratos Árabes Unidos, la región china de Hong Kong, el Territorio Británico del Océano Índico y entre otros. El francés se utiliza en las naciones de la Península de Indochina como Camboya, Laos y Vietnam, parte de la India como en el territorio de Puducherry y Líbano, como segunda lengua oficial junto con el árabe, la otra lengua oficial.

La lengua española es hablada por minorías en las Filipinas, aunque del español en esta parte de la Región Oriental del Sudeste Asiático, ha derivado una lengua criolla como el chabacano. Hablado en determinados sectores de las Filipinas, así también al norte del estado de Sabah en Malasia, que está reconocido como lengua oficial junto con el malayo y el inglés y en la ciudad de Ternate en Indonesia, todas ellas antiguamente pertenecieron a España. También el español es hablado por las comunidades sefardíes en Oriente Próximo, principalmente en países como Israel y zonas de Turquía, también lo hablan en algunos sectores de Palestina y Líbano. Además del español ha derivado también, un dialecto hablado por los sefarditas como el judeoespañol o ladino y que está presente también en Oriente próximo.

El griego es hablado en la isla de Chipre junto con el turco y el portugués es lengua oficial junto con el tetun en Timor Oriental y se habla también en algunos sectores de la India como Goa, Dadra y Nagar Haveli y Damán y Diu, en el estado de Malaca en Malasia y en la región china de Macao.

Asia es el mayor productor global de alimentos, pero también el mayor consumidor y tiene las mayores reservas de la mayoría de los minerales. Asia es el continente que concentra el mayor crecimiento económico, consume la mayoría del crédito global, tiene un 80% del crecimiento económico mundial, también el mayor crecimiento de inversión en ciencia y tecnología, inversión en educación, y en el sector económico.
La cooperación entre el gobierno, las industrias y el dominio de la tecnología han llevado a Asia al éxito económico.

En minería, el continente tiene una gran producción de oro (principalmente en China, Rusia, Indonesia, Kazajistán y Uzbekistán);[29]​ plata (principalmente en China y Rusia);[30]​ cobre (principalmente en China, Rusia y Kazajistán);[31]​ platino (Rusia);[32]​ mineral de hierro (China, India, Rusia, Kazajistán y Turquía);[33]​ zinc (China, India, Kazajistán y Rusia);[34]​ molibdeno (China, Armenia, Irán, Rusia, Mongolia y Turquía);[35]​ litio (China);[36]​ plomo (China, Rusia, India, Turquía, Tayikistán y Kazajistán);[37]​ bauxita (China, India, Indonesia, Kazajistán, Rusia, Vietnam y Malasia);[38]​ estaño (China, Indonesia, Myanmar, Vietnam, Malasia, Rusia y Laos);[39]​ manganeso (China, India, Myanmar, Malasia, Vietnam, Kazajistán y Georgia);[40]​  mercurio (China, Tayikistán y Kirguistán);[41]​ antimonio (China, Rusia, Tayikistán, Myanmar, Turquía, Irán, Vietnam, Kazajistán y Laos);[42]​ niquel (Indonesia, Filipinas, Rusia y China);[43]​ renio (Corea del Sur, China, Kazajistán, Uzbekistán y Armenia);[44]​ yodo (Japón, Turkmenistán, Azerbaiyán, Indonesia y Rusia),[45]​ otros . 

En la producción de petróleo, el continente tenía 14 de los 30 mayores productores mundiales en 2020: Rusia (segundo), Arabia Saudita (tercero), Irak (quinto), China (sexto), Emiratos Árabes Unidos (séptimo) , Irán (9º), Kuwait (10º), Kazajistán (12º), Qatar (15º), Omán (18º), Indonesia (21º), Azerbaiyán (22º), India (23º) y Malasia (25º). [46]​

En la producción de gas natural, el continente tenía 18 de los 30 mayores productores mundiales en 2015: Rusia (2º), Irán (3º), Qatar (4º), China (6º), Arabia Saudita (8º), Indonesia (9º), Turkmenistán (10º), Malasia (13º), Emiratos Árabes Unidos (14º), Uzbekistán (15º), Tailandia (22º), Pakistán (23º), Omán (24º), Azerbaiyán (25º), India (26º), Bangladés (27º), Kazajistán (28º) y Baréin (30º). [47]​[48]​

En la producción de carbón, el continente tenía 10 de los 30 mayores productores mundiales en 2018: China (1°), India (2°), Indonesia (5°), Rusia (6°), Kazajistán (10°), Turquía (11°), Mongolia (14°), Vietnam (18°), Tailandia (23°) y Filipinas (28°).[49]​

En la producción de  vehículos, el continente tenía 10 de los 30 mayores productores mundiales en 2020: China (1°), Japón (3°), Corea del Sur (5°), India (6°), Rusia (10°), Tailandia (11°), Turquía (14°), Irán (18°), Indonesia (20°) y Malasia (23°). [50]​

En la producción de acero, el continente tenía 12 de los 30 mayores productores mundiales en 2019: China (1°), India (2°), Japón (3°), Rusia (5°), Corea del Sur (6°), Turquía (8º), Irán (10º), Taiwán (12º), Vietnam (14º), Bangladés (20º), Indonesia (25º) y Arabia Saudita (29º). [51]​[52]​

Desde 2004 la UE es el principal socio comercial de China, que a su vez es el segundo socio comercial de la organización europea.

En 2005, China se convirtió en la sexta economía mundial. Con un crecimiento oficial del 9,5 % anual, la economía china está considerada como la de mayor crecimiento del planeta, manteniendo una tasa media superior al 8% desde los años 1980.

También se destaca el Asia meridional con crecimientos anuales de 8 %.

Muchas zonas de Asia están económicamente subdesarrolladas. Un elevado porcentaje de la población del continente se dedica a la agricultura, pese a lo cual gran parte de la actividad agrícola se caracteriza por cosechas y productividad laboral relativamente bajas. En conjunto, una minoría de los asiáticos está empleada en actividades de manufactura; en muchas ocasiones los centros urbanos y las industrias no se han integrado adecuadamente con el sector rural. Los sistemas de transporte locales e internacionales de los países asiáticos todavía están poco desarrollados en muchas zonas, pero han mejorado notablemente en los últimos años.

Sin embargo, hay un creciente número de excepciones. Japón ha modernizado con éxito su economía, al igual que Israel, Corea del Sur, Singapur, Hong Kong y, en menor grado, Indonesia, Malasia, Tailandia, Turquía y los estados petrolíferos de la península arábiga. En general han conseguido tasas de crecimiento económico que superan el 5% anual, un porcentaje que se aleja de sus tasas de crecimiento demográfico. En cambio, aunque los países del suroeste de Asia han hecho progresos, la distribución de los ingresos ha quedado más concentrada que en otros países. Estimulada por las inversiones extranjeras a gran escala, la rápida privatización y la industrialización, la República Popular China consiguió el crecimiento más rápido de Asia a principios de la década de 1990. Se estima que la economía china creció un 12% en 1992, aunque los niveles de renta per cápita permanecieron relativamente bajos. Vietnam y Laos, dos de los países más pobres de Asia, están empezando a conseguir un significativo crecimiento económico y a captar un notable nivel de inversión extranjera.

En julio de 1997, la moneda tailandesa (el baht) se devaluó, contradiciendo las repetidas declaraciones de las autoridades gubernamentales de que eso no ocurriría. En cuestión de días, las monedas de Indonesia, Filipinas y Malasia fueron fuertemente atacadas y comenzaron a derrumbarse. Para fines de octubre, el won de Corea del Sur se colapsó y la crisis se generalizó en el continente asiático.

Puede sonar paradójico que apenas unos cuantos meses antes de las crisis estas economías eran vistas como sólidas y estables, con muy buen futuro económico. Incluso, días antes de las crisis, los analistas financieros y económicos parecían no tener la mínima noción de lo que pasaría. El hecho de que ambas economías formaran parte del grupo de las economías más vigiladas por la comunidad financiera internacional hace a estas dos características todavía más insólitas.


El hecho es que la mayoría de las economías de Asia Oriental sufrió una severa recesión en 1998; el PIB se desplomó: 14 % en Indonesia, 9 % en Tailandia, 7% en Malasia, 6 % en Corea, 5 % en Hong Kong y 3 % en Japón.Turismo
Con el creciente turismo regional con el dominio de los visitantes chinos, MasterCard ha publicado el Índice de Ciudades de Destino Global 2019 con 13 de 20 dominados por las ciudades de la región de Asia y el Pacífico y también por primera vez una ciudad de un país de Asia (Bangkok) en el primer puesto con 22.78 visitantes internacionales más. Dubái (4 °), Singapur (5 °), Kuala Lumpur (6 °), Estambul (8 °), Tokio (9 °), Seúl (11 °), Osaka (12 °), La Meca (13 °), Phuket (14 °), Pattaya (15 °) , Bali (19), Hong Kong (20).[53]​[54]​

Asia es un continente de gran riqueza espiritual y de diametral importancia religiosa ya que las religiones más practicadas del mundo surgieron en Asia, siendo cuna de las cinco grandes religiones mundiales – el judaísmo, el cristianismo, el islam, el budismo y el hinduismo. Debido a ello, países y Estados asiáticos como Israel y Palestina (lugares sagrados para judíos, cristianos y musulmanes), Arabia Saudita (donde se encuentran las dos ciudades más sagradas para los musulmanes) e India (donde se encuentran los lugares santos de hindúes y budistas), reciben millones de peregrinos de todo el mundo.

Otras importantes religiones surgidas en Asia son el bahaísmo (Irán), el confucianismo (China), el jainismo (India), el sijismo (India), el taoísmo (China), el zoroastrismo (Irán) y el sintoísmo (Japón).

Entre diferentes pueblos asiáticos también se practica el animismo y el chamanismo, desde los seguidores del culto Bön en Tíbet, y la religión tradicional china, hasta las creencias animistas de diferentes tribus indígenas de todo el continente asiático.

La religión mayoritaria de Asia es el budismo, practicado principalmente en el Este y Sudeste Asiático, aunque en muchos países es practicado de forma sincrética al lado de otra religión nacional (por ejemplo, el taoísmo, confucianismo y la religión tradicional china en China).
De cerca le sigue el islam predominante en el Medio Oriente y el Asia Central (desde la península del Sinaí hasta Pakistán) más Malasia e Indonesia. El hinduismo es otra de las grandes religiones de Asia aunque más geográficamente concentrada en el subcontinente indio, es mayoritario solo en India y Nepal, pero con minorías en muchos países asiáticos.

El judaísmo es mayoritario en Israel (único país donde es mayoría en el mundo), y el cristianismo es mayoritario en Filipinas y Timor Oriental, y tiene un alto porcentaje en Corea del Sur y Armenia[20]​ pero tiene importantes minorías en Asia, por ejemplo en Líbano, Siria, Turquía e Israel.

A diferencia de otros continentes, la religión en Asia es de gran importancia y en muchos países es vista como parte de la identidad étnica, lo que ha motivado más de un conflicto étnico, como lo son el conflicto árabe-israelí, el enfrentamiento entre hindúes y musulmanes en India, Pakistán y Bangladés, entre hindúes y budistas en Sri Lanka, entre musulmanes y budistas en Indonesia, entre musulmanes y cristianos en Líbano y entre hindúes y sikhs en India.

En el continente se practican diferentes estilos deportivos, aunque en el Lejano Oriente, los más populares son las artes marciales como el Judo, el Karate, el Aikido, el Kenjutsu, el Bojutsu, el Ninjutsu, el Kenpō, entre otros originarios de Japón. El Taichi, el Kung-fu, el Choy Lee Fut, el Shaolin, entre otros originarios de la China. El Taekwondo, el Hapkido, el Taekkyon, el Tangsudo, entre otros en Corea. El Eskrima, el Kali filipino y otros son originarios de Filipinas. Además también en el continente se han hecho famosos otros deportes como el Béisbol, Fútbol, Baloncesto, Natación, Esgrima, Tenis, Voleibol, etc...

Ciertos países asiáticos, agrupados a través del Consejo Olímpico de Asia, han destacado en diferentes torneos a nivel continental o internacional, como los Juegos Asiáticos celebrados a partir de 1951 en diferentes países del continente; los Juegos de la Mancomunidad; los Juegos de la Francofonía; Filipinas y Timor Oriental han participado en uno de los eventos deportivos de los Juegos Iberoamericanos.

Japón fue el primer país del continente en inaugurar los Juegos Olímpicos como Tokio 1964. También han sido sede olímpica Corea del Sur en Seúl 1988 y China en Pekín 2008. Además por primera vez dos países juntos, Japón y Corea del Sur, fueron los anfitriones de la Copa Mundial de Fútbol de 2002. También será Catar sede de la Copa Mundial de Fútbol de 2022. Yao Ming es el mejor deportista de la historia china juega en la NBA en los Houston Rockets, también Ji Sung Park uno de los mejores futbolistas sur-coreano que juega al fútbol. Japón es el seleccionado que más veces ha conquistado la Copa Asiática, en cuatro ocasiones.

También cabe destacar que Australia, a pesar de estar geográficamente ubicada en Oceanía, forma parte desde 2006 de la confederación de Asia. Con la obtención de la Copa Asiática 2015 se convirtió en la primera selección del mundo en lograr el curioso récord de ser el único país campeón de dos continentes distintos, ya que anteriormente conquistó la Copa de las Naciones de la OFC en cuatro ocasiones.

El ecologismo (en ocasiones llamado el movimiento verde o ambientalista) es un variado movimiento político, social y global, que defiende la protección del medio ambiente.[1]​

Habitualmente, el ecologismo (también llamado ambientalismo) se defiende desde posiciones antropocéntricas, es decir, para satisfacer una necesidad humana, incluyendo necesidades de salud y sociales. En esos términos, los ecologistas hacen una crítica social más o menos implícita, proponiendo la necesidad de reformas legales y concienciación social tanto en gobiernos, como en empresas y colectivos sociales. El movimiento ecologista está unido con un compromiso para mantener la salud del ser humano en equilibrio con los ecosistemas naturales, se considera la humanidad como una parte de la naturaleza y no algo separada de ella. Una defensa pura del ecologismo se hace desde planteamientos ecocéntricos, dando prioridad a los ecosistemas y a las especies sobre los individuos -sean humanos o de otras especies. Este posicionamiento se enmarca normalmente dentro del ecologismo radical.

La existencia de organizaciones ecologistas está estrechamente ligada al desarrollo de los sistemas democráticos y al progreso de las libertades civiles. El movimiento está representado por una amplia y variada gama de organizaciones no gubernamentales, desde el nivel global hasta la escala local. Algunos cuentan con decenios de historia y disponen de importantes infraestructuras a nivel internacional; aunque la mayoría lo forman organizaciones locales de carácter más o menos espontáneo.

La ecología política se enfoca en conseguir modificaciones significativas en las políticas ambientales de todos los Estados del mundo. Hay quienes proponen un cambio radical en el sistema de Estado y se niega la necesidad de más desarrollo en el sentido convencional o capitalista, mientras otros solo proponen un cambio en la política ambiental, y otros un cambio profundo en la forma de las relaciones sociales y ambientales de producción.

Posiblemente esta política nace en el momento en que se hace patente el deterioro del medio ambiente a causa de los experimentos o el desconocimiento de la actividad humana. En el informe  Los límites del crecimiento derivado del club de Roma de los Estados Unidos, nace la inquietud y surgen multitud de grupos políticos ambientalistas o ecologistas en ese país. La ecología política y el ecologismo no siempre son partidarios del ecocentrismo absoluto, sino que, generalmente, suele partir de posiciones antropocentristas.

El movimiento ecologista surge entre los años setenta y ochenta en Occidente, a partir de la denuncia social del dominio hacia la naturaleza con fines de desarrollo.[2]​ [3]​ El movimiento ecologista tiene tres raíces principales: conservación y regeneración de los recursos naturales, preservación de la vida silvestre, y el movimiento para reducir la contaminación y mejorar la vida urbana.

El 26° presidente de los Estados Unidos de América Theodore Roosevelt, prominente conservacionista, fue el primero en tratar el tema de la Conservación ambiental en la agenda política de los Estados Unidos, aunque más centrado en condiciones de vida saludables que en cuestiones ecológicas.

El movimiento ecologista moderno se expresó de forma más apasionada en la cúspide de la era industrial: cerca del tercer cuarto del siglo XX. Los clásicos ecologistas modernos empezaron en ese período con el trabajo de Rachel Carson que proveyó el primer toque de atención científica sobre la muerte del planeta debido a la actividad humana.

Durante los años 50, 60 y 70, ocurrieron varios eventos que avivaron la conciencia medioambiental del daño al entorno causado por el hombre. En 1954, los 23 miembros de la tripulación del buque pesquero Daigo Fukuryū Maru fueron expuestos a radiactividad de una prueba de bomba de hidrógeno en el atolón Bikini. En 1969 hubo un vertido en una excavación petrolífera en el Canal de Santa Bárbara de California. Otros hechos importantes fueron la protesta de Barry Commoner contra los ensayos nucleares, el libro Silent Spring (Primavera silenciosa) de Rachel Carson así como The Population Bomb (La bomba demográfica) de Paul R. Ehrlich. Estos libros aumentaron la inquietud e interés sobre el medio ambiente.

El movimiento ecologista inicial se centraba fuertemente en la reducción de la contaminación y en la protección de las reservas de recursos naturales tales como agua y aire. Las presiones de desarrollo en rápida expansión también acuciaron considerables esfuerzos para preservar territorios únicos y hábitats de vida silvestre, para proteger las especies en peligro de extinción antes de que desapareciesen. En los Estados Unidos, durante la década de 1970 se aprobaron leyes como el Clean Water Act, Clean Air Act, Endangered Species Act y National Environmental Policy Act (Decreto Ley de Agua Limpia, Decreto Ley de Aire Limpio, Decreto Ley de Especie en Peligro de Extinción, y Decreto Ley de Política Medioambiental Nacional, respectivamente), las cuales han sido los cimientos para los estándares medioambientales.

Gracias al movimiento ecologista, la conciencia pública y las ciencias del medioambiente han mejorado en los últimos años. Las preocupaciones medioambientales se han ampliado, incluyendo conceptos como la sostenibilidad, el agujero en la capa de ozono, el cambio climático, la lluvia ácida, y la contaminación genética.

La mayoría de los ecologistas tienen objetivos similares, aunque pueden no estar de acuerdo en los detalles como el énfasis, las prioridades o el comportamiento individual. Los movimientos ecologistas a menudo interaccionan o están ligados con otros movimientos sociales con puntos de vista morales parecidos, como el movimiento pacifista, los derechos humanos o los derechos de los animales; contra las armas nucleares o la energía nuclear, las enfermedades endémicas, la pobreza, el hambre, etc.

«La ecología sin lucha social es simplemente jardinería»
Los ecologistas, desde sus inicios, se vieron atravesados por las diferentes ideologías que existían en el ámbito de las sociedades. Los movimientos vinculados con la concepción tradicional liberal no hacían hincapié en la gestión del capitalismo en la relación en la distribución de los recursos. Los movimientos socialistas seguían la ideología del desarrollo económico vigente. Sin embargo, parte de estos comenzaron a tener en cuenta que los recursos son limitados. De ahí, que naciera la convergencia entre el socialismo tradicional y el ecologismo, y diese lugar al ecosocialismo. Este movimiento trata la distribución de los recursos, qué posibilidades de gestión hay de estos a través del modelo que hoy tenemos, los límites de los recursos y la distribución de los riesgos. Al igual que el socialismo se va fragmentando, el feminismo también. Pues se comienzan a atender las diferencias entre los países de la periferia y los del centro, y surge en la periferia el ecofeminismo, que ha subrayado, entre otras muchas cosas, que las mujeres suelen ser en la gran mayoría de los países una de las partes más perjudicadas en los casos de injusticias y desastres medioambientales.

Así bien, en la década de los 70 ante la crisis petrolífera se acrecientan los problemas de contaminación medioambiental, la masificación urbana y una serie de catástrofes dan lugar a la puesta en marcha de un proceso de conciencia del ecologismo y surgimiento de numerosas plataformas, organizaciones y movimientos de tipo ecologistas en todo el mundo para tratar de encontrar y fomentar un respeto por el medio ambiente. El eco del movimiento ecologista comienza a alcanzar una resonancia internacional, rebasando los límites de los grupos activistas para comenzar a instalarse en la conciencia de la opinión pública, especialmente en los países industrialmente avanzados, donde la degradación del medio ambiente comienza a deteriorar los niveles de calidad de vida. Los primeros grupos que aparecen son diversos y se caracterizan por presentar diferentes tendencias: conservacionistas, institucionales y radicales.

En esta década se destaca la aparición de organizaciones de carácter institucional como las ONG ecologistas y los partidos políticos verdes, movimientos de izquierdas interesados en resolver los problemas medioambientales que surgen a partir de los años 1970 y 80. Se observan dos importantes agrupaciones como Greenpeace, una asociación que se forma de manera espontánea por un grupo de activistas antinucleares canadienses en 1971. Es una organización no gubernamental, que no depende política ni económicamente del Estado, cuyo objetivo es defender y proteger el medio ambiente realizando campañas de conciencia, protección medioambiental o actos directos de intento de boicot de empresas o instituciones que tratan de perjudicar al medio ambiente. Otra organización que nace por entonces es WWF/Adena, en 1968, como consecuencia de una actuación militante a favor de la protección de los espacios naturales. Una organización de carácter radical fue Frente de Liberación Animal (FLA), que surge de manera clandestina a principio de los 70 y se caracteriza por el empleo de la acción directa como método de lucha.

El ecologismo se desarrolla bajo diversos enfoques o ámbitos que atienden distintas preocupaciones relacionadas con el medio ambiente:

Un informe publicado en 1972 por el Club de Roma de los Estados Unidos, llamado The Limits to Growth (Los límites del crecimiento) esbozó algunas de las preocupaciones de los ecologistas. Otro informe del mismo país, llamado The Global 2000 Report to the President (El Informe Global al Presidente), publicado más tarde por el Consejo de Calidad Medioambiental, informaba hallazgos similares pero fue ampliamente ignorado. Más recientemente el Millennium Ecosystem Assessment (Evaluación del Ecosistema del Milenio) aporta vindicación al movimiento.

El movimiento ecologista, en la defensa de sus valores, reivindica y promueve iniciativas y reformas en diversas áreas. Entre ellas se pueden mencionar las siguientes:[5]​

El derecho ambiental pertenece a la rama del derecho social y es un sistema de normas jurídicas que regulan las relaciones de las personas con la naturaleza, con el propósito de preservar y proteger el medio ambiente en su afán de dejarlo libre de contaminación, o mejorarlo en caso de estar afectado. Sus objetivos son la lucha contra la contaminación, la preservación de la biodiversidad, y la protección de los recursos naturales, para que exista un entorno humano saludable.

Es una rama del Derecho bastante reciente, pero de gran desarrollo y futuro, surgiendo a mediados del siglo XX por la concienciación de la sociedad a consecuencia de algunos desastres ecológicos como la contaminación de la bahía de Minamata, el gran smog londinense, los escapes de Seveso o Bophal, y el accidente de Chernóbil, entre otros. Su origen, como tal especialización del Derecho, surge en la Conferencia de Naciones Unidas sobre el Medio Humano, celebrada en Estocolmo en 1972.

Entre sus características está su carácter multidisciplinario, ya que requiere la pericia y el asesoramiento de profesionales ajenos al Derecho (médicos, biólogos, ambientólogos, físicos, químicos, ingenieros, etc.) y estar en continuo cambio y actualización, en la misma medida que se producen avances científicos y técnicos.[15]​

El derecho ambiental se caracteriza por tener el trabajo del estado enfocado en realizar una "Zonificacion Ecológica y Económica" mediante los gobiernos regionales y locales, planificar bien las áreas destinadas para la vida en sociedad, el turismo, la producción agrícola. Evitando que la sociedad ocupe estos lugares destinados para un uso sostenible y generando una producción económica y un bienestar social, mediante los estudios de urbanización y producción del Gobierno Local.[16]​

Los objetivos del derecho ambiental se apuntan como fines de esta materia: tomar viable un objetivo primario, macro-objetivo, ligado con la sustentabilidad y el "estado socio-ambiental del derecho" según Antonio H. Benjamín,[17]​ y de varios objetivos secundarios, micro objetivos secundarios, tales como la protección de la salud y seguridad humanas, salvaguarda de la biosfera por si, conservación del patrimonio estético, turístico, paisajístico, prevención, reparación y represión del daño ambiental,[18]​ facilidad de acceso a la justicia, transparencia y libre circulación de la información ambiental, eficiencia económica, tutela de la propiedad, conocimiento científico y tecnológico, estabilidad social, democratizacion de los procesos decisorios ambientales, etc.

Las grandes organizaciones ecologistas que trabajan por el medio ambiente o por algún aspecto específico del mismo en el ámbito internacional comprenden fundamentalmente el Fondo Mundial para la Naturaleza (WWF), Amigos de la Tierra, Greenpeace, BirdLife Internacional (dedicada de modo concreto a la protección de las aves y sus hábitats), así como el Movimiento Mundial por los Bosques Tropicales (WRM) (centrado en la conservación de los bosques y selvas tropicales), entre otras.

El ecoterrorismo es el uso de prácticas terroristas en apoyo a causas ecologistas, medioambientales,[19]​ o de derechos de los animales.[20]​ No debe confundirse con el terrorismo ambiental, o ataques terroristas contra el medioambiente, que consisten en el uso ilegal de la fuerza contra recursos ambientales in situ con el fin de despojar a las poblaciones de sus beneficios y/o en la destrucción de propiedades ajenas.[19]​La palabra es un neologismo y su aplicación es controvertida, dado que a veces puede tener ambos sentidos.

Según la organización Global Witness en 2017 fueron asesinadas 197 personas por enfrentarse a gobiernos y empresas que robaron sus tierras y dañaron el medio ambiente, y por denunciar las prácticas corruptas e injustas que lo permitieron. El diario The Guardian en un informe especial ha publicado la lista de nombres.[24]​ En ese año América Latina mantiene el primer lugar del ranking de asesinatos. México en el mismo ocupa el cuarto lugar en la lista mundial de países más peligrosos para ser activista a favor del medio ambiente -antes ocupaba el puesto número 14-. Honduras, Filipinas y Brasil se sitúan en los primeros lugares donde se producen estos asesinatos.[24]​Entre los asesinatos de 2017 se encuentran el mexicano Isidro Balenegro López, activista mexicano y ganador del Premio Goldman del medio ambiente o Wayne Lotter, destacado activista contra la comercialización del marfil en Tanzania asesinado en agosto de 2017.[25]​[26]​

En gran parte debido a esta crítica política y confusión, una preocupación creciente por los problemas de salud ambiental causados por los pesticidas, algunos biólogos serios y ecologistas crearon un movimiento ecológico científico que no confundiese datos empíricos con visiones de un mundo futuro mejor.

En la actualidad es la ciencia de la ecología, más que los objetivos estéticos, la que provee la base de unidad a la mayoría de ecologistas. Todos aceptarían cierto nivel de contraste científico en las decisiones acerca de biodiversidad o uso forestal. La Biología de la conservación es un campo de importancia y en rápido desarrollo.

No obstante, el movimiento ambiental actualmente persiste en muchos grupos locales pequeños, frecuentemente dentro de ecorregiones, promoviendo los valores estéticos y espirituales que Thoreau o aquellos que reescribieron la respuesta del Jefe Seattle reconocerían. Algunos se asemejan al antiguo movimiento de conservación estadounidense - cuya expresión moderna la forman Nature Conservancy, National Audubon Society y National Geographic Society - organizaciones estadounidenses de influencia mundial.

Estos grupos políticamente neutrales tienden a evitar conflictos globales y ver el acuerdo de un conflicto entre humanos separado de lo que respecta a la naturaleza - en contradicción directa con el movimiento de la Ecología y el movimiento por la Paz que tienen un número creciente de estrechos enlaces: Mientras que partidos verdes, la Sea Shepherd Conservation Society, Greenpeace, y grupos como The Activist Magazine, por ejemplo, ven la ecología y la biodiversidad y un final de las extinciones como algo absolutamente básico para la paz, algunos grupos locales puede que no, y pueden ver un alto nivel de competición global y conflicto como justificable si les permite preservar sus propia identidad local. Esto les resulta egoísta a algunos. No obstante, esos grupos no tienden a quemarse sino a sostenerse por largos períodos, incluso generaciones, protegiendo tesoros locales. La Water Keepers Alliance es un buen ejemplo de este tipo de grupos que se aferran a las cuestiones locales.

Las visiones y confusiones, sin embargo, persisten. La nueva visión tribalista de la sociedad, por ejemplo, se hace eco de las preocupaciones de los primeros ecologista en cierto grado. Y un número en aumento de grupos locales encuentran el beneficio de la colaboración, como con métodos de decisiones por consenso o políticas simultáneas, o confiando en recursos legales comunes, o incluso un glosario común. A pesar de esto, las diferencias entre los distintos grupos que componen el movimiento medioambiental moderno tienden a tener más peso que esas similitudes, y raramente cooperan directamente excepto en las cuestiones globales más importantes.

Grupos como The Bioregional Revolution están haciendo una llamada sobre la necesidad de tender un puente entre estas diferencias, pues afirman que los problemas que convergen en el siglo XXI nos obligan a tomar una acción decisiva. Promueven el biorregionalismo, la permacultura, y las economías locales como solución a estos problemas, la sobrepoblación, el cambio climático, las epidemias globales, y la escasez de agua pero más notablemente a la Teoría del pico de Hubbert -- la predicción de que es probable de que lleguemos a un máximo en la producción global de petróleo que podría significar cambios drásticos en muchos aspectos de nuestra vida diaria.

A lo largo de los años, la problemática de la contaminación y destrucción del medio ambiente se ha intentado plantear desde múltiples perspectivas y soportes, buscando siempre causar un efecto modificador de la conducta que realmente nos lleve a una definitiva concienciación sobre la necesidad de conservar nuestro entorno.[cita requerida] Sin embargo, aunque ciertamente se hayan conseguido algunos progresos, todavía queda mucho camino por andar.[cita requerida]

Uno de los medios que más se están empleando en los últimos tiempos para hacer llegar al público ideas medioambientales es el cine. El lenguaje cinematográfico tiene mucho de emocional, y tal vez esta vía sea más indicada que la racional a la hora de hablar directamente a la conciencia humana.[cita requerida]

Tal y como asegura el profesor Enrique Martínez-Salanova en su página web Cine y Educación,[27]​ "el cine no solamente ha puesto en contacto al hombre con la naturaleza, los paisajes exóticos y el documental de naturaleza, sino que además ha sido, y sigue siendo en ocasiones, militante activo en la lucha por la defensa del medio ambiente". Además, el cine ha sido, desde su nacimiento, el más poderoso vehículo de transmisión de conocimientos y de culturas, aportando a sus espectadores infinitas posibilidades de encuentro con paisajes, naturaleza, lugares y costumbres.

Películas tan antiguas como Tarzán (1918) o King Kong (1933), ya basaban su argumento en la intromisión del hombre blanco en las selvas vírgenes. El compromiso ya estaba, por tanto, presente desde los orígenes del cine, aunque con los años se ha ido haciendo más manifiesto.[cita requerida]

El Festival Internacional de Cine de Medio Ambiente (FICMA) se celebra en Barcelona desde 1993.[cita requerida]

La EFFN (Environmental Film Festival Network), aglutina festivales de cine ambiental de países como los Estados Unidos, Argentina, Perú, España, Portugal, Ghana, Israel, Francia y México [cita requerida].

Invasión alemana de Polonia

 Alemania nazi
 Reino de Italia
 Imperio del Japón

 Francia de Vichy
 Reino de Hungría
 Reino de Rumania
 Albania
 Eslovaquia
 Reino de Bulgaria
 Croacia
 Mengjiang
 Manchukuo
 Chetniks
 India libre
 Birmania
 Montenegro
 Vietnam
 Finlandia
 Serbia
 Corea
 Nankín
 Tailandia

 Reino Unido
 Unión Soviética
 China
 Estados Unidos
 Francia

 Polonia
 Checoslovaquia
 Noruega
 Países Bajos
 Dinamarca
 Bélgica
 Luxemburgo
 Islandia
 Resistencia italiana
 Imperio etíope
 Reino de Grecia 
 Reino de Yugoslavia
 Filipinas
 Raj británico
 Canadá
 Australia
 Nueva Zelanda
 Dominio de Terranova
 Argelia francesa
 Marruecos
 Brasil
 Túnez
 Sudáfrica
 Nigeria
 Indochina francesa
 Tanganica
 Malta
 Hong Kong
 Reino de Nepal
 Congo Belga
 Liberia
 Tonga
 Transjordania
 Malasia británica
 Indias Orientales Neerlandesas
 Colonias del Estrecho
 Nuevas Hébridas
 Sarawak 
 Birmania británica
 Brunéi
 Rodesia del Sur
 Reino de Egipto
 Mongolia
 Tannu Tuvá
 Cuba

Guerra del Pacífico

Mediterráneo y Oriente Medio

Otras campañas

Guerras contemporáneas

La Segunda Guerra Mundial fue un conflicto militar global que se desarrolló entre 1939 y 1945. En ella se vieron implicadas la mayor parte de las naciones del mundo —incluidas todas las grandes potencias, así como prácticamente todas las naciones europeas—, agrupadas en dos alianzas militares enfrentadas: los Aliados, por un lado, y las potencias del Eje, por otro. Fue la mayor contienda bélica de la historia, con más de cien millones de militares movilizados y un estado de guerra total en que los grandes contendientes destinaron toda su capacidad económica, militar y científica al servicio del esfuerzo bélico, borrando la distinción entre recursos civiles y militares. Marcada por hechos de enorme repercusión que incluyeron la muerte masiva de civiles —el Holocausto, los bombardeos intensivos sobre ciudades y el uso, por primera vez en un conflicto militar, de armas nucleares—, la Segunda Guerra Mundial fue la más mortífera de la historia, con un resultado de entre 50 y 70 millones de víctimas, el 2,5 % de la población mundial.[1]​

El comienzo del conflicto se suele situar en el 1 de septiembre de 1939, con la invasión alemana de Polonia, cuando Hitler se decidió a la incorporación de una de sus reivindicaciones expansionistas más delicadas: el Corredor Polaco, que implicaba la invasión de la mitad occidental de Polonia; la mitad oriental, junto con Estonia, Letonia y Lituania fue ocupada por la Unión Soviética, mientras que Finlandia logró mantener su independencia de los soviéticos (guerra de Invierno). El Reino Unido y Francia le declararon la guerra a Alemania, que esperaban como una repetición de la guerra de trincheras («guerra de mentira») para la que habían tomado toda clase de precauciones (línea Maginot) que demostraron ser del todo inútiles. Las maniobras espectaculares de la blitzkrieg (guerra relámpago) proporcionaron en pocos meses a Alemania el control de Noruega, Dinamarca, Países Bajos, Bélgica y la propia Francia, mientras que el ejército británico escapaba in extremis desde las playas de Dunkerque durante la batalla de Francia. La mayor parte del continente europeo estaba ocupado por el ejército alemán o por sus aliados, entre los que destacaba la Italia fascista, cuya aportación militar no fue muy significativa (batalla de los Alpes, guerra greco-italiana).

La batalla de Inglaterra, la primera completamente aérea de la historia, mantuvo durante el periodo siguiente la presión sobre el nuevo gobierno de Winston Churchill, decidido a la resistencia («sangre, sudor y lágrimas») y que finalmente venció, entre otras cosas gracias a una innovación tecnológica (el radar) y al decisivo apoyo estadounidense, que negoció en varias entrevistas con Franklin D. Roosevelt (Carta del Atlántico, 14 de agosto de 1941).

En 1941, la necesidad estratégica de ocupar los campos petrolíferos del Cáucaso impulsó a Alemania a invadir la Unión Soviética (operación Barbarroja), inicialmente exitosa, pero que se estancó en la batalla de Moscú y los sitios de Leningrado y Stalingrado. Al mismo tiempo, Japón, en su campaña de expansión por Asia y en venganza por el embargo económico que el gobierno estadounidense les había impuesto, atacó Pearl Harbor el 7 de diciembre de 1941; la agresión precipitó la entrada de Estados Unidos en la guerra. Pocos meses después, la batalla de Midway (en julio de 1942) marcaría un punto de inflexión en la guerra del Pacífico ante el debilitamiento de la capacidad de combate japonesa frente a los estadounidenses. En el norte de África, los británicos frenaron el avance de los Afrika Korps alemanes desde Libia hacia Egipto en la batalla de El Alamein (1942), después de la invasión italiana al canal de Suez (1940).

El periodo final de la guerra se caracterizó por las complejas operaciones necesarias para los desembarcos aliados en Europa (Sicilia, en julio de 1943; Anzio, en enero de 1944; Normandía, en junio de 1944) y por el hundimiento del frente oriental, en el que se libraron las operaciones con tanques más encarnizadas de la historia (batalla de Kursk, especialmente en Projorovka, julio de 1943), mientras en el frente occidental los alemanes experimentaban armas tecnológicamente muy desarrolladas (misiles V-1 y V-2) y soportaban bombardeos destructivos sobre sus ciudades a una escala nunca antes vista (bombardeo de Dresde, en febrero de 1945) y la destrucción total de su capital (batalla de Berlín, entre abril y mayo de 1945).

En el frente del Pacífico, los estadounidenses tuvieron que desalojar isla a isla a los japoneses, tanto en el sur del Pacífico (Guadalcanal, en agosto de 1942) como en Filipinas (Manila, en febrero de 1945); tras librar las mayores batallas navales de la historia (batalla del Mar del Coral, en mayo de 1942; batalla del Golfo de Leyte, en octubre de 1944), alcanzaron tierras niponas (Iwo Jima, en febrero de 1945 y Okinawa, en abril de 1945). En agosto de 1945, el presidente de Estados Unidos, Harry S. Truman ordenó bombardear con las recién inventadas armas nucleares las ciudades de Hiroshima y Nagasaki. La devastación causada por el ataque, que a la larga se cobraría la vida de 250 000 personas, precipitó la capitulación de Japón.

A diferencia de la Primera Guerra Mundial, la rendición (tanto la japonesa como la alemana) se produjo por derrota incondicional, sin pasar por ningún tipo de negociación. Las conversaciones decisivas fueron las que plantearon la división de Europa en zonas de influencia entre los aliados, y que se negociaron en sucesivas cumbres (conferencia de Teherán, el 1 de diciembre de 1943; conferencia de Yalta, en febrero de 1945; y conferencia de Potsdam, en julio de 1945).

La Segunda Guerra Mundial alteró las relaciones políticas y la estructura social del mundo. Tras la conflagración, se fundó la Organización de las Naciones Unidas con el fin de fomentar la cooperación internacional y de prevenir potenciales conflictos. La Unión Soviética y Estados Unidos se erigieron como superpotencias rivales, estableciéndose el escenario para la Guerra Fría, que se prolongó durante los siguientes 46 años. Al mismo tiempo, la influencia de las grandes potencias europeas entró en decadencia, materializada en el inicio de la descolonización de Asia y África. La mayoría de los países cuyas industrias habían sido perjudicadas abordaron la recuperación económica con la ayuda financiera del país americanos (plan Marshall), mientras que la integración política emergía como un esfuerzo para establecer las relaciones de posguerra.

En general se considera que la guerra comenzó en Europa el 1 de septiembre de 1939[2]​[3]​ con la invasión alemana de Polonia, que provocó la declaración de guerra de Reino Unido y Francia a Alemania dos días después. Las fechas de inicio de las hostilidades en la zona del océano Pacífico son varias y anteriores en el tiempo: la segunda guerra chino-japonesa que comenzó el 7 de julio de 1937[4]​[5]​ o incluso la invasión japonesa de Manchuria a partir del 19 de septiembre de 1931.[6]​[7]​

Otros coinciden con el historiador británico A. J. P. Taylor, que sostenía que la guerra chino-japonesa y la guerra en Europa y sus colonias ocurrieron simultáneamente y ambas se desataron en 1941. Otra fecha de inicio a veces usada para la Segunda Guerra Mundial es la invasión italiana de Etiopía desde el 3 de octubre de 1935.[8]​ El también historiador Antony Beevor opina que la conflagración comenzó con la batalla de Jaljin Gol entre Japón y las fuerzas de Mongolia y la URSS, de mayo a septiembre de 1939.[9]​ En este artículo se seguirá la datación convencional.

La fecha exacta del fin de la guerra tampoco tiene un consenso universal. Generalmente se ha aceptado que el conflicto terminó con el armisticio japonés del 14 de agosto de 1945, en lugar de la rendición formal de Japón, que se produjo el 2 de septiembre y que puso final definitivo a las hostilidades en Asia. En 1951 se firmó un tratado de paz con Japón.[10]​ Décadas después, en 1990, un tratado sobre el futuro de Alemania permitió la reunificación del país y resolvió muchos de los problemas de la posguerra en Europa.[11]​ Japón y la URSS no firmaron nunca un tratado de paz formal.[12]​

Las causas bélicas del estallido de la Segunda Guerra Mundial son, en Occidente, la invasión de Polonia por las tropas alemanas y, en Oriente, la invasión japonesa de China, las colonias británicas y neerlandesas y posteriormente el ataque a Pearl Harbor.

La Segunda Guerra Mundial estalló después de que estas acciones agresivas recibieran como respuesta una declaración de guerra, la resistencia armada o ambas, por parte de los países agredidos y aquellos con los que mantenían tratados. En un primer momento, los países aliados estaban formados tan solo por Polonia, Reino Unido y Francia, mientras que las fuerzas del Eje las constituían únicamente Alemania e Italia en una alianza llamada el Pacto de Acero. A medida que la guerra progresó, los países que iban entrando en ella (por ser atacados o tener tratados con los países agredidos) se alinearon en uno de los dos bandos, dependiendo de cada situación. Ese fue el caso de los Estados Unidos y la URSS, atacados respectivamente por Japón y Alemania. Algunos países, como Hungría o Italia, cambiaron sus alianzas en las fases finales de la guerra.

El Tratado de Versalles, establecía la compensación que Alemania debía pagar a los vencedores de la Primera Guerra Mundial. El Reino Unido obtuvo la mayor parte de las colonias alemanas en África y Oceanía (aunque algunas fueron a parar a manos de Japón y Australia). Francia, en cuyo suelo se libraron la mayor parte de los combates del frente occidental, recibió como pago una gran indemnización económica y la recuperación de Alsacia y Lorena, que habían sido anexionadas a Alemania por Otto von Bismarck tras la Guerra Franco-prusiana en 1870.

En el Imperio ruso, la Dinastía Románov había sido derrocada y reemplazada por un gobierno provisional que a su vez fue derrocado por los bolcheviques de Lenin y Trotsky. Después de firmar el Tratado de Brest-Litovsk, los bolcheviques tuvieron que hacer frente a una guerra civil, que vencieron, creando la URSS en 1922. Sin embargo, ésta había perdido mucho territorio por haberse retirado prematuramente de la guerra. Estonia, Letonia, Lituania y Polonia resurgieron como naciones a partir de una mezcla de territorios soviéticos y alemanes tras el Tratado de Versalles.

En Europa Central, aparecieron nuevos estados tras el desmembramiento del Imperio Austrohúngaro: Austria, Hungría, Checoslovaquia y Yugoslavia. Además, el extinto Imperio tuvo que ceder territorios a la nueva Polonia, a Rumanía y a Italia.

En Alemania, el Tratado de Versalles tuvo amplio rechazo popular: bajo su cobertura legal se había desmembrado el país, la economía alemana se veía sometida a pagos y servidumbres a los Aliados considerados abusivos, y el Estado carecía de fuerzas de defensa frente a amenazas externas, sobre todo por parte de la URSS, que ya se había mostrado dispuesta a expandir su ideario político por la fuerza. Esta situación percibida de indefensión y represalias abusivas, combinada con el hecho de que nunca se llegó a combatir en territorio alemán, hizo surgir la teoría de la Dolchstoßlegende (puñalada por la espalda), la idea de que en realidad la guerra se podía haber ganado si grupos extranjeros no hubieran conspirado contra el país, lo que hacía aún más injusto el ser tratados como perdedores. Surgió así un gran rencor a nivel social contra los Aliados, sus tratados, y cualquier idea que pudiera surgir de ellos.

La desmovilización forzosa del ejército hasta la fuerza máxima de 100 000 hombres permitida por el tratado (un tamaño casi testimonial respecto al anterior) dejó en la calle a una cantidad enorme de militares de carrera que se vieron obligados a encontrar un nuevo medio de subsistencia en un país vencido, con una economía en pleno declive, y tensión social. Todo eso favoreció la creación y organización de los Freikorps, así como otros grupos paramilitares. La lucha de los Freikorps y sus aliados contra los movimientos revolucionarios alemanes como la Liga Espartaquista (a veces con la complicidad o incluso el apoyo de las autoridades) hizo que tanto ellos como los segmentos de población que les apoyaban se fueran inclinando cada vez más hacia un ideario reaccionario y autoritario, del que surgiría el nazismo como gran aglutinador a finales de los años 20 e inicios de los 30. Hasta entonces, había sido un partido en auge, pero siempre minoritario; un intento prematuro de hacerse con el poder por la fuerza (el Putsch de Múnich) acabó con varios muertos, el partido ilegalizado y Hitler en la cárcel. Durante ese periodo de encarcelamiento Hitler escribió el Mein Kampf (Mi lucha), el libro en el que sintetizó su ideario político para Alemania.

El caldo de cultivo existente a nivel social, combinado con la Gran Depresión de inicios de los 30, hizo que la débil República de Weimar no fuera capaz de mantener el orden interno; los continuos disturbios y conflictos en las calles incrementaron la exigencia de orden y seguridad por parte de sectores de la población cada vez más amplios. Sobre esa ola de descontento y rencor, el Partido Nazi, liderado por Adolf Hitler se presentó como el elemento necesario para devolver la paz, la fuerza y el progreso a la nación. Los ideólogos del partido establecieron las controvertidas teorías que encauzarían el descontento y justificarán su ideario: la remilitarización era imprescindible para librarse del yugo opresor de las antiguas potencias aliadas; la inestabilidad del país era ocasionada por movimientos sociales de obediencia extranjera (comunistas) o grupos de presión no alemanes (judíos), culpables además de haber apuñalado por la espalda a la Gran Alemania en 1918; además, Alemania tiene derecho a recuperar los territorios que fueron suyos, así como asegurarse el necesario espacio vital (Lebensraum) para asegurar su crecimiento y prosperidad. Todas estas ideas quedaron plasmadas en el Mein Kampf.

Partiendo de la sensación de afrenta originada por el Pacto de Versalles, los nazis potenciaron, alimentaron y extendieron la necesidad de reparación en la sociedad alemana, mezclando los problemas reales con las necesidades de su propio programa político, presentando el militarismo y la adherencia a la disciplina fascista como las únicas vías capaces de reconducir la situación. Así se justificó la represión brutal de cualquiera que no pensara del mismo modo o fuera percibido como un enemigo del Estado. Y el clima existente a causa del Pacto hizo que aparte de la sociedad no le preocupase lo más mínimo el incumplimiento de cualquier tipo de tratado internacional. Hasta 1932, el NSDAP fue incrementando su cuota electoral en las elecciones federales, manteniendo un estilo político igual de bronco y agresivo que el que practicaba en la calle.

En noviembre de 1932 tienen lugar las octavas elecciones federales alemanas, en las que el NSDAP logra un 33,1 % de votos (aunque bajó algo más de un 4 %). Al ser la lista más votada y ante la imposibilidad de lograr una opción de consenso entre las demás fuerzas políticas, el presidente Hindenburg nombra canciller a Hitler y le ordena formar gobierno.

El 27 de febrero de 1933, un incendio arrasa el Reichstag, la sede del parlamento alemán. A raíz de este suceso, Hitler declara el estado de excepción. Pronto surge desde el partido nazi la acusación de que los comunistas son los instigadores de la quema, y Hitler logra que un Hindenburg ya muy mermado de salud firme el Decreto del Incendio del Reichstag, aboliendo tanto al partido comunista como a cualquier organización afín a ese partido.

Con sus principales enemigos políticos ilegalizados, Hitler procedió a convocar las novenas elecciones federales alemanas el 5 de marzo de 1933. Esta vez logra un 43,9 % de votos y pasa a gobernar, en coalición con el DNVP, en mayoría absoluta. Una vez conseguido el poder político, para lograr el apoyo de la cúpula del ejército (Reichswehr), ordenó asesinar a los dirigentes de las SA, en la llamada noche de los cuchillos largos, la noche del 30 de junio al 1 de julio de 1934.

Hitler restauró en Alemania el servicio militar generalizado que había sido prohibido por el Tratado de Versalles, remilitarizó la Renania en 1936 y puso en práctica una política extranjera agresiva, el pangermanismo, inspirada en la búsqueda del Lebensraum, destinada a reagrupar en el seno de un mismo estado a la población germana de Europa central, comenzando por Austria (Anschluss) en marzo de 1938.

El principal objetivo declarado de la política exterior alemana de la época inmediatamente anterior a la guerra era, por una parte, la recuperación de esos territorios, así como del Corredor polaco y la Ciudad libre de Dánzig, en los antiguos territorios de Prusia perdidos por Alemania después de 1918. Esas reclamaciones territoriales constantes constituían elementos importantes de inestabilidad internacional, pues Berlín reivindicaba abiertamente su restitución, de forma cada vez más agresiva, con la intención de reconstruir la Gran Alemania Großdeutschland.

El apoyo al levantamiento militar del general Francisco Franco en España por parte de Italia y Alemania con tropas y armamento desafió abiertamente al acuerdo de no-intervención en el conflicto civil (Guerra Civil Española) de las naciones extranjeras. Hitler había firmado ya el Pacto de Acero con Mussolini, el único de los dirigentes europeos con un ideario similar. El apoyo a las fuerzas franquistas fue un intento de establecer un Estado fascista controlando el acceso al Mediterráneo con vistas a una futura guerra europea, algo que solo funcionó a medias.

El oeste de Checoslovaquia (la región conocida como los Sudetes) era el hogar de una gran cantidad de población de ascendencia germana, cuyos derechos, según el gobierno alemán, estaban siendo infringidos. La anexión de los Sudetes fue aceptada en los Acuerdos de Múnich en septiembre de 1938 tras una conferencia tripartita entre Alemania, Francia y Gran Bretaña, donde el francés Édouard Daladier y el primer ministro británico Neville Chamberlain, siguiendo una Política de apaciguamiento, confiaron en que sería la última reivindicación de la Alemania nazi. Hitler había transmitido personalmente esa idea a Chamberlain, tras entregarle un conjunto de informes con supuestas atrocidades cometidas contra habitantes alemanes en los Sudetes. La postura inglesa y francesa se debía en gran parte a la reticencia de sus poblaciones a verse envueltos de nuevo en una guerra a escala mundial, así como al convencimiento (sobre todo por parte de ciertos sectores de la sociedad inglesa) de que realmente el Tratado de Versalles había sido excesivo.

Sin embargo, en marzo de 1939 los ejércitos de Alemania entraron en Praga tomando el control de los territorios checos restantes. Al día siguiente, Hitler, desde el Castillo de Praga, proclamó el establecimiento del Protectorado de Bohemia y Moravia, a la vez que propició la aparición del Estado títere de Eslovaquia. También se apoderó del territorio de Memel, perteneciente a Lituania.

El fracaso del apaciguamiento demostró a las potencias occidentales que no era posible confiar en los tratados que pudieran firmarse con Hitler, así como que sus aspiraciones expansionistas no podían seguir siendo toleradas. Polonia rechaza ceder Dánzig a Alemania y firma con Francia un acuerdo de mutua defensa el 19 de mayo de 1939 y en agosto también lo suscribió con Gran Bretaña.

Por su parte, Alemania y la URSS firmaron el 23 de agosto del mismo año el Pacto Ribbentrop-Mólotov, que incluía un protocolo secreto por el que ambas potencias se dividían Europa central en esferas de influencia, incluyendo la ocupación militar. El tratado establecía el comercio e intercambio de petróleo y comida de la URSS a Alemania, reduciendo así el efecto de un futuro bloqueo por parte de Gran Bretaña como el que casi había ahogado a Alemania en la Primera Guerra Mundial. Hitler pasó entonces a centrarse en la preparación del futuro conflicto con los Aliados cuando, como pretendía, invadiera Polonia con el fin de incorporarla a Alemania. La ratificación del tratado de defensa entre Polonia y el Reino Unido no alteró sus planes.

Benito Mussolini se había convertido en líder indiscutido de Italia durante ese mismo período de entreguerras. Expulsado del Partido Socialista Italiano por apoyar la participación de Italia en la Primera Guerra Mundial, en 1919 fundó los Fasci italiani di combattimento, grupo militar integrado por excombatientes, que reprimían a los movimientos denominados obreros y al partido socialista; era por tanto análogo a los Freikorps alemanes tanto en ideario como en actuación. El fascismo creado por Mussolini defendía un régimen militarista, autoritario, nacionalista, que centralizara el poder en una persona y un movimiento (Partido Nacional Fascista en el caso italiano) y contrario a las instituciones democráticas. Los fascistas tomaron como emblema el fascio, antiguo símbolo de poder entre los romanos, consistente en un haz de varas con un hacha en el centro.

En estos años los movimientos obrero y campesino se manifestaron de manera más radical al tomar las fábricas y las tierras bajo su control, en un intento por imitar la Revolución Rusa. Los industriales y terratenientes, asustados por esta amenaza a sus intereses, apoyaron económicamente a los Fasci di combattimento. En septiembre de 1922 los camisas negras, como también eran conocidos los fascistas, organizaron una marcha sobre Roma, para presionar al gobierno por la incapacidad de resolver la situación económica. En respuesta, Víctor Manuel III nombró a Mussolini primer ministro. Este empezó a autodenominarse Duce ('Caudillo'), y estableció un gobierno totalitario. Creó el Gran Consejo Fascista que controló el Parlamento. Persiguió a los sindicatos, al Partido Socialista, a la prensa contraria a su gobierno, y a la Iglesia. Suprimió las libertades individuales y el derecho de huelga. Controló los medios de comunicación y solo permitió propaganda que exaltara el nacionalismo y el fascismo. También introdujo el militarismo en el sistema educativo italiano.

Del mismo modo que Hitler en Alemania, Mussolini defendía el derecho de Italia a la expansión territorial, de grado o por fuerza. Mussolini comenzó una gran campaña expansionista conocida como el colonialismo italiano. Estableció colonias en Somalia, Eritrea y Libia, y conquistó por la fuerza Abisinia y Albania, ignorando las protestas de la Sociedad de Naciones.

A pesar de ser nominalmente una democracia parlamentaria, el Ejército y la Marina de Japón eran dirigidos por los ministros de Guerra y Marina (que debían ser obligatoriamente generales o almirantes retirados o activos), los cuales no estaban sujetos a la autoridad del primer ministro, sino directamente a la del Emperador. De las 29 personas que recibieron el cargo de primer ministro durante el periodo 1885-1945, 15 eran almirantes o generales retirados o activos (durante el período 1932-45 fueron 8 de 11).

Esta anómala situación, combinada con el paso de un ejército permanente a otro reclutado (lo que obligaba a dar instrucción militar a todos los jóvenes del país), favoreció la progresiva militarización de la sociedad japonesa; el ejército y la marina, escasamente controlados por el poder civil, definían sus propios objetivos y se peleaban por los recursos presupuestarios disponibles, pero ambos coincidían en su desprecio a la clase política. Se formaron grupos de opinión enfrentados dentro de las fuerzas armadas que llevaban una "política paralela" a la del gobierno. Japón, un conjunto de islas con gran cantidad de población pero falto de recursos naturales, entró en el siglo XX con el firme propósito de imitar el sistema económico de las potencias occidentales, incluyendo el colonialismo, como forma de mantener su propio desarrollo, y volvió sus ojos hacia el continente asiático.

En 1894 Japón, que ya hacía tiempo que se disputaba la península de Corea con el Imperio Chino, inició la Primera Guerra Sino-japonesa con un ataque sin previo aviso. Para sorpresa de todos, el pequeño Imperio de Japón aplastó a las fuerzas del mastodóntico Imperio Chino, forzando un tratado de paz que le supuso la concesión de Taiwán, de las Islas Pescadores y de Liao-dong. La Rusia Imperial intentó limitar el dominio local de la emergente potencia: subvencionó el pago de las deudas de guerra chinas con Japón y, apoyada por Alemania y Francia, humilló a Tokio e impuso la restitución de Liao-dong a China.

Rusia y Japón se vieron desde ese momento implicadas en la lucha por la influencia en la parte noroeste de China. Rusia obtuvo la concesión para la construcción del ferrocarril Transmanchuriano, y aumentó su presencia militar en el sector con la creación de una base naval en Port Arthur, en la parte sur de la península de Liao-dong. La política rusa se encaminaba a desarrollar su influencia sobre toda Manchuria y Corea. Japón se inquietó e intentó en un principio negociar una repartición de áreas de influencia en Manchuria, aunque sin éxito. De modo que en 1904 la Marina Imperial Japonesa atacó y destruyó (de nuevo sin previa declaración de guerra) la flota rusa estacionada en Port Arthur. Japón estaba bien preparado, dominaba los mares de la zona en conflicto y sus bases estaban cerca de la zona. Por el contrario, Rusia estaba minada por tensiones internas, dirigida en el este por un mando incompetente e incapaz de asegurar un enlace eficaz con el oeste, ya que el Transiberiano era su única vía terrestre, por lo que no pudo plantar cara. La Guerra Ruso-japonesa terminó en 1905 con un armisticio que humilló a Rusia y dejó Liao-dong en manos de Japón, junto con la mitad meridional de la isla Sajalín y la preeminencia absoluta sobre Corea. En 1914, Japón declaró la guerra a Alemania, consiguiendo al final de la Primera Guerra Mundial las posesiones alemanas del Océano Pacífico septentrional.

En la década de los 30 la posición política de los militares en Japón era cada vez más dominante. El poder político estaba controlado por los grupos de presión dentro del Ejército y la Armada, hasta el punto de que ocurrieron varios golpes de estado y atentados por parte de cadetes y oficiales jóvenes del Ejército y la Marina contra ministros y altos cargos que estorbaban los intereses de las camarillas militares. Estas acciones llegaron a costar la vida incluso de un primer ministro en 1932, lo que supuso el final a todos los efectos de cualquier intento de controlar al ejército desde el gobierno: la clase política era consciente de que simplemente emitir en público una opinión desfavorable hacia las fuerzas armadas significaba arriesgarse a morir a manos de un ultranacionalista en un arranque de patriotismo.

En 1931, usando como casus belli unos supuestos incidentes transfronterizos, Japón invadió Manchuria, que convirtió en 1932 en Manchukuo, estado independiente bajo protectorado japonés, junto con Jehol. Las críticas internacionales por esta acción llevaron a Japón a retirarse de la Sociedad de Naciones al año siguiente. En 1937, necesitado de recursos naturales y aprovechando la debilidad china provocada por la guerra civil entre comunistas y republicanos, Japón inició la Segunda Guerra Sino-japonesa, y ocupó la parte noreste de ese país. Los Estados Unidos de América y Gran Bretaña reaccionaron en apoyo del Kuomintang concediéndole créditos, ayuda militar encubierta, pilotos y aeroplanos, y también levantando embargos cada vez mayores contra Japón de materias primas y petróleo (su comercio exterior llegó a caer en un 75%, mientras que las importaciones de petróleo lo hicieron en un 89%).

La Segunda Guerra Sino-japonesa comenzó en 1937, tras el Incidente del Puente de Marco Polo, cuando Japón atacó en profundidad a China desde Manchukuo.[14]​ Pekín, es atacada el 25 de junio, siendo finalmente tomada el día 8 de agosto junto a Tianjin. Los japoneses terminaron de ocupar el norte rápidamente, pero fueron detenidos finalmente en la batalla de Shanghái. Después de combatir alrededor de la ciudad durante más de tres meses, Shanghái finalmente cayó ante los japoneses en noviembre de 1937. La capital china, Nankín, cayó poco después. Como resultado, el Gobierno nacionalista chino trasladó su sede a Chongqing durante el resto de la guerra. Las fuerzas japonesas cometieron brutales atrocidades contra los civiles y los prisioneros de guerra en la masacre de Nankín, matando unos 300 000 civiles en un mes. Ni Japón ni China declararon oficialmente la guerra por razones similares: Japón deseaba evitar la intervención de potencias extranjeras, sobre todo el Reino Unido y los Estados Unidos, que era su primer proveedor de acero y hubiera debido imponer un embargo en virtud de las Leyes de Neutralidad vigentes en dicho país; mientras que China temía que la declaración le granjeara la enemistad de las potencias occidentales en la zona.

Las tensiones entre Japón y la Unión Soviética, países cuya enemistad se remonta a la Guerra ruso-japonesa y a la Intervención japonesa en Siberia, aumentan considerablemente tras el inicio de la guerra total en China. Entre julio y agosto de 1938 tiene lugar la Batalla del Lago Jasán, en territorio soviético, finalizada con un alto al fuego entre la URSS y Japón. Más importante fue la Batalla de Jaljin Gol entre mayo y septiembre de 1939 que concluye con una aplastante victoria de los soviéticos y sus aliados de la República Popular de Mongolia sobre nipones y manchúes finalizando así la guerra no declarada entre el Imperio Japonés y la URSS.[15]​

El Imperio japonés establece una serie de gobiernos títere en China. En diciembre de 1937 se instaura el Gobierno provisional de la República de China con capital en Pekín. Al año siguiente se crea el Gobierno Reformado de la República de China con capital en Nankín.[16]​ En 1939 se crea Mengjiang, un estado colaboracionista mongol situado en la región de la Mongolia Interior, el territorio de mayoría étnica mongola de China. Los tres gobiernos se fusionan en 1940 dando lugar a un único estado, de nombre oficial República de China y con capital en Nankín. Estaría gobernado por Wang Jingwei, exmiembro del Kuomintang y principal rival de Chiang durante el ascenso al poder de este último.

El 1 de septiembre de 1939, Alemania invadió Polonia, usando el pretexto de un ataque polaco simulado en un puesto fronterizo alemán. La llanura polaca ofrecía una ventaja para el desplazamiento de los blindados alemanes, aunque los bosques y las carreteras mal construidas eran problemas que hacían más arduo el avance. Alemania avanzó usando la blitzkrieg ('guerra relámpago'). El Reino Unido y Francia le dieron dos días a Alemania para retirarse de Polonia. Una vez que pasó la fecha límite, el 3 de septiembre, el Reino Unido, Australia, y Nueva Zelanda le declararon la guerra a Alemania, seguidos rápidamente por Francia, Sudáfrica y Canadá.

Los franceses se movilizaron lentamente y después solo hicieron una ofensiva de «demostración» en el Sarre, que pronto abandonaron, mientras que los británicos no pudieron hacer ninguna acción directa en apoyo de los polacos en el tiempo disponible (véase Traición occidental). Mientras, el 8 de septiembre, los alemanes alcanzaban Varsovia, tras haber penetrado a través de las defensas polacas, y comenzaron el asedio de Varsovia (8-28 septiembre). Durante este tiempo (9-22 de septiembre), hubo un contraataque polaco y la mayor batalla de la campaña conocido como batalla de Bzura.

El 17 de septiembre, la Unión Soviética, siguiendo su acuerdo secreto con Alemania, invadió Polonia desde el este, convirtiendo las defensas polacas en un caos mediante la apertura de un segundo frente. La defensa polaca no aguantaría la lucha en dos frentes a la vez. Un día más tarde, tanto el presidente polaco como el comandante en jefe huyeron a Rumanía. El 1 de octubre, después de un mes de asedio de Varsovia, las fuerzas hostiles entraron en la ciudad. Las últimas unidades polacas se rindieron el 6 de octubre. Polonia, sin embargo, nunca se rindió oficialmente a los alemanes. Algunas tropas polacas se fueron a países vecinos. Como consecuencia de la Campaña de septiembre, la Polonia ocupada consiguió crear un poderoso movimiento de resistencia y contribuyó con fuerzas militares significativas al esfuerzo aliado durante el resto de la Segunda Guerra Mundial.

Tras la conquista de Polonia, Alemania se tomó una pausa para reagruparse durante el invierno de 1939-1940, mientras británicos y franceses se mantenían a la defensiva. Los periodistas llamaron a este período la «guerra de broma» o Sitzkrieg (drôle de guerre, en francés), debido a que casi no existieron combates. Durante este período, la Unión Soviética atacó Finlandia el 30 de noviembre, con lo que comenzó la Guerra de Invierno. A pesar de superar a las tropas finesas en número de 4 a 1, el Ejército Rojo encontró que su ataque se volvía muy difícil, lo cual resultó muy embarazoso y la fuerte defensa finlandesa evitó una invasión completa. Finalmente, los soviéticos acabaron por imponerse y el tratado de paz vio cómo Finlandia cedía áreas estratégicamente importantes en la frontera cerca de Leningrado, así como en la Carelia. Esto sentó un precedente de flaqueza en el ejército Rojo, el cual los alemanes se tomarían en serio para la futura invasión.

Alemania invadió Dinamarca y Noruega el 9 de abril de 1940, en la Operación Weserübung, en parte para contrarrestar la amenaza de una inminente invasión Aliada de Noruega. Dinamarca no resistió, pero Noruega luchó. La defensa noruega fue socavada desde el interior por la colaboración de Vidkun Quisling, cuyo nombre es hoy en día sinónimo de «traidor». Tropas del Reino Unido, cuya propia invasión estaba preparada, desembarcaron en el norte de Noruega. A últimos de junio, los Aliados habían sido derrotados y se retiraban, Alemania controlaba la mayor parte de Noruega, y las Fuerzas Armadas de Noruega se habían rendido, mientras que la Familia real noruega escapaba a Londres. Alemania usó Noruega como base para ataques navales y aéreos contra los convoyes árticos que se dirigían a la Unión Soviética con armas y suministros. Los partisanos noruegos continuarían la lucha contra la ocupación alemana durante toda la guerra.

Los alemanes acabaron la «guerra de broma» el 10 de mayo de 1940, cuando invadieron Luxemburgo, Bélgica, los Países Bajos y Francia. Los Países Bajos fueron arrollados rápidamente y la ciudad neerlandesa de Róterdam fue destruida en un bombardeo aéreo. La Fuerza Expedicionaria Británica (BEF) y el Ejército Francés, avanzaron hacia el norte de Bélgica y planeaban hacer una guerra móvil en el norte, mientras mantenían un frente continuo y estático a lo largo de la línea Maginot más al sur. Los planes Aliados fueron desbaratados inmediatamente por el más clásico e importante ejemplo en la historia de la Blitzkrieg.

En la primera fase de la invasión, Fall Gelb, el Panzergruppe von Kleist de la Wehrmacht, se precipitó a través de las Ardenas, una región con espesos bosques que los Aliados habían pensado que sería impenetrable para un ejército mecanizado moderno. Los alemanes rompieron la línea francesa en Sedán, sostenida por reservistas más que por tropas de primera línea, para luego girar hacia el oeste a través del norte de Francia hacia el Canal de la Mancha, dividiendo en dos a los Aliados.

La BEF y las fuerzas Francesas, rodeadas en el norte, fueron evacuadas desde Dunkerque en la Operación Dinamo. La operación fue una de las evacuaciones más grandes de la historia militar, cuando 338 000 soldados británicos, franceses y belgas fueron evacuados a través del Canal de la Mancha en barcos de guerra y civiles. La ofensiva pudo haber sido más satisfactoria para los alemanes de no haber sido parada por Hitler para que sus tropas cogieran aliento, cosa que en particular a Guderian no gustó nada.

El 10 de junio, Italia se unió a la guerra, atacando a Francia por el sur. Las fuerzas alemanas continuaron entonces la conquista de Francia con el llamado plan rojo o Fall Rot. Francia firmó un armisticio con Alemania el 22 de junio de 1940, que condujo a la ocupación directa alemana de París y de dos tercios de Francia, y al establecimiento de un gobierno títere alemán con sede en el sudeste de Francia conocido como la Francia de Vichy.

Alemania había empezado los preparativos en el verano de 1940 para invadir el Reino Unido en la Operación León Marino. Muchos de los suministros y de las armas pesadas del ejército británico se habían perdido en Dunkerque. Los alemanes no tenían ninguna esperanza de batir a la Marina Real británica, pero pensaron que tendrían una oportunidad de éxito si podían alcanzar la superioridad aérea. Para hacerlo, tenían que suprimir primero a la Royal Air Force (RAF). Entonces se inició un combate aéreo a finales del verano de 1940 entre alemanes y británicos que llegó a conocerse como la batalla de Inglaterra. La Luftwaffe (Fuerza Aérea de Alemania) tomó como objetivo inicialmente a los aeródromos y estaciones de radar del RAF Fighter Command (Mando de Cazas de la RAF).

Pero tras no tener los resultados esperados e impulsado por el contraataque inglés lanzado a Berlín, Hitler desvió los bombardeos directamente a las ciudades inglesas. Así se pudo ver en la operación Blitz, donde los nazis bombardearon por más de cinco meses las ciudades más importantes de Inglaterra, pero más potentemente en su capital Londres. Las urbes de Liverpool, Coventry, Bristol, Southampton, Birmingham, Swindon, Plymouth, Cardiff, Mánchester y Sheffield también fueron fuertemente bombardeadas. Pese a todos los fuertes ataques de Alemania, Inglaterra resistió firmemente y al final, la Luftwaffe fue rechazada por los Hurricanes y los Spitfires, mientras la Marina Real británica mantenía el control del canal de la Mancha. El Blitz provocó alrededor de 43 000 muertes, y destruyó más de un millón de viviendas, pero fracasó en alcanzar los objetivos estratégicos de sacar a Inglaterra de la guerra o dejarla incapaz de resistir una invasión. Así, los planes de invasión alemanes fueron pospuestos indefinidamente.

Después de la caída de Francia en 1940, el Reino Unido estaba sin recursos económicos. Franklin Delano Roosevelt persuadió al Congreso de los Estados Unidos, para que aprobase la Ley de Préstamo y Arriendo el 11 de marzo de 1941, que proveyó al Reino Unido y a otros 37 países con 50 000 millones de dólares en equipo militar y otros suministros. El Reino Unido y la Commonwealth recibieron 34,4 mil millones de dólares. Canadá realizó un programa similar que envió 4,7 mil millones de dólares en suministros al Reino Unido.

El control del sur de Europa, el mar Mediterráneo y de África del Norte era importante debido a que el Imperio británico dependía del tráfico marítimo a través del canal de Suez. Si el canal caía en las manos del Eje o si la Marina Real británica perdía el control del Mediterráneo, entonces el transporte entre el Reino Unido, la India, y Australia tendría que efectuarse alrededor del cabo de Buena Esperanza, un incremento de miles de millas.

Así, tras la rendición francesa, los británicos atacaron a la Armada Francesa anclada en el Norte de África en julio de 1940, por temor a que pudiese caer en manos alemanas, incrementando así su potencial naval y dificultando la posición británica. Esto contribuyó a un distanciamiento en las relaciones anglo-francesas durante los años siguientes. Con la flota francesa destruida, la Marina Real combatió contra la flota italiana por la supremacía en el Mediterráneo desde sus fuertes bases en Gibraltar, Malta y Alejandría (Egipto). En África, las tropas italianas invadieron y capturaron la Somalilandia Británica en agosto.

Italia invadió Grecia el 28 de octubre de 1940, desde Albania, entonces ocupada por Italia, pero fue rechazada rápidamente. A mediados de diciembre, el ejército griego avanzó incluso hacia el sur de Albania, apresando así en la campaña a 530 000 soldados italianos. Mientras tanto, en cumplimiento de la garantía británica dada a Grecia, la Marina Real atacó a la flota italiana el 11 de noviembre de 1940. Aviones torpederos que habían partido desde los portaaviones británicos atacaron a la flota italiana en Tarento, un puerto del sur. Un acorazado fue hundido y se pusieron temporalmente fuera de servicio otros buques. El éxito de los torpedos aéreos en Tarento, fue visto con mucho interés por el jefe naval japonés, Isoroku Yamamoto, que estaba ponderando los medios para neutralizar a la Flota del Pacífico de los Estados Unidos. La Grecia continental, cuyas mejores tropas se habían desgastado en contra de Italia en Albania, cayó finalmente ante una invasión alemana desde el nordeste, que atravesó Bulgaria.

Las tropas italianas pasaron a Egipto desde Libia para atacar las bases británicas en septiembre de 1940, comenzando así la Campaña en África del Norte. El objetivo era la captura del canal de Suez. Las fuerzas británicas, indias, y australianas contraatacaron en la Operación Compass, que terminó en 1941. Entonces, numerosas fuerzas australianas y de Nueva Zelanda (ANZAC) fueron transferidas a Grecia para defenderla del ataque alemán. Las fuerzas alemanas (conocidas más tarde como el Afrika Korps) bajo el mando del general Erwin Rommel desembarcaron en Libia en febrero de 1941 para renovar el asalto contra Egipto.

Alemania también invadió Creta, operación importante por el uso a gran escala de las tropas paracaidistas alemanas. Creta estaba defendida por unos 11 000 griegos y 28 000 tropas del ANZAC, que habían escapado hacía poco de Grecia sin su artillería y sus vehículos. Los alemanes atacaron los tres aeropuertos principales de la isla en Maleme, Rétino y Candía. Después de un día de lucha, no se había alcanzado ninguno de los objetivos y los alemanes habían sufrido bajas devastadoras. Los planes alemanes estaban en desorden y el comandante alemán, el general Kurt Student, estaba contemplando el suicidio. Durante el día siguiente, gracias a la mala comunicación y del fallo de los comandantes aliados en comprender la situación, el aeropuerto de Maleme en el oeste de Creta cayó en poder de los alemanes. La pérdida de Maleme hizo que los alemanes pudiesen enviar refuerzos pesados transportados en avión con los que arrollar a las fuerzas aliadas en la isla. Sin embargo, en vista de las fuertes bajas sufridas por los paracaidistas, Hitler prohibió realizar más operaciones aerotransportadas.

En el norte de África, las fuerzas de Rommel avanzaron rápidamente hacia el este, poniendo sitio al vital puerto de Tobruk. Fueron derrotados dos intentos aliados por liberar Tobruk, pero una ofensiva mayor a fines de año (Operación Crusader) rechazó a las fuerzas de Rommel después de intensos combates.

La guerra entre las armadas aliada e italiana cambió decisivamente a favor de los aliados el 28 de marzo de 1941, cuando los barcos del almirante Andrew Browne Cunningham encontraron a la flota principal italiana al sur del Cabo Matapán, en el extremo sur de la Grecia continental. Con un coste de un par de aviones derribados, los Aliados hundieron cinco cruceros italianos y tres destructores, y dañaron al moderno acorazado Vittorio Veneto. La Marina italiana fue anulada como fuerza de combate y se vio facilitada la tarea aliada de transportar tropas a través del Mediterráneo hacia Grecia.

El 6 de abril de 1941, fuerzas alemanas, italianas, húngaras y búlgaras invadieron Yugoslavia, provocando la rendición del ejército yugoslavo el 17 de abril y la creación de un estado títere en Croacia y Serbia. También el 6 de abril, Alemania invadió Grecia desde Bulgaria. El ejército griego que defendía la línea Metaxas, fue superado en número y en capacidad de maniobra por el rápido avance alemán a través de Yugoslavia y colapsó. Atenas cayó el 27 de abril, aunque el Reino Unido consiguió evacuar unos 50 000 soldados, especialmente a Creta.

La resistencia comenzó en Yugoslavia a mediados de 1941, centrada en dos movimientos: los partisanos comunistas, AVNOJ, liderados por Tito, y el grupo realista Chetniks, liderado por Draza Mihailovic. Los dos grupos paramilitares cooperaron brevemente en 1941, pero se enfrentaron pronto, cuando los chetniks asumieron un papel más ambivalente, poniéndose frecuentemente del lado de las fuerzas de ocupación, y en contra de los comunistas.

En abril-mayo de 1941, hubo una corta guerra en Irak que resultó en una renovación de la ocupación británica. En junio, fuerzas Aliadas invadieron Siria y el Líbano, y capturaron Damasco el 17 de junio. Más tarde, en agosto, tropas del Reino Unido y del Ejército Rojo ocuparon el neutral Irán, asegurando su petróleo y una línea de suministro por el sur para la Unión Soviética.

Al comienzo de 1942, las fuerzas Aliadas en el norte de África fueron debilitadas al mandar destacamentos al Lejano Oriente. Rommel una vez más recapturó Bengasi. Entonces derrotó a los Aliados en la batalla de Gazala y conquistó Tobruk, haciendo miles de prisioneros y apoderándose de grandes cantidades de suministros, antes de continuar más profundamente dentro de Egipto.

En junio de 1942 se registró la Batalla de Mediados de Junio, en que los aliados se enfrentaron a las fuerzas aéreas y navales de Italia y Alemania durante el desarrollo de dos operaciones de abastecimiento de la isla de Malta conducidas por la Royal Navy británica (que comprendía también unidades australianas y polacas), llamadas en código Harpoon y Vigorous. Tales operaciones – ejerciéndose en una zona del Mediterráneo muy amplia – estuvieron definidas en base a las rutas: de Alejandría de Egipto la Vigorous y de Gibraltar la Harpoon, convergentes ambas sobre Malta en un arco temporal de cinco días.

La Primera Batalla de El Alamein tuvo lugar en julio de 1942. Las fuerzas Aliadas se habían retirado al último punto defendible antes de Alejandría y el canal de Suez. El Afrika Korps, sin embargo, había agotado sus suministros y los defensores pararon su empuje. La Segunda Batalla de El Alamein ocurrió entre el 23 de octubre y el 3 de noviembre. El teniente general Bernard Montgomery estaba al mando de las fuerzas Aliadas conocidas como el 8.º Ejército. Los Aliados iniciaron entonces su ofensiva y, a pesar de una dura resistencia inicial de los italianos y alemanes, triunfaron al final. Después de la derrota alemana en El Alamein, las fuerzas del Eje efectuaron con éxito una retirada estratégica hacia Túnez.

La Operación Torch fue efectuada por los Estados Unidos, Gran Bretaña y las fuerzas de la Francia libre el 8 de noviembre de 1942, para ganar el control del Norte de África por medio de desembarcos simultáneos en Casablanca, Orán y Argelia, seguidos unos pocos días después por un desembarco en Bône, la puerta de entrada a Túnez. Las fuerzas locales de la Francia de Vichy opusieron una resistencia mínima antes de someterse a la autoridad de la Francia libre del general Henri Giraud. Como represalia, Hitler invadió y ocupó la Francia de Vichy, mientras Mussolini ocupó Córcega y la costa azul francesa hasta el Ródano. Las fuerzas alemanas e italianas, que habían ocupado Túnez, fueron cogidas en un movimiento de pinza por los avances aliados, desde Argelia en el oeste y desde Libia en el este. La victoria táctica de Rommel contra las fuerzas inexpertas de los estadounidenses en la batalla del paso de Kasserine, solo pospuso un tiempo la futura rendición de las fuerzas del Eje en el Norte de África en mayo de 1943.

En 1943, el Eje casi tuvo éxito en la supresión de la resistencia partisana yugoslava. Desde enero a abril, se forzó a las guerrillas a huir hacia el este, en condiciones invernales sobre el duro terreno de Bosnia. Sufrieron graves pérdidas, y cruzaron el río Neretva (batalla del Neretva), asegurando su puesto de mando y su hospital. Continuaron hacia el este, incapacitando las fuerzas chetniks del área, y cayeron en un embolsamiento alemán casi fatal en el valle del Sutjeska a últimos de mayo (batalla del Sutjeska).

Italia había ganado el control de Eritrea y de la Somalía Italiana durante la Repartición de África, y había tomado Etiopía antes del comienzo de la Segunda Guerra Mundial durante la Invasión de Etiopía (1935-36). Estas tres colonias fueron reorganizadas en el dominio del África Oriental Italiana.

A principios de 1940, las fuerzas coloniales italianas consistían en 80 000 soldados italianos y 200 000 soldados nativos, mientras que las fuerzas británicas en toda la Somalilandia Británica, Kenia y Sudán solamente totalizaban unos 17 000.[20]​ Los italianos primero se desplegaron para la toma de la Somalilandia Francesa (hoy en día conocida como Yibuti). Este ataque fue cancelado debido al colapso del Ejército francés y la formación del Gobierno colaboracionista de la Francia de Vichy. En julio, las ciudades en la frontera con Sudán, Kassala y Gallabat fueron ocupadas por una fuerza italiana de 50 000 hombres,[21]​ y en agosto de 1940, el ejército colonial italiano atacó y tomó la Somalilandia Británica utilizando una fuerza de 25 000 hombres. Esto le dio a Italia el control de casi todo el Cuerno de África.

En septiembre de 1940, las fuerzas aliadas fallaron, durante la batalla de Dakar, en la captura de la capital de Senegal, luchando contra las tropas de la Francia de Vichy que la defendían; el África Occidental Francesa permaneció en manos de Vichy hasta los desembarcos de la Operación Torch en el norte de África en noviembre de 1942. Aunque en noviembre los Aliados tuvieron éxito en la batalla de Gabón, consolidando su control sobre el África Ecuatorial Francesa para las fuerzas de la Francia libre.

También en noviembre de 1940, los británicos empezaron una contraofensiva desde el Sudán con solamente 7000 soldados, atacando Gallabat ocupada por los italianos, pero fueron incapaces de tomarla.[22]​ Sin embargo, en enero de 1941, el ejército italiano retiró sus fuerzas desde las ciudades fronterizas del Sudán a un terreno más defendible al este de Kassala.[23]​ Con refuerzos adicionales provenientes del ejército de la India Británica y de Sudáfrica, la campaña empezó a hacer progresos. La Somalilandia Británica fue reconquistada en marzo, y Adís Abeba, capital de Etiopía, fue capturada el 6 de abril. El emperador Haile Selassie I volvió a la ciudad el 5 de mayo. Sin embargo, una fuerza de italianos continuó luchando una guerra de guerrillas en Etiopía, hasta la rendición italiana de septiembre de 1943.

Madagascar, como colonia francesa que era, estaba considerada territorio enemigo por los británicos desde la creación del régimen colaboracionista de Vichy. Era también la tierra sugerida a la que los judíos europeos deberían ser deportados, en una propuesta antisemita conocida como el «Plan Madagascar». Mientras los británicos controlasen Egipto y el Canal de Suez, estos planes alemanes eran imposibles, y eventualmente fueron archivados en favor de una campaña de genocidio, que se llamó la Solución final. Con la entrada de los japoneses en la guerra en diciembre de 1941, y la rendición de Singapur en febrero de 1942, los Aliados llegaron a preocuparse cada vez más de que Madagascar pudiese caer en manos del Eje. Por lo tanto, realizaron una invasión, conocida como la Operación Ironclad en mayo de 1942. La lucha contra los defensores franceses de Vichy duró hasta noviembre, porque los franceses estaban respaldados por varios submarinos japoneses. En diciembre, la Somalilandia Francesa también fue conquistada por los británicos.

Después de los desembarcos de la Operación Torch, el resto de los territorios de Vichy en África quedaron bajo el control de los Aliados. Con el control del sur del continente seguro, aparte de la insurgencia italiana en Etiopía, los Aliados volvieron su atención a otros teatros de la guerra.

La batalla de Grecia (Operación Marita) y la invasión de Yugoslavia retrasaron la invasión alemana seis semanas críticas, como posteriormente se puso de manifiesto. Tres grupos de ejércitos alemanes, junto con otras unidades militares del Eje, que sumaban unos 3,5 millones de hombres, se lanzaron a la invasión de la Unión Soviética el 22 de junio de 1941. El Grupo de Ejércitos Norte estaba desplegado en Prusia Oriental y estaba compuesto por los ejércitos de infantería 18.º y 16.º y un ejército Panzer, el 4.º al mando de los generales Busch, Von Küchler y Hoepner, todos bajo las órdenes del mariscal Ritter Von Leeb, apoyados por la 1.ª Flota aérea del general Koller totalizando 450 000 combatientes del Eje. Sus objetivos principales eran asegurar los estados bálticos y tomar Leningrado. Oponiéndose al Grupo de Ejércitos Norte estaban tres Ejércitos soviéticos compuestos por 450 000 hombres en un principio, pero con las nuevas movilizaciones se aumentó el número a 600 000 al mando del mariscal Voroshilov. Los alemanes lanzaron sus 600 tanques contra el punto de contacto de los dos Ejércitos soviéticos en ese sector. El objetivo del 4.º Ejército Panzer era cruzar los ríos Niemen y Dvina, que eran los dos mayores obstáculos en la ruta hacia Leningrado. En el primer día, los tanques cruzaron el río Niemen y penetraron 80 kilómetros. Cerca de Rasienai, los Panzers fueron contraatacados por 300 tanques soviéticos. Los alemanes tardaron 4 días en rodear y destruir a los tanques soviéticos. Los Panzers cruzaron después el río Dvina cerca de Dvinsk.

Los alemanes estaban ahora a una distancia suficiente como para atacar Leningrado; sin embargo, Hitler ordenó a los Panzers mantener su posición mientras los Ejércitos de infantería los alcanzaban. Las órdenes de mantener la posición durarían cerca de una semana, y dieron tiempo suficiente a los soviéticos para que fortaleciesen sus defensas alrededor de Leningrado. Los soviéticos recibieron apoyo de la flota soviética del Báltico, hasta que los Stukas alemanes lograron hundir a los acorazados Marat y Revolución de Octubre. Después de que Hitler dio la orden de ataque, el 4.º Ejército Panzer trató de perforar la plaza desde el 10 de agosto hasta el 8 de septiembre. Voroshilov movilizó a toda la población civil para evitar que la ciudad cayera, lo que consiguió con enormes pérdidas que oscilan entre 500 000 y 1 500 000 de bajas solamente en el bando soviético.

El Grupo de Ejércitos Centro estaba desplegado en Polonia y comprendía a los ejércitos 9.º, al mando del general Strauss, 4.º, al mando del general Von Kluge, al 2.º, comandado por el general Von Weichs, y dos ejércitos Panzer, el 2.º y el 3.º, bajo las órdenes de los generales Guderian y Hoth respectivamente, todos a su vez dirigidos por el mariscal Fedor von Bock. Su objetivo principal era la captura de Moscú. Oponiéndose al Grupo de Ejércitos del Centro estaban cuatro Ejércitos soviéticos con 3500 tanques, bajo el mando del mariscal Timoshenko. Los soviéticos ocupaban un saliente que se introducía en terreno alemán con su centro en Bialystok. Más allá de Bialystok estaba Minsk, un nudo de ferrocarriles clave, que guardaba la principal carretera a Moscú. El 3.º Ejército Panzer penetró a través de la unión de los dos Ejércitos soviéticos desde Prusia y cruzó el río Niemen, y el 2.º Ejército Panzer cruzó el río Bug desde el sur para lo cual se emplearon 80 tanques capaces de caminar bajo el agua. Mientras atacaban los panzers, los ejércitos de infantería golpeaban en el saliente y rodeaban a las tropas soviéticas en Bialystok. El objetivo de los ejércitos panzer era encontrarse en Minsk e impedir una retirada soviética. El 27 de junio, tras cinco días de operaciones, los ejércitos Panzer II y III se encontraron en Minsk habiendo avanzado 350 kilómetros en territorio soviético. En la enorme bolsa entre Minsk y la frontera polaca quedaron rodeadas 32 divisiones de infantería soviéticas y 8 divisiones de tanques, totalizando 400 000 soldados soviéticos con más de 3500 tanques (tres veces más que los atacantes) y 2000 cañones que en la batalla de Bialystok-Minsk fueron atacados y cercados en un triángulo que inicialmente tenía unos 300 km de lado.

La batalla de cerco duró 14 días, del 27 de junio al 10 de julio, y al desplomarse la resistencia fueron capturados 323 898 soldados soviéticos, aunque consiguieron escapar unos 250 000 más, capturaron o destruyeron 3332 tanques y 1909 cañones (más del total de tanques enviados a la lucha por Francia); el aniquilamiento de esa cantidad de material blindado dio confianza al mando alemán, ya que los tanques disponibles de Alemania para la invasión de Rusia eran solamente 2434, y se creyó que se había logrado acabar con la mayoría de los blindados soviéticos. En realidad era falso, pues el Ejército Rojo tenía una imponente masa de 20 000 máquinas en 1941, aunque se debe matizar que el 92 % de esos carros eran viejos tanques de los años 30 de los que en la primera semana se averió casi el 50 % de ellos debido a problemas mecánicos: el 90 % de los T-35 se averiaron sin luchar, solo un 5 % eran T-34s y un 3 % KV-1s. En ese mismo tiempo la Luftwaffe había organizado 2800 aviones en tres flotas comandados por Loehr, Kesselring y Keller. En los primeros días de lucha, numerosas escuadrillas de tres bombarderos se internaron en territorio soviético volando casi a ras de suelo, sin cruzar ciudades, para atacar los principales aeródromos en un radio de 300 km. En esos dos primeros días de lucha se reportaron 2700 aviones derribados o destruidos en sus bases, pero tras ocupar los aeródromos por tierra se comprobó que fueron destruidos 2700 aparatos, de los cuales unos 1800 en el primer día.

El Grupo de Ejércitos Sur estaba desplegado al sur de Polonia y en Rumanía y estaba compuesto por los ejércitos 6.º, 11.º, y 17.º, y un Ejército Panzer, el 1.º, junto con dos Ejércitos rumanos y varias divisiones italianas, croatas, eslovacas y húngaras. Su objetivo era capturar los campos petrolíferos del Cáucaso. En el sur, los comandantes soviéticos habían reaccionado rápidamente al ataque alemán y sus fuerzas de tanques superaban con mucho a las alemanas. Oponiéndose a los alemanes en el sur había tres ejércitos soviéticos. Los alemanes atacaron en los puntos de contacto de los tres ejércitos soviéticos, pero el 1.º Ejército Panzer golpeó justo a través del Ejército soviético con el objetivo de capturar Brody. El 26 de junio, cinco cuerpos de ejército mecanizado soviéticos con unos mil tanques montaron un contraataque masivo contra el 1.er Ejército Panzer. La batalla de Lutsk-Brody-Rovno fue una de las más feroces de la invasión y duró varios días. Al final de ella los alemanes resultaron vencedores, pero los soviéticos infligieron duras pérdidas al 1.er Ejército Panzer. Con el fracaso de la ofensiva soviética, se habían acabado las últimas fuerzas substanciales de tanques soviéticos.


El 3 de julio, apenas terminada la batalla de Bialystock-Minsk Hitler dio su consentimiento a los panzers para que relanzasen su empuje hacia el este, después que los ejércitos de infantería los hubiesen alcanzado. Fedor von Bock lanzó la vanguardia de sus nueve divisiones blindadas y sus siete motorizadas, seguidas por treinta y cinco divisiones de infantería hacia el frente. A las orillas del río Beresina los alemanes se enfrentaron a un nuevo tipo de tanque soviético desconocido hasta entonces. Era el T-34, con 45 milímetros de blindaje, coraza frontal inclinada, y cañón de 76,2 mm de diámetro, eficaz a 1500 m. Los efectivos de la 18.ª División Blindada de Guderian se enfrentaron a él, pasando serias dificultades antes de descubrir que tenía mala visibilidad por detrás y una comunicación por radio muy deficiente (los carros no solían tener radio y se hacía por señas entre ellos). Iguales dificultades pasaron al repeler al tanque pesado KV-1, mejor blindado que el T-34. Después de la sorpresa se destruyeron varias unidades soviéticas encabezadas por el VIII Cuerpo de Ejército, en la que militaba el hijo de Stalin Yákov Dzhugashvili, que fue hecho prisionero. A pesar de todo, Stalin se negó a hacer un trato con los nazis para el intercambio de su hijo.
El siguiente objetivo del Grupo de Ejércitos Centro sería la ciudad de Smolensk que dominaba la carretera a Moscú. Frente a los alemanes estaban las fortificaciones no concluidas de la Línea Stalin, apoyadas sobre el río Dniéper, y al perforarla consiguieron capturar Perekov. El 6 de julio, los soviéticos lanzaron un ataque con 700 tanques contra el 3.º Ejército Panzer. Los alemanes tenían una abrumadora superioridad aérea en calidad; los soviéticos poseían la flota aérea más numerosa de todas las naciones, pero sus cazas J-15 y sus bombarderos que eran relativamente lentos y de los más diversos modelos, no podían competir contra los Messerschmitt 109 ni contra los Junkers Ju 87 (Stukas) más rápidos. El 2.º Ejército Panzer cruzó el río Dnieper y se acercó a Smolensk desde el sur, mientras que el 3.er Ejército Panzer, después de derrotar el contraataque soviético, se aproximó a Smolensk desde el norte. Tres Ejércitos soviéticos quedaron embolsados. El 26 de julio, los Panzers cerraron la trampa y entonces comenzó la eliminación de la bolsa, cogiendo 310 000 prisioneros soviéticos, 3205 tanques y 3210 cañones, de un total de 3600 tanques, 3500 cañones y 460 000 combatientes soviéticos. Hitler ahora, se vio en un dilema: sus generales querían continuar con el empuje hacia Moscú, pero el problema para continuar con la ofensiva del sector central era que, en el sur, los ejércitos comandados por Gerd von Rundstedt se encontraban atascados a la entrada de Kiev, donde el mariscal Budenny tenía cinco ejércitos con más de 700 000 hombres, parapetados en poderosas defensas, y otro ejército soviético se encontraba en la región de Gómel con más de 100 000 hombres; este conjunto de tropas preocupaba a Hitler, ya que las líneas de abastecimiento de los ejércitos de Von Bock se encontraban demasiado extendidas. Tanques del Grupo de Ejércitos Centro fueron desviados en apoyo de los Grupos de Ejércitos Norte y Sur. Los generales de Hitler se opusieron vehementemente a esta medida, ya que Moscú se encontraba solo a 350 kilómetros del Grupo de Ejércitos Centro y el grueso del Ejército Rojo estaba desplegado en ese sector y solamente un ataque allí tenía esperanzas de acabar la guerra rápidamente. Pero Hitler fue inflexible y los tanques del Grupo de Ejércitos Centro se fueron a reforzar al 4.º Ejército Panzer en el norte, atravesando las defensas soviéticas el 8 de agosto, llegando al final de agosto a solo 50 km de Leningrado. Mientras tanto los finlandeses habían atacado hacia el sudeste, a ambos lados del lago Ladoga, alcanzando la antigua frontera soviética.

En el sur, a mediados de julio, más allá de los Pantanos de Pinsk, los alemanes se habían quedado a solo unos kilómetros de Kiev. El 1.er Ejército Panzer entonces fue hacia el Sur, mientras que el 17.º Ejército alemán, que estaba en el flanco sur del 1.er Ejército Panzer, golpeó hacia el este y entre los dos atraparon tres ejércitos soviéticos cerca de Uman. Cuando los alemanes eliminaron la bolsa, los tanques giraron hacia el norte y cruzaron el Dnieper; mientras tanto el 2.º Ejército Panzer, que había sido desviado del Grupo de Ejércitos Centro por orden de Hitler, había cruzado el río Desna con el 2.º Ejército en su flanco derecho. Los dos ejércitos Panzer atraparon ahora cuatro ejércitos soviéticos y parte de otros dos. El embolsamiento de las fuerzas soviéticas en Kiev fue conseguido el 16 de septiembre. Los rodeados soviéticos no abandonaron fácilmente, siguió una batalla salvaje (véase Batalla de Kiev) que duró diez días, después de la cual los alemanes declararon que habían capturado 600 000 soldados soviéticos. Hitler la llamó la batalla más grande de la historia. Después de Kiev, los alemanes no estaban superados en número por el Ejército Rojo, y los soviéticos no tenían más reservas próximas. A Stalin le quedaban 800 000 hombres para defender Moscú.

En el norte, el 25 de agosto, el Grupo de Ejércitos Norte capturaró Chúdovo, en la línea principal de ferrocarril entre Moscú y Leningrado. Cinco días más tarde tomaron el importante nudo ferroviario de Mga, y el 8 de septiembre, la 20.º División Motorizada ocupó Shlisselburg, en la esquina sureste del lago Ládoga, a treinta y siete kilómetros al este de la ciudad, completando de esta manera el cerco de la ciudad.[25]​

Los enormes cañones de la Flota del Báltico frenaron en seco la primera ofensiva alemana en 1941 a tan solo siete kilómetros de Leningrado, sus poderosos cañones eran capaces de lanzar por los aires los tanques alemanes, tan solo una de estas baterías navales fue capaz de destruir treinta y cinco tanques alemanes, doce posiciones de artillería, un batallón de infantería y un tren militar alemán cargado de soldados y municiones.[26]​ Entonces el mariscal de campo Wilhelm von Leed Comandante del Grupo de Ejércitos Norte, decidió que fuera la Luftwaffe quien despejara el camino a Leningrado hundiendo los barcos de la Flota del Báltico, la primera víctima fue el viejo Acorazado Marat (antiguo Petropavlovsk), hundido en aguas someras tras el impacto directo de dos bombas de 1000 kg. Sin embargo, tres de sus torretas principales estaban intactas junto al resto del casco y los soviéticos lo pusieron en servicio, por lo cual, el Marat continuó como batería estacionaria durante el resto del cerco.[27]​[28]​

La poderosa demostración de fuego de la Flota Soviética y el traslado de la mayor parte de la Luftwaffe y de las unidades blindadas del Grupo de Ejércitos Norte para apoyar la gran ofensiva alemana contra Moscú (véase Operación Tifón) impidió a la Wehrmacht ocupar rápidamente Leningrado, por lo que el Alto Mando Alemán ordenó al Grupo de Ejércitos Norte, atrincherarse y dejar morir de hambre a la población y a la guarnición de la ciudad.[29]​ El 6 de septiembre de 1941, Adolf Hitler, emitió la directiva del Führer n.º 35 ordenaba que tres cuerpos motorizados y el VIII Cuerpo aéreo se pusieran bajo el control del Grupo de Ejércitos Centro para participar en la ofensiva sobre Moscú. Con sus dos divisiones Panzer y sus dos divisiones motorizadas restantes, el Grupo de Ejércitos Norte fue incapaz de hacer progresos en los ataques terrestres. En su lugar comenzaron a bombardear la ciudad con artillería pesada y ataques de la Luftwaffe. El día 12, las bombas alemanas destruyeron el principal almacén de alimentos de la ciudad, hecho que marcaría el comienzo de dos años de hambruna y sufrimiento.[30]​

La Batalla de Moscú (Operación Tifón) comenzó el 2 de octubre. Frente al Grupo de Ejércitos Centro estaba una serie de elaboradas líneas de defensa. Los alemanes sobrepasaron fácilmente la primera línea de defensa cuando el 2.º Ejército Panzer, volviendo desde el sur, tomó Orel que estaba 110 kilómetros detrás de la primera línea soviética de defensa. Entonces los alemanes avanzaron y en el vasto embolsamiento capturaron a 663 000 soldados soviéticos. Los soviéticos solo tenían ahora 90 000 hombres y 1500 tanques para la defensa de Moscú.

Casi desde el principio de la Operación Tifón el clima había ido empeorando, haciendo más lento el avance alemán hacia Moscú, hasta llegar a ser de 3 kilómetros diarios. El 31 de octubre, el Alto Mando del Ejército alemán ordenó un alto en la Operación Tifón para que los ejércitos pudiesen reorganizarse. La pausa dio tiempo a los soviéticos para organizar nuevos ejércitos y traer tropas desde el este, cuando el Pacto de Neutralidad firmado por soviéticos y japoneses en abril de 1941, le aseguraba a Stalin que ya no sería amenazado por los japoneses por más tiempo.

El 15 de noviembre, los alemanes reiniciaron una vez más el ataque a Moscú. Frente a los alemanes esperaban seis ejércitos soviéticos. Los alemanes intentaron que los 3.º y 4.º ejércitos Panzer cruzaran el Canal de Moscú y rodearan Moscú desde el nordeste. El 2.º Ejército Panzer atacaría Tula y después se acercaría a Moscú desde el sur y el 4.º Ejército atacaría en el centro. Pero el 22 de noviembre, las tropas siberianas soviéticas atacaron el 2.º Ejército Panzer en el sur, e infligieron una sorprendente derrota a los alemanes. El 4.º Ejército Panzer tuvo éxito en cruzar el Canal de Moscú y el 2 de diciembre había penetrado hasta siuarse a 25 kilómetros del Kremlin. Pero empezaron las primeras tormentas del invierno y, por falta de previsión, la Wehrmacht no estaba equipada para la guerra de invierno y las congelaciones y enfermedades causaron más bajas que el propio combate; los muertos y heridos ya habían alcanzado un número de 155 000 en tres semanas. Las divisiones estaban a mitad de potencia y el frío causaba grandes problemas a los cañones y al resto de equipo. Los ataques soviéticos solían realizarse muy temprano, dado que las armas alemanas no funcionaban bien a tan bajas temperaturas, mientras que las de los soviéticos sí. Las condiciones climatológicas hacían que la Luftwaffe quedase en tierra. Las tropas soviéticas recién reclutadas cerca de Moscú, eran de cerca de 500 000 hombres, y el general Zhukov lanzó un contraataque masivo el 5 de diciembre que hizo retroceder a los alemanes cerca de 325 kilómetros, aunque no consiguió una brecha definitiva. La invasión de la Unión Soviética había costado a los alemanes hasta la fecha unos 250 000 muertos y 500 000 heridos, así como gran parte de sus tanques.

Hitler ocultó a los japoneses su plan de invadir la Unión Soviética. La URSS, temiendo una guerra en dos frentes, decidió hacer la paz con Japón. El 13 de abril de 1941, la URSS y Japón firmaron el Pacto de Neutralidad, permitiendo que los japoneses concentrasen su atención en la inminente guerra en Asia y el Pacífico.

En el verano de 1941, los Estados Unidos, el Reino Unido y los Países Bajos comenzaron un embargo de petróleo contra el Japón, amenazando con impedir su capacidad para librar una guerra importante tanto en el mar como en el aire. Sin embargo, las fuerzas japonesas continuaron avanzando hacia el interior de China. Durante los meses de verano, Japón trató de sondear las posibilidades de lograr que los Estados Unidos levantasen el embargo de petróleo contra el Imperio de Japón.[14]​ La respuesta estadounidense fijaba como condición sine qua non la retirada de las tropas japonesas en China. Rechazando estas condiciones, Japón planeó un ataque a Pearl Harbor para mermar gravemente a la Flota del Pacífico de los Estados Unidos, y después apoderarse de los campos de petróleo de las Indias Orientales Neerlandesas.[14]​

El primer ministro, príncipe Fumimaro Konoe, era muy reticente a iniciar una guerra contra los Estados Unidos y los países de la Commonwealth. Sin embargo, el emperador Hirohito se inclinó finalmente por las tesis del sector más belicista, como el propio Konoe admitiría ante su jefe de gabinete, Kenji Tomita.[32]​ Ante su aislamiento en el Gobierno y la falta de apoyo del emperador, Konoe se vio forzado a dimitir el 16 de octubre de 1941. Para reemplazarlo, Hirohito eligió, de acuerdo con la recomendación del Señor del Sello Privado, Koichi Kido, al hasta entonces ministro de la Guerra, general Hideki Tōjō, una de las figuras más destacadas del sector belicista, encargándole la organización del ataque contra la flota estadounidense en el Pacífico. El 1 de diciembre, en una Conferencia Imperial celebrada en Tokio, Hirohito dio su aprobación oficial al comienzo de la guerra.[14]​

El 7 de diciembre, Japón lanzó ataques por sorpresa, prácticamente simultáneos, contra Pearl Harbor, Tailandia y los territorios británicos de Malaya y Hong Kong. Una flota de portaaviones japoneses lanzó un ataque aéreo por sorpresa sobre Pearl Harbor. El ataque destruyó la mayor parte de los aviones estadounidenses de la isla y dejó fuera de combate a la principal flota de batalla estadounidense (tres acorazados fueron hundidos, y cinco más gravemente dañados, aunque solo se perdieron definitivamente el USS Arizona y el USS Oklahoma, los otros seis acorazados fueron reparados y pudieron regresar al servicio activo). Sin embargo, los cuatro portaaviones estadounidenses (que eran el principal objetivo del ataque japonés) estaban fuera, en alta mar. En Pearl Harbor, el muelle principal, las instalaciones de suministro y de reparación fueron reparadas rápidamente. Más aún, las instalaciones para el almacenaje de combustible de la base, cuya destrucción habría dejado gravemente mermada a la flota del Pacífico, fueron dejadas intactas. El ataque unió a la opinión pública estadounidense pidiendo venganza contra el Japón. Al día siguiente, el 8 de diciembre, los Estados Unidos declararon la guerra al Japón.

A la vez que atacaban Hawái, los japoneses atacaron la isla de Wake, un territorio estadounidense en el Pacífico Central. El intento de desembarco inicial, fue rechazado por la guarnición de Marines, y una resistencia muy dura continuó hasta el 23 de diciembre. Los japoneses enviaron un gran número de refuerzos, y la guarnición se rindió cuando estuvo claro que no estaba viniendo ninguna fuerza de auxilio estadounidense.

Japón también invadió las Filipinas, un protectorado de los Estados Unidos, el 8 de diciembre. Las fuerzas estadounidenses y filipinas, bajo el mando del general Douglas MacArthur, fueron forzadas a retirarse a la península de Bataán. Una fiera resistencia continuó hasta abril, comprando un tiempo precioso para los Aliados. Después de su rendición, los supervivientes fueron conducidos a la Marcha de la Muerte de Bataán. La resistencia Aliada continuó por un mes más en la isla fortaleza de Corregidor, hasta que también se rindieron. El general MacArthur, al que se le había ordenado retirarse a Australia, prometió: «Volveré».

Un desastre golpeó a los británicos el 10 de diciembre, cuando perdieron 2 barcos de guerra importantes, el HMS Prince of Wales y el HMS Repulse. Ambos buques fueron atacados por 85 bombarderos y torpederos japoneses con base en Saigón, en la Indochina francesa, y 840 marineros británicos perecieron. Winston Churchill dijo acerca del suceso: «En toda la guerra, nunca recibí un golpe más directo».

Alemania declaró la guerra a los Estados Unidos el 11 de diciembre, aunque no estaba obligada a hacerlo bajo el acuerdo del Pacto Tripartito. Hitler esperaba que Japón apoyaría a Alemania atacando a la Unión Soviética. Japón no lo hizo porque había firmado un tratado de no agresión, prefiriendo concentrarse en expandir su imperio en China, Sudeste de Asia, y el Pacífico. Más que abrir un segundo frente sobre la URSS, el efecto de la declaración de guerra alemana fue el de borrar cualquier oposición significativa dentro de los Estados Unidos, para unirse a la lucha en el Teatro Europeo.

Los Aliados Cuatro Grandes —Reino Unido, los Estados Unidos, la Unión Soviética y China[33]​ — fueron creados oficialmente a través de la Declaración de las Naciones Unidas el 1 de enero de 1942. Poco después se formó el Comando estadounidense-británico-neerlandés-australiano, en inglés (ABDACOM), para unificar las fuerzas Aliadas en el Sureste de Asia. Fue el primer mando supremo Aliado de la guerra.

Las fuerzas navales ABDACOM casi fueron destruidas en la batalla del Mar de Java, la batalla naval más grande de la guerra hasta ese momento, desde el 28 de febrero hasta el 1 de marzo. El mando conjunto se acabó poco después, para reemplazarse por tres mandos supremos Aliados en el Sudeste de Asia y en el Pacífico.

En abril, la incursión Doolittle, la primera incursión aérea Aliada sobre Tokio, levantó la moral en los Estados Unidos e hizo que Japón gastase recursos en la defensa de la tierra madre, pero causó poco daño real.

A principios de mayo, los japoneses empezaron a realizar la Operación Mo, un plan para conquistar Port Moresby, en Nueva Guinea. El primer paso fue abortado por las marinas de los Estados Unidos y de Australia en la batalla del Mar del Coral. Esta fue la primera batalla que se luchó entre portaaviones, y la primera batalla donde las flotas enemigas nunca tuvieron contacto visual directo entre ellas. El portaaviones estadounidense Lexington fue hundido y el Yorktown gravemente dañado, mientras que los japoneses perdieron el portaaviones ligero Shōhō y el gran portaaviones Shōkaku sufrió daño moderado. El Zuikaku perdió la mitad de su complemento aéreo, y junto con el Shōkaku, fue incapaz de participar en la consiguiente batalla en Midway. La batalla fue una victoria táctica para los japoneses, ya que infligieron más pérdidas sobre la flota estadounidense que las sufridas por ellos, pero fue una victoria estratégica estadounidense, ya que el ataque japonés sobre Port Moresby fue rechazado.

En los seis meses siguientes a Pearl Harbor, los japoneses habían conseguido casi todos sus objetivos navales. Su flota de 11 acorazados, 10 portaaviones, 18 cruceros pesados y 20 ligeros, permanecía relativamente intacta. Habían hundido o dañado de manera importante todos los acorazados de Estados Unidos en el Pacífico. Las flotas británica y neerlandesa del Lejano Oriente habían sido destruidas, y la Real Armada Australiana, había sido rechazada hacia sus puertos de origen.[34]​ Su anillo de conquistas se cimentaba en un perímetro defensivo de su elección, que se extendía desde el Pacífico Central hasta Nueva Guinea y Birmania.

La única fuerza estratégica aliada de importancia, que permanecía oponiéndose a todo esto, era la base naval de Pearl Harbor, incluyendo los tres portaaviones de la Flota del Pacífico de los Estados Unidos. Ambos bandos veían como algo inevitable una batalla decisiva entre portaaviones, y los japoneses confiaban en que si mantenían una ventaja numérica de 10:3 en portaaviones pesados, obtendrían la victoria.[35]​ También tenían un avión excelente basado en los portaaviones, el Zero. Los japoneses enviaron una flota hacia la Isla de Midway, una isla periférica de las Islas Hawái, con el objetivo de atraer lo que quedaba de la flota estadounidense a una batalla decisiva. El 5 de junio, bombarderos estadounidenses basados en portaaviones avistaron la fuerza japonesa y hundieron 4 de sus mejores portaaviones durante la batalla de Midway, a un coste de un solo portaaviones, el Yorktown. Esta fue una victoria muy importante para los Estados Unidos, y marcó el punto de inflexión en la guerra del Pacífico. La capacidad estadounidense en la construcción de barcos y aviones superaba ampliamente a la japonesa, y la flota japonesa nunca disfrutaría otra vez de tal superioridad numérica.

En julio, los japoneses intentaron un ataque por tierra sobre Port Moresby, a lo largo del sendero Kokoda, un sendero de tierra, en fila india, a través de la jungla y las montañas. Un batallón australiano, que estaba esperando el regreso de las unidades regulares desde el Norte de África y la llegada del ejército estadounidense, superado en número y mal equipado y entrenado, libró una lucha en retirada contra una fuerza japonesa de 5000 hombres.

El 7 de agosto, los Marines estadounidenses comenzaron la batalla de Guadalcanal. Durante los seis meses siguientes, las fuerzas estadounidenses lucharon contra las fuerzas japonesas por el control de la isla. Mientras tanto, se libraron muchos encuentros navales en las aguas cercanas, incluyendo la batalla de la isla de Savo, la batalla del Cabo Esperance, la batalla naval de Guadalcanal, y la batalla de Tassafaronga.

A finales de agosto y principios de septiembre, mientras se combatía en el sendero Kokoda y en Guadalcanal, fue derrotado un ataque de los marines japoneses por fuerzas australianas en la costa sur de Nueva Guinea, en la batalla de la Bahía de Milne. Esta fue la primera derrota de las fuerzas de tierra japonesas en la Guerra del Pacífico.

El 22 de enero, después de una dura batalla en Gona y Buna, las fuerzas australianas y estadounidenses recuperaron las cabezas de playa Japonesas más importantes en el este de Nueva Guinea.

Las autoridades estadounidenses declararon segura a Guadalcanal el 9 de febrero. Las fuerzas de Estados Unidos, Nueva Zelanda, Australia, y de las Islas del Pacífico, empezaron una larga campaña para recuperar las partes ocupadas de las Islas Salomón, Nueva Guinea, y las Indias Orientales Neerlandesas, sufriendo algunas de las resistencias más duras de toda la guerra. El resto de las Islas Salomón fueron recuperadas en 1943.

En 1940, la guerra había llegado a un punto muerto con ambos bandos consiguiendo solamente ganancias mínimas. Los Estados Unidos dieron un importante apoyo financiero a China, y crearon a los Flying Tigers ('Tigres Voladores'), una unidad aérea, para impulsar las fuerzas aéreas Chinas.

Las fuerzas Japonesas invadieron partes del norte de la Indo-China Francesa el 22 de septiembre. Las relaciones Japonesas con occidente se habían deteriorado rápidamente en los últimos años, y los Estados Unidos, que habían rechazado el Tratado de comercio entre Japón y los Estados Unidos de 1911, colocaron un embargo a las exportaciones a Japón de material de guerra y otras materias.

Menos de 24 horas después del ataque sobre Pearl Harbor, Japón invadió Hong Kong. Las Filipinas y las colonias británicas de Malasia, Borneo, y Birmania siguieron poco después, con la intención Japonesa de apoderarse de los campos petrolíferos de las Indias Orientales Neerlandesas. A pesar de la fiera resistencia de las fuerzas filipinas, australianas, neozelandesas, británicas, canadienses, indias y estadounidenses, todos estos territorios capitularon ante los Japoneses en cuestión de meses. Singapur cayó ante los japoneses el 15 de febrero. Aproximadamente 80 000 hombres de la Commonwealth Británica (junto con otros 50 000 que cayeron en Malasia), fueron a los campos de prisioneros japoneses, siendo la rendición más grande de un ejército conducido por los británicos hasta la fecha. Churchill consideraba la derrota británica en Singapur como una de las derrotas británicas más humillantes de toda la historia.

Japón lanzó una ofensiva importante en China después del ataque sobre Pearl Harbor. El objetivo de la ofensiva era el capturar la ciudad de Changsha, estratégicamente importante. Anteriormente los japoneses habían tratado de capturar la ciudad en dos ocasiones, fallando en ambas. Para el ataque, los japoneses reunieron 120 000 soldados en 4 divisiones. Los Chinos respondieron con 300 000 hombres, y pronto el ejército Japonés estaba rodeado, teniendo que retirarse.

El Ejército Nacionalista Chino del Kuomintang, bajo el mando de Chiang Kai-shek, y el Ejército Chino Comunista, bajo el mando de Mao Zedong, ambos se oponían a la ocupación Japonesa de China, pero nunca se aliaron realmente contra los Japoneses. El conflicto entre las fuerzas Nacionalistas y Comunistas, emergió mucho antes de la guerra; y continuó después y, hasta cierto punto, incluso durante la guerra, aunque de forma menos abierta.

Los Japoneses habían capturado gran parte de Birmania, cortando la Carretera de Birmania por la que los Aliados Occidentales habían estado suministrando a los Chinos Nacionalistas. Esta pérdida forzó a los Aliados a crear y sostener un gran puente aéreo desde la India, conocido como volar "The Hump" (la joroba). Bajo el mando del general estadounidense Joseph Stilwell, las fuerzas Chinas en la India fueron reentrenadas y reequipadas, mientras que se hicieron preparativos para construir la Carretera de Ledo, desde la India para reemplazar la Carretera de Birmania. Este esfuerzo se iba a convertir en una tarea de ingeniería enorme.

En el Atlántico Norte, los submarinos alemanes (U-Boot) intentaron cortar las líneas de suministro al Reino Unido hundiendo barcos mercantes.[36]​ En los primeros cuatro meses de guerra hundieron más de 110 barcos. Además de los barcos de suministro, los sumergibles atacaban ocasionalmente barcos de guerra británicos. Un submarino hundió al portaaviones británico HMS Courageous, mientras que el U-47 del legendario comandante Günther Prien consiguió hundir al acorazado HMS Royal Oaken su puerto base de Scapa Flow. Además de los submarinos, los corsarios de superficie también suponían una amenaza para la navegación aliada.

En el Atlántico Sur, el Acorazado de bolsillo  Admiral Graf Spee hundió nueve buques de la Marina mercante británica. Fue localizado más allá de la costa sur de Sudamérica, y después combatió con los cruceros HMS Ajax, HMS Exeter, y HMNZS Achilles en la batalla del Río de la Plata, y fue forzado a entrar en el puerto de Montevideo. Antes que volver a afrontar una nueva batalla, el capitán Hans Langsdorff se hizo a la mar y hundió su buque justo fuera del puerto.

El 24 de mayo de 1941, el acorazado alemán Bismarck partió de su puerto, amenazando con dirigirse hacia el Atlántico. Hundió al HMS Hood, uno de los mejores cruceros de batalla de la Marina Real británica. Siguió entonces una caza masiva, en la que el acorazado alemán fue hundido después de una persecución de 2700 kilómetros, durante la cual los británicos emplearon 8 acorazados y cruceros de batalla, 2 portaaviones, 11 cruceros, 21 destructores, y 6 submarinos. Los aviones torpederos Fairey Swordfish del portaaviones HMS Ark Royal alcanzaron al Bismarck, provocando el bloqueo de su timón y permitiendo que los escuadrones perseguidores de la Marina Real Británica lo alcanzasen y hundiesen.

En el verano de 1941, la Unión Soviética entró en la guerra al lado de los Aliados. Aunque su ejército era muy numeroso, había perdido mucho de su equipo y de su base industrial en las primeras semanas que siguieron a la invasión alemana. Los Aliados Occidentales intentaron remediarlo enviando los Convoyes Árticos, que viajaban desde el Reino Unido y los Estados Unidos hasta los puertos del norte de la Unión Soviética (Arjángelsk y Múrmansk). La traicionera ruta alrededor del Cabo Norte de Noruega, fue lugar de muchas batallas, donde los alemanes trataban continuamente de destruir los convoyes usando sumergibles, bombarderos con base en la costa noruega, ocupada por Alemania, y barcos de superficie.

Tras la entrada de los Estados Unidos en guerra, en diciembre de 1941, los submarinos alemanes hundieron barcos mercantes a lo largo de la Costa Este de los Estados Unidos, el Mar de las Antillas y el Golfo de México. Tuvieron un éxito inicial tan grande que llegó a ser conocido entre las tripulaciones de los sumergibles alemanes como los Segundos buenos tiempos. La institución de los apagones costeros y el sistema de convoyes llevaron a una disminución de los ataques y los submarinos volvieron a su anterior práctica de esperar a los convoyes aliados a mitad de su recorrido en el océano Atlántico.

El 9 de mayo de 1942, el destructor HMS Bulldog capturó al sumergible alemán U 110 y recobró, completa e intacta, una máquina Enigma, un ingenio de cifrado. La máquina se llevó a Bletchley Park, Inglaterra, donde se utilizó para descifrar el código concreto utilizado por los submarinos alemanes. Desde entonces los Aliados disfrutaron de ventaja, ya que podían interceptar y comprender algunas de las comunicaciones por radio alemanas, dirigiendo sus fuerzas navales al lugar donde podían ser más efectivas.

En diciembre de 1943, tuvo lugar la última batalla importante entre la Marina Real Británica y la Armada Alemana. En la batalla de Cabo Norte, el último crucero de batalla alemán, el Scharnhorst, fue hundido por el HMS Duke of York, HMS Belfast y varios destructores.

El momento en el que dio un vuelco la batalla del Atlántico fue a principios de 1943, cuando los Aliados refinaron sus tácticas navales, haciendo un uso efectivo de su nueva tecnología para contrarrestar los ataques de los sumergibles. Los Aliados producían barcos más rápidamente de lo que los submarinos lograban hundirlos, merced a la introducción de la producción en serie, y perdían además menos barcos adoptando el sistema de convoyes, que ya se había ensayado con éxito en la Primera Guerra Mundial. El desarrollo y mejora de la guerra antisubmarina rebajó la esperanza de vida de una tripulación de submarino alemán a meses. Los submarinos del tipo XXI, o elektroboote, con enormes mejoras con relación a los tipos clásicos, aparecieron cuando la guerra ya daba sus últimas bocanadas, demasiado tarde como para afectar su resultado, aunque sirvieron como referente a los vencedores Aliados para desarrollar nuevas clases de submarinos.

El 6 de enero de 1942, Stalin, confiado después de su victoria en Moscú, ordenó una contraofensiva general. Inicialmente los ataques tuvieron éxito cuando los embolsamientos soviéticos se cerraron alrededor de Demiansk (bolsa de Demyansk) y Viazma (bolsa de Viazma), y se hicieron amenazadores ataques hacia Smolensko y Briansk. Pero a pesar de estos éxitos, la ofensiva soviética pronto perdió fuerza. En marzo, los alemanes habían recobrado y estabilizado su línea, y asegurado el corredor de la bolsa de Viazma. Solamente en la bolsa de Demjansk existía alguna perspectiva seria de una gran victoria soviética, ya que allí una gran parte del 16.º Ejército Alemán había sido rodeado. Hitler ordenó que no hubiese ninguna retirada y los 92 000 hombres atrapados en la bolsa tuvieron que defender el terreno en el que estaban, mientras recibían los suministros desde el aire. Aguantaron durante diez semanas, hasta abril, cuando se abrió un corredor terrestre hacia el oeste. De esta manera, las fuerzas alemanas retuvieron Demiansk, hasta que se les permitió retirarse en febrero de 1943.

Con la primavera, ambos bandos decidieron reiniciar la ofensiva. Mientras que el Alto Mando Alemán decidió estabilizar el frente en Járkov, los soviéticos sin saberlo, decidieron atacar en el mismo sector para mantener la presión en el sur. Los soviéticos habían atacado en el sector de Járkov en enero, y habían establecido un saliente en la orilla oeste del río Donets.

El 12 de mayo, los soviéticos comenzaron su ofensiva con ataques concéntricos a cada lado de Járkov (Segunda batalla de Járkov) y, en ambos lados, atravesaron las líneas alemanas, quedando la ciudad seriamente amenazada. Como respuesta, los generales alemanes aceleraron sus planes para su propia ofensiva, que se lanzó cinco días más tarde.

El 6.º Ejército Alemán atacó el saliente desde el sur y rodeó completamente a todo el ejército soviético que estaba asaltando Járkov. En los últimos días de mayo, los alemanes destrozaron las fuerzas que se encontraban dentro de la bolsa. De las tropas soviéticas enbolsadas, 70 000 soldados fueron muertos, 200 000 capturados y solo 22 000 consiguieron escapar. Los alemanes no se percataron de la magnitud de la victoria conseguida y, aunque no lo sabían, a principios de junio las extensas estepas del Cáucaso estaban virtualmente sin defensa.

Tardíamente, Hitler se había dado cuenta que no contaba con tantas fuerzas como para llevar a cabo una ofensiva en todos los sectores del Frente Oriental. No obstante, pensó que si sus ejércitos lograban apoderarse del petróleo y de las tierras fértiles del sur de Rusia, obtendrían los medios para poder continuar la guerra, privando a su vez al Ejército Rojo de su vital fuente de combustible y cereales. En abril, Hitler confirmó sus planes para la campaña principal en Rusia, de nombre en código Operación Azul. Los objetivos de la Operación Azul serían la destrucción del frente sur del Ejército Rojo, la consolidación del control en Ucrania, al oeste del río Volga, y la captura de los campos petrolíferos del Cáucaso. Los alemanes reforzaron al Grupo de Ejército Sur, transfiriendo divisiones de otros sectores y obteniendo divisiones de los aliados del Eje. A finales de junio, Hitler tenía 74 divisiones listas para la ofensiva, aunque solo 54 de ellas eran alemanas.

El plan alemán era un ataque de tres puntas en el sur de Rusia:

Se esperaba que estos movimientos diesen como resultado una serie de grandes bolsas de tropas soviéticas como en la Operación Barbarroja. Aunque los oficiales de la inteligencia soviética no sabían de donde vendría la principal ofensiva alemana de 1942, Stalin estaba convencido que el principal objetivo alemán sería Moscú de nuevo, y un 50 % de todas las tropas del Ejército Rojo fueron desplegadas en esta región. Solo un 10 % de las tropas soviéticas estaban desplegadas en el sur de Rusia.

El 28 de junio de 1942, comenzó la Operación Azul. En todos frentes los soviéticos retrocedieron cuando los alemanes atravesaron sus defensas. El 5 de julio, elementos adelantados del 4.º Ejército Panzer alcanzaron el río Don cerca de Vorónezh y quedaron enzarzados en una amarga batalla para capturar la ciudad. Los soviéticos mantuvieron ocupado al 4.º Ejército Panzer, ganando un tiempo vital para reforzar sus defensas. De esta manera, por vez primera en la guerra, los soviéticos no estaban luchando para aguantar sin esperanza posiciones expuestas, sino para permitir una retirada organizada. Cuando la pinzas alemanas se cerraron, solamente encontraron rezagados y guardias de cobertura.

Enfadado con los retrasos, Hitler reorganizó al Grupo de Ejércitos Sur en dos Grupos de Ejércitos más pequeños: A y B. El Grupo de Ejércitos A incluía al 17.º Ejército, al 1.º Ejército Panzer y al 4.º Ejército Panzer. El Grupo de Ejércitos B incluía al 2.º Ejército, al 6.º Ejército, al 8.º Ejército Italiano, al 2.º Ejército Húngaro, y a los 3.º y 4.º Ejércitos Rumanos. El grueso de las fuerzas acorazadas ahora estaba concentrado en el Grupo de Ejércitos A, al que se le ordenó avanzar hacia los campo petrolíferos del Cáucaso, mientras que al Grupo de Ejércitos B se le ordenó capturar Stalingrado y defenderlo de cualquier contraataque soviético. La transferencia del 4.º Ejército Panzer lejos del 6.º Ejército ayudó al 1.er Ejército Panzer a cruzar la región baja del río Don, pero redujo el avance del 6.º Ejército a una marcha, dando más tiempo a los soviéticos a consolidar sus posiciones en Stalingrado.

El 23 de julio, el 6.º Ejército Alemán había tomado Rostov del Don, pero los soviéticos lucharon con una hábil acción de cobertura que enzarzó a los alemanes en una dura lucha urbana para tomar la ciudad. Esto también permitió que las principales formaciones soviéticas escapasen de un embolsamiento. Con el cruce del río Don asegurado en el sur y con el avance del 6.º Ejército yendo muy despacio, Hitler envió al 4.º Ejército Panzer para reunirse otra vez con el 6.º Ejército. A finales de julio, el 6.º Ejército reemprendió su ofensiva y el 10 de agosto limpió la orilla occidental del Don, pero los soviéticos aguantaron en algunas zonas, retrasando la marcha del 6.º Ejército hacia el este. En contraste, el Grupo de ejército A, después de cruzar el Don el 25 de julio, se había extendido en un amplio frente. El 17.º Ejército Alemán giró hacia el oeste, hacia el Mar Negro, mientras que el 1.er Ejército Panzer atacó hacia el sur y al este, barriendo un terreno abandonado en su mayor parte por los soviéticos en retirada. El 9 de agosto, el 1.er Ejército Panzer alcanzó las estribaciones de las montañas del Cáucaso, habiendo avanzando más de 450 kilómetros.

Después de limpiar de tropas soviéticas la orilla oeste del Don, el 6.º Ejército Alemán cruzó el río el 21 de agosto y empezó a avanzar hacia Stalingrado. La Luftwaffe bombardeó la ciudad matando 40 000 personas, convirtiendo gran parte de la ciudad en ruinas. El 6.º Ejército avanzó entonces sobre Stalingrado desde el norte, mientras que el 4.º Ejército Panzer avanzó desde el sur. Entre estos ejércitos y en el área desde el Don al Volga, se había creado un saliente. Dos ejércitos soviéticos defendían el saliente y, el 29 de agosto, el 4.º Ejército Panzer lanzó un gran ataque a través del saliente hacia Stalingrado. Se le ordenó al 6.º Ejército que hiciese lo mismo, pero los soviéticos montaron fuertes ataques contra el 6.º Ejército desde el norte que lo inmovilizaron durante tres días vitales, que hicieron posible que las fuerzas soviéticas escapasen al embolsamiento, y se retirasen hacia Stalingrado. Los soviéticos, que en este momento ya se habían dado cuenta que el plan alemán era apoderarse de los campos petrolíferos, empezaron a enviar un gran número de tropas desde el sector de Moscú para reforzar a sus tropas en el sur. Zhúkov asumió el mando del frente de Stalingrado y a principios de septiembre lanzó una serie de ataques desde el norte que retrasaron aún más el intento del 6.º Ejército de tomar la ciudad. A mediados de septiembre, el 6.º Ejército, después de neutralizar los contraataques soviéticos, reasumió otra vez la captura de la ciudad. El 13 de septiembre, los alemanes avanzaron a través de los suburbios del sur y para el 23 de septiembre de 1942, el principal complejo de fábricas estaba rodeado y la artillería alemana alcanzaba los muelles del río, a través de los cuales, los soviéticos evacuaban a los heridos y traían refuerzos. La lucha callejera feroz, el conflicto cuerpo a cuerpo de la clase más salvaje, se adueñaban ahora de Stalingrado. El agotamiento y las privaciones quitaban gradualmente las fuerzas a los hombres de ambos bandos, ya que una de las batallas más sangrientas de la Segunda Guerra Mundial acababa de comenzar.

El 6.º Ejército, al mando del general Friedrich Paulus, no había sido equipado para luchar en un ambiente urbano, y le pidió a Hitler poder retirarse para reorganizar sus fuerzas, pero este, que había llegado a obsesionarse con la toma de Stalingrado, rehusó contemplar una retirada. El general Paulus, desesperado, usando sus últimas reservas lanzó otro ataque a principios de noviembre, ya que en este momento los alemanes habían conseguido capturar el 90 % de la ciudad. Los soviéticos, sin embargo, habían estado acumulando fuerzas frescas en los flancos de Stalingrado, que estaban en este momento severamente bajas de hombres por parte del Eje, ya que el grueso de las fuerzas alemanas estaba concentrado en la captura de la ciudad, y las tropas de los Socios del Eje se habían dejado guardando los flancos. El 19 de noviembre de 1942, los soviéticos lanzaron la Operación Urano, con ataques simultáneos que rompieron los débiles flancos enemigos, custodiados por tropas rumanas e italianas, y se encontraron en la ciudad de Kalach cuatro días más tarde, embolsando al 6.º Ejército en Stalingrado.

Los generales pidieron permiso para intentar romper el cerco, lo cual fue rechazado por Hitler, que ordenó al 6.º Ejército permanecer en Stalingrado, y les prometió que serían enviados suministros desde el aire hasta que fuesen rescatados. La palabra de Göring se vio duramente puesta en entredicho, pues de las 500 toneladas diarias de suministros prometidos, para apoyar a los soldados alemanes asediados, no llegaron a Stalingrado ni la décima parte. 

Al mismo tiempo, los soviéticos lanzaron la Operación Marte en un saliente cerca de Moscú. Su objetivo era el inmovilizar al Grupo de Ejércitos Centro e impedir que pudiese reforzar a las fuerzas del Grupo de Ejércitos B en Stalingrado.

Mientras tanto, el avance del Grupo de Ejércitos A en el Cáucaso se había detenido cuando los soviéticos destruyeron las instalaciones petrolíferas, y se requeriría un año de trabajo para volverlas a hacer operativas, y los campos petrolíferos que quedaban, estaban al sur de las montañas del Cáucaso. Todo agosto y septiembre, las tropas de montaña alemanas sondearon para intentar encontrar un medio de pasar las montañas, pero para octubre, con el comienzo del invierno, no estaban más cerca de conseguir su objetivo. Con las tropas alemanas rodeadas en Stalingrado, el Grupo de Ejércitos A comenzó a replegarse.

En diciembre, el mariscal de campo Erich von Manstein, formó rápidamente una fuerza de socorro alemana compuesta con unidades del Grupo de Ejército A para liberar al aislado 6.º Ejército. Incapaz de obtener refuerzos del Grupo de Ejércitos Centro, la fuerza de socorro solo consiguió penetrar 50 kilómetros antes de ser forzada a retroceder por los soviéticos. Para final del año, el 6.º Ejército estaba en una situación desesperada, cuando la Luftwaffe solamente fue capaz de suministrar un sexto de los suministros que Hermann Goering había prometido.

En enero de 1943 se lleva a cabo la Operación «Iskra» Chispa, planeada por el Alto Mando soviético con el objetivo prioritario de romper el sitio de Leningrado. La planificación de la operación comenzó poco después del fracaso de la ofensiva de Siniávino (19 de agosto-10 de octubre de 1942).[38]​ La realización de la operación se encomendó al Frente de Leningrado y al Frente del Vóljov del Ejército Rojo[nota 1]​con el apoyo de la Flota del Báltico y de la Flotilla del Ládoga, del 12 al 30 de enero de 1943, con el objetivo de crear una conexión terrestre con Leningrado. Las fuerzas de ambos frentes se unieron el 18 de enero y el 22 de enero la línea del frente se había estabilizado. La operación abrió con éxito un corredor terrestre de entre ocho a diez kilómetros de ancho hasta la ciudad. Inmediatamente después de la operación, se construyó un ferrocarril a través del corredor que permitió que llegaran muchos más suministros a la ciudad que por el Camino de la Vida a través de la superficie congelada del lago Ládoga, reduciendo significativamente la posibilidad de captura de la ciudad y cualquier vínculo entre las tropas de Alemania y de Finlandia.[40]​ 

Poco antes de rendirse al Ejército Rojo el 2 de febrero de 1943, Friedrich Paulus fue ascendido a mariscal de campo. De esta manera, Hitler le indicaba a Paulus que se suicidase, porque ningún Mariscal de Campo alemán había rendido jamás sus tropas o había sido cogido prisionero. De los 300 000 hombres del 6.º Ejército, solo sobrevivieron 91 000 para ser cogidos como prisioneros, incluyendo 22 generales, pero solo unos 5 000 hombres volverían a Alemania después de la guerra. Ésta llegó a ser la batalla más grande, y más costosa en vidas humanas, de la historia. En ambos lados murieron o fueron heridos alrededor de dos millones de personas, incluyendo civiles, siendo las bajas del Eje de aproximadamente unas 850 000.

El 10 de febrero de 1943, el Cuartel General del Mando Supremo (Stavka) lanzó la operación Estrella Polar, menos de dos semanas después de que la operación chispa, levantara parcialmente el sitio de Leningrado. El objetivo de esta nueva ofensiva era derrotar de manera decisiva al Grupo de Ejércitos Norte, levantando el asedio por completo, pero solo logró ganancias muy modestas a costa de un gran número de bajas.[41]​ El Ejército Rojo realizó otros intentos en 1943 para renovar su ofensiva y levantar el sitio por completo, pero solo lograron avances limitados en cada uno de ellos. El estrecho corredor a través del cual, discurría el Camino de la Victoria, permaneció dentro del alcance de la artillería alemana. Al mismo tiempo la artillería alemana de largo alcance continuó bombardeando la ciudad de forma intermitente. El Ejército Rojo no levantó completamente el asedio hasta un año después, el 27 de enero de 1944.[42]​ La ofensiva terminó un mes después, el 1 de marzo, cuando la Stavka ordenó a las tropas del Frente de Leningrado realizar una operación de seguimiento a través del río Narva, mientras que el Segundo Frente Báltico debía defender el territorio ganado en persecución del XVI Cuerpo de Ejército alemán.[43]​

Aparte de Italia, Europa Occidental vio muy poca lucha desde septiembre de 1940 a junio de 1944. Fuerzas británicas y canadienses lanzaron un pequeño ataque en el pequeño puerto pesquero de la Francia ocupada en Dieppe, el 19 de agosto de 1942, cuyo objetivo era sondear y ganar información para una invasión de Europa que sucedería más tarde en la guerra. La batalla de Dieppe fue un desastre total, pero proporcionó información crítica acerca de las tácticas anfibias que serían utilizadas más tarde en la Operación Torch y la Operación Overlord.[44]​

En diciembre de 1941, siguiendo al ataque japonés en Pearl Harbor, que llevó a los Estados Unidos a la guerra, Churchill y Roosevelt se encontraron en la Conferencia Arcadia. Acordaron que la derrota de Alemania tenía prioridad sobre la derrota del Japón. Para aliviar la presión alemana sobre la Unión Soviética, los Estados Unidos propusieron una invasión de Francia cruzando el canal en 1942. Los británicos se opusieron a esto, sugiriendo en vez de ello una pequeña invasión de Noruega o desembarcos en el África del Norte Francesa. La Declaración de las Naciones Unidas fue emitida, y los Aliados Occidentales invadieron primero el Norte de África.[45]​

Con la entrada de los Estados Unidos en la contienda, la guerra aérea se volvió a favor de los Aliados a últimos de 1942. Las Fuerzas Aéreas del Ejército de los Estados Unidos comenzaron a llevar a cabo los primeros bombardeos diurnos sobre Alemania, lo que permitió apuntar de manera mucho más precisa, pero expuso a los bombarderos a un mayor peligro que en el bombardeo nocturno. Mientras tanto, los británicos y los canadienses tomaron como objetivos las ciudades alemanas y las industrias de guerra para el bombardeo nocturno. Este esfuerzo fue orquestado por el Primer Mariscal del Aire Harris, que llegó a ser conocido como Bombardero Harris. Los ataques en masa, que podían llegar a tener entre quinientos a mil bombarderos pesados[cita requerida], fueron realizados contra aeropuertos, centros industriales, bases de submarinos, centros de ferrocarril, depósitos de combustible y, en los últimos estados de la guerra, los lugares de lanzamiento para armas tales como el misil V-1 o el cohete V2. Aparte de instalaciones industriales y militares, las ciudades alemanas sufrieron duros bombardeos que se saldaron con cientos e incluso miles de civiles muertos.

Los aliados también empezaron misiones de sabotaje contra Alemania, tales como la Operación Antropoide, en la que Reinhard Heydrich, el arquitecto de la Solución final, fue asesinado en mayo de 1942 por agentes de la resistencia checa que habían volado desde el Reino Unido.[46]​ Hitler ordenó graves represalias contra los ocupantes del cercano pueblo checoslovaco de Lídice. Todo el tiempo, los Aliados continuaron construyendo e incrementando sus fuerzas en el Reino Unido para una eventual invasión de Europa Occidental que fue planeada para finales de primavera, o para principios del verano de 1944.

La rendición de las fuerzas del Eje en Túnez el 13 de mayo de 1943, dejó como resultado 250 000 prisioneros. La Guerra del Norte de África, resultó un desastre para Italia, y cuando los Aliados invadieron Sicilia el 10 de julio en la Operación Husky, capturando la isla en poco menos de un mes, el régimen de Benito Mussolini se colapsó. El 25 de julio, fue destituido de su cargo por Víctor Manuel III, el Rey de Italia, y arrestado con el consentimiento del Gran Consejo Fascista. Un nuevo gobierno, dirigido por Pietro Badoglio, tomó el poder y declaró ostensiblemente que Italia permanecería en la guerra. Badoglio ya había empezado a tener negociaciones secretas de paz con los Aliados.

Los Aliados invadieron la Italia continental el 3 de septiembre de 1943. Italia se rindió a los Aliados el 8 de septiembre, como había sido acordado en las negociaciones. La familia real y el gobierno de Badoglio escaparon hacia el sur, dejando al Ejército Italiano sin órdenes, mientras que los alemanes continuaron la lucha, forzando a los Aliados a una parada completa en el invierno de 1943-1944 en la Línea Gustav al sur de Roma.

En el norte, Mussolini, fue liberado por orden de Hitler, por un grupo de paracaidistas de las SS de Alemania bajo el mando de Otto Skorzeny el 12 de septiembre de 1943. Con el apoyo nazi, creó lo que era de hecho un gobierno títere, la República Social Italiana o República de Saló, llamada así por la nueva capital en Saló en el lago de Garda. En estos momentos, los grupos clandestinos de oposición a Mussolini y a la ocupación alemana se habían armado y habían comenzado una guerra de guerrillas para desestabilizar su poder. A este movimiento subversivo se le conoce como Resistencia italiana.

A mediados de 1943 se produjo la quinta y final ofensiva del Sutjeska de los alemanes contra los partisanos yugoslavos.

Siguiendo la rendición Italiana, las tropas alemanas tomaron la defensa de la península Itálica y establecieron la Línea Gustav en los Montes Apeninos del sur, al sur de Roma. Los Aliados fueron incapaces de romper esta línea, y así intentaron rodearla con un desembarco anfibio en Anzio el 22 de enero de 1944. El desembarco, llamado Operación Shingle, fue rodeado rápidamente por los alemanes y parado en seco, haciendo que Churchill comentase: «En vez de lanzar un gato salvaje a la costa, todo lo que tenemos es una ballena varada».

Incapaz de flanquear la Línea Gustav, los Aliados intentaron de nuevo, romperla mediante asaltos frontales. El 15 de febrero, el monasterio de Montecassino, fundado en el 524 por San Benito fue destruido por bombarderos estadounidenses B-17 y B-26. Paracaidistas de élite alemanes se lanzaron inmediatamente sobre las ruinas para defenderlas. Desde el 12 de enero hasta el 18 de mayo, fue asaltado cuatro veces por las tropas Aliadas, con el resultado de unas pérdidas de 54 000 bajas aliadas y de 20 000 soldados alemanes.

Después de unos meses, se rompió la línea Gustav y los Aliados avanzaron hacia el norte. El 4 de junio, Roma fue liberada, y el ejército Aliado alcanzó Florencia en agosto. Fue entonces detenido en la Línea Gótica en los Apeninos toscanos durante el invierno.

Después de la rendición del 6.º Ejército Alemán en Stalingrado el 2 de febrero de 1943, el Ejército Rojo lanzó ocho ofensivas durante el invierno. Muchas estaban concentradas a lo largo de la cuenca del Don cerca de Stalingrado. Estos ataques resultaron en ganancias iniciales, hasta que las fuerzas alemanas fueron capaces de tomar ventaja de la sobre extensión y debilitada condición del Ejército Rojo, y lanzar un contraataque para recapturar la ciudad de Járkov y áreas circundantes. Esta sería la última victoria estratégica importante de los alemanes en la Segunda Guerra Mundial.

Las lluvias de primavera impidieron las operaciones en la Unión Soviética, pero ambos lados usaron este tiempo para prepararse para la inevitable batalla que llegaría en el verano. La fecha del comienzo de la ofensiva se había movido repetidamente, debido a que retrasos en su preparación habían forzado a los alemanes a posponer el ataque. El 4 de julio, la Wehrmacht, después de reunir la concentración de poder de fuego más grande de toda la Segunda Guerra Mundial, lanzó su ofensiva contra la Unión Soviética en el saliente de Kursk. Los soviéticos conocían sus intenciones, y se apresuraron a defender el saliente con un sistema enorme de defensas en el terreno. Los alemanes atacaron a la vez desde el norte y el sur del saliente y esperaban encontrarse en el medio, cortar el saliente y atrapar a 60 divisiones soviéticas. La ofensiva alemana en el sector Norte fue abortada cuando consiguieron realizar muy pocos progresos a través de las defensas soviéticas, pero en el sector Sur hubo verdadero peligro de producirse una penetración alemana. Los soviéticos trajeron entonces sus reservas para contener el empuje alemán en el sector Sur, y la consiguiente batalla de Kursk, llegó a ser la batalla de tanques más grande de la guerra, cerca de la ciudad de Projorovka. Los alemanes ya no tenían reservas de consideración, habiendo agotado sus fuerzas acorazadas y no pudieron parar la contraofensiva soviética que los lanzó de vuelta a sus posiciones de partida.

Los soviéticos capturaron Járkov después de su victoria en Kursk, y con la amenaza de las lluvias del otoño, Hitler estuvo de acuerdo en una retirada general a la línea del Dnieper en agosto. A fines de septiembre, los alemanes encontraron la línea del Dnieper imposible de sostener cuando crecieron las cabezas de puente soviéticas. Ciudades importantes del Dniéper empezaron a caer, siendo la primera Zaporozhye, seguida por Dnepropetrovsk. A principios de noviembre los soviéticos penetraron a través de sus cabezas de puente a ambos lados de Kiev y recapturaron la capital ucraniana. El 1er Frente Ucraniano atacó en Korosten en Nochebuena, y el avance soviético continuó a lo largo de la línea del ferrocarril hasta que se alcanzó la frontera polaco-soviética de 1939.


El 26 de enero de 1944, después de la exitosa Ofensiva de Leningrado-Nóvgorod, Iósif Stalin declaró que el Sitio de Leningrado había sido levantado y que las fuerzas alemanas fueron expulsadas del óblast de Leningrado.[48]​ Poco después, Nóvgorod también fue liberada y en febrero el avance soviético se detuvo en la frontera con Estonia, después de haber hecho retroceder 100 kilómetros el frente y liberar por completo la región de Leningrado.
«De pronto, Leningrado emergió de la oscuridad ante nuestros ojos», escribió la poetisa Olga Bergholz. «Hasta las últimas grietas en los muros, la ciudad nos fue reveladaː bombardeada, acribillada y marcada con sus ventanas de madera contrachapada. Y vimos que a pesar de tantos golpes crueles, Leningrado conservaba su orgullosa belleza. Bajo las luces azuladas, rosadas, verdes y blancas, la ciudad nos pareció tan austera y conmovedora que no nos cansábamos de contemplarla».[49]​
Para marzo los soviéticos golpearon en Rumanía desde Ucrania. Las fuerzas soviéticas rodearon al 1.er Ejército Panzer, al norte del río Nistru. Los alemanes escaparon de la bolsa en abril, salvando a la mayoría de sus hombres pero perdiendo su equipo pesado. Durante abril, el Ejército Rojo lanzó una serie de ataques cerca de la ciudad de Iaşi, Rumanía, con el objetivo de capturar el sector, estratégicamente importante, que esperaban usar de trampolín para lanzarse hacia Rumanía para una ofensiva de verano. Cuando lanzaron el ataque a través del bosque de Târgu Frumos los soviéticos fueron rechazados por los alemanes y las fuerzas rumanas, al defender con éxito las fuerzas del Eje el sector a través del mes de abril.

Cuando las tropas soviéticas se acercaron a Hungría, las tropas alemanas ocuparon Hungría el 20 de marzo. Hitler pensó que el líder húngaro, el almirante Miklós Horthy ya no podía considerarse un aliado fiable. Otro de los aliados del Eje, Finlandia, había buscado una paz separada con Stalin en febrero de 1944, pero no aceptaron los términos iniciales que se les ofrecieron. El 9 de junio, la Unión Soviética comenzó la Ofensiva de Víborg-Petrozavodsk en el Istmo de Karelia que, después de tres meses, forzó a Finlandia a aceptar un armisticio.

Antes que los soviéticos pudiesen comenzar su ofensiva de verano hacia Bielorrusia, tenían que limpiar la península de Crimea de fuerzas del Eje. Restos del 17.º Ejército alemán del Grupo de Ejércitos Sur y algunas fuerzas rumanas, habían sido aisladas y dejadas atrás en la península cuando los alemanes se habían retirado de Ucrania. A principios de mayo, el 3.er Frente Ucraniano del Ejército Rojo atacó a los alemanes y la consiguiente batalla fue una victoria completa para las fuerzas soviéticas, fracasando un chapucero esfuerzo de evacuación a través del Mar Negro por parte de los alemanes (véase ofensiva de Crimea).

Con Crimea limpia, la largamente esperada ofensiva soviética de verano, de nombre en código, Operación Bagration, comenzó el 22 de junio de 1944, con 2,5 millones de hombres y 6000 tanques. Su objetivo era limpiar Bielorrusia de tropas alemanas, y aplastar al Grupo de Ejército Centro Alemán que estaba defendiendo ese sector. La ofensiva se organizó para coincidir con los desembarcos Aliados en Normandía, pero retrasos hicieron que la ofensiva tuviese que ser pospuesta por algunas semanas. La subsiguiente batalla resultó en la destrucción del Grupo de Ejército Centro Alemán, y en unas 800 000 bajas alemanas, la derrota más grande de la Wehrmacht durante la guerra. Los soviéticos continuaron imparables adelante, alcanzando los alrededores de Varsovia el 31 de julio.

La proximidad del Ejército Rojo, hizo que los polacos de Varsovia pensasen que serían liberados pronto. El 1 de agosto, se rebelaron como parte de la más amplia Operación Tempest. Casi 40 000 luchadores de la resistencia polaca tomaron el control de la ciudad. Los soviéticos, sin embargo, no avanzaron más. [2] La única ayuda que recibieron los polacos fue fuego de artillería, cuando unidades del ejército alemán, que se movían dentro de la ciudad para acallar la revuelta, recibieron disparos de artillería soviética. La resistencia acabó el 2 de octubre. Después unidades alemanas destruyeron la mayor parte de lo que había quedado de la ciudad.

Después de la destrucción del Grupo de Ejército Centro Alemán, los soviéticos atacaron a las fuerzas alemanas en el sur a mediados de julio de 1944, y en el plazo de un mes habían limpiado Ucrania de la presencia alemana, infligiéndoles graves pérdidas a los alemanes. Una vez que Ucrania fue limpiada, las tropas soviéticas golpearon en Rumanía. El 2.º y 3.er Frentes Ucranianos del Ejército Rojo, se enzarzaron con el Heeresgruppe Südukraine alemán, que estaba constituido por formaciones alemanas y rumanas, en un operación para ocupar Rumanía y destruir las formaciones Alemanas en el sector. El resultado de la batalla de Rumanía fue una victoria completa para el Ejército Rojo, y significó el paso de Rumanía desde el campo del Eje hacia el campo Aliado. Bulgaria se rindió al Ejército Rojo en septiembre. Siguiendo a los alemanes en retirada desde Rumanía, los soviéticos entraron en Hungría en octubre de 1944 pero el 6.º Ejército Alemán rodeó y destruyó tres cuerpos del Grupo Pliyev del Mariscal Rodión Yakovlevich Malinovsky cerca de Debrecen, en Hungría. Los soviéticos habían esperado con su rápido asalto la captura de Budapest, pero fueron rechazados y Hungría permanecería como aliada de Alemania hasta el fin de la guerra en Europa. Esta batalla sería la última victoria alemana en el Frente Oriental.

Los soviéticos se recobraron de su derrota en Debrecen, y las columnas adelantadas del Ejército Rojo colaboraron con los Partisanos yugoslavos en la liberaron Belgrado a últimos de noviembre y alcanzaron Budapest el 29 de diciembre de 1944, rodeando la ciudad y atrapando unas 188 000 tropas del Eje, incluyendo muchas Waffen-SS alemanas. Los alemanes aguantaron hasta el 13 de febrero de 1945, y el asedio se convirtió en uno de los más sangrientos de la guerra. Mientras tanto el 1.er, 2.º y 3.er Frentes del Báltico del Ejército Rojo entablaron combate con los restos del Grupo de Ejército Centro y del Grupo de Ejércitos Norte para capturar la región báltica de manos alemanas en octubre de 1944. El resultado de la consiguiente serie de batallas fue la pérdida permanente de contacto entre los Grupos de Ejército Norte y Centro, y la creación de la bolsa de Curlandia en Letonia, donde los ejércitos alemanes 16.º y 18.º fueron atrapados, con un total de unos 250 000 hombres, y allí permanecerían hasta el final de la guerra.

El 30 de junio, los Aliados lanzaron la Operación Cartwheel, una operación de gran estrategia para el Pacífico Sur y Suroeste, encaminada a aislar la base Japonesa más importante, Rabaul, antes de proceder a la campaña de «saltar de isla en isla» hacia Japón. Había tres objetivos principales: volver a capturar Tulagi y las Islas Santa Cruz; volver a conquistar la costa norte de Nueva Guinea, y las Islas Salomón centrales; y la toma de Rabaul y bases cercanas.

Para septiembre, las fuerzas australianas y estadounidenses en Nueva Guinea habían capturado las bases más importantes Japonesas en Salamaua y Lae. Poco después se lanzaron sobre la Península Huon, la cadena montañosa Finisterre, Bougainville, y las campañas de Nueva Bretaña.

En noviembre, los marines de Estados Unidos vencieron en la batalla de Tarawa. Este fue el primer asalto anfibio con una oposición muy fuerte en el teatro del Pacífico. La gran cantidad de bajas que sufrieron los Marines, desató una tormenta de protestas en los Estados Unidos, donde no se podía comprender que se sufriesen pérdidas tan grandes por una diminuta y aparentemente sin importancia isla. Los Aliados adoptaron una política de puentear algunas islas fuertes Japonesas y dejarlas "pudrirse en el árbol", rotos sus suministros y tropas de refresco.

El avance aliado continuó en el Pacífico con la captura de las Islas Marshall antes de finales de febrero. Unos 42 000 soldados del Ejército y Marines de los Estados Unidos desembarcaron en el atolón Kwajalein el 31 de enero. Se produjo una batalla muy dura, y la isla fue conquistada el 6 de febrero. Después los Marines de Estados Unidos volvieron a derrotar a los Japoneses en la batalla de Eniwetok.

El objetivo estratégico de los Estados Unidos era el conseguir bases aéreas para poder bombardear Japón con sus nuevos B29, en las Islas Marianas, especialmente Saipán, Tinian y Guam. El 11 de junio, la flota Naval de los Estados Unidos bombardeó Saipán, defendido por 32 000 tropas japonesas; La batalla de Saipán comenzó el día 15, cuando 77 000 marines desembarcaron, consiguiendo asegurar la isla el 9 de julio. Los Japoneses emplearon toda su menguante fuerza naval en la batalla del Mar de Filipinas, pero sufrieron graves pérdidas en barcos y aviones. Después de la batalla, la fuerza de portaaviones Japonesa ya no era efectiva militarmente. Con la captura de Saipán, Japón estaba al fin al alcance de los bombarderos B-29.

Guam fue invadida el 21 de julio y conquistada el 10 de agosto, pero los japoneses lucharon fanáticamente. Las operaciones de limpieza continuaron mucho tiempo después de que la batalla de Guam hubiese acabado oficialmente. La isla de Tinian fue invadida el 24 de julio y fue tomada el 1 de agosto. Esta operación vio el uso por vez primera del napalm en una guerra.[51]​

Las tropas del general MacArthur liberaron las Filipinas, desembarcando en la isla de Leyte el 20 de octubre. Los Japoneses se habían preparado, dispuestos a una defensa a toda costa, y usaron los últimos restos de sus fuerzas navales en un intento fallido para destruir la fuerza de invasión en la batalla del Golfo de Leyte, desde el 23 de octubre hasta el 26 de octubre de 1944, la batalla naval más grande de la historia del mundo moderno. Esta fue la primera batalla en la que los Japoneses emplearon ataques kamikaze. El acorazado Japonés Musashi, uno de los dos acorazados más grandes jamás construidos, fue hundido por 19 torpedos estadounidenses y 17 bombas.

A lo largo de 1944, los submarinos y aviones Aliados atacaron la marina mercante Japonesa, y privaron a la industria japonesa de las materias primas, por cuya obtención Japón había ido a la guerra. El principal objetivo era el petróleo, y Japón estaba casi seco a finales de 1944. En 1944, los submarinos hundieron unos dos millones de toneladas de carga,[52]​ mientras que los Japoneses solamente fueron capaces de reemplazar menos de un millón de toneladas.[53]​ El 6.º Ejército de los Estados Unidos desembarcó en Luzón, la principal isla de las Filipinas. Manila fue reconquistada en marzo.

Los Estados Unidos capturaron Iwo Jima en febrero. La isla era psicológicamente importante porque era un territorio tradicional Japonés, administrado por la prefectura de Tokio. Estaba fuertemente defendido con muchos túneles, trincheras y fuertes bajo tierra, pero eventualmente fue conquistado por los Marines, después de que hubiesen capturado el Monte Suribachi, un punto clave de la defensa. Iwo Jima probó ser de un valor incalculable debido a sus dos campos de aviación que fueron usados para los aterrizajes de emergencia de los B29, y porque estaba bastante cerca de Japón como para proveer escolta de cazas a los bombarderos, y así alcanzar las islas de origen japonesas.[54]​

Con la consiguiente captura de Okinawa (desde abril hasta junio), los Estados Unidos trajeron a la tierra natal de los Japoneses, dentro de un radio de acción más cómodo, para sus ataques navales y aéreos. Los japoneses defendieron la isla con tropas terrestres, kamikazes, y con la misión suicida del acorazado Yamato, que fue hundido por los bombarderos en picado estadounidenses. Junto con docenas de otras ciudades Japonesas, Tokio fue bombardeado con bombas incendiarias, y murieron cerca de 90 000 personas en el ataque inicial. Las condiciones de vida hacinadas alrededor de los centros de producción y las construcciones residenciales de madera contribuyeron a las cifras tan grandes de pérdidas humanas. Además, los puertos y las mayores áreas de tránsito marítimo de Japón fueron saturadas con minas colocadas desde el aire, en la Operación Starvation, que desorganizó totalmente la logística de la nación isla.

La última ofensiva importante en el área del Pacífico Sudoeste fue la Campaña de Borneo de mediados de 1945, cuyo objetivo era aislar más aún, a las fuerzas japonesas que quedaban en el Sureste de Asia, y asegurar la liberación de los prisioneros de guerra aliados.

En abril de 1944, los japoneses comenzaron la Operación Ichigo, para asegurar la ruta férrea entre Peking y Nankín, y para limpiar el sur de China de campos de aviación estadounidenses bajo el mando del general Chennault.[55]​ La operación tuvo éxito, ya que abrió un corredor continuo entre Peking e Indochina, y forzó la recolocación de los campos de aviación más tierra adentro.Sin embargo, falló en la destrucción del ejército de Chiang Kai-shek, y los estadounidenses pronto adquirieron las Marianas, desde las que podían bombardear las islas de origen japonesas.

Mientras los estadounidenses continuaban sin pausa la construcción de la carretera de Ledo desde la India hasta China, en marzo de 1944, los japoneses empezaron su propia ofensiva hacia la India. Esta "Delhi Chalo" ('Marcha hacia Delhi') fue iniciada por Netaji Subhas Chandra Bose,[56]​ el comandante del Ejército Nacional Indio (una fuerza compuesta de prisioneros de guerra del Ejército Indio Británico, que habían sido capturados por los japoneses y que habían decidido unirse a la guerra en un intento para librar a la India de sus gobernantes coloniales, y desde ahí obtener la independencia).[57]​ Los japoneses intentaron destruir a las principales fuerzas indias y británicas en Kohima e Imphal, resultando en algunos de los combates más feroces de la guerra. Mientras que las tropas aliadas que estaban cercadas eran reforzadas y suministradas por aviones de transporte hasta que tropas frescas consiguieron romper el asedio, los japoneses, debido en parte a las lluvias torrenciales, agotaron sus suministros y empezaron a pasar hambre. Las fuerzas supervivientes se retiraron eventualmente perdiendo 85 000 hombres, una de las derrotas más grandes del Japón durante la guerra.

Durante el monzón desde agosto hasta noviembre de 1944, los japoneses fueron perseguidos hasta el río Chindwin en Birmania. Con el comienzo de la estación seca a principios de 1945, las fuerzas estadounidenses y chinas finalmente completaban la carretera de Ledo, aunque demasiado tarde como para tener ningún efecto decisivo. El 14.º Ejército Británico, compuesto de unidades indias, británicas y africanas, lanzó una ofensiva en Birmania central. Las fuerzas Japonesas fueron derrotadas decisivamente, y los aliados los persiguieron hacia el sur, conquistando Rangún el 2 de mayo (véase Operación Drácula).

En la primavera de 1944, se habían completado las preparaciones aliadas para la invasión de Francia. Se habían reunido unas 120 divisiones con unos dos millones de hombres, de los cuales 1,3 millones eran estadounidenses, 600 000 eran británicos y el resto unidades canadienses, franceses libres y polacos. La invasión se emplazó para el 5 de junio pero debido al mal tiempo se pospuso para el 6 de junio de 1944.[59]​ Entre el 85 y el 90 por ciento de todas las tropas alemanas estaba desplegado en el Frente Oriental, y solo unos 400 000 alemanes en dos ejércitos, el 7.º Ejército alemán y el recién creado 5.º Ejército Panzer eran todo lo que Alemania podía reservar para defenderse contra la invasión aliada. Los alemanes habían construido también una serie de fortificaciones elaboradas a lo largo de la costa, llamadas el Muro del Atlántico para detener la invasión, pero en muchos sitios el Muro estaba incompleto o destruido a causa de los bombardeos aliados, cuya superioridad en aviación era apabullante. Las fuerzas aliadas, bajo el mando supremo de Dwight D. Eisenhower, habían lanzado una elaborada campaña de engaños, para convencer a los alemanes que los desembarcos ocurrirían en el área de Calais, lo que causó que los alemanes desplegaran gran parte de sus fuerzas en ese sector. Solamente 50 000 alemanes estaban desplegados en el sector de Normandía el día de la invasión.

La invasión comenzó cuando se lanzaron 17 000 paracaidistas en Normandía para servir como una fuerza de distracción e impedir que los alemanes atacasen las playas. Al apuntar el día, una flota naval inmensa apoyada por aviones bombardeó las defensas alemanas en las playas, pero debido al mar que estaba muy agitado, muchos barcos fallaron su blanco. Se desembarcó en cinco puntos conocidos en clave como Utah, Omaha, Gold, Juno y Sword. Los estadounidenses en particular, sufrieron fuertes pérdidas en la playa de Omaha debido a que las fortificaciones alemanas estaban intactas. Sin embargo, al final del primer día, se habían cumplido muchos de los objetivos aliados, incluso habiendo sido muy optimista el objetivo británico de capturar Caen. Los alemanes no lanzaron ningún contraataque significativo sobre las playas, salvo una contraofensiva de los panzer que separó Juno y Sword, ya que Hitler creía que los desembarcos eran una distracción. Solamente tres días más tarde, el Alto Mando alemán se dio cuenta de que Normandía era el lugar de la verdadera invasión, pero para entonces, los Aliados habían consolidado sus cabezas de playa.

Relato de un testigo del desembarco en Omaha, Cornelius Ryan, famoso tras la guerra por su libro "Normandía": 

El terreno «bocage» de Normandía, donde los estadounidenses habían desembarcado, era ideal para la guerra defensiva. No obstante, los estadounidenses progresaron de forma constante y capturaron el puerto de aguas profundas de Cherburgo el 26 de junio, uno de los objetivos primarios de la invasión. Sin embargo, los alemanes habían minado el puerto y destruido muchas de las instalaciones antes de rendirlo, y haría falta otro mes antes de que el puerto pudiese ser habilitado para un uso limitado. Los británicos lanzaron otro ataque el 13 de junio para capturar Caen, pero fueron rechazados debido a que los alemanes habían reforzado la ciudad con un gran número de tropas en la ciudad para retenerla. La ciudad permanecería todavía en manos alemanas durante otras 6 semanas.

El 23 de julio, en la Operación Cobra, las fuerzas mecanizadas estadounidenses consiguieron forzar la salida por el lado oeste de la cabeza de playa de Normandía gracias a su superioridad numérica, al poder de fuego aliado y a tácticas mejoradas. Cuando Hitler supo de la salida estadounidense, ordenó a sus fuerzas en Normandía que lanzasen una contraofensiva inmediata. Sin embargo, las fuerzas alemanas que se movían en campo abierto, eran un objetivo fácil para la aviación aliada, ya que al principio habían escapado de los ataques aéreos aliados, debido solamente a sus posiciones defensivas bien camufladas.

Los estadounidenses colocaron fuertes formaciones en sus flancos para que neutralizaran los ataques, y empezaron entonces a rodear al 7.º Ejército alemán y a grandes partes del 5.º Ejército Panzer en la bolsa de Falaise. Fueron capturados unos 50 000 alemanes, pero 100 000 consiguieron escapar de la bolsa, aunque sin sus tanques ni armamento pesado. Todavía peor para los alemanes, fue que los británicos y canadienses que habían estado bloqueados en su sector, ahora hicieron una brecha en las líneas alemanas. Se había desvanecido cualquier esperanza que tuviesen los alemanes de contener el avance aliado en Francia, formando una nueva línea defensiva. Los aliados se precipitaron por toda Francia, avanzando 1 000 kilómetros en dos semanas.[60]​ Las fuerzas alemanas se retiraron hacia el Norte de Francia, Países Bajos y Bélgica. Las fuerzas aliadas estacionadas en Italia invadieron la Riviera francesa el 15 de agosto de 1944, y enlazaron con las fuerzas de Normandía. La resistencia francesa clandestina en París, se levantó contra los alemanes el 19 de agosto, y una división acorazada francesa bajo el mando del general Philippe Leclerc, presionando a la vanguardia desde Normandía, recibió la rendición de las fuerzas alemanas de la ciudad, y liberó a la ciudad el 25 de agosto.

Los alemanes lanzaron la bomba volante V-1, el primer misil de crucero del mundo, para atacar blancos en el sur de Inglaterra y en Bélgica. Más tarde, emplearían el cohete V2, un misil balístico guiado de combustible líquido. Ninguna de estas armas era muy precisa y podían solamente ser apuntadas hacia blancos grandes, como las ciudades. Tuvieron muy poco impacto militar, pero su intención era más bien la desmoralización de los civiles.

Los problemas logísticos eran una constante en el avance aliado hacia el este, ya que las líneas de suministro todavía venían desde las playas de Normandía. Los paracaidistas aliados y las fuerzas acorazadas intentaron un avance para ganar la guerra, a través de los Países Bajos y el Rin con la Operación Market Garden en septiembre, pero fueron rechazados. Una victoria decisiva lograda por el 1.er Ejército canadiense en la batalla del Escalda, aseguró la entrada al puerto de Amberes, con lo cual se pudo usar para recibir suministros a últimos de noviembre de 1944. Mientras tanto, los estadounidenses lanzaron un ataque a través del bosque de Hurtgen en septiembre; los alemanes, a pesar de tener menor número de hombres, fueron capaces de rechazar a los estadounidenses durante cinco meses, usando el difícil terreno y buenas posiciones defensivas. En octubre, los estadounidenses capturaron, Aquisgrán, la primera ciudad importante alemana en ser ocupada.

Hitler había estado planeando desde mediados de septiembre una contraofensiva importante contra los aliados. El objetivo del ataque sería la captura de Amberes. La captura o destrucción de Amberes no solo cortaría los suministros a los ejércitos aliados, también dividiría a las fuerzas aliadas en dos, desmoralizando la alianza y forzando a sus líderes a negociar. Para el ataque, Hitler concentró lo mejor de lo que le quedaba de sus fuerzas, en el Oeste. El 5.º Ejército Panzer, el reconstruido 7.º Ejército y el recién creado 6.º Ejército Panzer, en total, 240 000 hombres en 28 divisiones, 1 200 tanques y cañones de asalto. La ofensiva empezó el 16 de diciembre de 1944, con una barrera artillera disparada por 900 cañones alemanes. Una hora más tarde, los tres ejércitos alemanes golpearon la línea estadounidense del frente. Hitler lanzó su golpe hacia Amberes a través de las Ardenas, en el sur de Bélgica, una región llena de colinas y en algunos lugares llena de espesos bosques, y el lugar de su victoria en 1940.

El ataque del 6.º Ejército Panzer tuvo un progreso lento, pero una de sus puntas de lanza consiguió penetrar en las líneas estadounidenses y lanzarse con rapidez hacia el río Mosa. En el Sur, el 5.º Ejército Panzer penetró a través de la inexperta infantería estadounidense. El avance alemán fue retrasado en Saint-Vith, población que las fuerzas estadounidenses defendieron durante varios días. En el vital nudo de carreteras de Bastogne, los alemanes sitiaron la ciudad, defendida por la 101.ª División Aerotransportada, pero no consiguieron tomarla. Algunas unidades alemanas sobrepasaron Bastogne, pero el avance principal fue bloqueado. La ofensiva alemana tuvo un gran impacto en los comandantes aliados, ya que no creían que los alemanes aún tuvieran capacidad para organizar una ofensiva a gran escala. Muchas de las tropas alemanas que atacaban eran veteranos del Frente Oriental, y sabían como combatir en invierno. Un cielo denso y cubierto había impedido el uso de sus aviones de reconocimiento y de ataque a tierra a los estadounidenses. Sin embargo, los aliados estaban empezando a recuperarse de su impacto inicial y el 1.er Ejército y el 9.º Ejército se reagruparon para bloquear cualquier intento de avance de los alemanes hacia el Norte. El 3.º Ejército de Patton hizo un giro rápido de 90 grados y golpeó el flanco sur alemán. El 26 de diciembre, el 3.º Ejército había liberado Bastogne. El clima en estos momentos había mejorado, permitiendo liberar todo el poder aéreo aliado, hasta detener el ataque terrestre alemán en Dinant. En un intento para mantener el impulso de la ofensiva los alemanes lanzaron un ataque aéreo masivo contra los campos de aviación aliados en los Países Bajos el 1 de enero de 1945. Los alemanes destruyeron 465 aviones pero perdieron 277 de sus propios aviones. Mientras que los aliados recuperaron sus pérdidas en cuestión de días, la Luftwaffe no, por lo que ya no fue capaz de lanzar más ataques aéreos importantes.[61]​ Las fuerzas aliadas del norte y el sur se encontraron en Houffalize, y a finales de enero habían empujado a los alemanes a sus posiciones de partida. Se habían desperdiciado meses de la producción de guerra del Reich, en un momento en el que las fuerzas alemanas del Frente Oriental necesitaban esos recursos desesperadamente, ya que el Ejército Rojo se estaba preparando para su masiva ofensiva contra Alemania.

Con los Balcanes y la mayor parte de Hungría limpias de tropas alemanas a últimos de diciembre de 1944, los soviéticos comenzaron un redespliegue masivo de sus fuerzas hacia Polonia para su inminente ofensiva de invierno. Los preparativos soviéticas todavía estaban en marcha, cuando Churchill le pidió a Stalin que lanzase su ofensiva tan pronto como fuera posible para aliviar la presión alemana en el Oeste. Stalin accedió y la ofensiva fue dispuesta para el 12 de enero de 1945. Los ejércitos de Kónev atacaron a los alemanes en el sur de Polonia y se expandieron desde su cabeza de puente en el río Vístula cerca de Sandomierz. El 14 de enero, los ejércitos de Rokossovsky atacaron desde el río Narew al norte de Varsovia. Los ejércitos de Zhúkov, situados en el centro, atacaron desde sus cabezas de puente cerca de Varsovia. La ofensiva combinada soviética rompió las defensas que cubrían Prusia Oriental, dejando el frente Alemán en un completo caos.

Zhúkov tomó Varsovia el 17 de enero y, ya el 19 de enero, sus tanques habían llegado a Łódź. Ese mismo día, las fuerzas de Kónev alcanzaron la frontera alemana anterior a la guerra. Al final de la primera semana de la ofensiva, los soviéticos habían penetrado 160 kilómetros en profundidad, en un frente que tenía 650 kilómetros de ancho. La apisonadora soviética se paró finalmente en el río Óder al final de enero, a solo 60 kilómetros de Berlín.

Los soviéticos habían esperado capturar Berlín para mediados de febrero, pero resultó una previsión demasiado optimista. La resistencia alemana que casi se había colapsado en la fase inicial del ataque, se había endurecido tremendamente. Las líneas soviéticas de suministro estaban sobreextendidas y la disciplina entre las tropas soviéticas en el momento que fueron lanzadas sobre suelo alemán se colapsó. El deshielo de primavera, la falta de apoyo aéreo, y el miedo a ser rodeados a través de ataques de flanco desde Prusia Oriental, Pomerania y Silesia, condujo a un alto general de la ofensiva soviética. El recién creado Grupo de Ejércitos Vístula, bajo el mando de Heinrich Himmler, intentó un contraataque en el flanco expuesto del Ejército Rojo pero había fallado para el 24 de febrero. Esto hizo que Zhúkov tuviese claro que el flanco tenía que ser asegurado antes que pudiese montarse cualquier ataque sobre Berlín. Los soviéticos reorganizaron entonces sus fuerzas y golpearon hacia el norte, limpiando Pomerania, y después atacaron hacia el sur y limpiaron Silesia de tropas alemanas. En el sur, tres intentos alemanes de liberar la asediada guarnición de Budapest fallaron, y la ciudad cayó ante los soviéticos el 13 de febrero. Los alemanes contraatacaron otra vez; Hitler insistía en la tarea imposible de recuperar el río Danubio. El 16 de marzo, el ataque había fallado, y el Ejército Rojo contraatacó ese mismo día. El 30 de marzo, entraron en Austria y capturaron Viena el 13 de abril.

Hitler creía que el objetivo principal para la inminente ofensiva Soviética sería en el sur cerca de Praga, y no Berlín, y había enviado las últimas reservas alemanas a defender en ese sector. El principal objetivo del Ejército Rojo era realmente Berlín y para el 16 de abril estaba listo para comenzar su asalto final sobre Berlín. Las fuerzas de Zhúkov golpearon por el centro y cruzaron el río Óder pero quedaron detenidas debido a la desesperada resistencia alemana en las Alturas Seelow. Después de tres días de lucha muy dura y de 33 000 soldados soviéticos muertos,[63]​ se penetraron las últimas defensas de Berlín. Kónev cruzó el río Óder desde el sur y se encontró que podía atacar Berlín pero Stalin le ordenó que guardase los flancos de las fuerzas de Zhúkov y que no atacase Berlín. Las fuerzas de Rokossovski cruzaron el Óder por el norte y enlazaron con las fuerzas del Mariscal de Campo Bernard Montgomery en el norte de Alemania mientras que las fuerzas de Zhúkov y Kónev capturaban Berlín.

Para el 24 de abril, grupos del ejército soviéticos habían rodeado al 9.º Ejército Alemán y a parte del 4.º Ejército Panzer. Estas eran las principales fuerzas que supuestamente tenían que defender Berlín, pero Hitler había dado órdenes a estas fuerzas que aguantasen donde estaban y que no retrocediesen. Así que las principales fuerzas alemanas que supuestamente debían defender Berlín, estaban atrapadas al sureste de la ciudad. Berlín fue rodeada más o menos en este momento, y como esfuerzo de resistencia final, Hitler llamó a los civiles, incluyendo a los adolescentes y ancianos, a que luchasen en la milicia Volkssturm, contra el Ejército Rojo que se estaba aproximando. Estas fuerzas marginales fueron aumentadas con los vapuleados restos alemanes que habían luchado contra los soviéticos en las Alturas Seelow. Hitler le ordenó al cercado 9.º Ejército, que rompiese el cerco y que enlazase con el 12.º Ejército del general Walther Wenck y que liberase Berlín. Una tarea imposible, las unidades supervivientes del 9.º Ejército fueron conducidas hacia los bosques que rodeaban Berlín, cerca del pueblo de Halbe, donde estuvieron envueltos en una lucha particularmente dura, tratando de romper las líneas soviéticas y de alcanzar al 12.º Ejército. Una minoría consiguió unirse al 12.º Ejército y dirigirse peleando hacia el oeste, para rendirse a los estadounidenses. Mientras tanto, la durísima lucha urbana continuaba en Berlín. Los alemanes habían almacenado una gran cantidad de panzerfausts, y consiguieron destruir una gran cantidad de tanques soviéticos en las calles llenas de escombros de Berlín. Sin embargo, los soviéticos emplearon las lecciones que habían aprendido en la lucha urbana en Stalingrado, y fueron avanzando lentamente hacia el centro de la ciudad. Las fuerzas alemanas en la ciudad resistieron tenazmente, en particular la unidad SS Nordland, que estaba compuesta de voluntarios SS extranjeros, porque estaban muy motivados ideológicamente y creían que no vivirían si eran capturados. La lucha fue casa por casa y cuerpo a cuerpo. Los soviéticos tuvieron 360 000 bajas; los alemanes 450 000 bajas incluyendo civiles, y además 170 000 capturados. Hitler y su personal se trasladaron al búnker de la Cancillería, donde se suicidó el 30 de abril de 1945, junto a Eva Braun, con la que había contraído matrimonio unas horas antes.

Roosevelt, Churchill, y Stalin llegaron a acuerdos para la Europa de posguerra en la Conferencia de Yalta en febrero de 1945. Su encuentro llegó a muchas resoluciones importantes, tales como la formación de las Naciones Unidas, elecciones democráticas en Polonia, las fronteras de Polonia se movieron hacia el oeste a expensas de Alemania, los nacionales soviéticos serían repatriados, y se acordó que la Unión Soviética atacaría a Japón a los tres meses de la rendición de Alemania.

Los Aliados reasumieron su avance hacia el interior de Alemania a finales de enero. El obstáculo final para los Aliados era el Río Rin, que fue cruzado a finales de marzo de 1945, ayudados por la captura fortuita del Puente de Ludendorff en Remagen. Una vez que los Aliados hubieron cruzado el Rin, los británicos se dirigieron en abanico hacia el nordeste en dirección a Hamburgo, cruzando el Río Elba y moviéndose hacia Dinamarca y el Mar Báltico.

El 9.º Ejército de los Estados Unidos se dirigió al sur para formar la pinza norte del embolsamiento del Ruhr, mientras que el 1.er Ejército fue hacia el norte como la pinza sur del embolsamiento. Estos ejércitos estaban comandados por el general Omar Bradley, que tenía bajos su mando a 1 300 000 hombres. El 4 de abril, el cerco estaba completado, y el Grupo de Ejército Alemán B, que incluía al 5.º Ejército Panzer, al 7.º Ejército y al 15.º Ejército comandados por el Mariscal de Campo Walther Model, estaban atrapados en la Bolsa del Ruhr. Se cogió a unos 300 000 soldados alemanes como prisioneros de guerra. El 1.er y 9.º ejércitos de los Estados Unidos giraron entonces hacia el este. Pararon su avance en el río Elba, donde se encontraron con las tropas soviéticas a mediados de abril.

Los avances Aliados hacia el norte de la Península Italiana, en el invierno de 1944-45, habían sido lentos debido al terreno montañoso y al redespliegue de tropas en Francia. Pero para el 9 de abril, el 15.º Grupo de Ejército Britoestadounidense, penetró a través de la Línea Gótica y atacó el valle del Po, cercando gradualmente las principales fuerzas alemanas. Milán se conquistó a finales de abril. El 5.º Ejército de Estados Unidos continuó su avance hacia el oeste y enlazó con unidades francesas, mientras que los británicos entraron en Trieste, y se encontraron con los partisanos yugoslavos. Unos pocos días antes de la rendición de las tropas alemanas en Italia, partisanos italianos capturaron a Mussolini, que trataba de escapar a Suiza. Fue ejecutado, junto con su amante Clara Petacci. Se llevaron sus cuerpos a Milán, donde fueron colgados boca abajo, para escarnio público.

Después de la muerte de Hitler, Karl Dönitz se convirtió en el jefe del gobierno alemán pero su poderío se desintegraba rápidamente. Las fuerzas alemanas en Berlín entregaron la ciudad a las tropas soviéticas el 2 de mayo de 1945. Las fuerzas alemanas en Italia se rindieron el 2 de mayo de 1945, en el cuartel general del general Alexander, y las fuerzas alemanas en el Norte de Alemania, Dinamarca y los Países Bajos se rindieron el 4 de mayo. El Alto Mando Alemán bajo el generaloberst Alfred Jodl rindieron incondicionalmente todo el resto de fuerzas alemanas el 7 de mayo en Reims, Francia. Los Aliados occidentales celebraron el «Día de la Victoria en Europa» el 8 de mayo. La Unión Soviética celebró el «Día de la Victoria» el 9 de mayo. Algunos restos del Grupo de Ejército Centro Alemán continuaron resistiendo hasta el 11 de mayo o el 12 de mayo (véase batalla de Praga).[65]​

La última conferencia aliada de la Segunda Guerra Mundial se celebró en la ciudad de Potsdam, cercana a Berlín, desde el 17 de julio hasta el 2 de agosto. Durante la Conferencia de Potsdam se alcanzaron acuerdos entre los Aliados sobre la política a llevar en la Alemania ocupada. También se lanzó un ultimátum a Japón pidiendo su rendición incondicional.

El presidente de los Estados Unidos Harry Truman decidió usar la nueva arma atómica para acelerar el final de la guerra. La batalla de Okinawa había mostrado que una invasión en las islas de origen japonesas (planeada para noviembre) significaría un gran número de bajas estadounidenses. La estimación oficial que fue dada por la Secretaría de Guerra era de 1,4 millones de bajas aliadas, aunque algunos historiadores discuten si esto habría sido el caso o no. La invasión habría significado la muerte de millones de soldados japoneses y civiles, que estaban siendo entrenados como milicia.

El 6 de agosto de 1945, un B-29 Superfortress, el Enola Gay, lanzó una bomba atómica apodada Little Boy sobre Hiroshima, destruyendo la ciudad. El 9 de agosto, un B-29 llamado Bockscar lanzó la segunda bomba atómica, apodada Fat Man, sobre la ciudad portuaria de Nagasaki.[67]​

El 8 de agosto, dos días después que se hubiese lanzado la bomba atómica sobre Hiroshima, la Unión Soviética, habiendo abolido su pacto de no agresión con Japón en abril, atacó a los japoneses en Manchukuo y Mengjiang, cumpliendo su promesa hecha en Yalta de atacar a los Japoneses tres meses después de que hubiese acabado la guerra en Europa. En menos de dos semanas, el ejército japonés en Manchuria, que consistía en aproximadamente un millón de hombres, había sido destruido por los soviéticos.[68]​[69]​ El Ejército Rojo, ayudado por tropas de la República Popular de Mongolia, se movió hacia Corea del Norte el 18 de agosto. Corea fue seguidamente dividida en el paralelo 38 en las zonas soviética y estadounidense.[70]​[71]​
[72]​

El uso estadounidenses de las armas atómicas contra Japón y la invasión soviética del Manchukuo, hicieron que Hirohito se apresurase a puentear al gobierno existente e interviniese para finalizar la guerra. En su alocución radiofónica a la nación, el Emperador no mencionó la entrada de la Unión Soviética en la guerra, pero en su «reescritura a los soldados y marineros» del 17 de agosto, ordenándoles el alto el fuego y entregar las armas, acentuó la relación entre la entrada de los soviéticos en la guerra y su decisión de rendirse, omitiendo cualquier mención a las bombas atómicas.

Los japoneses se rindieron el 14 de agosto de 1945, o el Día de la Victoria sobre Japón, firmando el Acta de Rendición de Japón el 2 de septiembre. Las tropas japonesas en China se rindieron formalmente el 9 de septiembre de 1945.

El uso generalizado de carros de combate es una primera ilustración de la tendencia a la motorización. Mientras que el Ejército francés escogió la dispersión de los blindados, al servicio de la infantería, los alemanes adoptaron una táctica basada en la agrupación de blindados y salieron victoriosos de la batalla de Francia. La concepción del carro de combate en sí mismo tiende a dos conceptos diferentes: la potencia y la maniobrabilidad.

El progreso de los carros de combate va acompañado del progreso del armamento antitanque: el uso de la carga hueca permite atravesar los blindajes aunque estos sean muy espesos. Tubos lanzacohetes como la bazuca permiten al soldado poseer contra los tanques la potencia de un artillero.[76]​

Paralelamente a la utilización de tanques se asiste a un aumento intensivo de la utilización de transportes motorizados de tropas, dejando de lado a los caballos, todavía muy presentes tanto del lado alemán como del lado francés durante la batalla de Francia o en el frente Este, principalmente por razones de logística. La división blindada estadounidense fue, por el contrario, totalmente motorizada.

Los inmensos progresos de la aviación realizados entre las dos guerras van a dar a los aviones de guerra un lugar preponderante. El mejoramiento de las estructuras del avión permiten a los cazabombarderos como el Stuka realizar bombardeos en picada y de este modo permitir los bombardeos a objetivos terrestres. Los bombarderos pesados como el Boeing B-17 Flying Fortress estadounidense, que tenían un radio de acción que alcanzó 5 000 kilómetros hacia el final de la guerra, fueron utilizados en campañas masivas de bombardeos de más de mil aviones, poniendo en práctica el concepto de bombardeo estratégico. Para contrarrestar a los bombarderos, los beligerantes hicieron uso de sus caza y de cañones de defensa contra aviones(DCA). La eficacia de los (DCA) obligó a organizar las operaciones de bombardeo nocturnas. A los aviones de caza se les encomendó asegurar el espacio aéreo sobre el campo de batalla o sobre un frente dado.[77]​

Por mar, después de la Primera Guerra Mundial, se privilegió la construcción de navíos de línea. Los cruceros de batalla, más rápidos que los acorazados estaban menos protegidos. Los primeros acorazados rápidos no aparecieron hasta el final de los años 1930. Pero esos navíos constituían un objetivo ideal para la aviación embarcada en los portaaviones, sobre todo los bombardeos en picada y los aviones de torpedo. A pesar de una fuerte defensa aérea, disponiendo a veces de un tiro radar, el acorazado era todavía vulnerable. Los portaaviones, que podían disponer de 50-60 aparatos, tuvieron un papel cada vez más determinante. Los portaaviones se convirtieron en una pieza central, que los estadounidenses llamaron "Task force", y los otros navíos fueron comúnmente utilizados de escoltas.[78]​

Al final de la Segunda Guerra Mundial, nuevas armas hicieron aparición en el campo de batalla, como el avión sin piloto V1 lanzado por primera vez por los alemanes sobre Inglaterra en la noche del 13 al 14 de junio de 1944, o el V2 lanzado por primera vez sobre Londres el 8 de septiembre de 1944.[78]​ Contrariamente a los temores de los aliados, los alemanes no tenían un proyecto de bomba atómica.[79]​ Los estadounidenses, por el contrario, dispusieron a partir de diciembre de 1941 gigantescos recursos en el proyecto Manhattan, que concluyó el 16 de julio 1945, después de la rendición de la Alemania Nazi, con la primera explosión nuclear en el desierto de Nuevo México y a los bombardeos atómicos de Hiroshima y Nagasaki el 6 y 9 de agosto de 1945.

A pesar de tratarse de un país neutral, en los primeros años de la guerra, un grupo de aviadores argentinos se alistaron como voluntarios en la Royal Air Force británica, dando lugar al 164.º Escuadrón de la RAF de voluntarios argentinos, el cual combatió en el norte de Francia y Bélgica. Se presentaron 776 argentinos como voluntarios en las fuerzas aéreas de Gran Bretaña, Canadá, Sudáfrica[83]​ En total, se estima que de 4000[84]​ a 5000[85]​ argentinos combatieron durante la Segunda Guerra Mundial como voluntarios de los aliados.

En diciembre de 1941 tras el ataque a Pearl Harbor, Cuba fue el único país independiente antillano que le declaró la guerra al Eje. En el país fueron arrestados varios agentes alemanes y se convirtió en el principal proveedor de azúcar a los aliados. Alemania hundió cinco buques mercantes cubanos, con un saldo de 82 muertos. Por su parte los cazasubmarinos cubanos hundieron al submarino alemán U-176. El siguiente país de América latina en declararle la guerra a las potencias del ejes es Honduras, curiosamente ese mismo día uno de sus barcos fue capturado en Shanghái por la armada imperial Japonesa y rebautizado como el Ekkai Maru.[86]​ Por consecuencias de estos, Honduras rompe relaciones con Japón y expulsa al cónsul de Alemania de su país.[87]​ Sus primeras acciones iniciaron en 1942 con el patrullaje aéreo, y contribuyo a la guerra enviando materias primas. 

También como consecuencia del ataque a Pearl Harbor, Venezuela rompe relaciones con las potencias del Eje en diciembre de 1941. A raíz de ello, el 16 de febrero de 1942, los tanqueros venezolanos Monagas y Tía Juana son torpedeados y hundidos por submarinos del Tercer Reich en aguas del golfo de Venezuela, tras lo cual, el gobierno del presidente Isaías Medina Angarita, aunque sin declarar la guerra, pasa a cooperar con el esfuerzo aliado de manera más estrecha, autorizando incluso el uso temporal de bases militares venezolanas por el Ejército y la Armada de los Estados Unidos, así como garantizando el suministro de combustible a dichas fuerzas.

En mayo de ese mismo año, Alemania hundió dos navíos petroleros mexicanos (el Potrero del Llano y el Faja de Oro); con este hecho se da inicio a la única participación de México en la Guerra Mundial. Ante la descortesía del Eje de no contestar a la nota de protesta enviada por la cancillería mexicana, el Congreso mexicano le declaró la guerra el 22 de mayo de 1942, siendo el tercer y último país norteamericano en entrar en la guerra. Desde fines de junio a principios de septiembre los submarinos alemanes hundirían 4 barcos más: Túxpam, Oaxaca, Las Choapas y Amatlán. De esta forma, la aviación mexicana conformada por el escuadrón 201 participó en la guerra del Pacífico.

Si bien varios países sudamericanos le declararon la guerra a las potencias del Eje, solo Brasil envió una fuerza expedicionaria (FEB) a combatir. Entre julio y agosto de 1942, submarinos alemanes hundieron 18 barcos brasileños y hasta el final de la guerra se llegó a 36 buques hundidos y alrededor de 1 100 muertos. Aunque el Gobierno de Brasil se mostraba reacio a entrar en el conflicto, la indignación pública empujó a Brasil a declarar la guerra a Alemania en noviembre de 1942, y a enviar una División completa de casi 30 000 hombres al frente de Italia onde participaron la rotura de la Línea Gótica y la ofensiva aliada final en aquel frente.

El país también participado con el suministro de bases en su Noreste con la Marina de Brasil proporcionando escolta para los convoyes que se dirigen al sur del continente americano y al norte de África, en total escoltaron los convoyes de 3.164 barcos y, junto a la Fuerza Aérea Brasileña, la vigilancia y  guerra submarina hundiendo algunos submarinos alemanes y el italiano Arquimede.[88]​La Fuerza Aérea Brasileña contribuido con un escuadrón de combate y otro de observación en Itália, completando misiones de ataque a tierra, escolta y observación.

Colombia declaró la guerra en 1943, porque un submarino alemán hundió uno de sus barcos, la goleta Resolute, que unos días antes había transportado soldados británicos a la isla de San Andrés. A raíz de esto, el Gobierno colombiano decidió hacer patrullajes para evitar más hundimientos. El 29 de marzo de 1944 el ARC Cabimas transportaba gasóleo en la ruta Cartagena-Panamá escoltado por el ARC Caldas, que detectó la presencia del submarino alemán U-154, hundiéndolo en el acto.[89]​[90]​

El resto de los países sudamericanos como Perú, Ecuador, Uruguay, Paraguay, Venezuela, Chile y Argentina, solo rompieron relaciones diplomáticas con los países del Eje entre 1942 y 1944. La mayor parte de los cuales declararon, finalmente, la guerra al Eje recién en febrero de 1945. Salvo Argentina, que le declaró la guerra a Alemania y a Japón el 27 de marzo de 1945, y Chile, que hizo lo propio con Japón el 12 de abril de ese año, siendo el último país en emitir una declaración de guerra.

Los países centroamericanos lo hicieron bien al lado de México, o bien al lado de Brasil; excepto Costa Rica, que declaró la guerra a Japón el 8 de diciembre de 1941, al mismo tiempo que los Estados Unidos.

El 23 de octubre de 1940 se celebró la llamada «entrevista de Hendaya», en la que Francisco Franco se reunió con Adolf Hitler en presencia de sus ministros de Asuntos Exteriores, Ramón Serrano Suñer y Joachim von Ribbentrop, para tratar la posible entrada de España en la guerra en el bando alemán. Tras ella, Franco cambió la declaración de «neutralidad» por la de «no beligerancia», para mostrar de esta forma el apoyo de España al Eje Roma-Berlín. En junio de 1941 se autorizó el reclutamiento de voluntarios para luchar contra el comunismo, dando origen a la División Azul, la cual combatió en el Ejército alemán durante la invasión de la Unión Soviética.

Franco, que había recibido el apoyo británico y estadounidense, lo seguía compensando con las explotaciones mineras británicas, como Riotinto, a la vez que permitía el paso de refugiados judíos o militares (principalmente pilotos) hacia Portugal. La intención era quedar bien con cualquiera que ganara la guerra. Esta posición se apreció especialmente desde que Franco pretendió suavizar la posición de su régimen con las destituciones del ministro germanófilo Ramón Serrano Suñer en 1942, y la repatriación de los voluntarios de la División Azul en 1943, después de la Conferencia de Casablanca.

Respecto a la guerra, Franco dijo:[cita requerida] 

Luchando contra el comunismo o en contra del fascismo, había españoles en casi todos los ejércitos:

La guerra y la dominación del continente europeo permitieron al régimen nazi de llevar al extremo su ideología racista. Según las palabras de Goebbels: «La guerra nos ofrece toda clase de posibilidades que la paz nos rechazaba».[93]​

Dentro de esas posibilidades mencionadas aparece un plan de destrucción étnica teniendo como objetivo los pueblos de la Europa del este. El mismo día de la entrada en guerra, septiembre de 1939, Hitler autoriza la exterminación de discapacitados mentales y otras personas en situación de enfermedad, la Aktion T4 conduce a la muerte por gas de más de 150 000 discapacitados.

A partir de 1939, los judíos son concentrados a la fuerza en guetos miserables, deliberadamente superpoblados y gestionados con falta de comida. Durante su exterminación sistemática, que se designa con el nombre de Shoah, es antes de todo puesta en marcha por la Wehrmacht y por los Einsatzgruppen en territorios polacos y soviéticos. En la URSS y en una parte de Polonia, la «Shoah por balas» da paso en 1942 al empleo metódico de "camiones de gas".

Además de los horrores propios de toda guerra, la Segunda Guerra Mundial introdujo formas de sufrimiento no achacables a la propia escala de la misma:

La Segunda Guerra Mundial contribuyó a que emergieran dos superpotencias que buscaban repartirse el mundo: Estados Unidos y la URSS. La Sociedad de Naciones, a la que se responsabilizó de contribuir a desatar la guerra, fue reemplazada por la ONU. La carta de las Naciones Unidas se firmó en San Francisco el 26 de junio de 1945.
En los Juicios de Núremberg y Tokio, parte de la jerarquía nazi y del Tenno nipón fue juzgada y condenada por crímenes contra la humanidad. La investigación científica y técnica, en su conjunto, se benefició de un fuerte impulso en particular: el dominio del átomo tras el Proyecto Manhattan. También contribuyó a la creación del helicóptero, los aviones de reacción y la creación del ICBM.

Los soviéticos, que se aliaron con EE. UU. y los aliados solo por conveniencia contra el enemigo común, Alemania, se convirtieron en enemigos por sus ideales contrarios, y así comenzó una era de guerra fría a nivel mundial, concentrándose en Europa.

En Alemania tras la firma del armisticio por parte del Eje, el Plan Marshall contribuyó a la reconstrucción de Alemania. Si bien los alemanes perdieron la guerra, sus adelantos en tecnología punta en cadenas de industrias, fabricación de componentes para cohetes, misiles y diversos tipos de armas ayudaron a los Aliados del Oeste y sirvieron para el llamado «milagro alemán».

Sin embargo se presentó la expulsión de alemanes en Europa central (Prusia, Checoslovaquia, Polonia y países bálticos) donde había asentamientos alemanes desde varios siglos atrás. Los alemanes de los Sudetes, que pedían su incorporación a Alemania, habían desencadenado el desmantelamiento de Checoslovaquia, acordado en los Acuerdos de Múnich de 1938.

Tras la toma de esos territorios por el ejército soviético, numerosos alemanes fueron expulsados o dejaron su tierra para ir a Alemania o Austria, en condiciones generalmente dramáticas.

Los Estados Unidos tomaron la iniciativa de una actitud «positiva». Impusieron la democracia (particularmente al Japón), a través de una depuración y de un control del Estado y la educación.

Las pérdidas de vidas humanas para Estados Unidos fueron, en comparación con el resto de los Aliados, muy inferiores en número porque en su territorio no se desarrolló la guerra y las pérdidas solo fueron militares.

En este contexto, la actitud francesa, país liberado tras la batalla de Normandía, según la historiografía francesa, estuvo marcada por la afirmación original de una voluntad de independencia, sobre todo debido a la personalidad de Charles de Gaulle, quien hizo jugar a Francia un papel en la ocupación de Alemania al lado de los vencedores y, por otra parte, desarrolló la investigación nuclear para afirmar su independencia de Estados Unidos. La liberación se acompaña de una depuración de personas sospechosas de ser colaboradores (gran parte de ellos ejecutados sin juicio previo) y la destrucción de ciudades como El Havre. Se forma un gobierno de unión, entre comunistas y gaullistas de una parte y representantes de la resistencia y radicales, de centro-izquierda.

Los otros aliados, si se exceptúa el Reino Unido, jugaron un rol menor o fueron descartados de las negociaciones referentes a la puesta en práctica de las dos zonas de influencia que siguieron a los acuerdos de Yalta y de Potsdam. Esta situación, que porta en sí misma los gérmenes de la Guerra Fría, llegaría a durar hasta 1991.

El Reino Unido salió considerablemente debilitado de la guerra que consagró el fin de su poderío colonial. Por consiguiente, las Islas Británicas conocieron una crisis sin precedentes que requirió la reconstrucción y reestructuración de su economía.

Se estima que alrededor de seis millones de judíos, junto con otros grupos étnicos, fueron asesinados por los nazis, principalmente mediante la deportación a campos de concentración, algunos tan conocidos como Auschwitz, Treblinka y Majdanek. La expresión hebrea Shoah (catástrofe) —también conocida como «Holocausto»— designa la exterminación en masa de los judíos perpetrada durante esta sangrienta guerra.

Al final del conflicto la Organización de las Naciones Unidas (ONU) reemplazó a la Sociedad de Naciones (SDN), fundada en 1919, y se otorgó a sí misma la misión de resolver los conflictos, en general bélicos, de carácter internacional.

La Unión Soviética se anexionó Estonia, Letonia, Lituania, el este de Polonia y partes de Finlandia y Rumanía. Polonia recibió territorios de Alemania (Pomerania, Silesia y la mitad de Prusia Oriental). Austria recuperó su independencia en 1955.

Alemania quedó dividida en cuatro zonas de influencia: Estados Unidos, Francia y Reino Unido unificaron sus respectivas zonas en la República Federal Alemana y la URSS hizo lo mismo con su zona que se convirtió en la República Democrática Alemana, hasta 1990, cuando los Länder que la conformaban se incorporaron a la República Federal de Alemania, dando lugar a la reunificación alemana y a la creación de la actual Alemania.

La guerra dejó al descubierto la debilidad de los países europeos y los movimientos de independencia de las colonias se generalizaron con el apoyo de las dos superpotencias. Los ejércitos de las potencias coloniales no tenían ya capacidad para controlar dichos movimientos, por lo que a lo largo de la segunda mitad del siglo XX se produjo la llamada descolonización.

El mundo quedó dividido en dos bloques:

Error en la cita: Existen etiquetas <ref> para un grupo llamado «Nota», pero no se encontró la etiqueta <references group="Nota"/> correspondiente.

Julio Garavito Armero (Bogotá, 5 de enero de 1865-Bogotá, 11 de marzo de 1920) fue un astrónomo, matemático, economista, poeta e ingeniero colombiano.[1]​ Sus investigaciones contribuyeron al desarrollo de las ciencias en Colombia durante el siglo XIX. Ha sido situado al mismo nivel de otros dos importantes científicos neogranadinos del siglo XIX, José Celestino Mutis y Francisco José de Caldas. 

En su honor, uno de los cráteres lunares del lado opuesto al visible desde la Tierra, fue bautizado con su nombre en el año 1970.[2]​

Hijo de Hermógenes Garavito, un comerciante de la ciudad, y de Dolores Armero, familia de origen santafereño.

Criticó a los políticos de su época, a pesar de haber sido concejal de Bogotá y diputado de la Asamblea de Cundinamarca.[3]​

En pedagogía mostró ideas novedosas, proponiendo métodos que él consideró lógicos y naturales para una enseñanza objetiva, fruto de su estudio de la psicología infantil.[cita requerida]

Manuel Antonio Rueda y Luis María Lleras, fueron algunos de sus profesores; este último lo calificó desde joven como promesa para las matemáticas.

Se graduó como bachiller en Filosofía y Letras, en 1884. En 1885, a la edad de 20 años, interrumpió sus estudios a causa de las numerosas guerras civiles que azotaron el país. Obtuvo sus estudios de matemático y de Ingeniero Civil en la Universidad Nacional de Colombia.[4]​ Garavito es, hasta donde se conoce información documentada, el primero en graduarse como profesor de matemáticas.[5]​

Presentó una segunda tesis que consistía en calcular en un manómetro todas las posibilidades matemáticas que tiene este instrumento. Finalmente, para optar al título de Ingeniero Civil elaboró un tipo de estructura triangular para construir puentes.[3]​

En 1902 propuso al gobierno un plan para que el Observatorio realizara la carta de Colombia, con métodos astronómicos, partiendo de la latitud de Santafé de Bogotá. El proyecto fue aprobado y se creó la Oficina de Longitudes, bajo la dirección de Garavito. Esta entidad se encargó de delimitar las fronteras del país y de publicar mapas generales y regionales de Colombia. Por lo que intervino decisivamente en la elaboración del mapa geográfico de Colombia, recurriendo a ingeniosos procedimientos que reemplazaban la ausencia de vértices geodésicos mediante el uso de vértices astronómicos, referidos y fijados por coordenadas a las ciudades y poblaciones más importantes del país y recurriendo a cambios de señales entre el Observatorio Astronómico y las estaciones telegráficas de cada una de tales poblaciones.[6]​

Como astrónomo del Observatorio, del que fue director durante 27 años,[7]​ realizó numerosos descubrimientos útiles, tales como la ubicación latitudinal de Bogotá, los estudios de los cometas que pasaron por la Tierra entre 1901 y 1910, este último, el Halley.

Su aporte más importante fue el estudio de la Mecánica celeste, que finalmente se convertiría en el estudio de las fluctuaciones lunares y su influencia en los comportamientos temporales, climáticos, hídricos y de los hielos polares, así como el análisis de la aceleración orbital terrestre, asunto que sería corroborado después.

También trabajó en áreas como la física óptica, labor que quedó inconclusa a su muerte; y la economía, gracias a lo cual ayudó a recuperar a su país de la guerra civil que pasó por su época, dando al papel moneda valor efectivo y no convencional. Para ello, realizó conferencias y congresos de economía, además de estudiar los ciclos de la riqueza y las influencias humanas que afectan la economía, tales como la guerra o la sobre población.

Posteriormente, fue jefe de la Comisión Corográfica, creada con el fin de promover el desarrollo de los ferrocarriles colombianos y la delimitación de la frontera con Venezuela.

Como docente, Garavito fue profesor de cálculo, mecánica racional y astronomía, cátedras que conservó hasta su muerte.

Se opuso a la Teoría de la Relatividad, probablemente por opiniones vagas (pues no la estudió en profundidad), opuestas y contradictorias acerca de esa teoría y su influencia en la ciencia clásica; además de la condena de ciertos sectores del clero colombiano, que en su condición de creyente acató. Fue representante de la ciencia colombiana de finales del siglo XIX y comienzos del XX: estaba, por un lado, parcialmente aislado de sus colegas en otros países -jamás asistió a un congreso internacional- y por otro, se encontró en un ambiente cultural interno completamente apático e indiferente. Poseía una admiración por la mecánica newtoniana y llegó a creer de buen modo que la mecánica celeste ya había dado su última palabra en desarrollo. Igual opinaba de la ciencia astronómica (1920). 

La Asamblea General de la Unión Astronómica Internacional (UAI), reunida en Moscú (Rusia) en 1958, inició el proceso de nomenclatura de los accidentes y detalles que ya se iban conociendo del lado opuesto de la Luna, el que nunca se ve desde la superficie de la Tierra.

Ya para el inicio de la década del 70, los innumerables detalles de esa otra cara de la Luna exigieron una consulta a nivel mundial para proponer nombres, de manera que el Observatorio Astronómico Nacional envió una lista, de la cual fue escogido el nombre de Garavito, cuyo trabajo sobre la Luna había sido citado por Brouwer y Clemente en Methods of Celestial Mechanics, en 1961.

El nombre de Garavito fue aceptado durante la reunión de la UIA celebrada en Brighton (Inglaterra), en 1970, y le correspondió un cráter del lado de la Luna oculto a la Tierra, situado en las coordenadas selenográficas de latitud 48° al sur y 157° de longitud oriental. Para ese entonces era el único latinoamericano con ese honor.[8]​

Julio Garavito falleció a los 55 años a consecuencia de una dolencia que había adquirido tras un trabajo en una mina de carbón. Su esposa, María Luisa Cadena, había muerto 4 años atrás.[9]​[10]​

Garavito alcanzó numerosas distinciones nacionales e internacionales como la de ser miembro de número y primer presidente de la Sociedad Geográfica de Colombia - Academia de Ciencias Geográficas, miembro supernumerario de la Sociedad Colombiana de Ingenieros,  de la Sociedad Geográfica de Lima, de la Sociedad Astronómica de Francia y de la Sociedad Belga de Astronomía. También fue candidato a formar parte de la Academia de Historia Hispanoamericana de Ciencias y Artes.

En 1919, poco antes del fallecimiento de Julio Garavito, el gobierno colombiano expidió un decreto por el que se ordenó honrar su memoria y actividad como científico colombiano, publicando con cargo al Estado todos sus trabajos científicos —muchos de los cuales se encontraban inéditos—, adquiriendo la primera edición de cada uno de ellos, y adoptándolos como textos de enseñanza en las universidades del país. Se ordenó también la erección de un busto en bronce.
Tan buenas intenciones sólo se cumplieron parcialmente, pues gran parte de las obras se acabaron publicando por el esfuerzo y empeño de Jorge Álvarez Lleras y de la Academia Colombiana de Ciencias Exactas, Físicas y Naturales durante las décadas de 1930 y 1940, más que por un efectivo interés por parte del Estado colombiano. El Congreso nacional también reconoció a Garavito como uno de los símbolos de la ingeniería colombiana, y dio el nombre de este científico a la orden que creó con el fin de honrar a los ingenieros colombianos.  

Se mencionan a continuación algunos de los reconocimientos establecidos en memoria de Julio Garavito:[12]​ 





El Reino Unido (United Kingdom en inglés),[nota 1]​ o de forma abreviada RU (UK en inglés), oficialmente Reino Unido de Gran Bretaña e Irlanda del Norte (United Kingdom of Great Britain and Northern Ireland en inglés),[nota 2]​ es un país soberano e insular ubicado al noroeste de la Europa continental. Su territorio está formado geográficamente por la isla de Gran Bretaña, el noreste de la isla de Irlanda y pequeñas islas adyacentes. Desde la independencia de la República de Irlanda, Irlanda del Norte ha sido la única parte del país con una frontera terrestre, hasta la inauguración del Eurotúnel que une por tierra a la isla de Gran Bretaña con Francia y las tierras continentales europeas. Gran Bretaña limita al norte y al oeste con el océano Atlántico, al este con el mar del Norte, al sur con el canal de la Mancha y al oeste con el mar de Irlanda.

El Reino Unido es un Estado unitario comprendido por cuatro naciones constitutivas: Escocia, Gales, Inglaterra e Irlanda del Norte.[8]​ Es gobernado mediante un sistema parlamentario con sede de gobierno y capitalidad en Londres, pero con tres administraciones nacionales descentralizadas en Edimburgo, Cardiff y Belfast, las capitales de Escocia, Gales e Irlanda del Norte, respectivamente. Es una monarquía parlamentaria, siendo Isabel II la jefa de Estado. Coloquial y erróneamente se denomina Gran Bretaña e Inglaterra, consecuencia del mayor peso de ambos (territorio y reino, respectivamente) dentro del Estado. Las dependencias de la Corona de las islas del Canal —Jersey y Guernsey— y la Isla de Man no forman parte del Reino Unido, si bien el Gobierno británico es responsable de su defensa y las relaciones internacionales.[9]​

El Reino Unido tiene catorce territorios de ultramar, todos ellos vestigios de lo que fue el Imperio británico, que en su territorio internacional llegó a alcanzar y a abarcar cerca de una quinta parte de la superficie terrestre mundial. Isabel II continúa estando a la cabeza de la Mancomunidad de Naciones y siendo jefa de Estado de cada uno de los Reinos de la Mancomunidad.

Es un país desarrollado que por su volumen neto de producto interno bruto es la sexta economía mundial (por su PIB nominal) y novena por su PIB PPA. Fue el primer país industrializado del mundo[10]​ y la principal potencia mundial durante el siglo XIX y el comienzo del siglo XX[11]​ (1815-1945), pero el costo económico de las dos guerras mundiales y el declive de su imperio en la segunda parte del siglo XX disminuyeron su papel en las relaciones internacionales. Sin embargo, aún mantiene una significativa influencia económica, cultural, militar y política, y es una potencia nuclear. Fue miembro de la Unión Europea entre 1973 y 2020, de la que se salió en el proceso conocido como Brexit.[nota 3]​ Es uno de los cinco miembros permanentes del Consejo de Seguridad de las Naciones Unidas con derecho a veto, miembro del G7, el G-20, la OTAN, la OCDE, la UKUSA, la Mancomunidad de Naciones y la Common Travel Area.

El nombre oficial del país es Reino Unido de Gran Bretaña e Irlanda del Norte (en inglés, United Kingdom of Great Britain and Northern Ireland), siendo Reino Unido o UK las formas abreviadas más utilizadas. El nombre fue propuesto por primera vez en el Acta de Unión de 1707, en la que los reinos de Inglaterra y Gales decidieron constituir un nuevo reino junto con Escocia, que tendría el nombre de Reino Unido de Gran Bretaña (United Kingdom of Great Britain).[13]​[14]​ Más tarde, con el Acta de Unión de 1800 la isla de Irlanda pasó a formar parte del país, por lo que el nombre cambió a Reino Unido de Gran Bretaña e Irlanda (United Kingdom of Great Britain and Ireland). En 1927, el país obtuvo su nombre actual Reino Unido de Gran Bretaña e Irlanda del Norte. (La Irlanda del Sur se convirtió en el Estado Libre Irlandés cuando obtuvo oficialmente el autogobierno independiente en 1922 y la independencia completa con el Estatuto de Westminster de 1931.)

Es denominado frecuentemente por el nombre de la isla que comprende la mayor parte de su territorio, Gran Bretaña, o también, por extensión, por el nombre de uno de sus países constituyentes, Inglaterra. El gentilicio del Reino Unido, así como el de la isla de Gran Bretaña es británico, aunque también, por extensión, se suele usar en el habla corriente el gentilicio inglés.[15]​[16]​

Aunque el Reino Unido, como Estado soberano, es un país, Inglaterra, Escocia, Gales, y en menor medida, Irlanda del Norte, también se consideran como "los países", a pesar de que no son Estados soberanos.[17]​ La página web del primer ministro británico ha utilizado la expresión "países dentro de un país" para describir al Reino Unido.[18]​

Algunos resúmenes estadísticos también se refieren a los países de Inglaterra, Escocia y Gales como "regiones", mientras que a Irlanda del Norte se le conoce como "provincia".

Los primeros asentamientos por seres humanos anatómicamente modernos en el actual territorio del Reino Unido se produjo en oleadas hace aproximadamente 30 000 años.[19]​ Se cree que, hacia fines del período prehistórico de la región, la población pertenecía a la cultura de los celta insulares, que comprende a los britanos y a la Irlanda gaélica.[20]​ La conquista romana, iniciada en el año 43 sometió al sur de la isla a ser una provincia del imperio por cuatro siglos. A esto, le siguió una serie de invasiones encabezadas por distintos pueblos germánicos —anglos, sajones y jutos—, que redujo el área británica hacia lo que iba erigirse como el actual territorio de Gales, Cornualles y el histórico Reino de Strathclyde.[21]​ La mayor parte de la región colonizada por los anglosajones se unificó en el Reino de Inglaterra en el siglo X.[22]​ Al mismo tiempo, los gaélico-hablantes en el noroeste de Bretaña —con conexiones hacia el nordeste de Irlanda y tradicionalmente se supone que han migrado desde allí en el siglo V—[23]​[24]​ se unieron con los pictos para crear el denominado Reino de Escocia en el siglo IX.[25]​

En 1066, los normandos invadieron Inglaterra desde Francia y después de su conquista, tomaron el poder de grandes partes de Gales, Irlanda y fueron invitados a establecerse en Escocia, introduciendo al feudalismo de cada país el modelo norteño-francés y la cultura normanda.[26]​ La élite normanda influenció en gran medida, pero fue asimilada con cada una de las culturas locales.[27]​ Por consiguiente, los reyes medievales ingleses conquistaron Gales y realizaron un intento fallido para anexar Escocia a su territorio. Tras la Declaración de Arbroath, Escocia mantuvo su estatus soberano, a pesar de las constantes tensiones con Inglaterra. Los monarcas ingleses, debido a la herencia que poseían sobre vastos territorios en Francia y por las reclamaciones a la corona francesa, mantuvieron varios conflictos en Francia, siendo el más notable de ellos la Guerra de los Cien Años. En ella, Escocia se alió con Francia y finalizó en 1453, con la retirada inglesa de tierras francesas.[28]​

La Edad Moderna estuvo marcada por conflictos religiosos en torno a la reforma protestante, donde se produjo a partir de allí la introducción de las iglesias protestantes estatales en cada país.[29]​ Gales fue incorporado totalmente al Reino de Inglaterra,[30]​ e Irlanda fue constituido como reino en unión personal con la corona inglesa.[31]​ Dentro del actual territorio norirlandés, las tierras de la nobleza católica gaélica independiente fueron confiscadas y dadas a los colonos protestantes de Inglaterra y Escocia.[32]​

En 1603, Jacobo VI de Escocia heredó la corona de Inglaterra e Irlanda, lo cual unió a los tres reinos y se trasladó su corte desde Edimburgo a Londres; no obstante, cada país seguía siendo una entidad política independiente, al mismo tiempo que conservaban sus instituciones políticas, legales y religiosas separadas.[33]​[34]​

A mediados del siglo XVII, los tres reinos estuvieron involucrados en una serie de guerras —incluyendo la Guerra Civil Inglesa— que desencadenaron en el derrocamiento temporal de la monarquía y el establecimiento de una república unitaria de la Mancomunidad de Inglaterra, Escocia e Irlanda.[35]​[36]​ Durante los siglos XVII y XVIII, se reportaron actos de piratería (corsario) de la flota británica, atacando y robando buques de las costas europeas y caribeñas.[37]​

Pese a restauración de la monarquía en 1660, el interregno aseguró, tras la Revolución gloriosa (1688) y la Declaración de Derechos de 1689 (en inglés, Bill of Rights) y la Ley de Derecho, que a diferencia de los demás países europeos, el absolutismo real no prevalecería, y que un profesado como católico jamás podría acceder al trono. La constitución británica se desarrollaría sobre la base de una monarquía constitucional y un sistema parlamentario.[38]​ Con la fundación de la Royal Society en 1660, el estudio de la ciencia aumentó notablemente. Durante este período, particularmente en Inglaterra, el desarrollo de la armada inglesa —dentro del contexto de la denominada «era de los descubrimientos») condujo a la adquisición y liquidación de colonias de ultramar, particularmente en América del Norte.[39]​[40]​

El 1 de mayo de 1707, se creó el Reino Unido de Gran Bretaña[14]​[41]​[42]​ por medio de la unión política celebrada entre el Reino de Inglaterra (del que formaba parte Gales) y el Reino de Escocia. Este evento fue el resultado del Tratado de Unión firmado el 22 de julio de 1706[43]​ y ratificado por los parlamentos inglés y escocés para crear el Acta de Unión de 1707. Casi un siglo después, el Reino de Irlanda, bajo el dominio inglés desde 1691, se unió con el Reino de Gran Bretaña para formar el Reino Unido de Gran Bretaña e Irlanda, según lo estipulado en el Acta de Unión de 1800.[44]​ Aunque Inglaterra y Escocia habían sido Estados separados antes de 1707, habían permanecido en una unión personal desde 1603, cuando se llevó a cabo la Unión de las Coronas.[45]​[46]​

En su primer siglo de existencia, el país desempeñó un papel importante en el desarrollo de las ideas occidentales sobre el sistema parlamentario, además de que realizó contribuciones significativas a la literatura, las artes y la ciencia.[47]​ La Revolución Industrial, liderada por el Reino Unido, transformó al país y dio sustento al creciente Imperio británico. Durante este tiempo, al igual que otras potencias, estuvo involucrado en la explotación colonial, incluyendo el comercio de esclavos en el Atlántico, aunque con la aprobación de la Ley de esclavos en 1807, el país fue uno de los pioneros en la lucha contra la esclavitud.[48]​

Después de la derrota de Napoleón Bonaparte en las Guerras Napoleónicas, la nación emergió como la principal potencia naval y económica del siglo XIX y continuó siendo una potencia eminente hasta el siglo XX. La capital, Londres, fue la ciudad más grande del mundo desde 1831 hasta 1925.[49]​ El Imperio británico alcanzó su máxima extensión en 1921, cuando después de la Primera Guerra Mundial, la Sociedad de Naciones le otorgó el mandato sobre las antiguas colonias alemanas y posesiones otomanas, las últimas como parte de la partición del Imperio otomano. Un año más tarde, se creó la Compañía de Radiodifusión Británica (British Broadcasting Company),[50]​ que posteriormente se convirtió en la British Broadcasting Corporation (BBC),[50]​ la primera radiodifusora a gran escala de todo el mundo.[51]​

En 1921, los conflictos internos en Irlanda sobre las demandas para un gobierno autónomo irlandés, finalmente condujeron a la partición de la isla.[52]​ Al mismo tiempo, la victoria del partido Sinn Féin en las elecciones generales de 1918, seguida por una guerra de independencia, llevaron a la creación del Estado Libre Irlandés; Irlanda del Norte optó por seguir formando parte del Reino Unido.[53]​ Como resultado, en 1927 el nombre formal del Reino Unido de la Gran Bretaña e Irlanda cambió a su nombre actual, el Reino Unido de Gran Bretaña e Irlanda del Norte. La Gran Depresión, estalló en un momento en el que el país todavía estaba lejos de recuperarse de los efectos de la Primera Guerra Mundial.

El Reino Unido formó parte con Estados Unidos, la Unión Soviética y Francia de entre los aliados de la Segunda Guerra Mundial. Tras la derrota de sus aliados europeos en el primer año de la guerra, el ejército británico continuó la lucha contra Alemania en una campaña aérea conocida como la batalla de Inglaterra. Después de la victoria, el país fue una de las tres grandes potencias que se reunieron para planificar el mundo de la posguerra. La Segunda Guerra Mundial dejó la economía nacional dañada. Sin embargo, gracias a la ayuda del plan Marshall y a los costosos préstamos obtenidos de los Estados Unidos y Canadá, la nación comenzó el camino de la recuperación.[54]​

Los años inmediatos a la posguerra vieron el establecimiento del Estado del bienestar, incluyendo uno de los primeros y más grandes servicios de salud pública del mundo. Los cambios en la política del gobierno también atrajeron a personas de toda la Mancomunidad, naciendo un Estado multicultural. A pesar de que los nuevos límites del papel político británico fueron confirmados por la Crisis de Suez de 1956, la propagación internacional del idioma inglés significó la influencia permanente de su literatura y su cultura, mientras que desde la década de 1960, su cultura popular también comenzó a tener gran influencia en el extranjero.

Tras un período de desaceleración económica mundial y los conflictos industriales de la década de 1970, el siguiente decenio vio la sustancial afluencia de ingresos obtenidos por la venta del petróleo del mar del Norte y el crecimiento económico. El mandato de Margaret Thatcher marcó un cambio significativo en la dirección del consenso político y económico de la posguerra; un camino que desde 1997 siguieron los gobiernos laboristas de Tony Blair y Gordon Brown. En 1982, hubo una breve guerra contra Argentina en las Malvinas que concluyó con victoria británica. En los años 80 hubo varias tragedias en estadios de fútbol provocadas, entre otros motivos por el apogeo del fenómeno hooligan, como la Tragedia de Heysel, la Tragedia de Valley Parade y la Tragedia de Hillsborough. En 1988, la plataforma petrolífera Piper Alpha, situada en el Mar del Norte, explotó y murieron 167 personas. Ese mismo año sucedió el atentado terrorista más sangriento cometido en Europa, cuando una bomba estalló en el interior del vuelo 103 de Pan Am y mató a 270 personas.[55]​

El Reino Unido fue uno de los doce miembros fundadores de la Unión Europea en su inicio en 1992 con la firma del Tratado de Maastricht. Con anterioridad, desde 1973 había sido miembro de la precursora de la Unión Europea, la Comunidad Económica Europea (CEE). El fin del siglo XX vio cambios importantes en el gobierno británico, con el establecimiento de las administraciones descentralizadas conferidas para Irlanda del Norte, Escocia y Gales.[56]​

El 16 de septiembre de 1992 se produjo el episodio llamado "miércoles negro" cuando unos especuladores financieros, entre otros, George Soros, apostaron contra la libra esterlina provocando unas perdidas multimillonarias al estado inglés,[57]​[58]​ el colapso del Banco de Inglaterra y obligando a este a retirarse del Mecanismo Europeo de Cambio de divisas.

En 1997 Reino Unido transfiere la soberanía de Hong Kong a China. Ese mismo año la muerte de Diana de Gales en un accidente automovilístico conmociona a todo el país. En 1998, tras casi dos años de negociaciones, se firmó el acuerdo de Viernes Santo[59]​ Para dicho acuerdo actuó como mediador el entonces presidente estadounidense Bill Clinton,[60]​ consumándose el proceso de paz en Irlanda del Norte y alto el fuego del grupo terrorista IRA, poniendo fin al conflicto de Irlanda del Norte (llamado por los ingleses The Troubles es decir, [Los Problemas]).

La política exterior durante el gobierno de Tony Blair (1997-2007) fue de un estrecho alineamiento con los Estados Unidos. Tras la participación del Reino Unido en la Operación Libertad Duradera en Afganistán iniciada en 2001, Blair tomo parte de la cumbre de las Azores en 2003 donde se adoptó la decisión de lanzar un ultimátum de 24 horas al régimen iraquí encabezado por Saddam Hussein para su desarme.[61]​ Este ultimátum finalmente desembocó en la invasión de Irak (Operación Libertad Iraquí) en 2003.

El terrorismo islámico golpeó Londres el 7 de julio de 2005 provocando 56 muertos y más de 700 heridos, el día siguiente de que Londres fuera la sede elegida para albergar los Juegos Olímpicos de Londres 2012.

La crisis financiera de 2008 afectó severamente la economía británica. Dos años después, los laboristas de Gordon Brown pierden las elecciones y asciende el gobierno conservador encabezado por David Cameron, que introdujo nuevas medidas de austeridad destinadas a hacer frente a los déficits públicos sustanciales que se dieron durante el período de crisis.[62]​ En 2014, el Gobierno escocés celebró un referéndum para la independencia de Escocia en septiembre de ese año, siendo rechazada la propuesta de independencia con un 55 % de los votos.[63]​[64]​ El 9 de septiembre del año 2015, la reina Isabel II se convirtió en la monarca con más tiempo de reinado en el país, habiendo superado así a su propia tatarabuela, la reina Victoria I.

En junio de 2016 se celebró un referéndum sobre la permanencia del Reino Unido en la Unión Europea con un 51,9 % de votos a favor de dejar la entidad europea, proceso que podría demandar hasta dos años[65]​[66]​ y que inició oficialmente el 29 de marzo de 2017. Como parte de la coalición antiyihadista en la guerra contra el Estado Islámico, el Reino Unido volvió a ser golpeado ese año por el terrorismo en ciudades como Londres y Mánchester.

El 1 de enero de 2020 se hizo efectivo el Brexit, la salida del Reino Unido de la Unión Europea.

El Reino Unido es una monarquía parlamentaria cuya jefa de Estado es Isabel II. Asimismo, es la jefa de Estado de los quince países que, en el marco de la Mancomunidad de Naciones, constituyen monarquías independientes, situando al Reino Unido en una unión personal con aquellas naciones. La reina tiene la soberanía sobre las dependencias de la Corona, la isla de Man y los bailiazgos de Jersey y Guernsey. Estos no forman parte del Reino Unido, aunque el Gobierno británico gestiona sus relaciones exteriores y la defensa, además de que el parlamento tiene autoridad para legislar en su nombre.

El Reino Unido no tiene un documento que sirva como constitución totalmente definida,[67]​ algo que solo ocurre en otros dos países del mundo, Israel y Nueva Zelanda.[68]​ La constitución del Reino Unido, por lo tanto, consiste principalmente en una colección de diferentes fuentes escritas, incluyendo leyes, estatutos, jurisprudencias y tratados internacionales. Como no hay ninguna diferencia técnica entre los estatutos ordinarios y la "ley constitucional", el parlamento puede realizar una "reforma constitucional" por el simple hecho de aprobar una ley, y en consecuencia, tiene el poder para cambiar o suprimir casi cualquier elemento escrito o no escrito de la constitución. Sin embargo, existen ciertas limitaciones para la aprobación de las leyes, por ejemplo, ninguna legislatura puede crear leyes que no se puedan cambiar en un futuro.[69]​

El Reino Unido cuenta con un gobierno parlamentario, basado en el sistema Westminster, el cual ha sido emulado alrededor del mundo, uno de los legados del Imperio británico. El parlamento del Reino Unido, que se reúne en el Palacio de Westminster tiene dos cámaras: la Cámara de los Comunes (elegida por el pueblo) y la Cámara de los Lores. Cualquier ley aprobada por el parlamento requiere el consentimiento real para convertirse en ley. El hecho de que el parlamento descentralizado en Escocia y las asambleas en Irlanda del Norte y Gales no sean órganos soberanos y puedan ser abolidos por el parlamento británico, hace que este último sea el órgano legislativo más importante en el país.

El puesto del jefe de gobierno del Reino Unido, el primer ministro,[70]​ lo ocupa el miembro del parlamento que obtiene la mayoría de votos en la Cámara de los Comunes, por lo general es el líder del partido político con más asientos en dicha cámara. El primer ministro y el gabinete son nombrados por el monarca para formar el "Gobierno de Su Majestad", aunque el primer ministro elige al Consejo de Ministros, y por convención, el monarca respeta su elección.[71]​

Tradicionalmente, el gabinete se conforma de miembros del mismo partido del primer ministro de ambas cámaras legislativas, en su mayoría de la Cámara de los Comunes. El poder ejecutivo es ejercido en nivel administrativo político por el primer ministro y el gabinete, quienes hacen su juramento delante del rey y siendo éste primero parte del Consejo Privado, de tal modo que se convierten en Ministros de la Corona. En las elecciones de 2010, el líder del Partido Conservador, David Cameron, puso fin a los trece años del mandato laborista y asumió el papel de primer ministro.[72]​ Cameron pudo repetir este éxito en las elecciones generales de 2015, en donde el Partido Conservador obtuvo mayoría absoluta.[73]​

Las elecciones generales son convocadas por el monarca. Aunque no existe ningún plazo mínimo para ocupar un puesto en el parlamento, la Ley del Parlamento de 1911 exige que se debe llamar a una nueva elección dentro del plazo de cinco años después de las elecciones anteriores. Anteriormente, para las elecciones a la Cámara de los Comunes, el territorio nacional se dividía en 646 distritos electorales, con 529 en Inglaterra, 18 en Irlanda del Norte, 59 en Escocia y 40 en Gales;[74]​ este número aumentó a 650 en las elecciones generales del 2010. Cada distrito electoral elige a un miembro del parlamento por mayoría simple.

El Partido Conservador, el Partido Laborista y el Partido Nacional Escocés (el cual se presenta solo en Escocia), son los principales partidos políticos; en las elecciones generales de 2015 ganaron 619 de los 650 escaños disponibles en la Cámara de los Comunes. La mayoría de los escaños restantes fueron ganados por partidos que, al igual que el Partido Nacional Escocés, solo compiten en una parte del país, como el Partido de Gales (solo en Gales), el Partido Unionista Democrático, el Partido Socialdemócrata y Laborista, el Partido Unionista del Ulster y el Sinn Féin (solo en Irlanda del Norte, aunque el Sinn Féin también compite en las elecciones en Irlanda),[75]​ además de los Liberal Demócratas (los cuales se presentan a nivel nacional y obtuvieron 8 escaños). Para las elecciones al Parlamento Europeo, el Reino Unido tiene actualmente 72 diputados elegidos por voto en bloque.[76]​ Las dudas sobre la verdadera soberanía de cada nación constitutiva surgieron tras la adhesión del Reino Unido a la Unión Europea.[77]​

El país no tiene un sistema jurídico único, ya que fue creado por la unión política de los países anteriormente independientes y el artículo 19 del Tratado de la Unión de 1707 garantiza la existencia por separado del sistema legal escocés.[78]​ Hoy en día, el país tiene tres diferentes sistemas jurídicos: el derecho de Inglaterra, el derecho de Irlanda del Norte y la ley escocesa. En octubre de 2009, los recientes cambios constitucionales trajeron consigo la creación de una nueva Corte Suprema para asumir las funciones de apelación de la Comisión de Apelación de la Cámara de los Lores.[79]​[80]​ El Comité Judicial del Consejo Privado es el tribunal de apelación más alto para varios países independientes de la Mancomunidad, los territorios de ultramar y las dependencias de la Corona británica.

El Reino Unido pertenece a varias organizaciones internacionales como lo son la Organización de las Naciones Unidas, la Mancomunidad de Naciones, el G-8, el G-7, el G-20, la Organización del Tratado del Atlántico Norte, la Organización para la Cooperación y el Desarrollo Económico, la Organización Mundial del Comercio, el Consejo de Europa, la Organización para la Seguridad y la Cooperación en Europa. Además es uno de los miembros permanentes del Consejo de Seguridad de Naciones Unidas con derecho de veto.
Este abandono definitivamente la Unión Europea el 31 de enero de 2020, lo que lo convirtió en frontera exterior de la misma. 
Este proceso, conocido popularmente como Brexit, estaba previsto que culminara en mayo de 2019. Sin embargo, debido a sucesivas ampliaciones a lo largo del año, por falta de acuerdo en el Parlamento Británico, fue efectiva a inicios de 2020. 

La alianza más notable entre el Reino Unido con otro país es su "relación especial" con los Estados Unidos, aunque también mantiene relaciones estrechas con varios miembros de la Unión Europea, de la OTAN, de la Mancomunidad y con otros países poderosos como Japón. La presencia global y la influencia británica se amplifican aún más a través de sus relaciones comerciales, su ayuda oficial al desarrollo y sus fuerzas armadas, que mantienen cerca de ochenta instalaciones militares y otras implementaciones alrededor del mundo.[81]​

El Ejército, la Marina Real y la Real Fuerza Aérea británica se conocen colectivamente como las Fuerzas Armadas británicas. Las tres fuerzas son administradas por el Ministerio de Defensa y controladas por el Consejo de Defensa, presidido por el Secretario de Estado para la Defensa. Las tropas británicas son unas de las que cuentan con un mejor entrenamiento, además de ser las más avanzadas tecnológicamente. Según diversas fuentes, incluyendo el Ministerio de Defensa, el Reino Unido tiene el tercer o cuarto presupuesto más alto para gastos militares a nivel internacional, a pesar de contar solo con el 25.º ejército más grande en términos de personal. Actualmente, el gasto total en defensa representa el 2,5 % del PIB.[82]​

La Marina Real es una flota de agua azul, una de las tres que sobreviven, junto con la Marina Nacional francesa y la Armada de los Estados Unidos.[83]​ El 3 de julio de 2008, el Ministerio de Defensa firmó varios acuerdos con un valor de 3,2 millones de £ para construir dos nuevos portaaviones.[84]​ El Reino Unido es uno de los de cinco países (junto con Estados Unidos, China, Rusia y Francia) que puede estar en posesión de armas nucleares,[85]​ utilizando un submarino de clase Vanguard, que cuenta con el sistema de misiles balísticos de Trident II D5.

Entre las principales funciones de las Fuerzas Armadas británicas se encuentran la protección y defensa del Reino Unido y sus territorios de ultramar, la promoción de los intereses de seguridad global y el apoyo a los esfuerzos internacionales por mantener la paz. Además, son participantes activos y regulares en la OTAN, la ONU y en otros organismos internacionales que buscan la resolución pacífica de los conflictos. Existen varias guarniciones de ultramar e instalaciones del ejército británico alrededor del mundo, principalmente en la Isla Ascensión, Belice, Brunéi, Canadá, Diego García, las Islas Malvinas/Falkland, Alemania, Gibraltar, Kenia, Chipre y Catar.[86]​

En 2010, el Ejército Británico reportó que contaba con 197 840 militantes.[87]​ Aparte, están los cuerpos de las Fuerzas Especiales de Reino Unido, las Fuerzas de Reserva y las Fuerzas de Auxilio Real. Con esto, la cifra de soldados se eleva a 435 500, incluyendo al personal activo y de reserva. A pesar de las capacidades militares del Reino Unido, una política reciente sobre cuestiones de defensa asume que "las operaciones más exigentes" podrían llevarse a cabo como parte de una coalición.[88]​ Dejando a un lado la intervención en Sierra Leona, las operaciones en Bosnia y Herzegovina, Kosovo, Afganistán e Irak pueden ser tomadas como precedentes de esta política. De hecho, la última guerra en la que el Ejército Británico luchó por su propia cuenta fue durante la guerra de las Malvinas en 1982, en la que derrotó al Ejército Argentino.

En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Reino Unido ha firmado o ratificado:

La organización territorial del Reino Unido es compleja y muy variada, ya que cada país constituyente tiene su propio sistema de demarcación geográfica y administrativa con orígenes anteriores a la unión entre ellos. En consecuencia, no hay «ninguna unidad administrativa en común entre los integrantes del Reino Unido».[95]​ Hasta el siglo XIX se realizaron pocos cambios a estas administraciones, pero desde entonces ha habido una evolución constante de su papel y función.[96]​ El cambio no ocurrió de manera uniforme en las naciones constitutivas, y la devolución del poder sobre la administración local a Escocia, Gales e Irlanda del Norte, hace que sea poco probable que los cambios administrativos futuros sean uniformes.

La organización del gobierno local en Inglaterra es compleja, debido a que la distribución de funciones varía de acuerdo a las disposiciones locales. La legislación local se lleva a cabo por el Parlamento británico y el gobierno del Reino Unido, porque Inglaterra no cuenta con un parlamento descentralizado. El nivel superior de las subdivisiones de Inglaterra son las nueve oficinas regionales de gobierno.[97]​ Desde 2000, la región de Londres cuenta con una asamblea electa y con un alcalde, después del gran apoyo dado a dicha propuesta en el referéndum de Londres de 1998.[98]​[99]​ Se pretendía que las otras regiones también contaran con su propia asamblea regional, pero el rechazo a esta idea en un referéndum realizado en 2004 en la región Nordeste de Inglaterra detuvo la reforma.[100]​ Por debajo del nivel de la región, Londres se conforma por 32 municipios y el resto de Inglaterra tiene consejos de distrito y diputaciones o autoridades unitarias. Los concejales son elegidos por sufragio directo, mediante voto sencillo o por bloque.[101]​

El gobierno local de Escocia se divide en 32 áreas de consejos, que tienen una amplia variación tanto en tamaño como en población. Las ciudades de Glasgow, Edimburgo, Aberdeen y Dundee son áreas de consejo especiales, así como el área de consejo de Highland, que incluye una tercera parte de la superficie de Escocia, pero solo poco más de 200 000 personas. El poder conferido a las autoridades locales es administrado por los concejales elegidos, que son actualmente 1222.[102]​ Las elecciones se llevan a cabo por voto único transferible, mediante elecciones en bloque de tres o cuatro concejales. Cada Consejo elige a un Administrador o un Coordinador General para presidir las reuniones del Consejo y para actuar como el representante de la zona. Los concejales están sujetos a un código de conducta impuesto por la Comisión de Normas para Escocia.[103]​ La organización representante de los funcionarios locales es la Convención de Autoridades Locales Escocesas (COSLA).[104]​

Desde 1973, el gobierno local en Irlanda del Norte se organiza en 26 consejos de distrito, en donde se celebran elecciones de voto único transferible, para elegir representantes con poderes limitados a servicios, como ser la recolección de residuos y el mantenimiento de parques y lugares públicos.[105]​ Sin embargo, el 13 de marzo de 2008, el poder ejecutivo propuso la creación de once consejos nuevos para reemplazar el sistema actual[106]​ y las próximas elecciones locales se postergarán hasta el 2011 para facilitar este proceso.[107]​

Por último, el gobierno local en Gales consta de 22 autoridades unitarias, incluyendo las ciudades de Cardiff, Swansea y Newport, que son autoridades unitarias independientes.[108]​ Las elecciones se celebran cada cuatro años por sufragio directo.[109]​ La Asociación del Gobierno Local de Gales representa a los intereses de las autoridades locales galesas.[110]​

Los territorios británicos de ultramar son catorce territorios dependientes del Reino Unido, pero que no conforman parte de él. Principalmente, se trata de pequeñas islas poco pobladas que representan los vestigios del antiguo Imperio británico. Juntos, representan un área que supera los 1 728 000 km² y una población de aproximadamente 260 000 personas.[111]​ Sin embargo, el más extenso de ellos (1 709 400 km², equivalente al 98,9% de los territorios de ultramar) es el Territorio Antártico Británico, que solo es reconocido por otros cuatro países, mientras que la mayoría de los signatarios del Tratado Antártico.[112]​ no reconocen soberanía británica sobre ese territorio y lo tratan apenas como una reclamación británica, mientras que otros dos países firmantes, Chile y Argentina, tienen sus propias reclamaciones. El territorio antártico reivindicado por Reino Unido se superpone parcialmente con el área reclamada por Chile (Territorio Chileno Antártico) y totalmente con la reclamada por Argentina (Antártida Argentina), al punto que este desacuerdo llevó a tensiónes diplomáticas, presiones e incidentes (como el de Isla Decepsión o el de Bahía Esperanza) en años anteriores a la firma del tratado, que pospone la resolución del asunto.. 

Las dependencias de la Corona británica son tres territorios semidependientes del monarca del Reino Unido, pero que tampoco forman parte del país. A diferencia de los territorios de ultramar, la legislación y otros asuntos de interés local corresponden a una asamblea legislativa local; además, los tratados internacionales y las normas de carácter nacional solo son aplicadas si esta asamblea las aprueba.[113]​ Estas dependencias ocupan cerca de 779 km² y tienen una población de más de 235 700 habitantes.[112]​

El Reino Unido tiene 244 820 km² de superficie.[1]​[114]​ que comprenden la isla de Gran Bretaña y la parte nororiental de la isla de Irlanda (Irlanda del Norte) y otras islas más pequeñas.[114]​ El país se encuentra entre el océano Atlántico y el mar del Norte, a 35 kilómetros de la costa noroeste de Francia, de la que se encuentra separado por el canal de la Mancha.[114]​

Gran Bretaña se ubica entre las latitudes 49° y 59° N (las islas Shetland se extienden casi a los 61° N) y las longitudes 8° O a 2° E. El observatorio de Greenwich, en Londres, es el punto de definición para el meridiano de Greenwich. Cuando se mide directamente de norte a sur, Gran Bretaña mide poco más de 1100 kilómetros de longitud y poco menos de 500 kilómetros en su parte más ancha. Sin embargo, la mayor distancia entre dos puntos en la isla es de 1350 kilómetros entre el final de la tierra en Cornualles (cerca de Penzance) y John o' Groats en Caithness (cerca de Thurso). Irlanda del Norte comparte una frontera de tierra de 443 km con la República de Irlanda.[1]​

Inglaterra acapara poco más de la mitad de la superficie total del Reino Unido, con 130 410 kilómetros cuadrados de superficie. La mayor parte del país consiste de tierras bajas, con un poco de terreno montañoso en la zona noroeste, donde se encuentra la línea de Tees-Exe, entre las montañas de Cumbria y los montes Peninos. La montaña más alta de la región es Scafell Pike (978 m s. n. m.) y se ubica dentro de esta zona. Los principales ríos y estuarios de Inglaterra son el Támesis, el Severn y el Humber.

Escocia representa menos de un tercio del área total del Reino Unido, cubriendo 78 772 kilómetros cuadrados;[115]​ esta cifra incluye las casi ochocientos islas,[116]​ que en su mayoría se encuentran al oeste y al norte de Gran Bretaña, destacando las Hébridas, las islas Orcadas y las islas Shetland. La topografía de Escocia se distingue por la falla de las Highlands, que atraviesa el territorio escocés de Helensburgh a Stonehaven. La falla separa las dos principales regiones escocesas: las tierras altas del norte y oeste y las tierras bajas del sur y este. La región montañosa contiene la mayoría de las montañas de Escocia, incluyendo el Ben Nevis, que con sus 1343 m s. n. m., es el punto más alto en las islas británicas.[117]​ Las tierras bajas, especialmente la franja estrecha de tierra entre el fiordo de Clyde y el fiordo de Forth conocida como el "Cinturón Central", son más planas y en ellas se encuentra la mayoría de las comunidades escocesas, incluyendo Glasgow, la ciudad más grande de la región, y Edimburgo, la capital y centro político del país.

Gales ocupa menos de una décima parte del total del área del Reino Unido, cubriendo solo 20 758 kilómetros cuadrados. Gales es principalmente montañosa, aunque la zona sur es menos montañosa que el norte y el centro. Por eso, las principales zonas industriales están en Gales del Sur, formadas por las ciudades costeras de Cardiff, Swansea y Newport. Las montañas más altas son las Snowdonia, donde se encuentra el pico más alto de la región: el Snowdon con 1085 m s. n. m.. Las catorce (o quince) montañas más altas de Gales sobrepasan los 914,4 metros (3000 pies) y se conocen comúnmente como las "Gales 3000's".[118]​ Hay varias islas que se extienden delante de los más de 1200 km de costa, la más grande de ellas es Anglesey (Ynys Môn), ubicada al noroeste del país.

Irlanda del Norte acapara solo 14 160 kilómetros cuadrados y su territorio es en su mayoría montañoso. Se encuentra separado de la isla británica por el mar de Irlanda y el canal del Norte. El pico más alto de esta región es el Slieve Donard con 849 m s. n. m., localizado en los montes de Mourne. En Irlanda del Norte se encuentra el Lough Neagh, que con sus cerca de 388 kilómetros cuadrados, es el cuerpo de agua más grande en el Reino Unido.[119]​

El Reino Unido tiene un clima templado y un clima oceánico con abundantes lluvias durante todo el año.[114]​ La temperatura varía con las estaciones, pero rara vez cae por debajo de -10 °C, o se eleva por encima de los 35 °C. El viento predominante proviene del suroeste, trayendo consigo el clima húmedo y cálido desde el océano Atlántico.[114]​ La parte oriental se encuentra más protegida de este viento y por lo tanto tiene un clima más seco. Las corrientes atlánticas, calentadas por la corriente del Golfo, hacen que los inviernos no sean tan severos, especialmente en el oeste, donde los inviernos son húmedos. Los veranos son más cálidos en el sureste de Inglaterra, siendo la parte más cercana al continente europeo, y más frescos conforme se avanza hacia el norte. Las nevadas ocurren durante el invierno y la primavera, aunque las nevadas intensas rara vez caen en las tierras bajas.

En el Reino Unido, como resultado del cambio climático, hay una tendencia hacia inviernos más cálidos y veranos más cálidos, el nivel del mar en la costa británica aumenta aproximadamente 3 mm cada año y hay signos de un cambio en los patrones de precipitación.[120]​ Los científicos del clima esperan que las olas de calor, como las de 2003, se conviertan en la norma en la década de 2040 como resultado de la crisis climática.[120]​ Los cálculos del modelo de 2019 muestran que Londres sería reubicado en otra zona climática si ocurre el escenario RCP4.5.[121]​ El clima en Londres en 2050 se parece más al clima anterior en Barcelona que al clima anterior en Londres.[121]​ Incluso los fenómenos meteorológicos extremos son cada vez más frecuentes e intensos.[122]​ Se ha demostrado que las inundaciones en Inglaterra 2013-2014 se remontan al cambio climático provocado por el hombre.[122]​

En la mayoría de Gran Bretaña hay un clima templado que recibe altos niveles de precipitaciones y niveles medios de insolación. Hacia el norte, el clima se hace más frío y los bosques de coníferas sustituyen en gran medida a las especies caducifolias de los bosques del sur.

Hay algunas variaciones en el clima británico, con algunas áreas con condiciones subárticas tal como ocurre en las Tierras Altas de Escocia y Teesdale, e incluso subtropical en las islas Sorlingas. Los cambios estacionales que se producen en todo el archipiélago condicionan a las plantas que deben hacer frente a los cambios en los niveles de luz solar, precipitación y temperatura, así como el riesgo de nieve y las heladas durante el invierno.

Dentro de la isla existen varios ecosistemas como los bosques templados, pantanos, marismas, etc. El roble, el olmo, el haya, el fresno, el pino y el abedul son algunos de los árboles más comunes dentro de los bosques británicos.[123]​ Anteriormente, las islas británicas se encontraban repletas de bosques de árboles caducifolios y coníferas, pero para la década de 2000, solamente cerca del 10 % del territorio nacional se encontraba cubierto por bosques, concentrándose en el noreste de Escocia y en el sureste de Inglaterra, debido en gran parte a la tala descontrolada y al crecimiento urbano.[124]​ El área que rodea a las zonas urbanas está cubierta principalmente por pastos y plantas con flores[123]​

La isla de Gran Bretaña, junto con el resto del archipiélago conocido como las islas británicas, alberga una fauna típica de clima templado oceánico, poco diversa si se compara a nivel mundial y similar a la de otros países de Europa del Norte.

Entre los mamíferos que más abundan en el país se incluyen los zorros, conejos, ciervos, erizos, ratones, comadrejas y musarañas.[123]​ Como otras islas ubicadas en latitudes similares, son escasos los ejemplares de reptiles y anfibios.[125]​ En todo el territorio nacional se han descubierto más de 21 000 especies de insectos y cerca de 230 especies de aves, algunas de las cuales están amenazadas por la caza y la destrucción de su hábitat.[123]​ Los principales ríos británicos, como el río Támesis, son la principal fuente de agua para la fauna de los ecosistemas locales, a la vez de que son el hábitat de varias especies de peces y aves acuáticas.[126]​

La biodiversidad disminuyó severamente durante la última glaciación, y en poco tiempo (en términos geológicos) se separó del continente por la formación del canal de la Mancha.

El hombre ha perseguido a las especies de mayor tamaño que interferían con sus actividades (el lobo,[127]​ el oso pardo y el jabalí) hasta provocar su extinción en la isla, aunque por supuesto siguen existiendo las formas domesticadas como el perro y el cerdo. El jabalí se volvió a introducir posteriormente.[128]​

Desde mediados del siglo XVIII, Gran Bretaña ha sufrido una gran industrialización y aumento de urbanización. Un estudio de DEFRA publicado en 2006 sugirió que 100 especies de animales se han extinguido en el Reino Unido durante el siglo XX, lo que supone cerca de 100 veces la tasa de extinción de fondo.[129]​ Esto ha tenido un gran impacto en las poblaciones de animales autóctonos, particularmente en los paseriformes, siendo cada vez más escasas. La pérdida de hábitat ha afectado principalmente a las especies de mamíferos de mayor tamaño. Sin embargo, algunas especies se han adaptado al entorno urbano en expansión, en particular el zorro, la rata, y otros animales como la paloma torcaz.

La economía del Reino Unido se compone (en orden descendente de tamaño) de las economías de Inglaterra, Escocia, Gales e Irlanda del Norte. Basado en las tasas de cambio del mercado, el Reino Unido es la quinta economía más grande del mundo y la segunda más grande en Europa después de Alemania, por delante de Francia.[131]​

La Revolución Industrial se inició en el Reino Unido, en un proceso donde se dio una gran concentración de las industrias pesadas en todo el país, como la construcción naval, la extracción del carbón, la producción de acero y la industria textil. La extensión del Imperio creó un mercado exterior enorme para los productos británicos, permitiendo que la nación dominara el comercio internacional en el siglo XIX. Más tarde, como le sucedió a otras economías industrializadas, junto con el declive económico después de las dos guerras mundiales, el Reino Unido comenzó a perder su ventaja competitiva y la industria pesada disminuyó. Aunque la fabricación sigue siendo una parte importante de la economía, en 2003 solo representaba una sexta parte de los ingresos del país.[132]​

La industria automovilística es una parte importante del sector manufacturero, aunque ha disminuido con el colapso del MG Rover Group y actualmente la mayor parte de la industria es propiedad extranjera. La producción de aviones civiles y de defensa es liderada por BAE Systems, el mayor contratista de defensa en el mundo,[133]​ y por la firma europea EADS, el propietario del Airbus. Rolls-Royce tiene una parte importante del mercado mundial de motores aeroespaciales. La industria química y farmacéutica son importantes en el Reino Unido, ya que las compañías británicas de GlaxoSmithKline y AstraZeneca son la segunda y sexta empresa farmacéutica más grandes del mundo, respectivamente.[134]​

Sin embargo, durante las últimas décadas el sector terciario aumentó considerablemente y ahora produce cerca del 73 % del PIB.[135]​ El sector de servicios está dominado por los servicios financieros, especialmente bancos y aseguradoras. Esto hace a Londres el centro financiero más grande del mundo,[136]​ ya que aquí se encuentran las sedes de la Bolsa de Londres, el London International Financial Futures and Options Exchange y el Lloyd's of London; además de ser el líder de los tres "centros de comando" de la economía mundial (junto con Nueva York y Tokio).[137]​ Además, cuenta con la mayor concentración de sucursales de bancos extranjeros en el mundo. En la última década, un centro financiero rival de Londres ha crecido en la zona de Docklands, donde el HSBC, el banco más grande del mundo,[138]​[139]​ y el Barclays reubicaron sus sedes. Muchas empresas multinacionales que no son de propiedad británica han elegido Londres como el lugar para su sede europea o extranjera: un ejemplo es la firma estadounidense de servicios financieros Citigroup. La capital de Escocia, Edimburgo, también es uno de los grandes centros financieros de Europa[140]​ y es la sede del Royal Bank of Scotland Group, uno de los bancos más importantes del mundo.

El turismo es muy importante para la economía británica. Con los más de 27 millones de turistas que arribaron al país en 2004, el Reino Unido está clasificado como el sexto destino turístico más importante en el mundo.[141]​ Londres, por un margen considerable, es la ciudad más visitada en el mundo con 15,6 millones de visitantes en 2006, por delante de Bangkok (10,4 millones de visitantes) y de París (9,7 millones).[142]​ Las industrias creativas aportaron el 7 % del PIB de 2005 y crecieron a una tasa promedio anual del 6 % entre 1997 y 2005.[143]​

En julio de 2007, el Reino Unido tenía una deuda pública del 35,5 % del PIB.[144]​ Esta cifra aumentó a 56,8 % del PIB en julio de 2009.[145]​ La moneda nacional es la libra esterlina, representada con el símbolo £. El Banco de Inglaterra es el banco central, responsable de la emisión de moneda, aunque los bancos de Escocia e Irlanda del Norte tienen derecho a emitir sus propios billetes. La libra esterlina también se utiliza como moneda de reserva por otros gobiernos e instituciones y es la tercera moneda con mayor cantidad de reservas, después del dólar estadounidense y del euro.[146]​ El Reino Unido decidió no participar en el lanzamiento del euro como moneda, y el anterior primer ministro británico, Gordon Brown, ha descartado la adopción del euro en un futuro cercano, argumentando que la decisión de no unirse al proyecto había sido la mejor opción para el país y para Europa.[147]​ El anterior gobierno de Tony Blair se comprometió a celebrar un referéndum público para decidir si el país realizaría las "cinco pruebas económicas". En 2005, más de la mitad de los británicos (55 %) estaban en contra de la adopción del euro como moneda, mientras que solo el 30 % estaban a favor.[148]​

El 23 de enero de 2009, cifras de la Oficina Nacional de Estadísticas mostraron que la economía británica estaba oficialmente en recesión por primera vez desde 1991.[149]​ Se informó que fue en el último trimestre del 2008 cuando la economía cayó en una recesión que fue acompañada por el creciente desempleo, el cual aumentó de 5,2 % en mayo de 2008 a 7,6 % en mayo de 2009. La tasa de desempleo para adultos entre 18 a 24 años, aumentó de 11,9 % a 17,3 % en el mismo periodo.[150]​

La línea de pobreza relativa en el Reino Unido se define comúnmente por debajo del 60 % del ingreso promedio. Entre 2007 y 2008, el 13,5 millones de personas, o sea, el 22 % de la población, vivían por debajo de esta línea. Se trata de uno de los niveles de pobreza relativa más altos entre los miembros de la Unión Europea.[151]​ Después de tomar en cuenta los costos de la vivienda, se demostró que en el mismo lapso 4 millones de niños, 31 % del total, vivían en hogares que estaban por debajo de la línea de pobreza. Esto representa una disminución de 400 000 niños comparado con el periodo entre 1998 y 1999.[152]​

Entre 2007 y 2015, el Reino Unido registró la mayor disminución de los salarios reales (ajustados por inflación) de todos los países avanzados, al mismo nivel que Grecia (-10,4%). El Reino Unido tiene la mayor desigualdad de ingresos entre los países de la OCDE y las mayores disparidades regionales de Europa. La parte de los ingresos capturada por el 1% más rico se ha duplicado en los últimos 30 años, pasando de alrededor del 4% a más del 8,5% del producto interno bruto (PIB) en 2018.[153]​

Las principales carreteras británicas forman una red de 46 904 kilómetros, de los cuales más de 3520 kilómetros son autopistas. Además, hay cerca de 213 750 kilómetros de caminos pavimentados.[114]​ La red ferroviaria, con 16 116 kilómetros en Gran Bretaña y 303 kilómetros en Irlanda del Norte,[114]​ diariamente transporta más de 18 000 trenes de pasajeros y 1000 trenes de mercancías. Las redes ferroviarias urbanas están bien desarrolladas en Londres y otras ciudades importantes. Llegaron a existir más de 48 000 km de vías férreas en todo el país, sin embargo, la mayoría se redujo entre 1955 y 1975, en gran parte después de un informe del asesor de gobierno Richard Beeching a mediados de la década de 1960 (conocido como el hacha de Beeching). Actualmente se consideran nuevos planes para construir nuevas líneas de alta velocidad para el año 2025.[154]​

La Agencia de Carreteras es la agencia ejecutiva responsable de los caminos y autopistas en Inglaterra, aparte de la empresa privada M6 Toll.[157]​ El Departamento de Transporte afirma que la congestión vehicular es uno de los más graves problemas en materia de transporte y que si no se controla, para el año 2025 podría costarle a Inglaterra más de 22 000 millones de libras esterlinas.[158]​ De acuerdo con el Informe Eddington de 2006 realizado por el gobierno británico, la congestión está en peligro de perjudicar la economía, a menos que se contrarreste con el cobro de peajes y la expansión de la red de transporte.[159]​[160]​

Las vías y medios de transporte de Escocia son responsabilidad del Departamento de Transportes del gobierno local, siendo Transportes Escocia la agencia gubernamental encargada del mantenimiento de las carreteras y redes ferroviarias del país.[161]​ La red de ferrocarriles de Escocia tiene alrededor de 340 estaciones y 3000 kilómetros de vías y transporta a más de 62 millones de pasajeros cada año.[162]​ En 2008, el gobierno escocés estableció planes de inversión para los próximos 20 años, con prioridades para incluir un nuevo puente en la carretera de Forth y la electrificación de la red ferroviaria.[163]​

El aeropuerto de Londres-Heathrow, situado a 24 km al oeste de la capital, es el aeropuerto más concurrido del Reino Unido y tiene el mayor nivel de tráfico de pasajeros internacionales en el mundo.[155]​[156]​ Entre octubre de 2009 y septiembre de 2010, los aeropuertos británicos recibieron a 211,4 millones de pasajeros. Asimismo, es la base de operaciones de aerolíneas como British Airways, Virgin Atlantic y bmi.[164]​

La televisión es el principal medio de comunicación en el Reino Unido. Las principales cadenas de carácter nacional son: BBC One, BBC Two, ITV1, Channel 4 y Five. En Gales, S4C es el principal canal en idioma galés.

La BBC es la principal compañía emisora de carácter público del Reino Unido y la más grande y antigua emisora del mundo.[166]​ Opera varios canales de televisión y estaciones de radio en el país y en el extranjero. El servicio de televisión internacional de la BBC, BBC World News, se retransmite en todo el mundo y el servicio de radio internacional, BBC World Service, emite en treinta y tres idiomas.

En cuanto a la radio, el principal servicio es BBC Radio que cuenta con diez estaciones nacionales, entre las que se encuentran las dos con mayor popularidad: BBC Radio 1 y BBC Radio 2; y cerca de cuarenta estaciones regionales. Además existen servicios en otros idiomas dentro de las fronteras británicas, como BBC Radio Cymru en galés y BBC Radio nan Gàidheal en gaélico escocés; algunos programas de BBC Radio Ulster son emitidos en irlandés para la población norirlandesa. Existen además cientos de estaciones privadas de carácter local.

Internet es otro de los medios de comunicación más importantes en el país, además de que ha tenido un gran aumento desde la última década, de tal modo que con 41 817 847 de usuarios, es el séptimo país con la mayor cantidad de internautas en el mundo.[167]​ El dominio de Internet para el Reino Unido es .uk. El sitio web más popular con terminación ".uk" es la versión británica de Google, seguido por la página de la BBC.[168]​

The Sun es el periódico de mayor circulación nacional, con 3,1 millones de ejemplares diarios, acaparando aproximadamente un cuarto del mercado.[169]​ Su publicación hermana, News of the World era el periódico semanal de mayor circulación, cancelado tras un escándalo de escuchas ilegales, que se centraba normalmente en historias de celebridades.[170]​ The Daily Telegraph, un periódico de derecha, es considerado el periódico de "calidad" más vendido en el país.[169]​ The Guardian es otro periódico de "calidad", aunque de tendencia más liberal; Financial Times es el principal diario financiero del país, caracterizado por imprimirse en hojas color salmón.

Impreso desde 1737, The News Letter de Belfast, es el periódico en inglés más antiguo aún en circulación. Uno de sus competidores norirlandeses, The Irish News, ha sido calificado como el mejor periódico regional del Reino Unido en varias ocasiones.[171]​ Además de los periódicos, algunas publicaciones británicas cuentan con circulación internacional, entre los que destacan las revistas The Economist y Nature.

El consumo de energía eléctrica en el país asciende a 345 800 millones de kWh anuales, siendo el 12.º país con mayor consumo de electricidad en el mundo.[114]​ Sin embargo, produce 1,54 millones de barriles de petróleo diarios y 69,9 millones de m³ de gas natural anuales.[114]​ Actualmente, la mayor parte de la energía eléctrica proviene de fuentes no renovables, principalmente del carbón y petróleo. Esto ha hecho que el gobierno comience a implementar medidas para reducir la dependencia de los combustibles fósiles en materia de producción de energía y se pretende que para 2020 el 40 % de la electricidad provenga de fuentes de energía alternativas como la solar, la eólica y la mareomotriz.[172]​

El Reino Unido tiene una pequeña reserva de carbón, junto con reservas importantes, pero en continua disminución, de gas natural y petróleo.[173]​ Se han identificado unos 400 millones de toneladas de carbón en el país.[174]​ En 2004, el consumo de carbón total (incluyendo las importaciones) fue de 61 millones de toneladas,[175]​ permitiendo al país ser autosuficiente en carbón por apenas 6,5 años, aunque con los niveles de la extracción actual, el periodo aumenta a 20 años.[174]​ Una alternativa a la generación de energía eléctrica con carbón es la gasificación del carbón subterráneo (GCS). La GCS es un sistema que inyecta vapor y oxígeno dentro de un pozo, donde se extrae gas del carbón y empuja la mezcla de gases a la superficie (un método de extracción de carbón con emisiones de carbono potencialmente bajas). Tras la identificación de áreas terrestres que tienen el potencial para la GCS, las reservas de gas se calculan entre 7 000 millones y 16 000 millones de toneladas.[176]​ Basado en el consumo de carbón actual en el país, estos volúmenes representan reservas que podrían durar entre 200 y 400 años.[177]​

La educación en el Reino Unido es una cuestión descentralizada, ya que cada nación constituyente tiene su propio sistema de educación. La educación en Inglaterra es responsabilidad de la Secretaría de Estado para los Niños, Escuelas y Familias, aunque la administración y financiación de las escuelas estatales corresponden a las autoridades locales.[178]​ La universalidad en la educación en Inglaterra y Gales fue introducida en 1870 para la educación primaria y en 1900 para la educación secundaria.[179]​ Actualmente, la educación es obligatoria de los cinco a dieciocho años de edad. La mayoría de los niños son educados en escuelas del sector estatal, solo una pequeña porción estudia en escuelas especiales, principalmente por motivos de habilidades académicas. Las escuelas del Estado que tienen permitido seleccionar a los alumnos de acuerdo a su inteligencia y habilidad académica pueden lograr resultados comparables a las escuelas privadas más selectivas: en 2006, de las diez escuelas de mejor rendimiento académico, dos eran escuelas estatales de gramática. A pesar de una caída en las cifras reales, la proporción de niños en Inglaterra que asisten a escuelas privadas ha aumentado en más de 7 %.[180]​ Sin embargo, más de la mitad de los estudiantes de las principales universidades, Cambridge y Oxford, asistió a las escuelas estatales.[181]​ Inglaterra tiene algunas de las mejores universidades a nivel internacional; la Universidad de Cambridge, la Universidad de Oxford, el Imperial College London y la University College de Londres están clasificadas dentro de las diez mejores del mundo.[182]​ Según la TIMSS (Tendencias en el Estudio Internacional de Matemáticas y Ciencias), los alumnos en Inglaterra son los séptimos mejores en matemáticas y los sextos en ciencias. Los resultados sitúan a los alumnos ingleses por delante de otros países europeos, incluyendo Alemania y los países escandinavos.[183]​

La educación en Escocia es responsabilidad de la Secretaría de Educación y Aprendizaje, con la administración y financiación de las escuelas estatales a cargo de las autoridades locales. Dos organismos públicos no departamentales tienen un papel clave en la educación escocesa: la Autoridad Escocesa de Calificaciones[184]​ y Aprendizaje y Enseñanza de Escocia.[185]​ La educación se volvió obligatoria en Escocia en 1496.[186]​ La proporción de niños que asisten a escuelas privadas es apenas del 4 %, aunque ha ido aumentando lentamente en los últimos años.[187]​ Los estudiantes escoceses que asisten a universidades de Escocia no pagan colegiaturas ni los cursos para realizar algún posgrado, ya que todas estas cuotas fueron abolidas en 2001. La aportación monetaria a las universidades por parte de los alumnos egresados fue abolida en 2008.[188]​

La educación en Irlanda del Norte es administrada por el Ministerio de Educación y el Ministerio de Empleo y Aprendizaje, aunque a nivel local es responsabilidad de cinco juntas de educación, que cubren áreas geográficas determinadas. El Consejo para el Plan de Estudios, Exámenes y Evaluaciones (CCEA) es el organismo encargado de asesorar al gobierno sobre lo que debe enseñarse en las escuelas norirlandesas, el seguimiento de normas y la adjudicación de títulos.[189]​

La Asamblea Nacional de Gales tiene la responsabilidad de la educación en este país. Un número significativo de estudiantes galeses aprende, ya sea totalmente o en gran medida, en el idioma galés; las lecciones en galés son obligatorias para todos los alumnos hasta la edad de 16 años.[190]​ Hay planes para aumentar el número de escuelas de educación media que imparten clases en galés, como parte de la política para lograr un Gales totalmente bilingüe.

Un estudio publicado en diciembre de 2019 por la asociación The Equality Trust revela que sumando las fortunas de las cinco familias más ricas del Reino Unido —por un total de 46.000 millones de euros— obtenemos la suma que poseen los 13 millones de personas más pobres del país. En términos más generales, el 1 % más rico de los británicos posee por sí solo tanto dinero como el 80 % de la población total.[191]​

Entre 2017 y 2018, la tasa de pobreza en el país aumentó del 22,1 % al 23,2 %, el mayor incremento desde 1988. Se cree que el aumento de la inflación y los recortes presupuestarios del Gobierno conservador para 2015, en particular en lo que respecta a las asignaciones familiares y los subsidios de vivienda, son las principales causas. Cuatro millones de británicos viven con menos de la mitad de la línea de pobreza y 1,5 millones no pueden cubrir sus necesidades básicas.[192]​

Cada diez años se efectúa un censo simultáneamente en todas las regiones del Reino Unido.[194]​ La Oficina Nacional de Estadísticas es la responsable de la recopilación de datos para Inglaterra y Gales, mientras que en Escocia e Irlanda del Norte los responsables de llevar a cabo los censos son la Oficina de Registro General y la Agencia de Estadísticas e Investigación, respectivamente.[195]​

En el más reciente censo realizado en 2001, el total de la población del Reino Unido fue de 58 789 194 personas, la tercera más grande en la Unión Europea, la quinta más grande en la Mancomunidad y la vigésimo primera en el mundo. A mediados de 2008, se estimó que había crecido a los 61 383 000 habitantes.[196]​ En 2008, el crecimiento natural de la población superó la migración neta como el principal contribuyente al crecimiento de la población, la primera vez que ocurre desde 1998.[196]​ Entre 2001 y 2008, la población aumentó en una tasa media anual del 0,5 %. Esto se compara con el 0,3 % anual en el período de 1991 a 2001 y al 0,2 % en la década de 1981 a 1991.[196]​ Publicado en 2008, la estimación de la población de 2007 reveló que, por primera vez, el Reino Unido era hogar de más personas en edad de jubilación que de niños menores de 16 años.[197]​

A mediados de 2008, del total de unos 61 millones de británicos, la población de Inglaterra se estimó en 51 383 000 habitantes.[196]​ De esta forma, Inglaterra es uno de los países más densamente poblados del mundo con 383 habitantes por kilómetro cuadrado,[198]​ con una concentración particular en Londres y en el sureste del país.[199]​ Las estimaciones de ese mismo periodo ponen la población de Escocia en 5,17 millones, de Gales en 2,99 millones y de Irlanda del Norte en 1,78 millones,[196]​ con mucho menor densidad de población que Inglaterra. En comparación con los 383 habitantes ingleses por kilómetro cuadrado, las cifras correspondientes fueron 142 h/km² en Gales, 125 h/km² para Irlanda del Norte y solo 65 h/km² para Escocia.[198]​ Irlanda del Norte tenía la población de más rápido crecimiento en términos de porcentaje de todos los cuatro países constituyentes del Reino Unido.[196]​

Ese mismo año, la tasa de fertilidad promedio en todo el Reino Unido fue de 1,96 hijos por mujer.[200]​ Mientras que una creciente tasa de natalidad contribuye al crecimiento de la población actual, aún permanece considerablemente por debajo del baby boom de 1964, donde cada mujer tenía en promedio 2,95 hijos,[201]​ pero superior al récord más bajo en 2001, de 1,63 hijos por mujer.[200]​ En 2008, Escocia tenía la tasa de fecundidad más baja con solo 1,8 niños por mujer, mientras que Irlanda del Norte tuvo la más alta con 2,11 niños.[200]​

El Reino Unido no tiene un idioma oficial, pero el más predominante es el inglés, una lengua germánica occidental que desciende del anglosajón, que cuenta con un gran número de préstamos del nórdico antiguo, del francés normando y del latín. Debido en gran medida a la expansión del Imperio británico, el idioma inglés se esparció por el mundo y se convirtió en el idioma internacional de los negocios, así como la segunda lengua más divulgada en el mundo.[202]​

El escocés (Lallans), una lengua emparentada con el inglés que también desciende del inglés medio hablado en el noreste de Inglaterra, es reconocido a nivel europeo.[203]​ También hay cuatro lenguas celtas en uso: el galés, el irlandés, el gaélico escocés y el córnico. En el censo de 2001, más de una quinta parte de la población de Gales dijo que sabía hablar galés (21%),[204]​[205]​ Además, se estima que cerca de 200 000 galesoparlantes viven en Inglaterra.[206]​

El censo de 2001, en Irlanda del Norte se demostró que 167 487 personas (10,4 % de la población) tenían "cierto conocimiento del irlandés", casi exclusivamente en la población católica y nacionalista del país. Más de 92 000 personas en Escocia (justo por debajo del 2 % de la población) poseían algún entendimiento de la lengua gaélica, incluyendo el 72 % de los habitantes de las Hébridas Exteriores.[207]​ Está aumentando el número de escuelas que enseñan en galés, gaélico escocés e irlandés.[208]​ Estos idiomas también son hablados por pequeños grupos alrededor del mundo; en Nueva Escocia, Canadá se habla irlandés, mientras que existe una población que habla galés en la Patagonia argentina.[209]​

Generalmente, es obligatorio para los alumnos británicos estudiar un segundo idioma en algún momento de su trayectoria escolar: a la edad de 14 años en Inglaterra,[210]​ y hasta la edad de 16 en Escocia. El francés y el alemán son los dos idiomas más estudiados en Inglaterra y Escocia. En Gales, todos los alumnos de 16 años deben haber aprendido el galés como segunda lengua.[211]​

En el Acta de Unión de 1707, que llevó a la formación del Reino Unido, se aseguró que el protestantismo seguiría existiendo, así como un vínculo entre la Iglesia y el Estado que permanece hasta el siglo XXI. De esta forma, el cristianismo es la religión con más seguidores, seguida por el islam, el hinduismo, el sijismo y el judaísmo, según los datos obtenidos en el censo de 2001.[114]​

En el mismo censo el 71,6 % de los encuestados dijo que el cristianismo era su religión,[215]​ aunque encuestas que emplean una pregunta "más específica" tienden a encontrar proporciones menores; tal es el caso del Estudio de Tearfund de 2007, el cual reveló que el 53 % se identificaron como cristianos,[216]​ y del Estudio británico de Actitudes Sociales de 2007, que encontró que era casi un 47,5 %.[217]​ Sin embargo, el Estudio de Tearfund demostró que solo uno de cada diez británicos realmente asistía a la iglesia semanalmente.[218]​

El Estudio británico de Actitudes Sociales de 2007, que abarca a Inglaterra, Gales y Escocia, pero no a Irlanda del Norte, indicó que 20,87 % de la población eran parte de la Iglesia de Inglaterra, 10,25 % cristianos sin denominación, 9,01 % católicos, 2,81 % presbiterianos (Iglesia de Escocia), 1,88 % metodistas, 0,88 % bautistas y 2,11 % cristianos de otro tipo. Entre otras religiones, los musulmanes ocupaban el 3,30 %, los hinduistas el 1,37 %, los judíos el 0,43 %, los sijistas el 0,37 % y los adeptos a otras religiones el 0,35 %. Una gran proporción afirmó no tener ninguna religión (45,67 %).[217]​

En el censo de 2001, 9,1 millones de personas (15 % de la población) afirmaron ser ateos, con más de 4,3 millones de personas (7 %) que no indicaron una preferencia religiosa en específico.[219]​ Existe una disparidad entre las cifras para aquellos que se identifican con una religión en particular y para aquellos que proclaman la creencia en un dios: una encuesta del Eurobarómetro realizada en 2005 mostró que el 38 % de los encuestados cree que "hay un dios", 40 % cree que "hay algún tipo de espíritu o fuerza vital" y 20 % dijo que "no creo que exista algún tipo de espíritu, dios o fuerza vital".[220]​ El druidismo es desde 2010 reconocido como una de las religiones oficiales de Reino Unido y como una de las más antiguas del país.[221]​

Al igual que la educación, la asistencia médica es un asunto descentralizado, por lo que Inglaterra, Irlanda del Norte, Escocia y Gales cuentan con su propio sistema de atención de la salud, junto con terapias alternativas, holísticas y complementarias. El National Health Service (NHS) (Servicio Nacional de Salud) es el organismo encargado de brindar asistencia médica a todos los residentes permanentes del Reino Unido de manera gratuita. En 2000, la Organización Mundial de la Salud situó al National Health Service como el decimoquinto mejor en Europa y el decimoctavo en el mundo.[223]​[224]​

Además del National Health Service, existen varios organismos encargados del cuidado de la salud que son administrados por el gobierno, como el Consejo Médico General y el Consejo de Obstetricia y Enfermería, mientras que otros corresponden a la iniciativa privada, como los Colegios Reales. Sin embargo, la política y la administración del National Health Service corresponden a cada nación constitutiva. Cada National Health Service tiene diferentes políticas y prioridades, resultando en grandes contrastes entre uno y otro.[225]​[226]​

Desde 1979, los gastos del servicio médico han aumentado significativamente, acercándose al gasto promedio de la Unión Europea.[227]​ El Reino Unido gasta alrededor de un 8,4 % de su PIB en el cuidado de la salud, lo que está un 0,5 % por debajo del promedio de la Organización para la Cooperación y el Desarrollo Económico y alrededor de un 1 % por debajo del promedio de la Unión Europea.[228]​

Londres
Birmingham
Glasgow

Liverpool
Edimburgo
Leeds

La cultura del Reino Unido, también llamada "cultura británica", puede ser descrita como el legado de la historia de un país insular desarrollado, una gran potencia y también como el resultado de la unión política de cuatro países, cada uno conservando sus elementos distintivos de las tradiciones, costumbres y simbolismos. Como resultado del dominio del Imperio británico, la influencia de la cultura británica se puede observar en el idioma, las tradiciones, las costumbres y los sistemas jurídicos de muchas de sus antiguas colonias, como Canadá, Australia, Nueva Zelanda, India y los Estados Unidos.

El arte y la cultura han sido influenciados históricamente por la ideología occidental.[230]​ Desde la expansión del Imperio británico, la experiencia del poder militar, político y económico llevó a una técnica, gusto y sensibilidad únicos de los artistas del Reino Unido.[231]​ Los británicos usaban su arte "para ilustrar sus conocimientos y liderar el mundo natural", mientras que los colonos de América del Norte, Australasia y Sudáfrica "se embarcaron hacia la búsqueda de una expresión artística distintiva y apropiada para su identidad nacional".[231]​ El imperio estuvo "en el centro, más que en los márgenes, de la historia del arte británico", y las artes visuales de la época victoriana han sido fundamentales para la construcción, celebración y expresión de la identidad británica.[232]​

El arte del Reino Unido abarca todas las manifestaciones artísticas realizadas desde la fundación del país hasta la actualidad. Sin embargo, gran parte del denominado arte británico proviene de antes de 1707, siendo Stonehenge la manifestación artística más antigua en el país, ya que data del año 2500 a. C.[233]​ Desde entonces, el arte en el territorio comprendido por el Reino Unido se fue desarrollando con el paso de los siglos, y para la época de la unión de las cuatro naciones, cada una ya contaba con una tradición artística definida.

La época de mayor auge para las artes británicas fue durante el Imperio, cuando el Reino Unido se ubicó a la cabeza de varios movimientos artísticos en los que además de representar momentos históricos, bíblicos y mitológicos, plasmaron momentos de la vida cotidiana que podían trascender en el arte.[234]​ Además, gracias a la expansión imperial los artistas pudieron tomar influencias de las culturas de los países bajo el dominio británico, tales como India, Estados Unidos, etc., al mismo tiempo que las obras británicas dejaban su huella y legado dentro de los artistas de las colonias. Durante el siglo XX, el arte británico comenzó a expandirse a las corrientes del arte moderno y contemporáneo, como el posimpresionismo, el cubismo y el impresionismo.[235]​

Actualmente, existen varias instituciones artísticas en el Reino Unido, de las cuales han surgido varios movimientos artísticos y artistas destacados dentro de su campo. Entre estas se encuentran la Royal Academy, el Royal College of Art, la Royal Society of Arts y la galería Tate. Además, dentro de sus fronteras también se ubican varios museos y galerías de prestigio internacional, como el Museo Británico, la National Gallery de Londres, la Galería Nacional de Escocia, el Museo de Ciencias de Londres o el Museo de Yorkshire, entre otros.[236]​

La arquitectura británica se caracteriza por la combinación ecléctica de distintos estilos arquitectónicos, variando desde aquellos que se encontraban antes de la creación del país, como la arquitectura romana, hasta la arquitectura contemporánea del siglo XXI.[237]​ Irlanda del Norte, Escocia y Gales desarrollaron estilos arquitectónicos únicos y jugaron papeles importantes en la historia de la arquitectura mundial.[237]​ Aunque existen estructuras prehistóricas y clásicas en las islas Británicas, la historia de la arquitectura británica comienza con las primeras iglesias anglosajonas, construidas poco después de la llegada de Agustín de Canterbury a Gran Bretaña en el año 597.[237]​ Desde el siglo XII, la arquitectura normanda se esparció en Gran Bretaña e Irlanda, en forma de castillos e iglesias para ayudar a imponer la autoridad normanda en sus dominios.[237]​ La arquitectura gótica inglesa, que floreció entre 1189 y 1520, fue traída desde Francia, pero rápidamente desarrolló sus propias características.[237]​

Por todo el país, la arquitectura medieval secular se desarrolló en forma de castillos, la mayoría de ellos se encuentra cerca de la frontera entre Inglaterra y Escocia, y datan del siglo XVI, la época de las guerras de independencia de Escocia.[238]​ La invención de las armas de fuego y el cañón hicieron a los castillos inútiles y el renacimiento inglés dio paso al desarrollo de nuevos estilos artísticos para la arquitectura nacional: el estilo Tudor, el barroco inglés y el palladianismo.[238]​ La arquitectura georgiana y neoclásica avanzaron después de la Ilustración Escocesa y a partir de la década de 1930 aparecieron varios estilos modernistas. Sin embargo, la lucha por la conservación de las antiguas estructuras y la resistencia de los movimientos tradicionalistas ha cobrado fuerza, además de ser apoyados por figuras públicas como Carlos de Gales.[239]​

El Reino Unido fue una fuerte influencia en el desarrollo del cine, con los Estudios Ealing que reclaman el título de ser los estudios más antiguos en el mundo. A pesar de una historia de producciones importantes y exitosas, esta industria se caracteriza por un debate en curso sobre su identidad y las influencias del cine estadounidense y europeo. El mercado británico es muy pequeño para que la industria cinematográfica británica pueda producir exitosamente blockbusters al estilo de Hollywood por un período sostenido.[240]​ En comparación con la estadounidense, la industria cinematográfica británica no ha sido capaz de producir éxitos comerciales a nivel internacional; por lo que mantiene una actitud compleja y dividida hacia Hollywood. No obstante, cabe destacar que ocho de las diez películas más taquilleras de todos los tiempos tienen alguna dimensión británica, sea histórica, cultural o creativa: Titanic, dos episodios de El Señor de los Anillos, dos de la trilogía de los Piratas del Caribe y tres películas de la saga de Harry Potter.[241]​

La literatura británica se refiere a la literatura asociada con el Reino Unido, la isla de Man y las islas del Canal, así como a la literatura de Inglaterra, Gales y Escocia antes de la formación del país. La mayor parte de las obras de la literatura británica fue escrita en el idioma inglés. El Reino Unido publica cerca de 206 000 libros cada año, convirtiéndolo en el mayor editor de libros en el mundo.[242]​ La capital de Escocia, Edimburgo, fue declarada como "Ciudad de Literatura" por la UNESCO.[243]​

El poeta y dramaturgo inglés William Shakespeare es ampliamente considerado como el mayor dramaturgo de todos los tiempos.[244]​[245]​[246]​ Entre los escritores en inglés más reconocidos se encuentran Geoffrey Chaucer (siglo XIV), Thomas Malory (siglo XV), Thomas More (siglo XVI) y John Milton (siglo XVII). A Samuel Richardson, escritor del siglo XVIII, se le atribuye la invención de la novela epistolar, además de Daniel Defoe el creador de Robinson Crusoe. En el siglo XIX, siguieron más representantes de la literatura británica: la innovadora Jane Austen, la novelista gótica Mary Shelley, el escritor de cuentos para niños Lewis Carroll, las hermanas Emily, Charlotte y Anne Brontë, el activista social Charles Dickens, el naturalista Thomas Hardy, el poeta visionario William Blake, el poeta romántico William Wordsworth y sir Arthur Conan Doyle creador de Sherlock Holmes.

Los escritores más famosos del siglo XX incluyen al novelista de ciencia ficción H. G. Wells, los escritores de clásicos infantiles Rudyard Kipling y A. A. Milne, el controvertido D. H. Lawrence, la modernista Virginia Woolf, el satírico Evelyn Waugh, el novelista George Orwell, el popular novelista Graham Greene, la novelista policíaca Agatha Christie, el creador de James Bond Ian Fleming, los escritores de fantasía J. R. R. Tolkien, C. S. Lewis y más recientemente J. K. Rowling; así como los poetas Ted Hughes y John Betjeman.

Desde su fundación, el Reino Unido ha estado a la cabeza de los avances científicos y tecnológicos, así como en la investigación y desarrollo. La Royal Society es la sociedad científica más antigua del Reino Unido, y una de las más antiguas en el mundo. Durante sus más de 300 años de historia, se ha encargado de promover, proteger y divulgar el conocimiento y las ciencias en el país y en el mundo entero.[247]​ Dentro de esta sociedad participaron varios científicos que contribuyeron al avance de sus respectivas áreas de conocimiento; entre estos se encuentran: Robert Boyle, John Wallis, Isaac Newton, Robert Hooke, Thomas Willis, entre otros.[248]​ El Consejo de Facilidades para la Ciencia y Tecnología es otro de los organismos encargados de promover y dar apoyo a las investigaciones científicas en el país. Durante los años 2008 y 2009, este consejo invirtió más de 1200 millones de dólares estadounidenses para brindar recursos a varios institutos y sociedades científicas británicas.[249]​ Con respecto a la investigación biomédica, uno de los grandes avances en este país ha sido la secuenciación del genoma de 10 000 personas británicas para conocer las variantes genéticas raras y de baja frecuencia implicadas en la salud y la enfermedad.[250]​

Como país líder de la Revolución Industrial, los inventores del Reino Unido le brindaron al mundo varias innovaciones, principalmente en el campo de la textilería, la maquinaria de vapor, los ferrocarriles y la ingeniería. Dentro de este periodo destacan los inventores George Stephenson, James Watt y Robert Stephenson. Desde entonces, los inventos e inventores británicos han destacado y sido numerosos. Entre estos nuevos innovadores se encuentran Alan Turing, Alexander Graham Bell, John Logie Baird, Frank Whittle, Charles Babbage, Alexander Fleming, entre muchos otros.[251]​ En 2007, el Reino Unido contaba con 79 855 patentes en vigor, el séptimo país con mayor número de ellas.[252]​ La inversión de las empresas del Reino Unido en tecnología y ciencias fue de 9700 millones de USD entre 2010-2015.[253]​

El Reino Unido es famoso por la tradición del "empirismo británico", una rama de la filosofía del conocimiento que indica que el único conocimiento válido es aquel que se comprueba por la experiencia; y de la "Filosofía escocesa", que a veces se denomina el "la escuela escocesa del sentido común". Los filósofos más famosos del empirismo británico son: John Locke, George Berkeley y David Hume, mientras que Dugald Stewart, Thomas Reid y William Hamilton fueron los principales exponentes de la escuela escocesa del sentido común. Gran Bretaña también es notable por una teoría de la filosofía moral, el utilitarismo, usado por primera vez por Jeremy Bentham y posteriormente por John Stuart Mill, en su obra homónima Utilitarismo. Otros eminentes filósofos del Reino Unido y de los Estados que lo precedieron incluyen a Duns Scoto, John Lilburne, Mary Wollstonecraft, Francis Bacon, Adam Smith, Thomas Hobbes, Guillermo de Ockham, Bertrand Russell y Alfred Jules Ayer.

Existen varios estilos musicales bastante populares en el Reino Unido, desde la música folclórica de Inglaterra, Irlanda, Escocia y Gales, hasta el heavy metal. Entre los compositores británicos de música clásica más notables se encuentran: William Byrd, Henry Purcell, Edward Elgar, Gustav Holst, Arthur Sullivan (más conocido por trabajar con el libretista W. S. Gilbert), Ralph Vaughan Williams y Benjamin Britten, pionero de la ópera moderna británica. Peter Maxwell Davies es uno de los compositores vivos más destacados en el país y el actual maestro de música de la reina. También aquí se encuentran varias orquestas sinfónicas y coros de renombre internacional, como la Orquesta Sinfónica de la BBC y el Coro de la Sinfónica de Londres. El compositor barroco Georg Friedrich Händel, aunque nació en Alemania, obtuvo la ciudadanía británica[257]​ y algunas de sus mejores obras, como El Mesías, fueron escritas en inglés.[258]​

Los británicos más prominentes que han influenciado la música popular de los últimos cincuenta años incluyen a The Beatles, Queen, Elton John, David Bowie, Bee Gees, Led Zeppelin, Oasis, Blur, Pink Floyd y The Rolling Stones, todos ellos con ventas que superan los doscientos millones de discos en todo el mundo.[259]​[260]​[261]​[262]​[263]​[264]​[265]​ Asimismo, The Beatles tienen el récord de ventas musicales, con más de mil millones de discos vendidos a nivel internacional.[254]​[255]​[256]​ Un gran número de ciudades británicas son conocidas por su escena musical: estadísticamente, los artistas de Liverpool son los que tienen más éxito en la lista UK Singles Chart.[266]​ Por su lado, la contribución de Glasgow a la escena musical fue reconocida en 2008, cuando fue nombrada por la UNESCO como "Ciudad de la Música", título que comparte con Bolonia, Sevilla y Gante.[267]​

Históricamente, la gastronomía del Reino Unido ha sido etiquetada como «platos desabridos hechos con ingredientes de baja calidad, mezclados con salsas simples para acentuar el sabor, en vez de disfrazarlo.»[268]​ Sin embargo, la cocina británica ha absorbido la influencia cultural de los inmigrantes establecidos en el país, produciendo varios platillos híbridos, como el pollo tikka masala, considerado «el verdadero platillo nacional británico».[269]​

Los platos tradicionales de la cocina británica incluyen el fish and chips, el Sunday roast, el steak and kidney pie y el bangers and mash. La gastronomía del Reino Unido tiene múltiples variantes nacionales y regionales, como son las gastronomías propias de Inglaterra, Escocia y Gales, las cuales han desarrollado su propios platillos regionales, tales como el queso Cheshire, el Yorkshire pudding y el pastel galés. Como en otros países occidentales, el consumo de comida rápida es muy amplio, lo que ha ocasionado un problema de salud pública tan grave como el que sufre Estados Unidos.[270]​

El té es la bebida más popular en el país y de hecho también es una de las tradiciones gastronómicas más conocidas de la cocina del Reino Unido.[271]​ Originada durante el siglo XIX, la tea time (literalmente, «la hora del té», pero mejor traducido como «la hora de la merienda»), no es exclusivamente para consumir té, sino que es una de las comidas centrales de los británicos, similar a una merienda o incluso la cena.[272]​ El tea break y el tea sandwich son dos variaciones de esta comida.[273]​

El deporte es un elemento clave de la cultura británica. Gran cantidad de deportes fueron creados en el Reino Unido, incluyendo el fútbol, el rugby, el tenis y el golf, siendo el primero el deporte más popular en el país. Internacionalmente, Inglaterra, Escocia, Gales e Irlanda del Norte compiten separadamente en la mayoría de los deportes colectivos (aunque Irlanda del Norte en muchos deportes, como es el caso del rugby o el golf, continua unida al resto de la isla de Irlanda), así como en los Juegos de la Mancomunidad. Sin embargo, en algunos deportes el Reino Unido participa como un único equipo, como en el baloncesto.[274]​

En los Juegos Olímpicos, el Reino Unido también participa como un único equipo, representado por el comité olímpico nacional del Reino Unido, la British Olympic Association. El país ha participado en cada una de las ediciones de los Juegos Olímpicos de la era moderna y ha sido anfitrión de tres, de las ediciones de 1908, en donde se ubicó en el primer lugar del medallero, de 1948, de 2012, realizados en Londres.[275]​

Se afirma que el críquet se inventó en Inglaterra (aunque investigaciones recientes sugieren que en realidad originó en Bélgica)[276]​ y la selección inglesa, controlada por la Junta de Críquet de Inglaterra y Gales,[277]​ es el único equipo nacional del Reino Unido con estatus de test críquet. Los miembros de la selección son de nacionalidad galesa e inglesa, a diferencia de las selecciones de otros deportes como el fútbol y el rugby. Algunos norirlandeses y escoceses han jugado para la selección inglesa, debido a que sus respectivas selecciones no cuentan con estatus de test críquet. Todas las naciones constitutivas han competido en la Copa Mundial de Críquet, con Inglaterra llegando a la final en más de tres ocasiones.

Como en otros deportes colectivos, en el rugby Inglaterra, Escocia, Gales e Irlanda del Norte compiten como países separados en las diversas competencias internacionales, pero con la diferencia de que Irlanda del Norte lo hace en conjunto con la República de Irlanda, por lo que existe una selección de rugby de Irlanda que representa a toda la isla. Sin embargo, cada cuatro años los Leones británico-irlandeses, equipo compuesto por jugadores de todo el Reino Unido más Irlanda, hacen una gira por distintas partes del mundo. Mientras la selección de rugby de Inglaterra logró el campeonato de la Copa Mundial de Rugby de 2003, la mejor actuación de Gales ha sido un tercer lugar, Escocia un cuarto lugar e Irlanda ha alcanzado a llegar a los cuartos de final.

Una variante del rugby, el rugby league, también conocido como rugby a 13, se practica en todo el país, pero en el norte de Inglaterra (el lugar donde se originó) es el deporte más importante en muchas áreas, en especial en Yorkshire, Cumbria y Lancashire, aunque también tiene presencia en Londres y Gales del Sur. Anteriormente una selección del Reino Unido representaba al país en competiciones internacionales, pero desde 2008 cada nación cuenta con su propia selección de rugby league.[278]​ En 2013, el Reino Unido será la sede de la Copa Mundial de Rugby League por quinta ocasión.[279]​[280]​

El tenis se inventó en la ciudad de Birmingham entre los años 1859 y 1865. Desde 1877, cada verano se efectúa en Wimbledon (Londres) el Campeonato de Wimbledon, que es el tercer Grand Slam del año. A nivel de logros, el Reino Unido ha alcanzado la Copa Davis en 10 ocasiones, siendo la última la alcanzada en el año 2015, y ha alcanzado el subcampeonato de la Fed Cup en cuatro ocasiones.[281]​

El golf es el sexto deporte más popular del país, en términos de participación. Aunque The Royal and Ancient Golf Club of St Andrews, en Escocia, es la cuna de este deporte, el campo de golf más antiguo del mundo es el Musselburgh Links' Old Golf Course.[282]​ El shinty (o camanachd) es un deporte muy popular en la región escocesa de Highlands, a veces atrayendo a miles de espectadores de toda la nación, especialmente para ver la final del principal torneo, la Copa Camanachd.[283]​

En cuanto al automovilismo, el Reino Unido es uno de los países con mayor participación en este deporte, ya que la mayoría de los equipos de Fórmula 1 tienen su base en el país y los conductores británicos han ganado más títulos en conjunto que ningún otro.[284]​ En el Circuito de Silverstone se organiza anualmente el Gran Premio de Gran Bretaña, válido para la Fórmula 1. Otros eventos automovilísticos que se organizan en el país son el Campeonato británico de Turismos y una fecha del Campeonato Mundial de Rally. Asimismo, el Reino Unido es el hogar de varios de los principales equipos de Fórmula 1, destacándose entre ellos los múltiples campeones de constructores y pilotos McLaren, Williams F1, Team Lotus y Red Bull Racing. En el caso de esta última, a pesar del origen austríaco de la marca de bebidas propietaria del equipo, posee sus cuarteles generales en el Reino Unido, debido a la adquisición que hiciera de la franquicia del exequipo Jaguar F1 para poder ingresar al Campeonato Mundial de Fórmula 1.

Otros deportes populares a escala nacional incluyen las carreras de caballos y el hockey sobre césped. Particularmente en Irlanda del Norte, sobre todo dentro de la población católica, son muy populares el fútbol gaélico y el hurling, ambos regidos por la Asociación Atlética Gaélica.

El fútbol tiene sus orígenes en el Reino Unido, además de que fue en este país donde se formalizó y estandarizó, convirtiéndose en el deporte más popular.[285]​ Cada uno de los países constituyentes posee su propia asociación de fútbol, selección nacional y sistema de ligas independiente, aunque algunos clubes compiten fuera de sus países de origen debido a razones históricas o logísticas.

La cuestión por la que Inglaterra, Gales, Escocia e Irlanda del Norte pueden disputar las competiciones internacionales por separado, y no sucede lo mismo con otras regiones europeas, es por un motivo histórico. En el momento en el que la FIFA fue creada, en 1904, ya existía la Asociación de Fútbol de Inglaterra (Football Association, 1863), la Asociación de Fútbol de Escocia (Scottish Football Association, 1873), la Asociación de Fútbol de Gales (Football Association of Wales, 1876) y la Asociación Irlandesa de Fútbol (Irish Football Association, 1880), cuyas selecciones ya habían disputado partidos internacionales y contaban con sus propias competiciones domésticas. Por eso, en cuanto la FIFA —así como la UEFA, más de cuarenta años después— solicitó a esas federaciones que se afiliaran, estas aceptaron, pero siempre y cuando se mantuvieran intactos sus estatutos, cada uno por separado.[286]​

Distinta es la situación durante los Juegos Olímpicos. El COI dejó en claro desde su fundación, en 1894, que solo iba a permitir la participación de estados soberanos. Existen otros deportes en los que ingleses, escoceses, galeses y norirlandeses van por separado, todos ellos de enorme tradición, y que por supuesto no son olímpicos (hablamos por ejemplo del cricket o el rugby).
Es por este motivo que la isla ha disputado a través de una selección unificada los Juegos Olímpicos realizados entre 1908 y 1972 (bajo el nombre oficial de Gran Bretaña e Irlanda del Norte) y el de 2012, ocasión para la cual se conformó una selección olímpica compuesta mayoritariamente por futbolistas ingleses y algunos galeses, aunque sin escoceses ni norirlandeses.

El sistema de ligas de Inglaterra incluye cientos de ligas interconectadas con miles de divisiones. La máxima categoría, la Premier League, es la liga de fútbol con mayor audiencia en el mundo.[287]​ Bajo la Premier League, está la Football League, que consiste en tres divisiones, y luego la Football Conference, que consiste en una división nacional y dos divisiones regionales. Los equipos ingleses han obtenido buenos resultados en las competiciones europeas, incluyendo los que han ganado la Copa de Europa/Liga de Campeones de la UEFA: Liverpool en seis ocasiones, Manchester United en tres ocasiones, Nottingham Forest y Chelsea en dos y Aston Villa en una. En total, los clubes de Inglaterra han ganado cuarenta competiciones internacionales de la UEFA.[288]​ El principal coliseo deportivo de Inglaterra es el Estadio de Wembley, donde juega de local la selección inglesa de fútbol, que cuenta con una capacidad para 90 000 personas.

El sistema de ligas de Escocia tiene dos ligas nacionales: La Premier League de Escocia, máxima categoría, y la Football League de Escocia, que tiene tres divisiones. Un club de Inglaterra, el Berwick Rangers, compite en el sistema escocés de fútbol. Los equipos más importantes de Escocia son el Celtic Football Club y el Rangers Football Club, ambos de Glasgow: el Celtic se proclamó campeón de la Copa de Europa, actual Champions, en 1967, siendo el primer equipo británico en hacerlo, y el Rangers fue campeón de la Recopa de Europa en 1972. Además, el Heart of Midlothian es el tercer equipo más importante del país y el Aberdeen también fue campeón de la Recopa y de la Supercopa de Europa en 1983. La selección escocesa de fútbol juega de la mayoría de las veces de local en Hampden Park.

El sistema de ligas de Gales se compone de la Welsh Premier League y varias ligas regionales. El equipo de la Welsh Premier League, The New Saints, juega sus encuentros de local en Oswestry, ciudad fronteriza de Inglaterra, mientras tanto, algunos equipos de Gales como el Cardiff City, el Swansea City y el Wrexham, entre otros, compiten bajo el sistema de ligas de Inglaterra. El Millenium Stadium de Cardiff es el estadio en el que juega de local la selección de fútbol de Gales.

El sistema de ligas de Irlanda del Norte incluye la IFA Premiership, que es la máxima división. Un equipo de Irlanda del Norte, el Derry City, compite fuera de las fronteras del Reino Unido, en el fútbol de la República de Irlanda. La selección de fútbol de Irlanda del Norte juega sus partidos de local en el Windsor Park de Belfast.

Ludwig van Beethoven[a]​ (Bonn, Arzobispado de Colonia; 16 de diciembre de 1770[b]​-Viena, 26 de marzo de 1827) fue un compositor, director de orquesta, pianista y profesor de piano alemán. Su legado musical abarca, cronológicamente, desde el Clasicismo hasta los inicios del Romanticismo. Es considerado uno de los compositores más importantes de la historia de la música y su legado ha influido de forma decisiva en la evolución posterior de este arte.

Siendo el último gran representante del clasicismo vienés (después de Christoph Willibald Gluck, Joseph Haydn y Wolfgang Amadeus Mozart), Beethoven consiguió hacer trascender la música del Romanticismo, influyendo en diversidad de obras musicales del siglo xix. Su arte se expresó en numerosos géneros y aunque las sinfonías fueron la fuente principal de su popularidad internacional, su impacto resultó ser principalmente significativo en sus obras para piano y música de cámara.

Su producción incluye los géneros pianístico (treinta y dos sonatas para piano), de cámara (incluyendo numerosas obras para conjuntos instrumentales de entre ocho y dos miembros), concertante (conciertos para piano, para violín y triple), sacra (dos misas, un oratorio), lieder, música incidental (la ópera Fidelio, un ballet, músicas para obras teatrales), y orquestal, en la que ocupan lugar preponderante Nueve sinfonías.

La familia de Beethoven vivía bajo condiciones modestas. Su abuelo paterno, llamado también Ludwig,[c]​ (Malinas, 1712-1773), era descendiente de una familia de campesinos y granjeros originarios de Brabante, en la región de Flandes (Bélgica), que se trasladaron a Bonn en el siglo XVIII. La partícula van de su nombre,[1]​ contrario a lo que pudiera creerse, no posee orígenes nobles, mientras que Beethoven probablemente pudo haberse derivado de Bettenhoven (Fr.: Bettincourt), una localidad de Lieja, aunque otra hipótesis apunta a que el apellido proviene de Beeth (que en flamenco quiere decir «remolacha») y Hoven, que es el plural de Hof («granja»). De esta forma, «Beethoven» vendría a significar «granjas de remolachas».[2]​

En marzo de 1733 su abuelo emigró a Bonn, en donde trabajó como director y maestro de capilla de la orquesta del príncipe elector de Colonia. El 17 de septiembre de ese mismo año contrajo matrimonio con Maria Josepha Poll, cuyos testigos fueron el organista Gilles van den Aeden y Johann Riechler. Tuvieron tres hijos: Maria Bernarda Ludovica (bautizada el 28 de agosto de 1734, muerta el 17 de octubre de 1735), Marcus Josephus (bautizado el 25 de abril de 1736, muerto poco después en una fecha indeterminada) y Johann, de cuyo nacimiento o bautismo no se conserva registro; se supone que nació a finales de 1739 o comienzos de 1740. El tercero fue el único que sobrevivió a la infancia y fue el padre de Beethoven. Johann fue músico y tenor de la corte electoral.

El 12 de noviembre de 1767 se casó en la iglesia de San Remigio en Bonn con Maria Magdalena Keverich (19 de diciembre de 1746-17 de julio de 1787), una joven viuda e hija de un cocinero de Tréveris. Por ese motivo, el matrimonio de sus padres contó con la oposición de su abuelo, que por aquel entonces ya era el prestigioso maestro de capilla de la corte y consideraba a la joven de una clase social inferior a la de su hijo, lo cual no era cierto ya que en su familia había concejales e incluso senadores.[2]​

El matrimonio se trasladó al n.º 515 de la Bonngasse y dos años después, en 1769, nació su primer hijo, bautizado como Ludwig Maria van Beethoven. Sin embargo, apenas seis días después de su bautizo, el niño falleció. El 17 de diciembre de 1770 fue bautizado su segundo hijo, en la iglesia de San Remigio de Bonn, con el nombre de «Ludovicus van Beethoven» (Ludwig van Beethoven) según se describe en el acta de bautismo. Su fecha de nacimiento, generalmente aceptada como el 16 de diciembre de 1770, no cuenta con documentación histórica que pueda respaldarla. Maria Magdalena tuvo aún cinco hijos más, de los que solo sobrevivieron dos: Kaspar Anton Karl van Beethoven, bautizado el 8 de abril de 1774, y Nikolaus Johann van Beethoven, bautizado el 2 de octubre de 1776. Los biógrafos no tienen claras las fechas de nacimiento exactas de ninguno de los hijos de Maria Magdalena Keverich.[3]​

El padre de Beethoven estaba muy gratamente impresionado por el hecho de que Wolfgang Amadeus Mozart diese conciertos a los siete años y quería que su hijo siguiera sus pasos. Con la intención de hacer de Ludwig un nuevo niño prodigio, comenzó a enseñarle piano, órgano y clarinete a temprana edad.[4]​ Sin embargo, el estudio musical coartó el desarrollo afectivo del joven, que apenas se relacionaba con otros niños.[5]​ En mitad de la noche, Ludwig era sacado de la cama y era obligado a tocar el piano para los conocidos de Johann, a quienes quería impresionar; esto causaba que estuviera cansado en la escuela. Era habitual que dejara de asistir a clases y se quedara en casa para practicar música.

El padre era alcohólico, lo que supuso que perdiera el puesto de director de la orquesta de Bonn —puesto heredado del abuelo Ludwig—, y la madre estaba frecuentemente enferma. Aunque la relación con Johann era distante, Ludwig amaba mucho a su madre, a la que denominaba su «mejor amiga».[4]​

El 26 de marzo de 1778, cuando tenía siete años, Beethoven realizó su primera actuación en público en Colonia. Su padre afirmó que la edad de Ludwig era de seis años, para destacar, de esta manera, la precocidad de su hijo; por ello, siempre se creyó que Beethoven era más joven de lo que era en realidad. Debido a que el talento musical y pedagógico de su padre era limitado, Ludwig comenzó a recibir clases de otros profesores. Sus avances fueron significativos, sobre todo en la interpretación del órgano y la composición, guiado por músicos experimentados como Christian Gottlob Neefe. Neefe fue un profesor muy importante e influyente en su instrucción y supo valorar inmediatamente el nivel excepcional de Ludwig. Además de transmitirle conocimientos musicales, Neefe dio a conocer a Beethoven las obras de los pensadores más importantes, tanto antiguos como contemporáneos.[4]​

En 1782, cuando contaba once años de edad, Beethoven publicó su primera composición, titulada Nueve variaciones sobre una marcha de Ernst Christoph Dressler (WoO 63). Un año después, Neefe escribió en la Revista de Música acerca de su alumno: «Si continúa así, como ha comenzado, se convertirá seguramente en un segundo Wolfgang Amadeus Mozart».[6]​En junio del siguiente año, Ludwig fue contratado como intérprete de viola en la orquesta de la corte del príncipe elector de Colonia Maximiliano Francisco, por recomendación de Neefe. Este puesto le permitió frecuentar la música de los viejos maestros de capilla, además de facilitarle la entrada en nuevos círculos sociales, en los que se encontraban algunos de los que serían amigos suyos durante toda su vida, como la familia Ries, los von Breuning (en cuya casa conoció a los clásicos y aprendió a amar la poesía y la literatura) o el doctor Franz Gerhard Wegeler (con quien años más tarde se volvería a encontrar en Viena).[4]​

Beethoven encontró una vía de escape de la presión familiar en 1787 cuando, con 16 años, marchó a la capital austriaca apoyado por su mecenas, el conde Ferdinand von Waldstein, quien sufragó los gastos del viaje y, lo más importante, lo convenció de sus posibilidades de éxito. Parece que durante este viaje a Viena tuvo lugar un fugaz encuentro con Mozart. En relación con este encuentro, solo existen textos de discutible autenticidad. De cualquier modo, la leyenda dice que Mozart habría dicho: «Recuerden su nombre, este joven hará hablar al mundo».[4]​

Al poco tiempo, su madre enfermó gravemente de tuberculosis y su padre le pidió por carta que regresara a Bonn inmediatamente. La madre murió el 17 de julio de 1787. Tras este hecho, su padre entró en una depresión y su alcoholismo se agravó, llegando a ser detenido y encarcelado por este hecho. Después de esto, el joven Ludwig tuvo que responsabilizarse de sus jóvenes hermanos y se vio obligado a mantenerlos, tocando el violín en una orquesta y dando clases de piano durante cinco años, mientras que su padre seguía preso. Su padre falleció finalmente el 18 de diciembre de 1792.[4]​

En 1792 el príncipe elector de Bonn volvió a financiarle un viaje a Viena, ciudad en la que permaneció el resto de su vida componiendo, tratando de alcanzar un reconocimiento social a su persona por medio del arte y sufriendo un mal particularmente terrible para él: la sordera. Allí, Beethoven recibió clases de composición con Joseph Haydn, de contrapunto con Johann Georg Albrechtsberger y Johann Baptist Schenk y de lírica con Antonio Salieri.[4]​

Durante este período tuvo varios duelos musicales con otros pianistas. El primero fue en 1792 ―con 21 años de edad―, durante un viaje con la orquesta de la corte, en el cual tocó con Franz Sterkel, ejecutando obras de dicho compositor. En 1800 tuvo lugar el famoso duelo en el palacio de Lobkowitz, en el que el pianista y compositor Daniel Steibelt lo retó a que tocasen juntos. En dicha ocasión, Beethoven tomó partituras de una obra de este, modificándolas al mismo tiempo que las iba tocando, con tanta gracia que Steibelt declaró que no volvería a Viena mientras Beethoven viviera allí y abandonó la ciudad, radicándose en París.[4]​

Con veinticuatro años publicó su primera obra importante: tres tríos para piano, violín y violonchelo (Opus 1), y el año siguiente, en 1795, realizó su primer concierto público en Viena como compositor profesional, en el que interpretó sus propias obras. Ese mismo año le propuso matrimonio a Magdalena Willman, pero esta se negó. Posteriormente, realizó una gira por Praga, Dresde, Leipzig, Berlín y Budapest. En 1796 publicó tres sonatas para piano (Opus 2). La corte, la nobleza y la Iglesia vienesas acogieron la música de Beethoven y se convirtieron en mecenas y protectores del joven músico. Eran frecuentes las disputas entre estos estamentos y el compositor, debido al carácter fuerte e impulsivo del músico, pero este hecho le hizo granjearse un gran respeto en la ciudad. Entre sus mecenas se encontraban personalidades como el príncipe Karl von Lichnowsky y el barón Gottfried van Swieten.[4]​ Por esa época se desligó de Haydn, con el que no coincidía musicalmente pero a quien, a pesar de esto, dedicó los tres tríos.

En 1800, Beethoven organizó un nuevo concierto en Viena en el que realizó la presentación de su Primera sinfonía. Su actividad musical iba en aumento y también impartió clases de piano entre las jóvenes aristócratas, con las que mantuvo romances esporádicos. Al año siguiente, Beethoven se confesó preocupado por su creciente sordera a su amigo Wegeler. En Heiligenstadt, el año siguiente, escribió el conocido Testamento de Heiligenstadt, en el que expresa su desesperación y disgusto ante la injusticia de que un músico pudiera volverse sordo, algo que no podía concebir ni soportar. Incluso llegó a plantearse el suicidio, pero la música y su fuerte convicción de que podía hacer una gran aportación al género hicieron que siguiera adelante. En dicho testamento escribió que sabía que todavía tenía mucha música por descubrir, explorar y concretar.[4]​

Su música inicial, fresca y ligera, cambió para convertirse en épica y turbulenta, acorde con los tiempos revolucionarios que vivía Europa. Eran años en que las potencias monárquicas europeas se habían aliado para derrotar a la Francia revolucionaria. En una deslumbrante campaña en el norte de Italia, en la que el ejército austríaco fue derrotado, adquirió notoriedad Napoleón Bonaparte, que se convirtió en un ídolo entre los sectores progresistas. De esta época son la Sonata para piano n.º 8, llamada Patética, y la Sonata para piano n.º 14, llamada Claro de luna. Su Tercera sinfonía, llamada La Heroica (traducción de la denominación en italiano Eroica), estaba escrita en un principio en «memoria de un gran hombre», Napoleón, que era visto en ese momento como un liberador de su pueblo. Cuando se declaró a sí mismo emperador, Beethoven se enfureció y borró violentamente el nombre de Napoleón de la primera página de la partitura. La Heroica se estrenó finalmente el 7 de abril de 1805.[4]​

Muy pronto, Beethoven dejó de necesitar los conciertos y recitales en los salones de la corte para sobrevivir. Los editores se disputaban sus obras; además, la aristocracia austriaca, quizás avergonzada por la muerte de Wolfgang Amadeus Mozart en la pobreza, le asignó una pensión anual. Debido a la pérdida de sus capacidades auditivas, se entregó a una febril actividad creadora, y, a la par, sufrió penalidades personales producidas por dos desengaños amorosos. No llegó a casarse nunca, pero se le atribuyen varios romances, sobre todo entre damas de la nobleza. Antonie von Birkenstock, casada con el banquero alemán Franz Brentano, fue uno de los grandes amores de su vida.[7]​

Entre 1804 y 1807, estuvo enamorado de la joven y bella condesa Josephine Brunswick, viuda de Joseph Graf Deym. Su amor era correspondido por parte de la condesa, pero este no pudo concretarse debido a las rígidas restricciones sociales de la época y a la estricta separación entre la nobleza y el vulgo, por lo que la relación cesó. Durante este período, Beethoven había terminado Leonore, su única ópera. Compuso hasta cuatro oberturas diferentes y finalmente cambió el nombre de dicha ópera a Fidelio, en contra de sus deseos. El 20 de noviembre de 1805 fue la fecha de la primera representación, que tuvo poca afluencia de público, ya que esa misma semana las tropas de Napoleón habían entrado por primera vez en Viena. En los años siguientes, Beethoven incrementó su actividad creadora y compuso muchas obras, entre ellas la Quinta sinfonía, la Sexta sinfonía o Sinfonía Pastoral, la Obertura Coriolano y la bagatela para piano Para Elisa.[4]​

Sus apariciones en público eran cada vez más infrecuentes. El 22 de diciembre de 1808 Beethoven dio uno de sus últimos conciertos en vivo, en una larga jornada que incluyó el estreno de la Fantasía para piano, orquesta y coro Op. 80, las sinfonías Quinta y Sexta, el Concierto para piano n.º 4 Op. 58, el aria Ah perfido! y tres movimientos de la Misa en do mayor Op. 86. Tuvo como alumno al archiduque Johann Joseph Rainer Rudolph, hermano del emperador, y eventualmente se convirtió también en su más grande benefactor. En 1809 Beethoven no estaba conforme con su situación en Viena, especialmente bajo el aspecto económico. Entonces se planteó la invitación de Jerónimo Bonaparte, para dejar Viena y trasladarse a Wesfalia. Su vieja amiga la condesa Anna Marie Erdödy logró convencer a Beethoven para que se mantuviera en Viena con la ayuda de sus más ricos admiradores, entre los que se encontraban el archiduque Rudolf, el príncipe Lobkowitz y el príncipe Kinsky, que ofrecieron a Beethoven una pensión anual de 4000 florines, lo que le permitió vivir sin preocupaciones económicas. La única condición que le pusieron fue no abandonar la ciudad de Viena, condición aceptada por el compositor. Dicha pensión lo convirtió en el primer artista y compositor independiente de la historia, ya que anteriormente la mayoría de los músicos y compositores (Haydn y Mozart incluidos) eran sirvientes en las casas de la aristocracia, formando parte de su personal doméstico y componiendo e interpretando según sus amos les pedían. En cambio, las condiciones del arreglo al que llegó Beethoven con sus benefactores daban libertad al compositor de componer lo que él quisiera, y cuando él quisiera.[4]​

En 1812, Beethoven se trasladó al balneario de Teplitz (Teplice) y durante su estancia escribió la carta a su «Amada inmortal», que provocó multitud de especulaciones sobre su destinataria aunque nunca se ha podido averiguar con exactitud. En 1977, el musicólogo estadounidense Maynard Solomon afirmó que la carta iba dirigida a Antonie Brentano, la esposa de un mercader de Fráncfort del Meno y madre de cuatro hijos. Debido a su sentido ético y su miedo al matrimonio, Beethoven abandonó esta relación, a pesar de los conflictos emocionales que le causó.[8]​ En julio de ese año, Bettina von Arnim organizó un encuentro entre el compositor y Johann Wolfgang von Goethe. Más tarde, la condesa publicó su correspondencia con Goethe y en una de sus cartas al conde Hermann von Pückler-Muskau relató cierto suceso que al parecer habría ocurrido en dicho balneario ese mismo verano, cuando Beethoven y Goethe se encontraron por primera vez. Ambos paseaban por la alameda del balneario y de pronto apareció frente a ellos la emperatriz María Luisa de Austria-Este con su familia y la corte. Goethe, al verlos, se hizo a un lado y se quitó el sombrero. En cambio, el compositor se lo caló todavía más y siguió su camino sin reducir el paso, haciendo que los nobles se hicieran a un lado para saludar. Cuando estuvieron a cierta distancia se detuvo para esperar a Goethe y decirle lo que pensaba de su comportamiento «de lacayo».

Según Elisabeth von Arnim, el mismo Beethoven le habría contado esta anécdota. Sin embargo, su veracidad es muy discutida y hoy existe un cierto acuerdo en considerarla, si no por completo al menos en buena parte, invento de Elisabeth. En su carta a von Pückler-Muskau, le pregunta si le gusta la historia, Kannst du sie brauchen? («¿Puedes utilizarla?»). Von Arnim, sin embargo, decidió utilizarla ella misma, y en 1839 publicó en la revista Athenäum una carta, supuestamente de Beethoven, en la que este contaba la anécdota. El original de esta carta no apareció nunca; solo la copia y algunos detalles (como la fecha) indican que Beethoven no la escribió nunca, o al menos no tal como fue transcrita. Independientemente de su autenticidad, el incidente encantó a la sociedad vienesa, que lo creyó verdadero durante mucho tiempo.[9]​

Beethoven había entablado contacto con el inventor Johann Mäzel, que le construyó varios instrumentos para ayudarlo con sus dificultades auditivas, como cornetas acústicas o un sistema para escuchar el piano. Su obra orquestal La victoria de Wellington fue compuesta en 1813 para ser interpretada con un panarmónico, otro de los inventos de Mäzel. Esta obra era un homenaje a la victoria sobre los ejércitos napoleónicos en la batalla de Vitoria por parte del duque de Wellington y alcanzó gran popularidad, además de volver verdaderamente famoso al compositor, lo que le procuró grandes ingresos. Sin embargo, él mismo la calificó como «basura» (algo que no dijo de ninguna otra obra suya) y hoy está completamente olvidada. El invento de Mäzel que más impresionó al compositor fue el metrónomo, y escribió cartas de recomendación a editores y comenzó a realizar anotaciones en las partituras con los tiempos del metrónomo para que sus obras se interpretaran al tempo que él había concebido. En esa época comenzaron los problemas económicos del compositor, ya que uno de sus mecenas, el príncipe Lobkowitz, sufrió una quiebra económica y el príncipe Kinsky falleció al caerse de su caballo, tras de lo cual sus herederos decidieron no pagar las obligaciones financieras que el príncipe había contraído con el músico.[4]​

En 1814, acabó las Séptima y Octava sinfonías y reformó la ópera Fidelio, que fue un gran éxito, tanto de afluencia de público como económico, al igual que el resto de conciertos que realizó en esa época. Ese mismo año tuvo lugar el Congreso de Viena, que reunió en la ciudad a numerosos mandatarios que decidían el futuro de Europa después de la derrota de Napoleón. Este fue uno de los momentos de gloria de Beethoven, ya que fue invitado en muchas ocasiones a participar en los múltiples conciertos que se dieron en las celebraciones y fue recibido con admiración y reconocimiento.[4]​

Algunas fuentes apuntan a que el último concierto público de Beethoven tuvo lugar el 11 de abril de ese mismo año (1814)[10]​ y consistió en el estreno del Trío op. 97, junto al violinista Ignaz Schuppanzigh y el violonchelista Joseph Lincke.[11]​

Tras la muerte de su hermano Kaspar Karl el 15 de noviembre de 1815, tomó la decisión de acoger a su sobrino Karl, de nueve años de edad, en contra de la voluntad de su cuñada. En los años comprendidos entre 1815 y 1820, dedicó gran parte de sus energías y su tiempo a la batalla legal para ganar la custodia de su sobrino Karl. Este esfuerzo le supuso dejar prácticamente de componer (lo que no le impidió escribir seis ciclos de Lieder y la sonata Hammerklavier). En el testamento del hermano se le establecía a él como tutor de Karl, pero en el lecho de muerte, a petición de la cuñada, se estableció una tutoría conjunta. Ludwig, quien aborrecía a su cuñada, tuvo que llevar su causa ante la justicia. Los tribunales ordinarios no lo conocían y le costaba hacer valer sus influencias, aunque finalmente ganó el caso, y desde entonces se dedicó a la formación musical de Karl con falsas esperanzas, ya que el chico no tenía dotes musicales. Uno de los profesores con los que contó su sobrino fue Carl Czerny, que posteriormente fue profesor de Franz Liszt y antes había sido alumno del propio Beethoven. Además, la relación con su hijo adoptivo no era excelente; constantemente tenía que encontrarle nuevos tutores, ya que tenía conflictos con ellos, y este escapaba con su madre y peleaba constantemente con el tío. La preocupación por el dinero, que acompañó a Beethoven desde los días de la infancia en que tuvo que proveer para la familia, lo ocupó en este periodo como nunca. Los editores no confiaban en él, pues no cumplía sus promesas de exclusividad y pedía constantemente más dinero por sus obras. Según su biógrafo, Emil Ludwig, de este periodo no hay ni una sola carta en la que no se traten, al menos tangencialmente, problemas de dinero.[12]​

Después de 1815, Napoleón fue definitivamente derrotado y el canciller austriaco Klemens von Metternich instauró un régimen policial para impedir rebrotes revolucionarios. Beethoven fue una voz crítica del régimen. En esa época, su nombre era muy respetado en el Imperio y en Europa Occidental, sobre todo en Inglaterra, en parte gracias al éxito de La victoria de Wellington. Pero el ascenso de Gioachino Rossini y la ópera italiana, que Beethoven consideraba poco seria, lo colocó en segundo plano.

En 1816, realizó el primer esbozo de la Novena sinfonía y dos años más tarde su antiguo alumno y benefactor, el archiduque Rudolf, fue nombrado cardenal, motivo por el cual Beethoven comenzó a componer la Misa en re, aunque no estuvo terminada antes de la ceremonia de entronización. En 1822, Beethoven tuvo un encuentro con Rossini en Viena, ciudad en la que este estaba cosechando grandes éxitos. Debido a las dificultades con el idioma y la sordera de Beethoven, el encuentro fue breve.[4]​

Beethoven pasó los últimos años de su vida casi totalmente aislado por la sordera, relacionándose solamente con algunos de sus amigos a través de los «cuadernos de conversación», que le sirvieron como medio de comunicación. Su último gran éxito fue la Novena sinfonía, terminada en 1823. En los tres años finales, se dedicó a componer cuartetos de cuerda y la Missa Solemnis. El 13 de abril de ese año conoció a Franz Liszt, que entonces tenía once años, durante un concierto del compositor húngaro y lo felicitó por su interpretación. Años más tarde, Liszt transcribió todas las sinfonías de Beethoven para piano y fue un destacado intérprete de su obra. El estreno de la Novena sinfonía tuvo lugar el 7 de mayo de 1824 y fue un rotundo éxito a pesar de las dificultades técnicas que entrañaba la obra. Este éxito no se tradujo en una ganancia financiera y los problemas económicos continuaron acuciando al compositor, que aunque tenía el dinero que estaba ahorrando, no lo podía utilizar ya que estaba destinado como herencia para su sobrino.[4]​

La salud del maestro decayó inexorablemente durante su estancia en la casa de su hermano en Gneixendorf, a pesar de los cuidados de su familia. Su hermano Nikolaus Johann recordaba: «Al almuerzo comía únicamente huevos pasados por agua, pero después bebía más vino, y así a menudo padecía diarrea, de modo que se le agrandó cada vez más el vientre, y durante mucho tiempo lo llevó vendado». Tenía edemas en los pies y se quejaba continuamente de sed, dolores de vientre y pérdida de apetito.[13]​ En esa época, comenzó la composición de la Décima sinfonía.

El 1 de diciembre de 1826, Beethoven y Karl volvieron a Viena. La premura de la decisión determinó que carecieran de un transporte adecuado y solamente pudieron conseguir un viejo carromato descubierto. El viaje resulta catastrófico para una persona en el estado en que se encontraba Beethoven, quien llevaba ropa de verano y se vio obligado a pasar la noche en una taberna de la aldea, donde la habitación no tenía calefacción ni persianas que lo protegieran del frío. Hacia la medianoche sufrió un escalofrío febril y comenzó una tos seca acompañada de sed intensa y fuertes dolores en los costados. Estando así, el maestro bebió grandes cantidades de agua helada que solo agravaron su condición. Sin embargo, logró recuperarse de su crisis gracias a la atención del doctor Wawruch y consiguió llegar a la capital. El 20 de diciembre, se le extrajeron fluidos abdominales. Karl permaneció durante todo el mes a su lado hasta su incorporación, en enero, a su regimiento. El joven se había reconciliado totalmente con su tío tras el lamentable episodio del suicidio:[d]​ «Mi querido padre: vivo satisfecho y solo me pesa verme separado de ti».

Casi en la miseria, a pesar de tener una gran fortuna en acciones financieras, escribió a sus amigos en Londres para pedir algún dinero. La respuesta llegó de inmediato, junto con cien libras esterlinas prestadas incondicionalmente. Cuando se difundió en Viena el estado terminal de Beethoven, todos sus antiguos amigos que aún vivían acudieron a su domicilio de la Schwarzspanierhaus para expresarle sus deseos de una pronta recuperación, aunque en realidad su propósito era despedirse del envejecido compositor.[14]​

A pesar de los cuidados de su médico y el cariño de sus amigos, la maltrecha salud del músico, que había padecido problemas hepáticos durante toda su vida, empeoró. Esos últimos días le acompañaron Franz Schubert, quien en realidad no se atrevió a visitar al maestro, pero un amigo de ambos le mostró al moribundo las partituras de sus lieder, que Beethoven tuvo oportunidad de admirar y se le atribuye la frase: «Es verdad que en este Schubert se encuentra una chispa divina», recordando el comentario que sobre él hiciera Mozart y haciendo el cumplido que no hiciera a ningún otro músico.[15]​ El 20 de marzo escribió: «Estoy seguro de que me iré muy pronto». Y el día 23, entre los estertores del moribundo, algunas fuentes indican que exclamó: «Aplaudid amigos, comedia finita est» («La comedia ha terminado»), un final típico de la comedia del arte, aunque en 1860 Anselm Hüttenbrenner negó que Beethoven hubiera pronunciado tales palabras.[16]​ Esa misma tarde, tomó la pluma para designar a su sobrino Karl legatario de todos sus bienes.

Al día siguiente, 24 de marzo de 1827, Beethoven recibió la extremaunción y la comunión según el rito católico. Cabe señalar que las creencias personales de Beethoven fueron muy poco ortodoxas. Esa misma tarde entró en coma para no volver a despertar hasta dos días más tarde. Su hermano Nikolaus Johann, su cuñada y su admirador incondicional Anselm Hüttenbrenner lo acompañaron al final, ya que sus pocos amigos habían salido a buscar una tumba. Sus últimas palabras fueron dirigidas al vino del Rin, que llegó después de mucho esperar el encargo, y que se esperaba surtiera buenos efectos sobre la salud del músico: «Demasiado tarde, demasiado tarde...».[17]​ Hüttenbrenner relató los últimos momentos del compositor el 27 de marzo de 1827 de la siguiente forma:

Tres días después de su fallecimiento, el 29 de marzo, tuvo lugar el funeral. Se celebró en la iglesia de la Santa Trinidad, distante un par de manzanas del domicilio de Beethoven, y en él se interpretó el Réquiem en re menor de Wolfgang Amadeus Mozart. Al mismo asistieron más de 20 000 personas, entre las que se encontraba Schubert, gran admirador suyo. El actor Heinrich Anschütz leyó la oración fúnebre, que fue escrita por el poeta Franz Grillparzer, a las puertas del cementerio de Währing, ahora Schubert Park.[4]​

En su escritorio de trabajo se encontró el Testamento de Heiligenstadt, redactado en 1802, en donde explica a sus hermanos el porqué de su profunda amargura. También se encontró la mencionada desgarradora carta de amor dirigida a su «Amada inmortal», a la que llama «mi ángel, mi todo, mi mismo yo».[4]​

A lo largo de su vida, Beethoven visitó gran cantidad de médicos para curar sus diversas dolencias físicas, como mala digestión, dolor abdominal crónico, cirrosis hepática, nefropatía, pancreatitis crónica, irritabilidad, depresión, así como otros síndromes sin etiología demostrada, tales como alteraciones gastrointestinales, bronquiales, articulares y oculares.[19]​ En una carta a un amigo, expresó su deseo de que, después de su muerte, sus restos fueran usados para determinar la causa de su enfermedad y evitar que otros padecieran su mismo sufrimiento.[20]​ Desde su fallecimiento, a los cincuenta y seis años de edad, en 1827, han existido muchas especulaciones y estudios sobre las causas de sus múltiples dolencias y su muerte, pero no han conseguido determinar una patología sistémica para explicar al menos gran parte de sus síndromes.[20]​[19]​

Estudios realizados en el Centro de Tratamiento Pfeiffer en Warrenville (Illinois) en 2005 han revelado algunos datos sobre las causas de su muerte. Los análisis de un mechón de su cabello y de un fragmento de su cráneo dieron como resultado la existencia de altas concentraciones de plomo, lo que indica que el compositor podría haber padecido saturnismo. Aparentemente, Beethoven ingería agua contaminada con plomo, que se obtenía de un arroyo campestre creyendo que tenía propiedades curativas. Las últimas investigaciones señalan que fue por el plomo de los vasos que usaba para beber. A raíz de dichos estudios, se pudo saber que el compositor padecía problemas estomacales desde los veinte años y que, en ocasiones, sufría crisis depresivas.[21]​ Los problemas estomacales y el cambio de personalidad que sufrió a los veinte años concuerdan con el diagnóstico de envenenamiento por plomo.[20]​ En estos estudios, también se destaca la ausencia de niveles perceptibles de cadmio o mercurio en el mechón y el hueso, elementos a los que previamente se había identificado como causantes de la enfermedad de Beethoven. La presencia de plomo en el cráneo confirmó que la exposición del músico a dicho elemento no fue un hecho puntual en su vida, sino que tuvo que estar sometido a él durante muchos años. Aunque existen algunos extraños casos de sordera provocados por envenenamiento por plomo, no hay ninguna evidencia sólida que sustente que fuese esta la causa de la que padeció Beethoven.[20]​

Otro estudio afirma que Beethoven falleció a causa de una insuficiencia hepática que desencadenó un coma hepático. Además, puede que con probabilidad padeciera una septicemia final, con el antecedente de una ascitis fistulizada e infectada. No se puede afirmar con certeza si una insuficiencia renal y una diabetes descompensada fueron motivo de su fallecimiento. En la medicina moderna, la causa de su muerte sería catalogada como un fallo multisistémico, originado por su insuficiencia hepática.[19]​

La vida personal de Beethoven fue problemática debido a su creciente sordera, que lo llevó a plantearse el suicidio, según afirmó en los documentos encontrados en el Testamento de Heiligenstadt. A menudo, Beethoven era irascible y puede que sufriera trastorno bipolar.[22]​ Sin embargo, tuvo un círculo íntimo de amigos fieles durante toda su vida, quizás atraídos por la fortaleza de su reputada personalidad. Hacia el final de su vida, los amigos de Beethoven compitieron en sus esfuerzos para ayudar al compositor a paliar sus incapacidades físicas.[23]​

Hay numerosas evidencias del desdén que sentía Beethoven hacia la autoridad y el sistema de clases sociales. Detenía su interpretación al piano si su audiencia comenzaba a hablar entre sí o si dejaban de prestarle total atención. En los eventos sociales, se negaba a interpretar si le invitaban a hacerlo sin previo aviso. Finalmente, y después de muchas confrontaciones, el archiduque Rudolf decretó unas normas básicas de etiqueta en la corte que no afectaban a Beethoven.[23]​

Beethoven es reconocido como uno de los más grandes compositores de la historia. Ocasionalmente, es mencionado como parte de «Las tres bes» (junto con Bach y Brahms), quienes personalizan esta tradición. También es la figura central de la transición entre el clasicismo musical del siglo XVIII y el romanticismo del siglo XIX, por la profunda influencia que ejerció sobre las siguientes generaciones de músicos.[23]​

Beethoven compuso obras en una amplia variedad de géneros y para una amplia gama de combinaciones de instrumentos musicales. Sus obras para orquesta sinfónica incluyen nueve sinfonías (la Novena sinfonía incluye un coro) y alrededor de una docena de piezas de música «ocasional». Compuso nueve conciertos para uno o más instrumentos solistas y orquesta, así como cuatro obras cortas que incluyen a solistas acompañados de orquesta. Fidelio es la única ópera que escribió y entre sus obras vocales con acompañamiento orquestal se incluyen dos misas y una serie de obras cortas.[8]​

Compuso un amplio repertorio de obras para piano, entre ellas treinta y dos sonatas para piano y numerosas obras cortas, incluidos los arreglos (para piano solo o dúo de piano), de algunas de sus otras obras. Las obras en las que usa el piano como instrumento de acompañamiento incluyen diez sonatas para violín, cinco sonatas para violonchelo y una sonata para corno francés, así como numerosos lieder.[8]​

La cantidad de música de cámara que produjo Beethoven fue notable. Además de los dieciséis cuartetos de cuerda, escribió cinco obras para quinteto de cuerda, siete para trío con piano, cinco para trío de cuerda y más de una docena de obras para gran variedad de combinaciones de instrumentos de viento.[8]​

Según el escritor ruso Wilhelm von Lenz, la carrera como compositor de Beethoven se divide en tres periodos: temprano, medio y tardío.[23]​ El periodo temprano abarca hasta alrededor de 1802, el periodo medio se extiende desde 1803 hasta cerca de 1814 y el periodo tardío va desde 1815 hasta el fallecimiento del compositor. Esta división de Lenz, realizada en Beethoven et ses trois styles (1852), ha sido utilizada ampliamente por otros musicólogos, con ligeros cambios, desde entonces.[24]​[25]​

En su período temprano, el trabajo de Beethoven estuvo fuertemente influido por sus predecesores, Joseph Haydn y Wolfgang Amadeus Mozart, pero también exploró nuevas direcciones y gradualmente amplió el alcance y la ambición de su obra. Algunas obras importantes de este periodo son la Primera y Segunda sinfonías, un conjunto de seis cuartetos de cuerda (Opus 18), los primeros dos conciertos para piano (los n.º 1 y n.º 2) y la primera docena de sonatas para piano, incluyendo la famosa Sonata Patética, Op. 13.[26]​

El periodo medio, también llamado heroico, comienza después de la crisis personal provocada por la creciente sordera del músico. Es destacada por las obras de gran escala que denotan el heroísmo y la lucha. Las composiciones de este periodo incluyen seis sinfonías (las n.º 3, 4, 5, 6, 7 y 8), los últimos tres conciertos para piano, el Triple concierto y el Concierto para violín, cinco cuartetos de cuerda (n.º 7 al 11), varias sonatas para piano (incluyendo las sonatas Claro de luna, Waldstein y Appassionata), la Sonata Kreutzer para violín, y su única ópera, Fidelio.[27]​[28]​

El periodo tardío comienza alrededor de 1815. Las obras de dicho periodo se caracterizan por su profunda carga intelectual, sus innovaciones formales y su intensidad, expresión sumamente personal. El Cuarteto de cuerda n.º 14, Op. 131 tiene siete movimientos enlazados y la Novena sinfonía incorpora la fuerza coral a una orquesta en el último movimiento.[23]​ Otras composiciones de este periodo son la Missa Solemnis, los cinco últimos cuartetos de cuerda (incluyendo la Grosse fugue) y las cinco últimas sonatas para piano.[27]​

En su prolífica trayectoria musical, Beethoven dejó para la posteridad un importante legado: nueve sinfonías, una ópera, dos misas, tres cantatas, treinta y dos sonatas para piano, cinco conciertos para piano, un concierto para violín, un triple concierto para violín, violonchelo, piano y orquesta, dieciséis cuartetos de cuerda, una gran fuga para cuarteto de cuerda, diez sonatas para violín y piano, cinco sonatas para violonchelo y piano e innumerables oberturas, obras de cámara, series de variaciones, arreglos de canciones populares y bagatelas para piano.

Beethoven compuso nueve sinfonías a lo largo de su trayectoria musical. Entre ellas se destacan la Tercera sinfonía, también llamada en castellano Heroica,[e]​ en mi♭ mayor, la Quinta sinfonía, en do menor y la Novena sinfonía, en re menor (cuyo cuarto movimiento está basado en la Oda a la Alegría, escrita por Friedrich von Schiller en 1785).

Compuso su Primera sinfonía entre 1799 y 1800, cuando tenía 30 años de edad, y continuó componiendo sinfonías hasta su muerte.

Existe controversia sobre la existencia de una Décima sinfonía, en la que estaría trabajando Beethoven cuando falleció.

Las diez oberturas de Beethoven son piezas cortas que, posteriormente, serían ampliadas y trabajadas para su incorporación en obras mayores. En el fondo, es música compuesta para musicalizar ballets (Las criaturas de Prometeo) y obras de teatro: Coriolano de William Shakespeare, Egmont de Johann Wolfgang von Goethe, etc. Se trata de composiciones cerradas y uniformes que expresan emociones e ideas llenas de heroísmo. El tema de la libertad está muy presente en este apartado de la producción del músico de Bonn. Por ejemplo, la Obertura Coriolano (Op. 62) ilustra musicalmente el drama homónimo de Shakespeare basado en el héroe que tiene que escoger entre la libertad de conciencia y su lealtad a las leyes romanas, Leonora n.º 3 (Op. 72a), por su parte, es una obertura destacada de las cuatro escritas para la ópera Fidelio. De idéntica valía son Las criaturas de Prometeo (Op. 43) y Egmont (Op. 84), siendo esta última un buen ejemplo de la típica composición beethoveniana.

Cada concierto para piano de Beethoven es distinto y desarrolla una escritura pianística de gran virtuosismo (él mismo fue un gran virtuoso en su juventud). Quizá el más famoso sea el Concierto para piano n.º 5 «Emperador», de 1809, en donde el virtuosismo y el sinfonismo se combinan a la perfección. Es una composición épica que tiene un originalísimo arranque y soberbias cadencias. El origen del sobrenombre de este concierto no se lo puso el propio compositor sino que se lo asignaron los primeros asistentes como público, dada la grandeza y majestuosidad de la obra.[29]​

El primer y segundo conciertos para piano destacan por su concepción alegre, mientras que el Concierto para piano n.º 3, de 1801, de tono serio, es de una amplitud y calidad incomparables. Por su parte, el Concierto para piano n.º 4, Op. 58, de 1808, apuesta por la profundidad lírica y ha sido considerado, por la Allgemeine musikalische Zeitung de mayo de 1809, como el «mejor concierto para instrumento solo jamás compuesto».[30]​ En cuanto a los conciertos en los que participan otros instrumentos, hay que señalar el Concierto para violín y el Triple concierto para violín, violonchelo, piano y orquesta, en donde Beethoven sustituye el sinfonismo por un entretenimiento muy del gusto de la época, dando ocasión a resonancias algo exóticas: Rondó alla polacca es su rítmico tercer movimiento.

Beethoven también compuso una Fantasía para piano, orquesta y coro, Op. 80, que es una triple fantasía: comienza el piano solo, se le une la orquesta y, cerca del final, hace su entrada el coro —un esquema similar al de la Novena sinfonía—.

El único Concierto para violín, Op. 61 (que cuenta además con una transcripción para piano, obra del mismo Beethoven, Op. 61b) fue en su tiempo una obra controvertida que atrajo poca atención en su estreno, con el violinista Franz Clement en la parte solista. Fue solo en 1850, de la mano del violinista Joseph Joachim, amigo de Johannes Brahms, cuando el concierto alcanzó notoriedad. La explicación de esta demora en imponerse es lo complejo de su interpretación, que hizo que pocos violinistas se atreviesen a tocarlo por años, argumentando que la participación del violín a la par con la orquesta les restaba protagonismo, lo que se unía a la fuerte exigencia de Beethoven. Dentro de esta categoría de obras para violín y orquesta deben incluirse además dos breves Romanzas para violín y orquesta.

Sus treinta y dos sonatas manifiestan la personalidad revolucionaria y de transición de Beethoven, y el compositor se sitúa como el más destacado de la forma sonata del periodo comprendido entre clasicismo y romanticismo. Fiel a la forma sonata, el compositor alemán se permite más de una innovación: sonata de dos (Op. 111), cuatro (Op. 109) o cinco movimientos, temas con variaciones, fugas, scherzi, etc.

Estas sonatas presentan nuevas sonoridades, audaces experimentos, y queda encerrado el mundo interior del compositor y también el recién llegado lenguaje expresivo de la revolución romántica. En la temprana Patética, en la tempestuosa Appassionata, en la brusca y laberíntica Hammerklavier, en las últimas sonatas Op. 110 y 111, el compositor llega a las fronteras de la exposición pianística, que serán alcanzadas en el Op. 120. Beethoven fue uno de los compositores que más exigió a los constructores de piano a mejorar la sonoridad y resistencia de los pianofortes decimonónicos.

El inadecuado entrenamiento que tuvo Beethoven en sus primeros años de estudios musicales se refleja en las tres sonatas para piano escritas en 1783. El piano súbito, los repentinos arranques, las figuras de arpegios (ejecutadas a altas velocidades en varias octavas de forma ascendente o descendente) conocidas como los «cohetes de Mannheim», son característicos de la personalidad musical y sentimental de Beethoven. Él es el primero en usar el acorde de novena sin preparar y que se puede observar en el primer movimiento de su Sonata para piano n.º 14 «Claro de luna», dedicada a otro de los grandes amores de su vida, la condesa Giulietta Guicciardi.

Las sonatas para piano de Beethoven transportaron la música a un nuevo orden. Después de 1800, Beethoven empezó a desarrollar el género con proyecciones románticas. La Sonata n.º 11 Op. 22, en si ♭ mayor, es la última sonata del primer período de composición, la cual Beethoven declaró como su sonata preferida. La Op. 26 en la ♭, la primera que compuso desde el comienzo del nuevo siglo, se abre con un tema lento con variaciones, sigue con un scherzo temerario y vertiginoso, una marcha fúnebre «a la muerte de un héroe» y concluye en un agitado final. A esta le siguieron las dos sonatas Quasi una fantasía Op. 27 (a la segunda se la suele llamar Claro de Luna) que formalmente son poco convencionales. Los siguientes hitos de su composición pianística coincidieron con la gran crisis que le produjo el agravamiento de su sordera. La brillante Waldstein (el apellido del conde a quien va dedicada, más conocida por Aurora en los países hispanoparlantes) y la arrolladora Appasionata fueron de concepción tan revolucionaria, que hasta el propio Beethoven se abstuvo de escribir para piano solo durante algunos años. Pero la cima de su pianismo son las cuatro últimas de las treinta y dos sonatas, desde la Op. 106, Hammerklavier —que es frecuentemente referida como «sinfónica», por sus cuatro movimientos—, hasta la Op. 111 en do menor, la tonalidad de la que se valía para su música Sturm und Drang, como por ejemplo, su Quinta sinfonía. Las sonatas exigían un virtuosismo pianístico sin precedentes hasta entonces y eran prácticamente intocables en la época. Franz Liszt fue quien demostró que eran «interpretables».

Destacan también las diez sonatas para violín y piano, en especial la Sonata para violín n.º 9 «Kreutzer», Op. 47, conocida por las exigencias que presenta para la parte del violín. Está dedicada a Rodolphe Kreutzer, conocido violinista de la época. Así mismo, la Sonata para violín n.º 5, conocida como Frühling (Primavera) y la Sonata para violín n.º 10 gozan de gran popularidad.

Beethoven se centró sobre todo en la música orquestal, compaginándola con la música de cámara y para piano. También desarrolló obras vocales, aunque con suerte muy diversa. Por ejemplo, su única ópera escrita, Fidelio, revisada desde 1805 hasta 1814, fue un fracaso el día de su estreno. El músico tuvo que esperar hasta el 23 de mayo de 1814 para ser aclamado de forma entusiasta por un público enfervorizado. La nueva versión representaba para el público más que la recreación de los principios de la Ilustración, como fue su primer objetivo en 1805, la celebración de las victorias sobre Napoleón y como una alegoría de la liberación de Europa. Fue entonces cuando, ruborizado ante tales muestras de apoyo y cariño del público, escribió en su libro de conversaciones: «Es evidente que uno compone más bellamente cuando lo hace para el gran público». Se trataba, sin duda, del mismo compositor que había gritado al editor, tras el desastre de su primer Fidelio: «No compongo para la galería, que se vayan todos al infierno», nueve años antes.[31]​

Lo cierto es que Beethoven no mostraría particular interés en escribir óperas. Un proyecto largamente conversado con Goethe para transformar en ópera el Fausto no llegaría jamás a concretarse por razones desconocidas hasta hoy. Sin embargo, algunos autores, basados principalmente en anotaciones del propio Beethoven, han descrito algunas de sus sinfonías como «óperas encubiertas». Tal carácter ha sido asignado tanto a la Sexta sinfonía como a la Tercera.

La Missa Solemnis, escrita entre 1819 y 1823, su segunda obra para la Iglesia católica, es un canto de fe a Dios y a la naturaleza del hombre. Es una de sus obras más famosas, compuesta por encargo de su alumno, el archiduque Rudolf, nombrado en esa época arzobispo de Olomouc. La Missa solemnis provocó no pocos problemas a Beethoven. La obra fue estrenada parcialmente junto con la Novena sinfonía.[32]​

Otras obras corales de Beethoven son la Fantasía coral para piano, coro y orquesta (Op. 80), la Misa en do mayor, Latina (Op. 86), así como numerosos lieder, arias, coros y cánones, un ciclo de melodías, la cantata «En la Muerte del Emperador José II» (Op. 196) y el oratorio Cristo en el monte de los Olivos, de 1803, así como el célebre presto de la Novena sinfonía.

Hay algunos críticos musicales que opinan que el género de los cuartetos de cuerda desarrollado por Beethoven es más representativo que el de las sonatas para piano y el de las sinfonías. De hecho, Beethoven murió componiendo cuartetos.[33]​

En los cuartetos se puede comprobar el desarrollo de Beethoven a través de sus «tres estilos»: los primeros cuartetos, fieles a Haydn; el segundo período dominado por los llamados Cuartetos rusos, compuestos por encargo del aristócrata Razumovski; pero los más significativos son los seis finales, compuestos entre 1824 y 1827, es decir, correspondientes a la última etapa, algunas veces llamada «esotérica». La importancia del género en Beethoven rebasa los límites del Romanticismo, al grado de que sus últimas obras son una anticipación estilística y técnica que influiría en Dmitri Shostakóvich, Béla Bartók y en la Segunda Escuela de Viena de inicios del siglo XX. Los cuartetos muestran al Beethoven más profundo y original.

El Op. 18 constituye el primer esfuerzo importante de Beethoven en este complejo género musical y engloba seis obras dedicadas a su maestro, Joseph Haydn: aunque se encuentran todavía evidencias de las obras anteriores de Mozart y Haydn, ya hay un deseo de mostrar la originalidad que se verá plasmada en sus trabajos posteriores, como el movimiento final del Cuarteto de cuerda n.º 6 en si♭ mayor, «La malinconia», el cual es una introducción lenta que casi rebasa los límites tonales para luego dar paso al rondó concluyente.

En el periodo medio ya se aprecia a un Beethoven maduro, plenamente consciente de su poderío como creador y artista, pero sumido en la lucha contra la sordera. La primera parte de este periodo medio se constituye con el poderoso opus 59 Razumovski, constituido por tres cuartetos. Varios críticos musicales han tratado de ver un ciclo en este grupo de piezas dedicadas al conde Razumovski, el cual le proporcionó a Beethoven acceso a diversas melodías rusas como motivo de inspiración, aunque esto no condujo a una influencia definitiva. Hay varios motivos para creer que el punto de vista cíclico es cercano a la realidad, tomando en cuenta que el primer movimiento del primer cuarteto es una especie de síntesis de la forma sonata y que justamente el último del tercero es una compleja fuga, la cual tiene bastantes elementos de herencia con respecto al movimiento final de la Sinfonía «Júpiter» de Mozart, aunque el desarrollo estilístico del cuarteto es, como es natural, mucho mayor que el de dicha sinfonía.

Los últimos cuartetos y la Grosse fugue trascienden el Romanticismo y son considerados como el verdadero legado musical de Beethoven por su complejidad melódica, armónica y de ejecución. En la época de su estreno no fueron bien recibidos, pero cuando le comentaron al compositor que la Grosse fugue había causado el rechazo general, este respondió «No importa, no la compuse para ellos, sino para el futuro». Sin embargo, Beethoven consintió quitarla del Cuarteto de cuerda n.º 13 (Op. 130), del que inicialmente formaba parte (ahora la pieza lleva por número de opus el 133) y compuso un nuevo final para este cuarteto.

La presencia de Beethoven como símbolo, y no solo como músico, es un fenómeno heredado del romanticismo. Tradicionalmente se ha difundido la visión del cuadro de Joseph Karl Stieler, cuyo enorme impacto se dejó traslucir tras el éxito de las serigrafías de Andy Warhol en 1987. Se han dejado de lado otras versiones, sea porque tuvieran una calidad artística considerada inferior, o que mostraran una imagen considerada en algún aspecto «poco verídica» respecto a la imagen que ya se encuentra establecida en el imaginario popular acerca del músico.[34]​

Existen numerosos sellos postales y otros documentos filatélicos y numismáticos de países de todo el mundo en honor a Ludwig van Beethoven. Alemania es el país más prolífico de todos, aunque existen sellos emitidos en Francia, Mónaco, Austria, países de África y Sudamérica, entre otros. En total hay alrededor de doscientos sellos dedicados al compositor alemán.[35]​ Beethoven también ha aparecido en diversas monedas y medallas, acuñadas en muchos casos con motivo de los aniversarios de su nacimiento, de su fallecimiento o para conmemorar algún hecho destacado en su ciudad natal, Bonn. Estas monedas o medallas han sido acuñadas en países de toda Europa, Estados Unidos o Asia.[36]​

El compositor ha sido mostrado biográficamente en numerosas ocasiones en el cine, en el teatro y en la televisión. Estas son algunas de ellas:

Además, su música ha sido usada en más de doscientas cincuenta películas y programas de televisión.[48]​



El término cultura (del latín cultūra)[1]​[2]​ tiene muchos significados interrelacionados, es decir, es un término polisémico. Por ejemplo, en 1952, Alfred Kroeber y Clyde Kluckhohn recopilaron una lista de 164 definiciones de cultura en Cultura: una reseña crítica de conceptos y definiciones, y han clasificado más de 250 distintas.[3]​
En el uso cotidiano, la palabra cultura se emplea para dos conceptos diferentes: 

Una primera distinción en el conocimiento científico es la que se establece entre naturaleza y cultura. Esa distinción significa que el mundo de la naturaleza es el que no ha sido creado por el hombre, al menos en sus orígenes; mientras que la cultura es el mundo creado por los seres humanos, tal como se explica extensamente en el magnífico libro The Man-Made World ([4]​).

Cuando el término cultura surgió en Europa, entre los siglos XVIII y  XIX, se refería a un proceso de tecnología o mejora, como en la agricultura u horticultura. En el siglo XIX, pasó primero a referirse al mejoramiento o refinamiento de lo individual, especialmente a través de la educación, y luego al logro de las aspiraciones o ideales nacionales. A mediados del siglo XIX, algunos científicos utilizaron el término «cultura» para referirse a la capacidad humana universal. Para el antipositivista y sociólogo alemán Georg Simmel, la cultura se refería a «la cultivación de los individuos a través de la injerencia de formas externas que han sido objetificadas en el transcurso de la historia».[5]​

En el siglo XX, la «cultura» surgió como un concepto central de la antropología, abarcando todos los fenómenos humanos que no son el total resultado de la genética. Específicamente, el término «cultura» en la antropología americana tiene dos significados: (1) la evolucionada capacidad humana de clasificar y representar las experiencias con símbolos y actuar de forma imaginativa y creativa; y (2) las distintas maneras en que la gente vive en diferentes partes del mundo, clasificando y representando sus experiencias y actuando creativamente. Después de la Segunda Guerra Mundial, el término se volvió importante, aunque con diferentes significados, en otras disciplinas como estudios culturales, psicología organizacional, sociología de la cultura y estudios gerenciales.

Algunos etólogos han hablado de «cultura» para referirse a costumbres, actividades o comportamientos transmitidos de una generación a otra en grupos de animales por imitación consciente de dichos comportamientos.[cita requerida]
Las creencias y prácticas de una cultura determinada pueden ser ejercidas como mecanismos de control que limitan la conducta social. La cultura se asocia con la libertad, ya que es el vehículo entre el conocimiento y nuevas formas de conciencia que permiten una desestabilización en la hegemonía. Además puede reconocerse como conjuntos o modos de vida y costumbres de una época o grupo social. El término cultura puede alcanzar extensión y usos diversos, como diversidad cultural, objeto del conocimiento empírico, y la diferencia cultural.[6]​

Otros conceptos de cultura:

La etimología del concepto moderno “cultura” tiene un origen antiguo. En varias lenguas europeas, la palabra “cultura” está basada en el término latino utilizado por Cicerón, en su Tusculanae Disputationes, quien escribió acerca de un cultivo del alma o “cultura animi”, utilizando así una metáfora agrícola para describir el desarrollo de un alma filosófica, que fue comprendida teleológicamente como uno de los ideales más altos posibles para el desarrollo humano. Samuel Pufendorf llevó esta metáfora a un concepto moderno, con un significado similar, pero ya sin asumir que la filosofía es la perfección natural del hombre. Para este autor, los significados de cultura, que muchos escritores posteriores retoman, “se refieren a todas las formas en la que los humanos comienzan a superar su barbarismo original y, a través de artificios, se vuelven completamente humanos”.[8]​


Como lo describe Velkley:[8]​ 
El término “cultura”, que originalmente significaba la cultivación del alma o la mente, adquiere la mayoría de sus posteriores significados en los escritos de los pensadores alemanes del siglo XVIII, quienes en varios niveles desarrollaron la crítica de Rousseau al liberalismo moderno y la Ilustración. Además, un contraste entre “cultura” y “civilización” está usualmente implícito por estos autores, aun cuando no lo expresen así. Dos significados primarios de cultura surgen de este período: cultura como un espíritu folclórico con una identidad única, y cultura como la cultivación de la espiritualidad o la individualidad libre. El primer significado es predominante dentro de nuestro uso actual del término “cultura”, pero el segundo juega todavía un importante rol en lo que creemos debería lograr la cultura, como la “expresión” plena del ser único y “auténtico”.
El término cultura proviene del latín cultus que a su vez deriva de la voz colere que significa cuidado del campo o del ganado. Hacia el siglo XIII, el término se empleaba para designar una parcela cultivada, y tres siglos más tarde había cambiado su sentido de estado de una cosa a la propia acción que lleva a dicho estado: el cultivo de la tierra o el cuidado del ganado (Cuche, 1999: 10), aproximadamente en el sentido en que se emplea en el español de nuestros días en vocablos como agricultura, apicultura, piscicultura y otros. Por la mitad del siglo XVI, el término adquiere una connotación metafórica, como el cultivo de cualquier facultad. De cualquier manera, la acepción figurativa de cultura no se extenderá hasta el siglo XVII, cuando también aparece en ciertos textos académicos.

El Siglo de las Luces (siglo XVIII) es la época en que el sentido figurado del término como “cultivo del espíritu” se impone en amplios campos académicos. Por ejemplo, el Dictionnaire de l'Académie Française de 1718. Y aunque la Enciclopedia lo incluye solo en su sentido restringido de cultivo de tierras, no desconoce el sentido figurado, que aparece en los artículos dedicados a la literatura, la pintura, la filosofía y las ciencias. Con el paso del tiempo, como cultura se entenderá la formación de la mente. Es decir, se convierte nuevamente en una palabra que designa un estado, aunque en esta ocasión es el estado de la mente humana, y no el estado de las parcelas.

La clásica oposición entre cultura y naturaleza también tiene sus raíces en esta época. En 1798, el Dictionnaire incluye una acepción de cultura en que se estigmatiza el “espíritu natural”. Para muchos de los pensadores de la época, como Jean Jacques Rousseau, la cultura es un fenómeno distintivo de los seres humanos, que los coloca en una posición diferente a la del resto de animales. La cultura es el conjunto de los conocimientos y saberes acumulados por la humanidad a lo largo de sus milenios de historia. En tanto una característica universal (el vocablo), se emplea en número singular, puesto que se encuentra en todas las sociedades sin distinción de etnias, ubicación geográfica o momento histórico.

También es en el contexto de la Ilustración cuando surge otra de las clásicas oposiciones en que se involucra a la cultura, esta vez, como sinónimo de la civilización. Esta palabra aparece por primera vez en la lengua francesa del siglo XVIII, y con ella se significaba la refinación de las costumbres. Civilización es un término relacionado con la idea de progreso. Según esto, la civilización es un estado de la Humanidad en el cual la ignorancia ha sido abatida y las costumbres y relaciones sociales se hallan en su más elevada expresión. La civilización no es un proceso terminado, es constante, e implica el perfeccionamiento progresivo de las leyes, las formas de gobierno, el conocimiento. Como la cultura, también es un proceso universal que incluye a todos los pueblos, incluso a los más atrasados en la línea de la evolución social. Desde luego, los parámetros con los que se medía si una sociedad era más civilizada o más salvaje eran los de su propia sociedad. En los albores del siglo XIX, ambos términos, cultura y civilización eran empleados casi de modo indistinto, sobre todo en francés e inglés (Thompson, 2002: 186).

Es necesario señalar que no todos los intelectuales franceses emplearon el término. Rousseau y Voltaire se mostraron reticentes a esta concepción progresista de la historia. Intentaron proponer una versión más relativista de la historia, aunque sin éxito, pues la corriente dominante era la de los progresistas. No fue en Francia, sino en Alemania donde las posturas relativistas ganaron mayor prestigio. El término Kultur en sentido figurado aparece en Alemania hacia el siglo XVII -aproximadamente con la misma connotación que en francés. Para el siglo XVIII goza de gran prestigio entre los pensadores burgueses alemanes. Esto se debió a que fue empleado para denostar a los aristócratas, a los que acusaban de tratar de imitar las maneras “civilizadas” de la corte francesa. Por ejemplo, Immanuel Kant apuntaba que “nos cultivamos por medio del arte y de la ciencia, nos civilizamos [al adquirir] buenos modales y refinamientos sociales” (Thompson, 2002: 187). Por lo tanto, en Alemania el término civilización fue equiparado con los valores cortesanos, calificados de superficiales y pretenciosos. En sentido contrario, la cultura se identificó con los valores profundos y originales de la burguesía (Cuche, 1999:13).

En el proceso de crítica social, el acento en la dicotomía cultura/civilización se traslada de las diferencias entre estratos sociales a las diferencias nacionales. Mientras Francia era el escenario de una de las revoluciones burguesas más importantes de la historia, Alemania estaba fragmentada en múltiples Estados. Por ello, una de las tareas que se habían propuesto los pensadores alemanes era la unificación política. La unidad nacional pasaba también por la reivindicación de las especificidades nacionales, que el universalismo de los pensadores franceses pretendía borrar en nombre de la civilización. Ya en 1774, Johann Gottfried Herder proclamaba que el genio de cada pueblo (Volksgeist) se inclinaba siempre por la diversidad cultural, la riqueza humana y en contra del universalismo. Por ello, el orgullo nacional radicaba en la cultura, a través de la que cada pueblo debía cumplir un destino específico. La cultura, como la entendía Herder, era la expresión de la humanidad diversa, y no excluía la posibilidad de comunicación entre los pueblos.

Durante el siglo XIX, en Alemania el término cultura evoluciona bajo la influencia del nacionalismo.[9]​ Mientras tanto, en Francia, el concepto se amplió para incluir no solo el desarrollo intelectual del individuo, sino el de la humanidad en su conjunto. De aquí, el sentido francés de la palabra presenta una continuidad con el de civilización: no obstante la influencia alemana, persiste la idea de que más allá de las diferencias entre “cultura alemana” y “cultura francesa” (por poner un ejemplo), hay algo que las unifica a todas: la cultura humana.[10]​

Para efecto de las ciencias sociales, las primeras acepciones de cultura fueron construidas a finales del siglo XIX. Por esta época, la sociología y la antropología eran disciplinas relativamente nuevas, y la pauta en el debate sobre el tema que aquí nos ocupa la llevaba la filosofía. Los primeros sociólogos, como Émile Durkheim, rechazaban el uso del término. Hay que recordar que en su perspectiva, la ciencia de la sociedad debía abordar problemas relacionados con la estructura social.[11]​ Si bien es opinión generalizada que Karl Marx dejó de lado a la cultura, ello se ve refutado por las mismas obras del autor, sosteniendo que las relaciones sociales de producción (la organización que adoptan los seres humanos para el trabajo y la distribución social de sus frutos) constituyen la base de la superestructura jurídico-política e ideológica, pero en ningún caso un aspecto secundario de la sociedad. No es concebible una relación social de producción sin reglas de conducta, sin discursos de legitimación, sin prácticas de poder, sin costumbres y hábitos permanentes de comportamiento, sin objetos valorados tanto por la clase dominante como por la clase dominada. El desvelo de las obras juveniles de Marx, tanto de La ideología alemana (1845-1846) en 1932 por la célebre edición del Instituto Marx-Engels de la URSS bajo dirección de David Riazanov, como de los Manuscritos económicos y filosóficos (1844) posibilitó que varios partidarios de sus propuestas teóricas desarrollaran una teoría de la cultura marxista (véase más adelante).

Generalmente, el significado de cultura se relaciona con la antropología. Una de las ramas más importantes de esta disciplina social se encarga precisamente del estudio comparativo de la cultura. Quizá por la centralidad que la palabra tiene en la teoría de la antropología, el término ha sido desarrollado de diversas maneras, que suponen el uso de una metodología analítica basada en premisas que en ocasiones distan mucho las unas de las otras. Fue Franz Boas,[12]​ frente a esta empresa etnocentrista, quien opera el gran cambio epistemológico en la antropología. A partir de Boas, padre del relativismo cultural, el antropólogo se hace traductor, pudiendo entrar en la cosmovisión del estudiado y entender el mundo de sus significaciones. Ese mundo de la significación,[13]​ irá conduciendo, lentamente, hacia un concepto que no es antropológico, y es el de la construcción social del sentido. Ante este aumento de la sensibilidad vinculada con las cuestiones del lenguaje, se desarrollarán disciplinas nuevas, vinculadas con el mundo de la significación humana y del lenguaje, que completarán la idea de la cultura entendida desde el mundo de la significación.[14]​

De acuerdo con la Declaración Universal sobre la Diversidad Cultural de la UNESCO "la cultura debe ser considerada como el conjunto de los rasgos distintivos espirituales y materiales, intelectuales y afectivos que caracterizan a una sociedad o a un grupo social y que abarca, además de las artes y las letras, los modos de vida, las maneras de vivir juntos, los sistemas de valores, las tradiciones y las creencias"[15]​ 

Los etnólogos y antropólogos británicos y estadounidenses de las postrimerías del siglo XIX retomaron el debate sobre el contenido de cultura. Estos autores tenían casi siempre una formación profesional en derecho, pero estaban particularmente interesados en el funcionamiento de las sociedades exóticas con las que Occidente se encontraba en ese momento.[16]​ En la opinión de estos pioneros de la etnología y la antropología social (como Bachoffen, McLennan, Maine y Morgan), la cultura es el resultado del devenir histórico de la sociedad. Pero la historia de la humanidad en estos escritores era fuertemente deudora de las teorías ilustradas de la civilización, y sobre todo, del darwinismo social de Spencer.

Como señala Thompson (2002:190), la definición descriptiva de cultura se encontraba presente en esos primeros autores de la antropología decimonónica. El interés principal en la obra de estos autores (que abordaba problemáticas tan disímbolas como el origen de la familia y el matriarcado, y las supervivencias de culturas antiquísimas en la civilización occidental de su tiempo) era la búsqueda de los motivos que llevaban a los pueblos a comportarse de tal o cual modo. En esas exploraciones, meditarente, o entre la tecnología y el resto del sistema social.

Uno de los más importantes etnógrafos de la época fue Gustav Klemm. En los diez tomos de su obra Allgemeine Kulturgeschichte der Menschheit (1843-1852)[17]​ intentó mostrar el desarrollo gradual de la humanidad por medio del análisis de la tecnología, costumbres, arte, herramientas, prácticas religiosas. Una obra monumental, pues incluía ejemplos etnográficos de pueblos de todo el mundo. El trabajo de Klemm habría de tener eco en sus contemporáneos, empeñados en definir el campo de una disciplina científica que estaba naciendo. Unos veinte años más tarde, en 1871, Edward B. Tylor publicó en Primitive Culture una de las definiciones más ampliamente aceptadas de cultura. Según Tylor, la cultura es:

De esta suerte, uno de los principales aportes de Tylor fue la elevación de la cultura como materia de estudio sistemático. A pesar de este notable avance conceptual, la propuesta de Tylor adolecía de dos grandes debilidades. Por un lado, sacó del concepto su énfasis humanista al convertir a la cultura en objeto de ciencia. Por el otro, su procedimiento analítico era demasiado descriptivo. En el texto citado arriba, Tylor plantea que “un primer paso para el estudio de la civilización[18]​ consiste en diseccionarla en detalles, y clasificar éstos en los grupos adecuados” (Tylor, 1995:33). Según esta premisa, la mera recopilación de los “detalles” permitiría el conocimiento de una cultura. Una vez conocida, sería posible clasificarla en una graduación de más a menos civilizada, premisa que heredó de los darwinistas sociales.

La propuesta teórica de Tylor fue retomada y reelaborada posteriormente, tanto en Gran Bretaña como en Estados Unidos. En este último país, la antropología evolucionaba hacia una posición relativista, representada en primera instancia por Franz Boas. Esta posición representaba un rompimiento con las ideas anteriores sobre la evolución cultural, en especial las propuestas por los autores británicos y el estadounidense Lewis Henry Morgan. Para este último, contra quien Boas dirigió sus críticas en uno de sus pocos textos teóricos, el proceso de la evolución social humana (tecnología, relaciones sociales y cultura) podía ser equiparado con el proceso de crecimiento de un individuo de la especie. Por lo tanto, Morgan comparaba el salvajismo con la “infancia de la especie humana”, y la civilización, con la madurez.[19]​ Boas fue sumamente duro con las propuestas de Morgan y el resto de los antropólogos evolucionistas contemporáneos. A lo que sus autores llamaban “teorías” sobre la evolución de la sociedad, Boas las calificó de “puras conjeturas” sobre el ordenamiento histórico de “fenómenos observados conforme a principios admitidos [de antemano]” (1964:184).

La crítica de Boas en contra de los evolucionistas es un eco de la perspectiva de los filósofos alemanes como Herder y Wilhelm Dilthey. El núcleo de la propuesta radica en su inclinación a considerar la cultura como un fenómeno plural. En otras palabras, más que hablar de cultura, Boas hablaba de culturas. Para la mayor parte de los antropólogos y etnólogos adscritos a la escuela culturalista estadounidense, el estado del arte etnográfico al principio del siglo XX no permitía la conformación de una teoría general sobre la evolución de las culturas. Por lo tanto, la labor más importante de los estudiosos del fenómeno debía ser la documentación etnográfica.[20]​ De hecho, Boas escribió muy pocos textos teóricos, en comparación con sus monografías sobre los pueblos indígenas de la costa pacífica de América del Norte.

Los antropólogos formados por Robin Reid hubieron de heredar muchas de las premisas de su maestro. Entre otros casos notables, están el de Ruth Benedict. En su obra Patterns of culture (1939), Benedict señala que cada cultura es un todo comprensible solo en sus propios términos[21]​ y constituye una suerte de matriz que da sentido a la actuación de los individuos en una sociedad. Alfred Kroeber, retomando la oposición entre cultura y naturaleza, también señalaba que las culturas son fenómenos sui generis pero, en sentido estricto, eran de una categoría exterior a la naturaleza. Por lo tanto, según Kroeber, el estudio de las culturas debía salirse del dominio de las ciencias naturales y encarar a las primeras como lo que eran: fenómenos superorgánicos.[22]​ Melville Herskovits y Clyde Kluckhohn retomaron de Tylor su definición cientificista del estudio de la cultura. Para el primero, también la recolección de rasgos definitorios de las culturas permitiría su clasificación. Aunque, en este caso, la clasificación no se realizaba en sentido diacrónico, sino espacial-geográfico que habría de permitir el conocimiento de las relaciones entre los diferentes pueblos asentados en un área cultural. Kluckhonn, por su parte, resume en su texto Antropología la mayor parte de los postulados vistos en esta sección, y reclama el dominio de lo cultural como el campo específico de la actividad antropológica.

Por su parte Javier Rosendo describe la cultura como el conjunto de rasgos que caracterizan a una región o grupo de personas, con respecto al resto, que puede ir cambiando de acuerdo a la época en la cual se vive. Estos rasgos pueden abarcar la danza, tradiciones, arte, vestuario y religión.

La característica más peculiar del concepto funcionalista de cultura se refiere precisamente a la función social de la misma. El supuesto básico es que todos los elementos de una sociedad (entre los que la cultura es uno más) existen porque son necesarios. Esta perspectiva ha sido desarrollada tanto en antropología como en sociología aunque, sin duda, sus primeras características fueron delineadas involuntariamente por Émile Durkheim. Este sociólogo francés muy pocas veces empleó el término como unidad analítica principal de su disciplina. En su libro Las reglas del método sociológico (1895), plantea que la sociedad está compuesta por entidades que tienen una función específica, integradas en un sistema análogo al de los seres vivos, donde cada órgano está especializado en el cumplimiento de una función vital. Del mismo modo en que los órganos de un cuerpo son susceptibles a la enfermedad, las instituciones y costumbres, las creencias y las relaciones sociales también pueden caer en un estado de anomia. Durkheim y sus seguidores, sin embargo, no se ocupan exclusiva ni principalmente de la cultura como objeto de estudio, sino de hechos sociales. A pesar de ellos, sus propuestas analíticas fueron retomadas por autores conspicuos de la antropología social británica y la sociología de la cultura de Estados Unidos.

Más tarde, el polaco Bronislaw Malinowski retomó tanto la descripción de cultura de Tylor como algunos de los planteamientos de Durkheim relativos a la función social. Para Malinowski, la cultura podía ser entendida como una «realidad sui generis» que debía estudiarse como tal (en sus propios términos). En la categoría de cultura incluía artefactos, bienes, procesos técnicos, ideas, hábitos y valores heredados (Thompson, 2002: 193). También consideraba que la estructura social podía ser entendida análogamente a los organismos vivos pero, a diferencia de Durkheim, Malinowski tenía una tendencia más holística. Malinowski creía que todos los elementos de la cultura poseían una función que les daba sentido y hacía posible su existencia. Pero esta función no era dada únicamente por lo social, sino por la historia del grupo y el entorno geográfico, entre muchos otros elementos. El reflejo más claro de este pensamiento aplicado al análisis teórico fue el libro Los argonautas del Pacífico Occidental (1922), una extensa y detallada monografía sobre las distintas esferas de la cultura de los isleños trobriandeses, un pueblo que habitaba en las islas Trobriand, al oriente de Nueva Guinea.

Años más tarde, Alfred Reginald Radcliffe-Brown, también antropólogo británico, retomaría algunas de las propuestas de Malinowski, y muy especialmente las que se referían a la función social. Radcliffe-Brown rechazaba que el campo de análisis de la antropología fuera la cultura, más bien se encargaba del estudio de la estructura social, un entramado de relaciones entre las personas de un grupo. Sin embargo, también analizó aquellas categorías que habían sido descritas con anterioridad por Malinowski y Tylor, siguiendo siempre el principio del análisis científico de la sociedad. En su libro Estructura y función en la sociedad primitiva (1975) Radcliffe-Brown establece que la función más importante de las creencias y prácticas sociales es la del mantenimiento del orden social, el equilibrio en las relaciones y la trascendencia del grupo en el tiempo. Sus propuestas fueron retomadas más tarde por muchos de sus alumnos, especialmente por Edward Evan Evans-Pritchard etnógrafo de los nuer y los azande, pueblos del centro de África. En ambos trabajos etnográficos, la función reguladora de las creencias y prácticas sociales está presente en el análisis de esas sociedades, a la primera de las cuales, Evans-Pritchard llamó “anarquía ordenada”.

Los orígenes de las concepciones simbólicas de cultura se remontan a Leslie White, antropólogo estadounidense formado en la tradición culturalista de Boas. A pesar de que en su libro La ciencia de la cultura afirma, en un principio, que esta es «el nombre de un tipo preciso o clase de fenómenos, es decir, las cosas y los sucesos que dependen del ejercicio de una habilidad mental, exclusiva de la especie humana, que hemos llamado 'simbolizante'», en el transcurso de su texto, White irá abandonando la idea de la cultura como símbolos para orientarse hacia una perspectiva ecológica.[23]​

El estructuralismo es una corriente más o menos extendida en las ciencias sociales. Sus orígenes se remontan a Ferdinand de Saussure, lingüista, quien propuso grosso modo que la lengua es un sistema de signos. Tras su conversión a la antropología (tal como la llama en Tristes trópicos), Claude Lévi-Strauss –influido por Roman Jakobson– habría de retomar este concepto para el estudio de los hechos de interés antropológico, entre los que la cultura era solo uno más. De acuerdo con Lévi-Strauss, la cultura es básicamente un sistema de signos[24]​ producidos por la actividad simbólica de la mente humana (tesis que comparte con White).

En Antropología estructural (1958) Lévi-Strauss irá definiendo las relaciones que existen entre los signos y símbolos del sistema, y su función en la sociedad, sin prestar demasiada atención a este último punto. En resumen, se puede decir que en la teoría estructuralista, la cultura es un mensaje que puede ser decodificado tanto en sus contenidos, como en sus reglas. El mensaje de la cultura habla de la concepción del grupo social que la crea, habla de sus relaciones internas y externas. En El pensamiento salvaje (1962), Lévi-Strauss apunta que todos los símbolos y signos de que está hecha la cultura son productos de la misma capacidad simbólica que poseen todas las mentes humanas. Esta capacidad, básicamente consiste en la clasificación de las cosas del mundo en grupos, a los que se atribuyen ciertas cargas semánticas. No existe un grupo de símbolos o signos (campo semántico) que no tenga uno complementario. Los signos y sus significados pueden ser asociados por metáfora (como en el caso de las palabras) o metonimia (como en el caso de los emblemas de la realeza) a fenómenos significativos para el grupo creador del sistema cultural. Las asociaciones simbólicas no necesariamente son las mismas en todas las culturas. Por ejemplo, mientras en la cultura occidental, el rojo es el color del amor, en Mesoamérica es el de la muerte.

Según la propuesta estructuralista, las culturas de los pueblos “primitivos” y “civilizados” están hechas de la misma materia y, por tanto, los sistemas del conocimiento del mundo exterior dominantes en cada uno —magia en los primeros, ciencia en los segundos—– no son radicalmente diferentes. Aunque son varias las distinciones que se pueden establecer entre culturas primitivas y modernas: una de las más importantes es el modo en que manipulan los elementos del sistema. En tanto que la magia improvisa, la ciencia procede sobre la base del método científico.[25]​ El uso del método científico no quiere decir —según Lévi-Strauss— que las culturas donde la ciencia es dominante sean superiores, o que aquellas donde la magia juega un papel fundamental sean menos rigurosas o metódicas en su manera de conocer el mundo. Simplemente, son de índole distinta unas de otras, pero la posibilidad de comprensión entre ambos tipos de culturas radica básicamente en una facultad universal del género humano.

En la perspectiva estructuralista, el papel de la historia en la conformación de la cultura de una sociedad no es tan importante. Lo fundamental es llegar a dilucidar las reglas que subyacen en la articulación de los símbolos en una cultura, y observar la manera en que estos dotan de sentido la actuación de una sociedad. En varios textos, Lévi-Strauss y sus seguidores (como Edmund Leach) parecen insinuar, como Ruth Benedict, que la cultura es una suerte de patrón que pertenece a todo el grupo social pero no se encuentra en nadie en particular. Esta idea también fue retomada del concepto de lenguaje propuesto por Saussure.

La antropología simbólica es una rama de las ciencias sociales cuyo desarrollo se relaciona con la crítica al estructuralismo lévi-straussiano. Uno de los principales exponentes de esta corriente es Clifford Geertz. Comparte con el estructuralismo francés la tesis de la cultura como un sistema de símbolos pero, a diferencia de Lévi-Strauss, Geertz señala que no es posible para los investigadores el conocimiento de sus contenidos:

Bajo la premisa anterior, Geertz y la mayor parte de los antropólogos simbólicos ponen en duda la autoridad de la etnografía. Señalan que a lo que pueden limitarse los antropólogos es a hacer “interpretaciones plausibles” del significado de la trama simbólica que es la cultura, a partir de la descripción densa de la mayor cantidad de puntos de vista que sea posible conocer respecto a un mismo suceso. En otro sentido, los simbólicos no creen que todos los elementos de la trama cultural posean el mismo sentido para todos los miembros de una sociedad. Más bien creen que pueden ser interpretados de modos diferentes, dependiendo, ya de la posición que ocupen en la estructura social, ya de condicionamientos sociales y psíquicos anteriores, o bien, del mismo contexto.[26]​

Tal como se señaló anteriormente, Karl Marx a pesar de la opinión generalizada, puso atención en el análisis de las cuestiones culturales, específicamente en su relación con el resto de la estructura social. Según la propuesta teórica de Marx, el dominio de lo cultural (constituido sobre todo por la ideología) es un reflejo de las relaciones sociales de producción, es decir, de la organización que adoptan los seres humanos frente a la actividad económica. La gran aportación del marxismo en el análisis de la cultura es que esta es entendida como el producto de las relaciones de producción, como un fenómeno que no está desligado del modo de producción de una sociedad. Asimismo, la considera como uno de los medios por los cuales se reproducen las relaciones sociales de producción, que permiten la permanencia en el tiempo de las condiciones de desigualdad entre las clases.

En sus interpretaciones más simplistas, la definición de la ideología en Marx ha dado lugar a una tendencia a explicar las creencias y el comportamiento social en función de las relaciones que se establecen entre quienes dominan el sistema económico y sus subalternos. Sin embargo, son muchas las posturas donde la relación entre la base económica y la superestructura cultural es analizada en enfoques más amplios. Por ejemplo, Antonio Gramsci llama la atención a la hegemonía, un proceso por medio del cual, un grupo dominante se legitima ante los dominados, y estos terminan por ver natural y asumir como deseable la dominación. Louis Althusser propuso que el ámbito de la ideología (el principal componente de la cultura) es un reflejo de los intereses de la élite, y que a través de los aparatos ideológicos del Estado se reproducen en el tiempo.

Así mismo, Michel Foucault –en el conocido debate de noviembre de 1971 en Holanda con Noam Chomsky– respondiendo la pregunta de que si la sociedad capitalista era democrática, además de contestar negativamente –argumentando que una sociedad democrática se basa en el efectivo ejercicio del poder por una población que no esté dividida u ordenada jerárquicamente en clases– sostiene que, de manera general, todos los sistemas de enseñanza –los cuales aparecen simplemente como transmisores de conocimientos aparentemente neutrales–, están hechos para mantener a cierta clase social en el poder, y excluir de los instrumentos de poder a otras clases sociales.

Si bien el estudio de la cultura nació como una inquietud por el cambio de las sociedades a lo largo del tiempo, el desprestigio en el que cayeron los primeros autores de la antropología fue un terreno fértil para que arraigaran en la reflexión sobre la cultura las concepciones ahistóricas. Salvo los marxistas, interesados en el proceso revolucionario hacia el socialismo, el resto de las disciplinas sociales no prestaron mayor atención al problema de la evolución cultural.

Para introducir las definiciones neoevolucionistas de cultura, es necesario recordar que los evolucionistas sociales de finales del siglo XIX (representados, entre otros, por Tylor), pensaban que las sociedades “primitivas” de su época eran residuos de antiguas formas culturales, por las que necesariamente habría pasado la civilización de Occidente antes de llegar a ser lo que era en ese momento. Como se indicó antes, Boas y sus discípulos echaron por tierra estos argumentos, señalando que nada probaba la veracidad de estas suposiciones. Sin embargo, en Estados Unidos, hacia la década de 1940 tuvo lugar un nuevo viraje del enfoque temporal de la antropología. Este nuevo rumbo es el neoevolucionista, interesado entre otras cosas, por el cambio socio-cultural y las relaciones entre cultura y medio ambiente.

Según el neoevolucionismo, la cultura es el producto de las relaciones históricas entre un grupo humano y su medio ambiente. De esta manera se pueden resumir las definiciones de cultura propuestas por Leslie White (1992) y Julian Steward (1992), quienes encabezaron la corriente neoevolucionista en su nacimiento.[27]​ El énfasis de la nueva corriente antropológica se movió del funcionamiento de la cultura a su carácter dinámico. Este cambio de paradigma representa una clara oposición al funcionalismo estructuralista, interesado en el funcionamiento actual de la sociedad; y el culturalismo, que aplazaba el análisis histórico para un momento en que los datos etnográficos lo permitieran.

Tanto Steward como White concuerdan en que la cultura es solo uno de los ámbitos de la vida social. Para White, la cultura no es un fenómeno que deba entenderse en sus propios términos, como proponían los culturalistas. El aprovechamiento energético es el motor de las transformaciones culturales: estimula la transformación de la tecnología disponible, tendiendo siempre a mejorar. Así, la cultura está determinada por la forma en la que el grupo humano aprovecha su entorno. Este aprovechamiento se traduce a su vez en energía. El desarrollo de la cultura de un grupo es proporcional la cantidad de energía que la tecnología disponible le permite aprovechar. La tecnología determina las relaciones sociales y esencialmente la división del trabajo como una prístina forma de organización. A su vez, la estructura social y la división del trabajo se reflejan en el sistema de creencias del grupo, que formula conceptos que le permiten comprender el entorno que le rodea. Una modificación en la tecnología y la cantidad de energía aprovechada se traduce, por tanto, en modificaciones en todo el conjunto.

Steward, por su parte, retomaba de Kroeber la concepción de la cultura como un hecho que se encontraba por encima y fuera de la naturaleza. Sin embargo, Steward sostenía que había un diálogo entre ambos dominios. Opinaba que la cultura es un fenómeno o capacidad del ser humano que le permite adaptarse a su medio biológico. Uno de los principales conceptos en su obra es el de evolución. Steward planteaba que la cultura sigue un proceso de evolución multilineal (es decir, no todas las culturas pasan de un estado salvaje a la barbarie, y de ahí a la civilización), y que este proceso se basa en el desarrollo de tipos culturales derivados de las adaptaciones culturales al medio físico de una sociedad. Steward introduce en las ciencias sociales el término de ecología, señalando con él: el análisis de las relaciones existentes entre todos los organismos que comparten un mismo nicho ecológico.

Dentro del tipo de ideas introducidas por White y Steward, cabe señalar el materialismo cultural propugnado por Marvin Harris y otros antropólogos estadounidenses. Esta corriente puede ser asimilada a una forma de ecofuncionalismo en el que se encajan ciertas divisiones introducidas por Marx. Para el materialismo cultural, entender la evolución cultural y la configuración de las sociedades depende básicamente de condiciones materiales, tecnológicas e infraestructurales. El materialismo cultural establece una triple división entre grupos de conceptos que atiende a su relación causal. Esos grupos se llaman: infraestructura (modo de producción, tecnología, condiciones geográficas, etc.), estructura (modo de organización social, estructura jerárquica, etc.) y supraestructura (valores religiosos y morales, creaciones artísticas, leyes, etc.).

Había por lo menos una gran distancia conceptual entre la propuesta de White y de Steward. El primero se inclinaba por el estudio de la cultura como fenómeno total, en tanto que el segundo se mantenía más proclive al relativismo. Por ello, entre las limitaciones que tuvieron que superar sus sucesores estuvo la de concatenar ambas posturas, para unificar la teoría de los estudios de la ecología cultural. De esta suerte, Marshall Sahlins propuso que la evolución cultural sigue dos direcciones. Por un lado, crea diversidad “a través de una modificación de adaptación: las nuevas formas se diferencian de las viejas. Por otra parte, la evolución genera progreso: las formas superiores surgen de las inferiores y las sobrepasan”.[28]​

La idea de que la cultura se transforma siguiendo dos líneas simultáneas fue desarrollada por Darcy Ribeiro, que introdujo el concepto de proceso civilizatorio[29]​ para comprender las transformaciones de la cultura.

Con el tiempo, el neoevolucionismo sirvió como una de las principales bisagras entre las ciencias sociales y las ciencias naturales, especialmente como puente con la biología y la ecología. De hecho, su propia vocación como enfoque holístico le ha convertido en una de las corrientes más interdisciplinarias de las disciplinas que estudian la humanidad. A partir de la década de 1960, la ecología entró en una relación muy estrecha con los estudios culturales de corte evolutivo. Los biólogos habían descubierto que los seres humanos no son los únicos animales que poseen cultura: se habían encontrado indicios de ella entre algunos cetáceos, pero especialmente entre los primates. Roy Rappaport introdujo en la discusión de lo social la idea de que la cultura forma parte de la misma biología del ser humano, y que la evolución misma del ser humano se debe a la presencia de la cultura. Señalaba que:

Los nuevos descubrimientos en la etología (ciencia que estudia el comportamiento de los animales) animaron a muchos biólogos a intervenir en el debate sociológico de la cultura. Algunos de ellos buscaban establecer relaciones entre la cultura humana y las formas primitivas de cultura observadas, por ejemplo, entre los macacos de Japón. Uno de los ejemplos más conocidos es el de Sherwood Washburn, profesor de antropología de la Universidad de California. Al frente de un equipo multidisciplinario, emprendió la tarea de buscar cuáles eran los orígenes de la cultura humana. Como primera parte de su proyecto, analizó el comportamiento social de los primates superiores. En segundo lugar, suponiendo que los bosquimanos !kung eran los últimos reductos de las formas más primitivas de cultura humana, procedió al estudio de su cultura. La tercera etapa del programa de Washburn (en el que colaboraron Richard Lee e Irven de Vore, y que se prolongó durante la primera mitad de los años sesenta) fue proceder a la comparación de los resultados de ambas investigaciones, y especuló sobre esta base acerca de la importancia de la cacería en la construcción de la sociedad y la cultura.

Esta hipótesis fue presentada en un congreso llamado Man, the Hunter, realizado en la Universidad de Chicago en 1966. Fuera porque la investigación se apoyaba en premisas sobre la evolución cultural que fueron desechadas desde los tiempos de Boas, o porque era una tesis que negaba la importancia de la mujer en la construcción de la cultura, la tesis de Washburn, Lee y De Vore no fue bien recibida.[30]​

Esta definición, atiende a la característica principal de la cultura, que es una obra estrictamente de creación humana, a diferencia de los procesos que realiza la naturaleza, por ejemplo, el movimiento de la tierra, las estaciones del año, los ritos de apareamiento de las especies, las mareas e incluso la conducta de las abejas que hacen sus panales, elaboran miel, se orientan para encontrar el camino de regreso pero, que a pesar de eso, no constituyen una cultura, pues todas las abejas del mundo hacen exactamente lo mismo, de manera mecánica, y no pueden cambiar nada. Exactamente lo contrario ocurren en el caso de las obras, ideas y actos humanos, ya que estos transforman o se agregan a la naturaleza, por ejemplo, el diseño de una casa, la receta de un dulce de miel o de chocolate, la elaboración de un plano, la simple idea de las relaciones matemáticas, son cultura y sin la creación humana no existirían por obra de la naturaleza.

En 1998, Jesús Mosterín publicó su libro ¡Vivan los animales!, donde explica qué es la cultura:[31]​

La definición clásica de cultura en la Iglesia católica se encuentra en el concilio Vaticano II:

En la definición destacan dos aspectos: el poner al individuo al centro, siendo la cultura un producto del hombre y al servicio del hombre; y el conjugar la formación de cada persona a través de la cultura, con la contribución específica de una comunidad al progreso de la humanidad. Este concepto de cultura es la base para explicar el proceso de la inculturación o inserción de la Iglesia católica en una cultura y expresión del cristianismo en una nueva modalidad y culturalidad.

El concepto científico de cultura hizo uso desde el principio de ideas procedentes de la teoría de la información, de la noción de meme introducida por Richard Dawkins, de los métodos matemáticos desarrollados en la genética de poblaciones por autores como Luigi Luca Cavalli-Sforza y de los avances en la comprensión del cerebro y del aprendizaje. Diversos antropólogos, como William Durham, y filósofos, como Daniel Dennett y Jesús Mosterín, han contribuido decisivamente al desarrollo de la concepción científica de la cultura. Mosterín define la cultura como la información transmitida por aprendizaje social entre animales de la misma especie. Como tal, se contrapone a la naturaleza, es decir, a la información transmitida genéticamente. Si los memes son las unidades o trozos elementales de información adquirida, la cultura actual de un individuo en un momento determinado sería el conjunto de los memes presentes en el cerebro de ese individuo en ese momento. A su vez, la noción vaga de cultura de un grupo social es analizada por Mosterín en varias nociones precisas distintas, definidas todas ellas en función de los memes presentes en los cerebros de los miembros del grupo.[32]​

La industria cultural la define la UNESCO como aquella que produce y distribuye bienes o servicios culturales que, «considerados desde el punto de vista de su calidad, utilización o finalidad específicas, encarnan o transmiten expresiones culturales, independientemente del valor comercial que puedan tener. Las actividades culturales pueden constituir una finalidad de por sí, o contribuir a la producción de bienes y servicios culturales».[33]​

La importante aportación de la psicología humanista de, por ejemplo, Erik Erikson con una teoría psicosocial para explicar los componentes socioculturales del desarrollo personal.

Así, el ser humano tiene la facultad de enseñar al animal, desde el momento en que es capaz de entender su rudimentario aparato de gestos y sonidos, llevando a cabo nuevos actos de comunicación; pero los animales no pueden hacer algo parecido con nosotros. De ellos podemos aprender por la observación, como objetos, pero no mediante el intercambio cultural, es decir, como sujetos.

La cultura se clasifica, respecto a sus definiciones, de la siguiente manera:

La cultura puede también ser clasificada del siguiente modo:

La cultura forma todo lo que implica transformación y seguir un modelo de vida.
Los elementos de la cultura se dividen en:

a) Materiales: Son todos los objetos, en su estado natural o transformados por el trabajo humano, que un grupo esté en condiciones de aprovechar en un momento dado de su devenir histórico: tierra, materias primas, fuentes de energía, herramientas, utensilios, productos naturales y manufacturados, etcétera.

b) De organización: Son las formas de relación social sistematizadas, a través de las cuales se hace posible la participación de los miembros del grupo cuya intervención es necesaria para cumplir la acción. La magnitud y otras características demográficas de la población son datos importantes que deben tomarse en cuenta al estudiar los elementos de organización de cualquier sociedad o grupo.

c) De conocimiento: Son las experiencias asimiladas y sistematizadas que se elaboran, es decir los conocimientos, las ideas y las creencias que se acumulan y trasmiten de generación a generación y en el marco de las cuales se generan o incorporan nuevos conocimientos.

d) De conducta: Son los comportamientos o las pautas de conducta comunes a un grupo humano.

e) Simbólicos: Son los diferentes códigos que permiten la comunicación necesaria entre los participantes en los diversos momentos de una acción. El código fundamental es el lenguaje, pero hay otros sistemas simbólicos significativos que también deben ser compartidos para que sean posibles ciertas acciones y resulten eficaces.

f) Emotivos: que también pueden llamarse subjetivos. Son las representaciones colectivas, las creencias y los valores integrados que motivan a la participación y/o la aceptación de las acciones: la subjetividad como un elemento cultural indispensable.

g) Pautada: Son sistemas integrados. Una persona no representa una cultura, pero si todo un grupo amplio.

Dentro de toda cultura hay dos elementos a tener en cuenta:

Los cambios culturales: son los cambios a lo largo del tiempo de todos o algunos de los elementos culturales de una sociedad (o una parte de la misma).

La cultura está basada en todos nosotros.

Un ejemplo claro de este arte hilemorfista actual es Lotty Rosenfeld, una artista visual chilena, adscrita al neo-vanguardismo. Quien por medio de sus obras modifica la realidad, llevando su arte a las calles y transformando la relación convencional que existe entre materia y forma. Tal es “Un millar de cruces sobre el pavimento” Que fue una obra hecha en la época de la dictadura militar chilena en la que Rosenfeld dibujaba una cruz utilizando las líneas de señalización de las calles y colocándose una línea transversal por cada desaparecido que había de la dictadura, haciendo una contra información a los medios tradicionales de ese momento que negaban la realidad en la que estaba consumido el país, y que no daban cifras o estadísticas de los desaparecidos que había.

Un elemento esencial de la cultura es el lenguaje, hay conceptos culturales dentro de los diferentes sistemas en los que encontramos características específicas de gramática y léxico. Por tanto, incluir la lingüística en el estudio de las culturas antiguas y contemporáneas es indispensable. El análisis de la cultura desde la lingüística en los últimos sesenta años ha evolucionado desde el pensamiento estructuralista hasta la variación cultural.[34]​ Los lingüistas también han desarrollado una investigación sobre la comunicación intercultural, y recientemente han creado conceptos nuevos tales como multilingüismo y multiculturalismo para definir nuevos fenómenos culturales.[35]​



Gabriel José García Márquez (Aracataca, 6 de marzo de 1927-Ciudad de México, 17 de abril de 2014)[nota 1]​[2]​ ( escuchar) fue un escritor y periodista colombiano. Reconocido principalmente por sus novelas y cuentos, también escribió narrativa de no ficción, discursos, reportajes, críticas cinematográficas y memorias. Fue conocido como Gabo, y familiarmente y por sus amigos como Gabito.[3]​[4]​ En 1982 recibió el Premio Nobel de Literatura[5]​ «por sus novelas e historias cortas, en las que lo fantástico y lo real se combinan en un mundo ricamente compuesto de imaginación, lo que refleja la vida y los conflictos de un continente».[6]​[7]​ 

Está relacionado de manera inherente con el realismo mágico y su obra más conocida, la novela Cien años de soledad, es considerada una de las más representativas de este movimiento literario, e incluso se considera que por el éxito de la novela es que tal término se aplica a la literatura surgida a partir de los años 1960 en América Latina.[8]​[9]​ En 2007 la Real Academia Española y la Asociación de Academias de la Lengua Española publicaron una edición popular conmemorativa de esta obra, por considerarla parte de los grandes clásicos hispánicos de todos los tiempos.[10]​
Fue famoso tanto por su genialidad como escritor como por su postura política.[11]​Su amistad con el líder cubano Fidel Castro fue bastante conocida en el mundo literario y político.[12]​

Hijo de Eligio García y Luisa Santiaga Márquez Iguarán, nació en Aracataca, departamento del Magdalena, Colombia, «el domingo 6 de marzo de 1927 a las nueve de la mañana...», como refiere el propio escritor en sus memorias.[13]​
Cuando sus padres se enamoraron, el padre de Luisa, el coronel Nicolás Ricardo Márquez Mejía, se opuso a esa relación, pues Gabriel Eligio telegrafista, no era el hombre que consideraba más adecuado para su hija, por ser hijo de madre soltera, pertenecer al Partido Conservador Colombiano y ser un mujeriego confeso.[13]​

Con la intención de separarlos, Luisa fue enviada fuera de la ciudad, pero Gabriel Eligio la cortejó con serenatas de violín, poemas de amor, innumerables cartas y frecuentes mensajes telegráficos. Finalmente, la familia capituló y Luisa consiguió el permiso para casarse con Gabriel Eligio, lo cual sucedió el 11 de junio de 1927 en Santa Marta. La historia y tragicomedia de ese cortejo inspiraría más tarde a su hijo la novela El amor en los tiempos del cólera.[13]​

Poco después del nacimiento de Gabriel, su padre se convirtió en farmacéutico y, en enero de 1928, se mudó con Luisa a Barranquilla, dejando a Gabriel en Aracataca al cuidado de sus abuelos maternos. Dado que vivió con ellos durante los primeros años de su vida, recibió una fuerte influencia del coronel Nicolás Márquez, quien de joven mató a Medardo Pacheco en un duelo y tuvo, además de los tres hijos oficiales, otros nueve con distintas madres. El coronel era un liberal veterano de la Guerra de los Mil Días, muy respetado por sus copartidarios y conocido por su negativa a callar sobre la Masacre de las bananeras, suceso en el que murieron cientos de personas a manos de las Fuerzas Armadas de Colombia durante una huelga de los trabajadores de las bananeras, hecho que García Márquez plasmaría en su obra.[13]​

El coronel, a quien Gabriel llamaba Papalelo, describiéndolo como su «cordón umbilical con la historia y la realidad», fue también un excelente narrador y le enseñó, por ejemplo, a consultar frecuentemente el diccionario, lo llevaba al circo cada año y fue el primero en introducir a su nieto en el «milagro» del hielo, que se encontraba en la tienda de la United Fruit Company.[13]​
Frecuentemente decía: «Tú no sabes lo que pesa un muerto», refiriéndose así a que no había mayor carga que la de haber matado a un hombre, lección que García Márquez más tarde incorporaría en sus novelas.[13]​[14]​[15]​
Su abuela, Tranquilina Iguarán Cotes, era de origen gallego, así lo manifestó el propio Gabo en diferentes ocasiones,[16]​ a quien García Márquez llama la abuela Mina y describe como «una mujer imaginativa y supersticiosa»[14]​ que llenaba la casa con historias de fantasmas, premoniciones, augurios y signos, fue de tanta influencia en García Márquez como su marido e incluso es señalada por el escritor como su primera y principal influencia literaria, pues le inspiró la original forma en que ella trataba lo extraordinario como algo perfectamente natural cuando contaba historias y sin importar cuán fantásticos o improbables fueran sus relatos, siempre los refería como si fueran una verdad irrefutable. El propio escritor, tendría manifestado en una entrevista en 1983 al periódico español El País que:[17]​

Además del estilo, la abuela Mina inspiró también el personaje de Úrsula Iguarán que, unos treinta años más tarde, su nieto usaría en Cien años de soledad, su novela más popular.[13]​[18]​

Su abuelo murió en 1936, cuando Gabriel tenía ocho años. Debido a la ceguera de su abuela, él se fue a vivir con sus padres en Sucre, población ubicada en el departamento homónimo de Sucre, donde su padre trabajaba como farmacéutico.

Según lo afirmó su hijo Rodrigo, Gabriel había perdido la visión del centro de su ojo izquierdo desde su infancia, cuando miró directamente un eclipse. [19]​

Su niñez está relatada en sus memorias Vivir para contarla.[13]​ Después de 24 años de ausencia, en 2007 regresó a Aracataca para un homenaje que le rindió el gobierno colombiano al cumplir sus 80 años de vida y los 40 desde la primera publicación de Cien años de soledad.

Poco después de llegar a Sucre, se decidió que Gabriel debía empezar su educación formal y fue mandado a un internado en Barranquilla, un puerto en la boca del río Magdalena. Allí adquirió reputación de chico tímido que escribía poemas humorísticos y dibujaba tiras cómicas. Serio y poco dado a las actividades atléticas, fue apodado El Viejo por sus compañeros de clase.[15]​

García Márquez cursó los primeros grados de secundaria en el colegio jesuita San José (hoy Instituto San José) desde 1940, en donde publicó sus primeros poemas en la revista escolar Juventud. Luego, gracias a una beca otorgada por el Gobierno, Gabriel fue enviado a estudiar a Bogotá de donde lo reubican en el Liceo Nacional de Zipaquirá, ciudad ubicada a una hora de la capital, donde culminará sus estudios secundarios.

Durante su paso por la casa de estudios bogotana, García Márquez destacó en varios deportes, llegando a ser capitán del equipo del Liceo Nacional de Zipaquirá en tres disciplinas, fútbol, béisbol y atletismo.

Después de su graduación en 1947, García Márquez permaneció en Bogotá para estudiar Derecho en la Universidad Nacional de Colombia, donde tuvo especial dedicación a la lectura. La metamorfosis de Franz Kafka «en la falsa traducción de Jorge Luis Borges»[20]​ fue una obra que le inspiró especialmente. Estaba emocionado con la idea de escribir, no literatura tradicional, sino en un estilo similar a las historias de su abuela, en las que se «insertan acontecimientos extraordinarios y anomalías como si fueran simplemente un aspecto de la vida cotidiana». Su deseo de ser escritor crecía. Poco después, publicó su primer cuento, La tercera resignación, que apareció el 13 de septiembre de 1947 en la edición del diario El Espectador.

Aunque su pasión era la escritura, continuó con la carrera de derecho en 1948 para complacer a sus padres. Después del llamado Bogotazo en 1948, unos sangrientos disturbios que se desataron el 9 de abril a causa del magnicidio del líder popular Jorge Eliécer Gaitán, la universidad cerró indefinidamente y su pensión fue incendiada. García Márquez se trasladó a la Universidad de Cartagena y empezó a trabajar como reportero de El Universal. En 1950, desiste de convertirse en abogado para centrarse en el periodismo y se traslada de nuevo a Barranquilla para trabajar como columnista y reportero en el periódico El Heraldo. Aunque García Márquez nunca terminó sus estudios superiores, algunas universidades, como la Universidad de Columbia de Nueva York, le han otorgado un doctorado honoris causa en letras.[15]​

Durante su etapa de niñez, cuando visitaba a sus padres en Sucre, conoció a Mercedes Barcha, también hija de un boticario, en un baile de estudiantes y decidió enseguida que tenía que casarse con ella cuando terminara sus estudios.[15]​ En efecto, García Márquez contrajo matrimonio en marzo de 1958 en la iglesia de Nuestra Señora del Perpetuo Socorro de Barranquilla con Mercedes «a la que le había propuesto matrimonio desde sus trece años».[13]​[21]​

Mercedes es descrita por uno de los biógrafos del escritor como «una mujer alta y linda con pelo marrón hasta los hombros, nieta de un inmigrante egipcio, lo que al parecer se manifiesta en unos pómulos anchos y ojos castaños grandes y penetrantes».[15]​ Y García Márquez se ha referido a Mercedes constantemente y con cariño orgulloso; cuando habló de su amistad con Fidel Castro, por ejemplo, observó, «Fidel se fía de Mercedes aún más que de mí».[12]​

En 1959 tuvieron a su primer hijo, Rodrigo, quien se convirtió en cineasta, y en 1961 se instalaron en Nueva York, en donde ejerció como corresponsal de Prensa Latina. Tras recibir amenazas y críticas de la CIA y de los disidentes cubanos, que no compartían el contenido de sus reportajes, decidió trasladarse a México y se establecieron en la capital. Tres años después, nació su segundo hijo, Gonzalo, actualmente diseñador gráfico en la capital mexicana.[15]​

Aunque García Márquez poseía residencias en París, Bogotá y Cartagena de Indias, vivió la mayor parte del tiempo en su casa de la Ciudad de México, donde fijó su residencia a principios de los años 60 y en donde escribió Cien años de soledad en el número 19 de la calle La Palma de la colonia San Ángel.[22]​[14]​[23]​

La notoriedad mundial de García Márquez comenzó cuando Cien años de soledad se publicó en junio de 1967 y en una semana vendió 8000 ejemplares. De allí en adelante, el éxito fue asegurado y la novela vendió una nueva edición cada semana, pasando a vender medio millón de copias en tres años. Fue traducido a más de veinticinco idiomas y ganó seis premios internacionales. El éxito había llegado por fin y el escritor tenía 40 años cuando el mundo aprendió su nombre. Por la correspondencia de admiradores, los premios, entrevistas y las comparecencias era obvio que su vida había cambiado. En 1969, la novela ganó el Chianciano Aprecian en Italia y fue denominado el «Mejor Libro Extranjero» en Francia. En 1970, fue publicado en inglés y fue escogido como uno de los mejores 12 libros del año en Estados Unidos. Dos años después le fue concedido el Premio Rómulo Gallegos y el Premio Neustadt y en 1971, Mario Vargas Llosa publicó un libro acerca de su vida y obra, titulado García Márquez: historia de un deicidio. Para contradecir toda esta exhibición, García Márquez regresó simplemente a la escritura. Decidido a escribir acerca de un dictador, se trasladó con su familia a Barcelona (España) que pasaba sus últimos años bajo el régimen de Francisco Franco.[15]​

La popularidad de su escritura también condujo a la amistad con poderosos líderes, incluyendo el expresidente cubano Fidel Castro, amistad que ha sido analizada en Gabo y Fidel: retrato de una amistad.[12]​
En una entrevista con Claudia Dreifus en 1982, dice que su relación con Castro se basa fundamentalmente en la literatura: «La nuestra es una amistad intelectual. Puede que no sea ampliamente conocido que Fidel es un hombre culto. Cuando estamos juntos, hablamos mucho sobre la literatura». Algunos han criticado a García Márquez por esta relación; el escritor cubano Reinaldo Arenas, en 1992 en sus memorias Antes que anochezca, señala que García Márquez estaba con Castro, en 1980 en un discurso en el que este último acusó a los refugiados recientemente asesinados en la embajada de Perú de ser «chusma». Arenas recuerda amargamente a compañeros del escritor homenajear por ello con «hipócritas aplausos» a Castro.[12]​ 

También debido a su fama y a sus puntos de vista sobre el imperialismo de Estados Unidos, fue etiquetado como subversivo y por muchos años le fue negado el visado estadounidense por las autoridades de inmigración.[18]​ Sin embargo, después de que Bill Clinton fuera elegido presidente de Estados Unidos, este finalmente le levantó la prohibición de viajar a su país y afirmó que Cien años de soledad «es su novela favorita».[24]​

En 1981, el año en el que le fue concedida la Legión de Honor de Francia, regresó a Colombia de una visita con Castro, para encontrarse una vez más en problemas. El gobierno del liberal Julio César Turbay Ayala lo acusaba de financiar al grupo guerrillero M-19. Huyendo de Colombia solicitó asilo en México, donde hasta su muerte continuará manteniendo una casa.[12]​

Desde 1986 hasta 1988, García Márquez vivió y trabajó en México D. F., La Habana y Cartagena de Indias. En 1987, hubo una celebración en América y Europa del vigésimo aniversario de la primera edición de Cien años de soledad. No solo había escrito libros, también había terminado escribiendo su primera obra de teatro, Diatriba de amor contra un hombre sentado. En 1988 se estrenó la película Un señor muy viejo con unas alas enormes, dirigida por Fernando Birri, adaptación del cuento del mismo nombre.[25]​

En 1995, el Instituto Caro y Cuervo publicó en dos volúmenes el Repertorio crítico sobre Gabriel García Márquez.[25]​

En 1996 García Márquez publicó Noticia de un secuestro, donde combinó la orientación testimonial del periodismo y su propio estilo narrativo. Esta historia representa la onda inmensa de violencia y secuestros que Colombia continuaba encarando.[26]​

En 1999, el estadounidense Jon Lee Anderson publicó un libro revelador acerca de García Márquez, para lo cual tuvo la oportunidad de convivir varios meses con el escritor y su mujer en su casa de Bogotá.[26]​

En 1999 le fue diagnosticado un cáncer linfático. Al respecto, el escritor declaró en una entrevista en el año 2000 a El Tiempo de Bogotá:


En la misma entrevista, García Márquez se refiere al poema titulado La marioneta, que le fue atribuido por el diario peruano La República a modo de despedida por su inminente muerte, desmintiendo tal información.[27]​
Negó ser el autor del poema y aclaró que «el verdadero autor es un joven ventrílocuo mexicano que lo escribió para su muñeco», refiriéndose al mexicano Johnny Welch.[28]​

En 2002, su biógrafo Gerald Martin voló a México D. F. para hablar con García Márquez. Su mujer, Mercedes, tenía gripe y el escritor tuvo que visitar a Martin en su hotel. Según dijo, Gabriel García Márquez ya no tenía la apariencia del típico sobreviviente de cáncer. Todavía delgado y con el pelo corto, completó Vivir para contarla ese año.[15]​

A principios de julio de 2012, por comentarios de su hermano Jaime, se rumoreó que el escritor padecía de demencia senil, pero un vídeo en que celebra su cumpleaños en marzo de 2012 sirvió para desmentir el rumor.[29]​

En abril de 2014 fue internado en el Instituto Nacional de Ciencias Médicas y Nutrición, en México D. F., debido a una recaída producto del cáncer linfático que le fue diagnosticado en 1999. El cáncer había afectado un pulmón, ganglios e hígado. García Márquez falleció el 17 de abril de 2014.[30]​[31]​ El presidente de Colombia Juan Manuel Santos señaló que el escritor fue «el colombiano que, en toda la historia de nuestro país, más lejos y más alto ha llevado el nombre de la patria», decretando tres días de duelo nacional por su muerte.[32]​ Su cenizas reposan en el claustro de La Merced de Cartagena de Indias, a donde fueron trasladadas el 22 de mayo de 2016.[33]​[34]​

García Márquez comenzó su carrera como periodista mientras estudiaba derecho en la universidad. En 1948 y 1949 escribió para el diario El Universal de Cartagena. Desde 1950 hasta 1952, escribió una «caprichosa» columna con el seudónimo de «Septimus» para el periódico local El Heraldo de Barranquilla.[15]​
García Márquez tomó nota de su tiempo en El Heraldo. Durante este tiempo se convirtió en un miembro activo del grupo informal de escritores y periodistas conocidos como el Grupo de Barranquilla, una asociación que fue una gran motivación e inspiración para su carrera literaria. Trabajó con figuras como José Félix Fuenmayor, Ramón Vinyes, Alfonso Fuenmayor, Álvaro Cepeda Samudio, Germán Vargas, Alejandro Obregón, Orlando Rivera «Figurita» y Julio Mario Santo Domingo, entre otros.[26]​
García Márquez utilizaría, por ejemplo, a Ramón Vinyes, que sería representado como un «sabio catalán», propietario de una librería en Cien años de soledad. En esa época, García Márquez leyó las obras de escritores como Virginia Woolf y William Faulkner, quienes le influyeron en sus técnicas narrativas, los temas históricos y la utilización de localidades provinciales. El entorno de Barranquilla proporcionó a García Márquez una educación literaria a nivel mundial y una perspectiva única sobre la cultura del Caribe. Con respecto a su carrera del periodismo, Gabriel García Márquez ha mencionado que le sirvió como una herramienta para «no perder contacto con la realidad».[25]​

A petición de Álvaro Mutis en 1954, García Márquez regresó a Bogotá para trabajar en El Espectador como reportero y crítico de cine. Un año después, García Márquez publicó en el mismo diario Relato de un náufrago, una serie de catorce crónicas sobre el naufragio del destructor A. R. C. Caldas, basándose en entrevistas con Luis Alejandro Velasco, joven marinero que sobrevivió al naufragio. La publicación de los artículos dio lugar a una controversia pública a nivel nacional cuando en el último escrito reveló la historia oculta, ya que desacreditó la versión oficial de los acontecimientos que había atribuido la causa del naufragio a una tormenta.[13]​
Como consecuencia de esta controversia, García Márquez fue enviado a París para ser corresponsal extranjero de El Espectador. Escribió sus experiencias en El Independiente, un periódico que sustituyó brevemente a El Espectador, durante el gobierno militar del general Gustavo Rojas Pinilla y que más tarde fue cerrado por las autoridades colombianas. Poco después, tras el triunfo de la revolución cubana en 1960, García Márquez viajó a La Habana, donde trabajó en la agencia de prensa creada por el gobierno cubano Prensa Latina e hizo amistad con Ernesto Guevara.

En 1974, García Márquez, junto con intelectuales y periodistas de izquierda, fundó Alternativa, que duró hasta 1980 y marcó un hito en la historia del periodismo de oposición en Colombia. Para el primer número, García Márquez escribió un artículo exclusivo sobre el bombardeo al Palacio de La Moneda durante el Golpe de Estado en Chile de 1973, lo que garantizó que se agotara la edición. Luego sería el único que firmaría los artículos.[35]​

En 1994, junto con su hermano Jaime García Márquez y Jaime Abello Banfi, Gabriel García Márquez creó la Fundación Nuevo Periodismo Iberoamericano (FNPI), que tiene como objetivo ayudar a jóvenes periodistas a aprender con maestros como Alma Guillermoprieto y Jon Lee Anderson, y estimular nuevas formas de hacer periodismo. La sede principal de la entidad está en Cartagena de Indias y García Márquez fue el presidente hasta su muerte.[36]​ En su honor, la FNPI creó el Premio Gabriel García Márquez de Periodismo, que se otorga desde el año 2013 a lo mejor del periodismo iberoamericano.

Su primer cuento, La tercera resignación, fue publicado en 1947 en el periódico El Espectador. Un año después, empezó su trabajo de periodismo para el mismo periódico. Sus primeros trabajos eran todos cuentos publicados en el mismo periódico desde 1947 hasta 1952. Durante estos años publicó un total de quince cuentos.[26]​

Gabriel García Márquez quería ser periodista y escribir novelas; también quería crear una sociedad más justa.[26]​
Para La hojarasca, su primera novela, le llevó varios años encontrar un editor. Finalmente se publicó en 1955, y aunque la crítica fue excelente, la mayor parte de la edición se quedó en bodega y el autor no recibió de nadie «ni un céntimo por regalías».[13]​
García Márquez señala que «de todo lo que había escrito, La hojarasca fue su favorita porque consideraron que era la más sincera y espontánea».[26]​

Gabriel García Márquez tardó dieciocho meses en escribir Cien años de soledad.[26]​
El martes 30 de mayo de 1967 salió a la venta en Buenos Aires la primera edición de la novela. Tres décadas después se había traducido a 37 idiomas y vendido 25 millones de ejemplares en todo el mundo. «Fue un verdadero bombazo, que hizo explosión desde el primer día. El libro salió a las librerías sin ningún tipo de campaña publicitaria, la novela agotó su primera edición de 8.000 copias a las dos semanas y pronto convirtió el título y su realismo mágico en el espejo del alma latinoamericana».[37]​
Cien años de soledad ha influido en casi todos los novelistas importantes en todo el mundo. La novela hace una crónica de la familia Buendía en el pueblo de Macondo, que fue fundado por José Arcadio Buendía. Puede ser considerada una obra de realismo mágico.[38]​

El amor en los tiempos del cólera se publicó por primera vez en 1985. Está basada en las historias de dos parejas. La historia de la joven pareja formada por Fermina Daza y Florentino Ariza está inspirada en la historia de amor de los padres de García Márquez.[13]​
Sin embargo, como García Márquez explica en una entrevista: «La única diferencia es que mis padres se casaron. Y tan pronto como se casaron, ya no eran interesantes como figuras literarias».[24]​
El amor de los ancianos se basa en una historia que leyó en un periódico sobre la muerte de dos estadounidenses, de casi ochenta años de edad, que se reunían todos los años en Acapulco. Estaban en un barco y un día fueron asesinados por el barquero con sus remos. García Márquez señala: «A través de su muerte, la historia de su romance en secreto se hizo conocida. Yo estaba fascinado con ella. Estaban cada uno casado con otra persona».[24]​

En 2003, García Márquez publicó el libro de memorias Vivir para contarla, el primero de los tres volúmenes de sus memorias, que el escritor había anunciado como:


La novela Memoria de mis putas tristes apareció en 2004 y es una historia de amor que sigue el romance de un hombre de noventa años y su pubescente concubina. Este libro causó controversia en Irán, donde se prohibió después de 5000 ejemplares impresos y vendidos. En México, una ONG amenazó con demandar al escritor por hacer apología de la prostitución infantil.[39]​

Si bien, hay ciertos aspectos que casi siempre los lectores pueden esperar encontrar en la obra de García Márquez, como el humor, no hay un estilo claro y predeterminado, de plantilla. En una entrevista con Marlise Simons, García Márquez señaló:


García Márquez también es conocido por dejar fuera detalles y eventos aparentemente importantes de tal manera que el lector se ve obligado a cumplir un papel más participativo en la historia desarrollada. Por ejemplo, en El coronel no tiene quien le escriba de los personajes principales no se dan nombres. Esta práctica se ve influida por las tragedias griegas, como Antígona y Edipo rey, en el que ocurren eventos importantes fuera de la representación que se dejan a la imaginación del público.[18]​

El tema de la soledad atraviesa gran parte de las obras de García Márquez. Pelayo observa que «El amor en los tiempos del cólera, como todos los trabajos de Gabriel García Márquez, explora la soledad de la persona y de la especie humana... retrato a través de la soledad del amor y de estar en amor».[41]​

Plinio Apuleyo Mendoza le preguntó: «Si la soledad es el tema de todos sus libros ¿dónde debemos buscar las raíces de este exceso? ¿En su infancia tal vez?». García Márquez respondió: «Creo que es un problema que todo el mundo tiene. Toda persona tiene su propia forma y los medios de expresar la misma. La sensación impregna la labor de tantos escritores, aunque algunos de ellos pueden expresar lo inconsciente».[14]​

En su discurso de aceptación del Premio Nobel, La soledad de América Latina, se refiere a este tema de la soledad relacionado con América Latina: «La interpretación de nuestra realidad con esquemas ajenos sólo contribuye a hacernos cada vez más desconocidos, cada vez menos libres, cada vez más solitarios».[42]​

Otro tema importante en la obra de García Márquez es la invención de la aldea que él llama Macondo. Él usa su ciudad natal de Aracataca como una referencia geográfica para crear esta ciudad imaginaria, pero la representación del pueblo no se limita a esta área específica. García Márquez comparte: «Macondo no es tanto un lugar como un estado de ánimo».[14]​

Este pueblo de ficción se ha vuelto notoriamente conocido en el mundo literario y «su geografía y los habitantes son constantemente invocados por profesores, políticos y agentes» [...] que hacen «difícil de creer que es una pura invención».[43]​
En La hojarasca, García Márquez describe la realidad del «auge del banano» en Macondo, que incluye un período aparente de «gran riqueza» durante la presencia de empresas de los Estados Unidos, y un período de depresión con la salida de las empresas estadounidenses relacionadas con el banano. Además, Cien años de soledad se lleva a cabo en Macondo y narra la historia completa de esta ciudad ficticia desde su fundación hasta su desaparición con el último Buendia.[44]​

En su autobiografía, García Márquez explica su fascinación por la palabra y el concepto Macondo cuando describe un viaje que hizo con su madre de vuelta a Aracataca:


Según algunos académicos, Macondo —la ciudad fundada por José Arcadio Buendía en Cien años de soledad— solamente existe como resultado del lenguaje. La creación de Macondo es totalmente condicionada a la existencia de la palabra escrita. En la palabra —como instrumento de comunicación— se manifiesta la realidad, y permite al hombre lograr una unión con circunstancias independientes de su entorno inmediato.[45]​

En varias de las obras de García Márquez, entre ellas El coronel no tiene quien le escriba, La mala hora y La hojarasca, hay sutiles referencias sobre «La Violencia», guerra civil entre conservadores y liberales que se prolongó hasta los años 1960, causando la muerte de varios cientos de miles de colombianos. Son referencias a situaciones injustas que viven diversos personajes, como por ejemplo el toque de queda o la censura de prensa. La mala hora, que no es una de las novelas más famosas de García Márquez, destaca por su representación de la violencia con una imagen fragmentada de la desintegración social que provoca. Se puede decir que en estas obras «la violencia se convierte en cuento, a través de la aparente inutilidad (o sirve)de tantos episodios de sangre y muerte».[25]​

Sin embargo, aunque García Márquez describe la naturaleza corrupta y las injusticias de esa época de violencia en Colombia, se niega a usar su trabajo como una plataforma de propaganda política. «Para él, el deber del escritor revolucionario es escribir bien, y el ideal es una novela que mueve al lector por su contenido político y social, y al mismo tiempo por su poder para penetrar en la realidad y exponer su otra cara».[44]​

En las obras de García Márquez se puede encontrar también una «obsesión por captar la identidad cultural latinoamericana y particularizar los rasgos del mundo caribeño».[46]​
Asimismo, trata de deconstruir las normas sociales establecidas en esta parte del mundo. Como ejemplo, el carácter de Meme en Cien años de soledad puede ser visto como una herramienta para criticar las convenciones y prejuicios de la sociedad. En este caso, ella no conforma a la ley convencional que «las jóvenes deben llegar vírgenes al matrimonio» porque ha tenido una relación ilícita con Mauricio Babilonia.[25]​ Se puede ver otro ejemplo de esta crítica de las normas sociales a través de la relación amorosa entre Petra Cotes y Aureliano Segundo. Al final de la obra —cuando los protagonistas son viejos— se enamoran más profundamente que antes. Así, García Márquez está criticando la imagen mostrada por la sociedad de que «los viejos no pueden amar».[25]​

En su juventud, al asociarse al grupo de Barranquilla, Gabriel García Márquez comenzó a leer la obra de Ernest Hemingway, James Joyce, Virginia Woolf y, más importante, de William Faulkner de quien recibe una trascendente influencia reconocida explícitamente por él mismo cuando en su discurso de recepción del premio Nobel menciona: «mi maestro William Faulkner».[42]​ En la obra de Gabriel García Márquez titulada Nabo, el negro que hizo esperar a los ángeles, publicada en 1951, ya aparecen elementos similares a los de Faulkner como la ambigüedad deliberada y una pintura temprana de la soledad.[44]​

También emprendió un estudio de las obras clásicas, encontrando enorme inspiración en la obra de Edipo Rey de Sófocles de quien, en muchas ocasiones, Gabriel García Márquez ha expresado su admiración por sus tragedias y utiliza una cita de Antígona al principio de su obra La hojarasca cuya estructura se ha dicho también que tiene la influencia del dilema moral de Antígona.[44]​

En una entrevista a Juan Gustavo Cobo Borda en 1981, García Márquez confesó que el movimiento poético iconoclasta denominado "Piedra y cielo" (1939) fue fundamental para él, afirmando que:


Como autor de ficción, García Márquez es siempre asociado con el realismo mágico. De hecho, es considerado, junto al guatemalteco Miguel Ángel Asturias, figura central de este género. El realismo mágico se usa para describir elementos que tienen, como es el caso en los trabajos de este autor, la yuxtaposición de la fantasía y el mito con las actividades diarias y ordinarias.

El realismo es un tema importante en todas las obras de García Márquez. Él dijo que sus primeros trabajos (con la excepción de La hojarasca), como El coronel no tiene quien le escriba, La mala hora y Los funerales de la Mamá Grande, reflejan la realidad de la vida en Colombia y este tema determina la estructura racional de los libros. Dice: «No me arrepiento de haberlas escrito, pero pertenecen a un tipo de literatura premeditada que ofrecen una visión de la realidad demasiado estática y exclusiva».[14]​

En sus otras obras ha experimentado más con enfoques menos tradicionales a la realidad, de modo que «lo más terrible, lo más inusual se dice con expresión impasible».[44]​
Un ejemplo comúnmente citado es la ascensión espiritual y física al cielo de un personaje mientras está colgando la ropa para secar, en Cien años de soledad. El estilo de estas obras se inscribe en el concepto de lo «real maravilloso» descrito por el escritor cubano Alejo Carpentier y ha sido etiquetado como realismo mágico.[48]​
El crítico literario Michael Bell propone una interpretación alternativa para el estilo de García Márquez, por cuanto la categoría de realismo mágico ha sido criticada por ser dicotomizadora y exotizadora: «Lo que está realmente en juego es una flexibilidad psicológica que es capaz de habitar nada sentimentalmente el mundo diurno mientras se mantiene abierta a las incitaciones de aquellos dominios que la cultura moderna tiene, por su propia lógica interna, necesariamente marginalizados o reprimidos».[41]​
García Márquez y su amigo Plinio Apuleyo Mendoza discuten su trabajo de un modo similar, «El tratamiento de la realidad en tus libros... ha recibido un nombre, el de realismo mágico. Tengo la impresión de que tus lectores europeos suelen advertir la magia de las cosas que tú cuentas, pero no ven la realidad que las inspira. Seguramente porque su racionalismo les impide ver que la realidad no termina en el precio de los tomates o de los huevos».[14]​

García Márquez crea un mundo tan semejante al cotidiano pero al mismo tiempo totalmente diferente a ello. Técnicamente, es un realista en la presentación de lo verdadero y de lo irreal. De algún modo trata diestramente una realidad en la que los límites entre lo verdadero y lo fantástico se desvanecen muy naturalmente.[44]​

García Márquez considera que la imaginación no es sino un instrumento de la elaboración de la realidad y que una novela es la representación cifrada de la realidad y a la pregunta de si todo lo que escribe tiene una base real, ha contestado:[14]​


Su discurso de aceptación fue titulado La soledad de América Latina.[42]​
Fue el primer colombiano y el cuarto latinoamericano en ganar un Premio Nobel de Literatura, después de lo cual declaró: «Yo tengo la impresión de que al darme el premio han tenido en cuenta la literatura del subcontinente y me han otorgado como una forma de adjudicación de la totalidad de esta literatura».

García Márquez ha recibido muchos otros premios, distinciones y homenajes por sus obras como los relacionados a continuación:[25]​



García Márquez es una parte importante del boom latinoamericano de la literatura. Sus obras han recibido numerosos estudios críticos, algunos extensos y significativos, que examinan la temática y su contenido político e histórico. Otros estudios se enfocan sobre el contenido mítico, las caracterizaciones de los personajes, el ambiente social, la estructura mítica o las representaciones simbólicas en sus obras más notables.[38]​

Mientras que las obras de García Márquez atraen a una serie de críticos, muchos eruditos elogian su estilo y creatividad. Por ejemplo, Pablo Neruda escribió sobre Cien años de soledad que «es la mayor revelación en lengua española desde el Don Quijote de Cervantes».[24]​

Algunas críticas arguyen que a García Márquez le falta la experiencia adecuada en la arena literaria y que solamente escribe de sus experiencias personales e imaginación. De esta manera, dicen que sus obras no deben ser significativas. En respuesta a esto, García Márquez ha mencionado que él está de acuerdo que a veces su inspiración no viene de libros, sino de la música.[24]​
Sin embargo, según Carlos Fuentes, García Márquez ha logrado una de las mayores características de la ficción moderna. Eso es la liberación del tiempo, a través de la liberación de un instante a partir del momento que permite a la persona humana recrear a sí mismo y a su tiempo.[51]​
A pesar de todo, nadie puede negar que García Márquez ha ayudado a rejuvenecer, reformular y recontextualizar la literatura y la crítica en Colombia y en el resto de América Latina.[52]​ Al este del Atlántico Cervantes, al oeste García Márquez, dos baluartes captaron la realidad honda de su momento y dejaron una visión encantada de un mundo no soñado, a flor de tierra.[53]​

En 1983, cuando se le preguntó a Gabriel García Márquez: «¿Es usted comunista?» el escritor respondió: «Por supuesto que no. No lo soy ni lo he sido nunca. Ni tampoco he formado parte de ningún partido político».[54]​
García Márquez contó a su amigo Plinio Apuleyo Mendoza: «Quiero que el mundo sea socialista y creo que tarde o temprano lo será».[55]​
Según Ángel Esteban y Stéphanie Panichelli, «Gabo entiende por socialismo un sistema de progreso, libertad e igualdad relativa» donde saber es, además de un derecho, un izquierdo (hay un juego de palabras que ambos autores utilizan para titular el capítulo de su libro: "Si saber no es un derecho, seguro será un izquierdo").[12]​
García Márquez viajó a muchos países socialistas como Polonia, Checoslovaquia, Alemania Oriental, la Unión Soviética, Hungría, y después escribió algunos artículos, mostrando su «desacuerdo con lo que allí ocurría».[56]​
En 1971, en una entrevista para la revista "Libre" (que patrocinaba) declaró: «Yo sigo creyendo que el socialismo es una posibilidad real, que es la buena solución para América Latina, y que hay que tener una militancia más activa».[57]​

En 1959, García Márquez fue corresponsal en Bogotá de la agencia de prensa Prensa Latina creada por el gobierno cubano después del comienzo de la revolución cubana para informar sobre los acontecimientos en Cuba. Allí «tenía que informar objetivamente sobre la realidad colombiana y difundir a la vez noticias sobre Cuba y su trabajo consistía en escribir y enviar noticias a La Habana. Era la primera vez que García Márquez hacia periodismo verdaderamente político».[12]​ Más tarde, en 1960, fundó con su amigo Plinio Apuleyo Mendoza una revista política, Acción Liberal, que quebró después de publicarse tres números.[58]​

Gabriel García Márquez conoció a Fidel Castro en enero de 1959 pero su amistad se formó después, cuando García Márquez estaba trabajando con Prensa Latina, viviendo en La Habana y se vieron de nuevo varias veces. Después de conocer a Castro, «Gabo estaba convencido de que el líder cubano era diferente a los caudillos, héroes, dictadores o canallas que habían pululado por la historia de Latinoamérica desde el siglo XIX, e intuía que solo a través de él esa revolución, todavía joven, podría cosechar frutos en el resto de los países americanos».[12]​

Según Panichelli y Esteban, «ejercer un poder es uno de los placeres más reconfortantes que el hombre puede sentir», y ellos piensan que eso es el caso con García Márquez «hasta una edad madura». Por eso, se ha cuestionado la amistad entre García Márquez y Castro y si es un resultado de la admiración de García Márquez por el poder.[12]​

Jorge Ricardo Masetti, exguerrillero y periodista argentino, piensa que Gabriel García Márquez «es un hombre a quien le gusta estar en la cocina del poder».[12]​

En opinión de César Leante, García Márquez tiene algo de obsesión con los caudillos latinoamericanos. También dice que «El apoyo incondicional de García Márquez a Fidel Castro cae en buena parte dentro del campo psicoanalítico […] cual es la admiración que el criador del Patriarca ha sentido, siempre y desmesuradamente, por los caudillos latinoamericanos brotados de las montoneras. Verbigracia, el coronel Aureliano Buendía, pero sobre todo el innominado dictador caribeño que como Fidel Castro envejece en el poder». Dice Leante que García Márquez «es considerado en Cuba como una especie de ministro de cultura, jefe de cinematografía y embajador plenipotenciario, no del Ministerio de Relaciones Exteriores, sino directamente de Castro, que lo emplea para misiones delicadas y confidenciales que no encarga a su diplomacia».[59]​

Juan Luis Cebrián ha llamado a Gabriel García Márquez «un mensajero político», debido a sus artículos.[56]​

Según el británico Gerald Martin, quien publicó en 2008 la primera biografía autorizada del novelista, García Márquez siente una «enorme fascinación por el poder». Señala que «Él ha querido ser siempre testigo del poder y es justo decir que esa fascinación no es gratuita, sino que persigue determinados objetivos» y menciona que muchos consideran como excesiva su proximidad al líder cubano Fidel Castro.[15]​ Martin recuerda que también se relacionó con Felipe González (expresidente del Gobierno español) o con Bill Clinton (expresidente de Estados Unidos) pero «todo el mundo se fija sólo en su relación con Castro».[60]​

Por otra parte, el diplomático, periodista, biógrafo y compadre del Nobel, Plinio Apuleyo Mendoza señala que «Él es amigo de Castro, pero no creo que sea partidario del sistema, porque nosotros visitamos el mundo comunista y quedamos muy desencantados».[61]​

García Márquez participó como mediador en las conversaciones de paz adelantadas entre el Ejército de Liberación Nacional (ELN) y el gobierno colombiano que tuvieron lugar en Cuba y entre el gobierno de Belisario Betancourt y el grupo Movimiento 19 de abril (M-19); igualmente participó en el proceso de paz entre el gobierno de Andrés Pastrana y la guerrilla de las Fuerzas Armadas Revolucionarias de Colombia (FARC) que sin embargo fracasó.[62]​

En 2006, García Márquez se unió a la lista de prominentes figuras de América Latina como Pablo Armando Fernández, Ernesto Sabato, Mario Benedetti, Eduardo Galeano, Thiago de Mello, Frei Betto, Carlos Monsiváis, Pablo Milanés, Ana Lydia Vega, Mayra Montero y Luis Rafael Sánchez que apoyan la independencia de Puerto Rico, a través de su adhesión a la "Proclama de Panamá" aprobada por unanimidad en el Congreso Latinoamericano y Caribeño por la Independencia de Puerto Rico, celebrado en Panamá en noviembre de 2006.[63]​

La política desempeña un papel importante en las obras de García Márquez, en las que utiliza representaciones de varios tipos de sociedades con diferentes formas políticas para presentar sus opiniones y creencias con ejemplos concretos, aunque sean ejemplos ficticios. Esa diversidad de maneras con que García Márquez representa al poder político es una muestra de la importancia de la política en sus obras. Una conclusión que puede ser derivada de sus obras es que «la política puede extenderse más allá o más acá de las instituciones propias del poder político».[11]​

Por ejemplo, en su obra Cien años de soledad tenemos la representación de un lugar «donde no existe todavía un poder político consolidado y no hay, por lo tanto, ley en el sentido de precepto votado por el Congreso y sancionado por el presidente, que regule las relaciones entre los hombres, entre estos y el poder público y la constitución y funcionamiento de este poder».[11]​En contraste, la representación del sistema político en El otoño del patriarca es la de una dictadura, en la que el líder es grotesco, corrupto y sanguinario y con un poder tan grande que alguna vez preguntó qué hora es y le habían contestado la que usted ordene, mi general».[11]​

Una de las primeras novelas de García Márquez, La mala hora, puede ser una referencia a la dictadura de Gustavo Rojas Pinilla y representa la tensión política y la opresión en un pueblo rural, cuyos habitantes aspiran a la libertad y la justicia pero sin éxito en conseguir ninguna de las dos.[26]​

García Márquez profesó un interés particular por el cine y la televisión, participando como guionista, mecenas y permitiendo la adaptación de su obra. Ya en su etapa juvenil en Barranquilla, conjuntamente con el pintor Enrique Grau, el escritor Álvaro Cepeda Samudio y el fotógrafo Nereo López, participó en la realización del cortometraje surrealista La langosta azul (1954).[64]​

Posteriormente, en los años cincuenta, estudió la carrera de cine en el Centro Sperimentale Di Cinematografia de Roma, teniendo como condiscípulos al argentino Fernando Birri y al cubano Julio García Espinosa, que más tarde serían considerados fundadores de la llamada Fundación del Nuevo Cine Latinoamericano. Estas tres personalidades han declarado en reiteradas oportunidades el impacto que supuso para ellos ver la película Milagro en Milán de Vittorio de Sica, así como también asistir al nacimiento del neorrealismo italiano, tendencia que los hizo vislumbrar la posibilidad de realizar cine en América Latina siguiendo las mismas técnicas. Es preciso anotar que esta estancia en Roma sirvió para que el escritor aprendiera varios de los entresijos que comporta el quehacer cinematográfico, en tanto y cuanto compartió largas horas de trabajo en moviola al lado del guionista Cesare Zavattini. Este particular afinó en García Márquez una precisión cinematográfica a la hora de narrar con imágenes, que más tarde usaría como parte de su trabajo en la Ciudad de México García Márquez presidió desde 1986 la Fundación del Nuevo Cine Latinoamericano, que tiene sede en La Habana.

Se tiene conocimiento de que muchas obras cinematográficas mexicanas de los años 1960 fueron escritas por García Márquez, quien al igual que muchos intelectuales de la época firmó los guiones con seudónimo. Memorables son, en todo caso, El gallo de oro (1964), de Roberto Gavaldón,[65]​ y Tiempo de morir (1966), de Arturo Ripstein. La primera, basada en el cuento homónimo de Juan Rulfo, coescrita junto con el propio autor y el también escritor mexicano Carlos Fuentes, fue protagonizada por Ignacio López Tarso, Narciso Busquets y Lucha Villa, y fotografiada por el insigne Gabriel Figueroa. La segunda, western filmado inicialmente por Ripstein, tuvo su secuela casi veinte años más tarde bajo la tutela de Jorge Alí Triana.

Además de las tres películas citadas, entre 1965 y 1985, García Márquez participó directamente como guionista en los siguientes filmes: En este pueblo no hay ladrones (1965), de Alberto Isaac; Juego peligroso (segmento "HO") (1966), de Luis Alcoriza y Arturo Ripstein; Patsy, mi amor (1968), de Manuel Michel; Presagio (1974), de Luis Alcoriza; La viuda de Montiel (1979), de Miguel Littín; María de mi corazón (1979), de Jaime Humberto Hermosillo; El año de la peste (1979), de Felipe Cazals (adaptación del libro de Daniel Defoe El diario de la peste), y Eréndira (1983), de Ruy Guerra.[66]​

En 1975 R.T.I. Televisión de Colombia produce la serie televisiva La mala hora dirigida por Bernardo Romero Pereiro, basada en la novela homónima de García Márquez y transmitida en 1977.[67]​

En 1986, conjuntamente con sus dos condiscípulos del Centro Sperimentale di Cinematografía, y apoyados por el Comité de Cineastas de América Latina, funda la Escuela Internacional de Cine y Televisión de San Antonio de Los Baños en Cuba, institución a la cual le dedicará tiempo y dinero de su propio bolsillo para apoyar y financiar la carrera de cine de jóvenes provenientes de América Latina, el Caribe, Asia y África. A partir del año siguiente, en dicho centro se dedicará a impartir el taller «Cómo se cuenta un cuento», fruto del cual salen innumerables proyectos audiovisuales, amén de varios libros sobre dramaturgia.

En 1987, Francesco Rosi dirige la adaptación de Crónica de una muerte anunciada, protagonizada por Rupert Everett, Ornella Muti, Gian Maria Volonté, Irene Papas, Lucía Bosé y Anthony Delon.

En 1988 se produjeron y exhibieron: Un señor muy viejo con unas alas enormes, de Fernando Birri, con Daisy Granados, Asdrúbal Meléndez y Luis Ramírez; Milagro en Roma, de Lisandro Duque Naranjo, con Frank Ramírez y Amalia Duque García; Fábula de la bella palomera, de Ruy Guerra, con Claudia Ohana y Ney Latorraca, y Cartas del parque, de Tomás Gutiérrez Alea, con Ivón López, Víctor Laplace, Miguel Paneque y Mirta Ibarra.

En 1990, García Márquez, viajó a Japón, haciendo escala en Nueva York para conocer al director contemporáneo cuyos guiones más admira: Woody Allen. La razón de su viaje al país oriental es la de encontrarse con Akira Kurosawa, en ese momento rodando Los Sueños de Akira Kurosawa, interesado en llevar a la gran pantalla la historia de El otoño del patriarca, ambientado en el Japón medieval. La idea de Kurosawa fue totalizadora, incrustar toda la novela en el celuloide sin importar el metraje; infortunadamente, para esta idea no existió posibilidad de financiación, y el proyecto quedó en eso.

En 1991, la televisión colombiana produce María, la novela de Jorge Isaacs, adaptada por García Márquez junto con Lisandro Duque Naranjo y Manuel Arias.

En 1996 se presentó Edipo Alcalde, adaptación de Edipo rey de Sófocles hecha por García Márquez y Estela Malagón, dirigida por Jorge Alí Triana, y protagonizada por Jorge Perugorría, Ángela Molina y Paco Rabal.

En 1999, Arturo Ripstein filma El coronel no tiene quien le escriba, protagonizada por Fernando Luján, Marisa Paredes, Salma Hayek y Rafael Inclán.

En 2001 aparece Los niños invisibles, de Lisandro Duque Naranjo.

En 2006 se rodó El amor en los tiempos del cólera, con guion del sudafricano Ronald Harwood y bajo la batuta del director británico Mike Newell. Filmada en Cartagena de Indias, los personajes son encarnados por Javier Bardem, Giovanna Mezzogiorno, John Leguizamo, Catalina Sandino y Benjamin Bratt.

En marzo de 2010, y en el marco del Festival Internacional de Cine de Cartagena, se estrenó la versión fílmica de Del amor y otros demonios, coproducción entre Colombia y Costa Rica dirigida por la costarricense Hilda Hidalgo.

Memoria de mis putas tristes, coproducción entre Dinamarca y México, dirigida por el danés Henning Carlsen y con la adaptación cinematográfica a cargo del francés Jean-Claude Carrière iba a ser filmada en el 2009 en el estado de Puebla, pero se suspendió por problemas de financiación al parecer por una polémica motivada por el tema[68]​ por la amenaza de demanda de una ONG calificando la novela y el guion como apología de la prostitución infantil y pederastia.[39]​ Finalmente, la película fue filmada en secreto en la ciudad de San Francisco de Campeche (México) en 2011, protagonizada por Emilio Echevarría y se estrenó en 2012.[69]​

García Márquez incursionó poco directamente en teatro, pues solo se conoce el monólogo Diatriba de amor contra un hombre sentado, montada por primera vez en 1988 en Buenos Aires y reestrenada en 1994 en el Teatro Nacional de Bogotá.[70]​

Su obra en el teatro en su mayoría han sido adaptaciones de sus novelas. En 1991, Juan Carlos Moyano adaptó y dirigió un espectáculo de teatro de calle y plaza pública llamado Memoria y olvido de Úrsula Iguarán, basado en la novela Cien años de soledad, que presentó en el Festival Internacional de Teatro de Manizales de 1991 y en el Festival Iberoamericano de Teatro de Bogotá de 1992. En el 2000, Jorge Alí Triana estrenó la versión teatral de Crónica de una muerte anunciada adaptación de la novela homónima, con gran éxito nacional e internacional.[71]​

Igualmente la obra de García Márquez ha sido adaptada al género de la ópera:



Se denomina fruta a aquellos frutos comestibles obtenidos de plantas cultivadas o silvestres que, por su sabor generalmente dulce-acidulado, su aroma intenso y agradable y sus propiedades nutritivas, suelen consumirse mayormente en su estado fresco, como jugo o como postre (y en menor medida, en otras preparaciones), una vez alcanzada la madurez organoléptica, o luego de ser sometidos a cocción.[1]​

La definición del diccionario de la Real Academia Española no es específica: "fruto  comestible de ciertas plantas cultivadas  ; p. ej. la pera, la guinda, la fresa, etc."[2]​ Sin embargo, por los ejemplos dados, se evidencia que el término fruta se refiere a frutos para uso prioritario (aunque no excluyente) como postre, producidos en su mayoría por plantas leñosas (es decir, árboles frutales; por ejemplo, manzano, peral, melocotonero o durazno, ciruelo, cerezo, albaricoquero o damasco, higuera, vid, naranjo, mandarino, limonero, mango, papaya, chirimoya, guayabo, etc.) o por plantas semileñosas (arbustos frutales; por ejemplo, arándano, zarzamora, frambuesa, etc.) y, en mucha menor medida, por plantas herbáceas (por ejemplo, frutilla o fresa, banano o plátano).

El consumo de frutas aporta pocas calorías y un alto porcentaje de agua (entre 80 y 95 % de su peso fresco), por lo que facilita la hidratación del organismo.[3]​ Coadyuva al correcto funcionamiento del aparato digestivo por el aporte de fibra alimentaria.[3]​ Salvo excepciones (por ejemplo, el coco y el aceite de palma obtenido del fruto de Elaeis guineensis), las frutas no aportan grasas saturadas. Algunas frutas son fuentes de ácidos grasos esenciales para el organismo, tales como los frutos secos y las paltas o aguacates.[3]​ Las frutas son además una importante fuente de energía para el organismo por su alto contenido en hidratos de carbono solubles de rápida disponibilidad.[3]​

Como alimento, las frutas realizan aportes a la dieta que son de suma importancia para la salud humana. En general, son ricas en vitaminas, sales minerales y antioxidantes.[3]​ Algunas vitaminas y minerales, como la vitamina C y el potasio, dependen en buena medida de las frutas como fuentes de suministro.[3]​ Existen frutas como la sandía o el melón que contienen un alto índice de agua.

Hay diferentes formas de clasificar la fruta, según sea su tipo, la forma de recolección o el proceso de maduración.

Siempre, la velocidad de maduración y la vida en postcosecha no se asocia con el carácter climatérico o no climatérico de las frutas, sino con la respiración: cuanto mayor es la tasa respiratoria (constante o no), mayor es la perecibilidad de la fruta.[5]​ Por ejemplo, la manzana es una fruta climatérica que evidencia un pico en la producción de etileno y en la tasa respiratoria durante su maduración. Sin embargo, su tasa respiratoria media-baja le asegura una vida en postcosecha más prolongada que la de algunos frutos no climatéricos, como las fresas, las zarzamoras o las frambuesas que poseen tasas respiratorias más elevadas.

Hay además, algunos grupos de frutas que se distinguen por tener ciertas características comunes:

La composición química de las frutas depende sobre todo del tipo de fruta y de su grado de maduración.

Las frutas pertenecen al grupo 5 de la rueda de alimentos, ricos en azúcares, vitaminas C y A y sales minerales, representada en dicha rueda de color verde. Por su alto contenido en vitaminas y sales minerales pertenece al grupo de alimentos reguladores. Las frutas se localizan en el segundo piso de la pirámide de alimentos, es decir, que se recomienda la ingesta de 4 piezas de fruta en niños y 2 piezas en el adulto al día. A pesar de que en la clasificación general por grupos, las verduras y frutas están en grupos diferentes, los nutrientes que contienen son similares, aunque en el caso de las frutas el contenido en hidratos de carbono es más elevado y ello las convierten en alimentos un poco más energéticos. Por lo tanto:

La fruta no puede ser substituida por otros postres más modernos sin desequilibrar nuestra alimentación. Forma parte de nuestro comportamiento alimentario tomar fruta después de las principales comidas, aunque hoy día se sustituye con frecuencia por productos lácteos, es preciso decir que esta sustitución no es adecuada si se hace de forma habitual, debiendo hacerse solo en ocasiones especiales.[9]​ El consumo adecuado de fruta recomendado, por la OMS es de 5 porciones diarias.[10]​

Las transformaciones que se producen en las frutas debido a la maduración son:

Estas transformaciones pueden seguir evolucionando hasta el deterioro de la fruta. El etileno es un compuesto químico que genera la fruta antes de madurar y es fundamental para que la fruta madure. El etileno es un compuesto derivado halogenado. En las frutas maduras su presencia determina el tiempo de la maduración, por lo que el control de su producción será clave para su conservación. En las no climatéricas la presencia de etileno provoca una intensificación de la maduración.

La manipulación de la maduración se puede hacer modificando la temperatura, la humedad relativa y los niveles de oxígeno, dióxido de carbono y etileno.

La fruta debe ser consumida, principalmente como fruta fresca. Un almacenamiento prolongado no es adecuado; tampoco sería posible para algunos tipos de fruta, como las cerezas o las fresas. Muchas especies de frutas no pueden ser conservadas frescas, porque tienden a descomponerse rápidamente. Para la conserva o almacenamiento de la fruta hay que tener en cuenta que la temperatura ambiental elevada favorece la maduración ya que la temperatura demasiado alta puede afectar al aroma y al color. La fruta que se almacena debe estar sana, no deteriorada y exenta de humedad exterior. No se aconseja guardar juntas diferentes variedades de fruta ni las frutas con hortalizas, sobre todo con la patata, ya que se piensa que puede influir en la maduración. Tampoco deben guardarse las manzanas junto con las zanahorias porque hacen que estas últimas se pongan amargas. No se aconseja guardar los plátanos en la nevera porque el aroma y el aspecto se deterioran. El resto de las frutas si pueden guardarse en el frigorífico. Se recomienda guardar las frutas delicadas como máximo dos días, una semana las frutas con hueso, y unos diez días los cítricos maduros. Las manzanas y peras pueden guardarse algunos meses en una habitación fresca a unos 12 grados, aireada y oscura con un 80 y 90 % humedad.

En la conservación a gran escala o industrial de la fruta el objetivo más importante para alcanzar dicha conservación será el control de su respiración, evitando la maduración de las frutas climatéricas e intentando que la maduración de las frutas no climatéricas sea lo más lento posible. La fruta antes de madurar se conserva en ambientes muy pobre en oxígeno, y si es posible con altas concentraciones de anhídrido carbónico. Deben colocarse en lugares oscuros y con temperaturas inferiores a los 20 °C. Estas condiciones controlan la producción de etileno.

La fruta ya madura debe mantenerse en condiciones de poca luz, bajas temperaturas entre 0 y 6 grados centígrados y alta humedad relativa, próxima al 90 %.

Hay que separar las frutas maduras de las que no lo están, ya que una sola pieza puede hacer madurar al resto. Especialmente las manzanas, cuando una está podrida hay que separarla inmediatamente y desecharla para que no se pudran las demás.

Para poder disfrutar de fruta todo el año, se procede a su conservación:

La posibilidad de utilizar frutas es aún mayor con las diversas preparaciones de frutas:

La India lidera la producción mundial de fruta a gran escala, posible gracias a su clima húmedo, seguida de Vietnam y China.

La producción en miles de dólares internacionales está calculada según precios internacionales de 1999-2001
Fuente: Organización de las Naciones Unidas para la Alimentación y la Agricultura: Dirección de estadística

Por otra parte, respecto a la fruta tropical, Filipinas es el principal productor, seguido de Indonesia e India.

La producción en miles de dólares internacionales está calculada según precios internacionales de 1999-2001
Fuente: Organización de las Naciones Unidas para la Alimentación y la Agricultura: Dirección de estadística



Efecto laxante

Las frutas se han considerado como un coadyuvante para prevenir el estreñimiento. La fibra dietaria o alimentaria presente en ellas, se consideran la clave para esta propiedad. Pueden tener dos tipos de fibra la soluble y la insoluble. La cantidad promedio que aportan de fibra las frutas oscila en 9 gramos por 100 gramos. Existen otros compuestos que pueden ser laxantes, como el sorbitol o compuestos fenólicos; las frutas recomendadas para este efecto son las ciruelas pasas (frescas o en zumo), tienen alto contenido de sorbitol (dosis laxante hombres 0,4 g/kg, mujeres 1 g/kg).[12]​

Frutas y cardiopatías

Las enfermedades cardiovasculares (enfermedad coronaria o infarto al miocardio), tienen una estrecha relación de incidencia con los hábitos dietéticos, muchos estudios reportan especial atención al consumo de frutas y vegetales en cuanto a los altos beneficios, ya que aportan ciertas sustancias, como antioxidantes, folatos, fibra, potasio, flavonoides y otros fitoquímicos (licopeno); los cuales intervienen en mecanismos para el control de hipertensión arterial, dislipidemias, control de procesos oxidativos (formación de radicales libres) responsables del desarrollo de la enfermedad cardiovascular.[12]​

Las recomendaciones del ACC/American Heart Association basan el tratamiento en cambios del estilo de vida, para la Disminución y control de la hipertensión arterial y lipoproteínas de baja Densidad, dentro de estos cambios se hace hincapié en el aumento del consumo de frutas, vegetales y cereales de grano entero. [13]​

Frutas y cáncer

Varios estudios prospectivos, multicentricos, han mostrado la evidencia significativa de una correlación inversa entre consumo de frutas y vegetales y el riesgo de padecer cáncer. Por ello en 1997 el World Cancer Research Fund, estableció la siguiente recomendación ¨comer entre 400-800 g o 5 o más porciones al día de una variedad de vegetales y frutas a lo largo del todo el año¨.[12]​

Las frutas son una fuente importante de vitamina C y otros antioxidantes, como carotenoides, polifenoles y flavonoides; en un meta análisis se estimó una reducción significativa del riesgo del 28 %/100 g diarios de fruta consumida . Cuando el análisis se restringió a los cítricos, la reducción significativa del riesgo fue del 25%/50 g/día de fruta consumida. En la cohorte europea European Prospective Investigation into Cancer and Nutrition (EPIC) se observó un 40%  de disminución significativa del riesgo para el quintil superior de consumo de fruta respecto al menor, las frutas cítricas particularmente reporto reducción del 24 %•[14]​ 

Frutas y metabolismo de la glucosa
Los estudios experimentales han demostrado que una alta ingesta de fibra reduce la concentración de glucosa e insulina en sangre de personas diabéticas y produce una baja tolerancia a la glucosa.  Los alimentos con índice glicémico bajo, sin relación a su contenido de fibra en particular, se asocian con una mejor respuesta de la glucosa postprandial y con una mejora global del control de la glucemia.[12]​

Frutas y obesidad

El alto consumo de frutas y vegetales en personas con obesidad, asegura la disminución de un 30% de la ingesta calórica diaria, pero no afecta la palatabilidad y la sensación de saciedad de las mismas. Pero el alto contenido de fibra insoluble de algunas frutas puede ayudar a controlar la saciedad, mediante el enlentecimiento del vaciamiento gástrico[12]​

La rueda es un elemento circular y mecánico que gira alrededor de un eje.[1]​ Puede ser considerada una máquina simple, y forma parte del conjunto denominado elementos de máquinas.

Es uno de los inventos fundamentales en la Historia de la humanidad, por su gran utilidad en la elaboración de alfarería, y también en el transporte terrestre,  como componente fundamental de máquinas. El conocimiento de su origen se pierde en el tiempo, pues nadie sabe quién la inventó[1]​ y sus múltiples usos han sido esenciales en el desarrollo del progreso humano: como por ejemplo las primeras carreras impulsadas por caballos después siguiendo las máquinas de vapor.

En su forma primitiva, una rueda es un bloque circular de un material duro y duradero en cuyo centro se ha perforado un orificio a través del cual se coloca un cojinete del eje sobre el cual gira la rueda cuando se aplica un par motor a la rueda alrededor de su eje. El conjunto de rueda y eje puede considerarse una de las seis máquinas simples. Cuando se coloca verticalmente debajo de una plataforma o caja de carga, la rueda que gira sobre el eje horizontal permite transportar cargas pesadas. Esta disposición es el tema principal de este artículo, pero hay muchas otras aplicaciones de una rueda que se tratan en los artículos correspondientes: cuando se coloca horizontalmente, la rueda que gira sobre su eje vertical proporciona el movimiento giratorio que se utiliza para dar forma a los materiales (por ejemplo, una rueda de alfarero ); cuando está montado en una columna conectada a un timón o al mecanismo de dirección de un vehículo con ruedas, puede usarse para controlar la dirección de un barco o vehículo (por ejemplo, el volante de un barco o el volante); cuando se conecta a una manivela o motor, una rueda puede almacenar, liberar o transmitir energía (por ejemplo, el volante). Una rueda y un eje con fuerza aplicada para crear torque en un radio puede traducir esto en una fuerza diferente en un radio diferente, también con una velocidad lineal diferente.

La palabra rueda viene del latín "rota" ("rueda"). Por su parte, la palabra inglesa wheel proviene del inglés antiguo hweol, hweogol, de la lengua  protogermánica hwehwlan, *hwegwlan, del Protoindoeuropeo *kwekwlo-,[2]​ una forma extendida de la raíz *kwel- "girar, moverse".
Los cognados dentro del indoeuropeo incluyen el Islandés hjól "rueda, neumático", griego κύκλος kúklos, y sánscrito chakra, los dos últimos significan "círculo" o "rueda".[3]​

El lugar y la época de la "invención" de la rueda se han atribuido durante mucho tiempo a la civilización mesopotámica.[4]​[5]​[6]​ Sin embargo, esto sigue sin estar claro, ya que las pruebas más antiguas no garantizan la existencia de un verdadero medio de transporte con ruedas o están fechadas de forma demasiado diferente.[7]​ La invención de la rueda de disco de madera maciza se sitúa en el Neolítico tardío, y puede verse en conjunción con otros avances tecnológicos que dieron lugar a la Edad del Bronce temprana. Esto implica el paso de varios milenios sin rueda incluso después de la invención de la agricultura y de la cerámica, durante el Neolítico Acerámico.

La mayoría de los autores  estiman que la rueda fue inventada en el V milenio a. C. en Mesopotamia, durante el período de El Obeid (hacia el 4500 a. C.), en la antigua región conocida como Creciente Fértil, inicialmente, con la función de rueda de alfarero.

Posteriormente se empleó en la construcción de carros; se difundió por el Viejo Mundo junto con los carros y los animales de tiro. Usualmente se cree que la rueda migró a Europa y Asia Occidental en el IV milenio a. C., y a la cultura del valle del Indo hacia el III milenio a. C. Sin embargo, la rueda de carro más antigua que se conoce se encontró en Eslovenia.

Barbieri-Baja (2000) aboga por la existencia de vehículos chinos con ruedas alrededor del 2000 a. C., aunque su referencia más antigua data de alrededor del 1200 a. C.

Entre las culturas americanas no prosperó, probablemente por la ausencia de grandes bestias que pudieran tirar de los vehículos, y porque las civilizaciones más avanzadas ocupaban terrenos escarpados. Han sido encontradas ruedas en objetos olmecas identificados como juguetes que datan de alrededor del 1500 a. C.

A veces se atribuye a la cultura Halaf de 6500-5100 a.C. la primera representación de un vehículo con ruedas, pero esto es dudoso, ya que no hay pruebas de que los halafianos utilizaran vehículos con ruedas ni siquiera ruedas de cerámica.[8]​ Los precursores de las ruedas, conocidos como "tournettes" o "ruedas lentas", se conocían en el Oriente Medio en el quinto milenio antes de Cristo. Uno de los primeros ejemplos se descubrió en Tepe Pardis, Irán, y se fechó entre el 5200 y el 4700 a.C. Estaban hechas de piedra o arcilla y se fijaban al suelo con una clavija en el centro, pero requerían un gran esfuerzo para girar. Las verdaderas ruedas de alfarero, que giran libremente y tienen un mecanismo de  rueda y eje, fueron desarrolladas en Mesopotamia (Irak) hacia el 4200-4000 a.C.[9]​ El ejemplo más antiguo que se conserva, que se encontró en Ur (el actual Irak), data de aproximadamente el 3100 a.C.[10]​ También se han encontrado ruedas en la Civilización del Valle del Indo, una civilización del IV milenio a.C. que abarca zonas de la actual India y Pakistán.[11]​

La evidencia indirecta más antigua de movimiento con ruedas se encontró en forma de ruedas de arcilla en miniatura al norte del Mar Negro antes del 4000 a.C. Desde mediados del IV milenio a.C. en adelante, la evidencia se condensa en toda Europa en forma de carros de juguete, representaciones o surcos.[12]​ En Mesopotamia, las representaciones pictográficas de vagones con ruedas encontradas en tablillas de arcilla en el distrito de Eanna de Uruk, de la civilización sumeria, están datadas en torno al 3500-3350 a.C.[13]​ En la segunda mitad del IV milenio a.C., aparecieron evidencias de vehículos con ruedas casi simultáneamente en el norte (Cultura de Maikop) y en el sur del Cáucaso y Europa oriental, (Cultura de Cucuteni). Las representaciones de un vehículo con ruedas aparecieron entre el 3631 y el 3380 antes de Cristo en la Vasija de arcilla Bronocice excavada en un asentamiento de la Cultura de los vasos de embudo en el sur de Polonia.[14]​ En la cercana Olszanica, se construyó una puerta de 2. 2 m de ancho para la entrada de carros; este granero tenía 40 m de largo con 3 puertas, databa de entre 5000 a 7000 años, y pertenecía a la cultura neolítica, de la Cultura de la cerámica de bandas.[15]​ Las pruebas que se conservan de una combinación de rueda y eje, procedentes de Stare Gmajne, cerca de Liubliana, en Eslovenia, como la «Rueda de madera de las marismas de Liubliana», están fechadas dentro de dos desviaciones estándar en 3340-3030 a.C., y el eje en 3360-3045 a.C. [16]​ Se conocen dos tipos de rueda y eje del Neolítico temprano europeo; un tipo de construcción de vagón de circumalpinas (la rueda y el eje giran juntos, como en la rueda de los pantanos de Ljubljana), y el de la cultura de Baden en Hungría (el eje no gira). Ambas están fechadas en torno al 3200-3000 a.C.[17]​ Algunos historiadores creen que hubo una difusión del vehículo con ruedas desde el Cercano Oriente a Europa hacia mediados del IV milenio a.C.[18]​

Las primeras ruedas eran simples discos de madera con un agujero para el eje. Algunas de las primeras ruedas se hacían con rodajas horizontales de troncos de árboles. Debido a la estructura irregular de la madera, una rueda hecha con una rodaja horizontal de un tronco de árbol tenderá a ser inferior a una hecha con piezas redondeadas de tablas longitudinales.

La rueda de radios se inventó más recientemente y permitió la construcción de vehículos más ligeros y rápidos. Los primeros ejemplos conocidos de ruedas de radios de madera se encuentran en el contexto de la cultura Sintashta, que data de c. 2000 a.C. cerca del Lago Krivoye. Poco después, las culturas ecuestres de la región del Cáucaso utilizaron durante la mayor parte de los tres siglos carros de guerra con ruedas de radios. Se adentraron en la península griega, donde se unieron a los pueblos mediterráneos existentes para dar lugar, finalmente, a la Grecia clásica tras la ruptura del dominio de la minoica y las consolidaciones lideradas por la Esparta preclásica y Atenas. Los carros celtas introdujeron una llanta de hierro alrededor de la rueda en el I milenio a.C.

En China, se han encontrado huellas de ruedas que datan de alrededor del 2200 a.C. en Pingliangtai, un yacimiento de la Cultura de Longshan.[19]​ También se encontraron huellas similares en Yanshi, una ciudad de la Cultura de Erlitou, que data de alrededor del 1700 a.C. Las primeras pruebas de ruedas de radios en China proceden de Qinghai, en forma de dos cubos de rueda de un yacimiento fechado entre 2000 y 1500 a.C.[20]​

En Gran Bretaña, una gran rueda de madera, de unos 1 m (3,3 pies) de diámetro, fue descubierta en el yacimiento de Must Farm en Anglia Oriental en 2016. El espécimen, que data de entre el 1100 y el 800 a.C., representa el más completo y antiguo de su tipo encontrado en Gran Bretaña. El cubo de la rueda también está presente. Una espina de caballo encontrada en las cercanías sugiere que la rueda podría haber formado parte de un carro tirado por caballos. La rueda se encontró en un asentamiento construido sobre pilotes en un humedal, lo que indica que el asentamiento tenía algún tipo de vínculo con la tierra firme.[21]​

Aunque el uso a gran escala de las ruedas no se produjo en las Américas antes del contacto europeo, se han encontrado numerosos artefactos pequeños con ruedas, identificados como juguetes para niños, en sitios arqueológicos mexicanos, algunos de los cuales datan de aproximadamente 1500 a.C.[22]​ Se cree que el principal obstáculo para el desarrollo a gran escala de la rueda en las Américas fue la ausencia de animales grandes domesticados que pudieran utilizarse para tirar de carros con ruedas.[23]​ El pariente más cercano del ganado presente en América en tiempos precolombinos, el bisonte americano, es difícil de domesticar y nunca fue domesticado por los nativos americanos; varias especies de caballos existieron hasta hace unos 12 000 años, pero finalmente se extinguieron.[24]​ El único animal de gran tamaño que fue domesticado en el hemisferio occidental, la llama, un animal de carga, pero no apto físicamente para ser utilizado como animal de tiro para arrastrar vehículos con ruedas,[25]​ y el uso de la llama no se extendió mucho más allá de los Andes en la época de la llegada de los europeos.

El Estandarte de Ur, hallado en una tumba datada entre los siglos XXVII y XXV a. C., en el período Dinástico Arcaico, representa diversas escenas de la vida cotidiana y de guerra.

Las primeras ruedas eran simples discos de madera con un agujero central para insertarlas en un eje. La posterior invención de la rueda con radios permitió la construcción de vehículos más rápidos y ligeros y surgió durante la cultura de Andrónovo (2000-1200 a. C.), al norte de Asia Central.

La inclusión de una cinta de hierro alrededor de las ruedas de los carros surgió en el primer siglo antes de Cristo entre los pueblos celtas que, además, fueron los primeros en usar un tipo rodamiento rudimentario en el eje consistente de unos discos de madera muy dura. Posteriormente los romanos utilizaron anillos de bronce como rodamiento, a modo de buje. Por esa época, constructores daneses también probaron con éxito un sistema de cojinetes con rodillos de madera que hacían girar la rueda con menor fricción.

No hubo grandes modificaciones hasta el siglo XIX, cuando se generalizó el uso de metales en la elaboración de maquinarias. En la década de 1880 se inventaron los neumáticos para ruedas (inicialmente para bicicletas) y en el siglo XX se construyen ruedas de las más variadas aleaciones. Ahora, la evolución de la rueda fue pareja con el desarrollo del automóvil, que exigía mayor resistencia, mayor adherencia al suelo y menor desgaste. El problema principal, los pinchazos, se resolvió con la aparición de las primeras cubiertas sin cámara, a partir de 1959.

En la Edad Media y el Renacimiento se idearon mejoras técnicas que se emplearon en ingenios de hidráulica y militares. Después, con la Revolución industrial, la rueda comenzó a utilizarse para la transmisión de pares motrices, preferentemente, siendo el principal elemento de la civilización de las máquinas.

En febrero de 2003, en unos pantanos 22 km al sur de Liubliana, capital de Eslovenia, se halló una rueda cuya antigüedad data desde 3350 a. C. al 3100 a. C. Se la halló junto con su eje; mide 72 cm de diámetro y está hecha de madera de fresno, mientras que el eje, que giraba junto con las ruedas, era de roble, más duro.[26]​ Por otro lado, en el llamado Estandarte de Ur, proveniente de la ciudad de Ur en la Mesopotamia meridional, que data de 2500 a. C. [27]​ aproximadamente, se representa un carro tirado por onagros, la representación más antigua conservada de la rueda empleada en un carro.

Tres partes de una rueda maciza de madera se encontraron en Blair Drummond Moss (Valle del Forth, Escocia). Son la evidencia más temprana de transporte rodado en Gran Bretaña al haberse datado en el 1255 a. C.[nb 1]​[29]​[30]​

La rueda, seguramente, merece un lugar de honor en cualquier lista de grandes inventos. Una civilización industrializada es inconcebible sin ella. Su invención era tal vez inevitable, pero tardó bastante en aparecer al lado del ser humano. Muchas civilizaciones, incluidos los incas y los aztecas, no tenían vehículos de ruedas.[31]​ La más antigua evidencia del uso de la rueda (un pictograma de Sumeria, en el moderno Irak) data del año 3500 antes de Cristo. A partir de allí, el invento se difundió rápidamente por el antiguo mundo Occidental.

Un vehículo con ruedas requiere mucho menos trabajo para moverse que simplemente arrastrando el mismo peso. La baja resistencia al movimiento se explica por el hecho de que el trabajo de fricción realizado ya no está en la superficie que recorre el vehículo, sino en los rodamientos.  En el caso más sencillo y antiguo, el cojinete es solo un agujero redondo por el que pasa el eje (un "Cojinete de deslizamiento").  Incluso con un cojinete liso, el trabajo de fricción se reduce considerablemente porque:

Se pierde energía adicional en la interfaz rueda-carretera. Esto se denomina resistencia a la rodadura que es predominantemente una pérdida por deformación. Depende de la naturaleza del suelo, del material de la rueda, de su inflado en el caso de un neumático, del par neto ejercido por el eventual motor y de muchos otros factores.

Una rueda también puede ofrecer ventajas al atravesar superficies irregulares si el radio de la rueda es lo suficientemente grande en comparación con las irregularidades.

La rueda por sí sola no es una máquina, pero cuando está unida a un  eje junto con un rodamiento, forma la  rueda y eje, una de las máquinas simples. Una rueda motriz es un ejemplo de rueda y eje. Las ruedas son anteriores a las ruedas motrices en unos 6000 años, siendo una evolución del uso de troncos redondos como rodillos para mover una carga pesada, una práctica que se remonta a la prehistoria tan lejos que no ha sido datada.

Rueda de las marismas de Liubliana , de alrededor del 3150 a. C. (modelo restaurado de la pieza de rueda de madera fechada exactamente por radiocarbono más antigua del mundo).

Rueda maciza del siglo XX hecha de tablas de madera, unida con una llanta de metal.

Ruedas de radios en el antiguo carro etrusco de Monteleone , segundo cuarto del siglo VI a. C.

Rueda de radios con chapa de bronce de Árokalja , de alrededor del 1000 a.C.

Radial - (izquierda) y tangencial - (derecha) ruedas de radios de alambre, ambas con neumáticos .

Llanta de aleación fundida en bicicleta plegable , con neumático .

La rueda también se ha convertido en una fuerte metáfora cultural y espiritual de un ciclo o repetición regular (véase chakra, reencarnación, Yin y Yang entre otros). Por ello, y debido a la dificultad del terreno, los vehículos con ruedas estaban prohibidos en la antiguo Tíbet. La rueda en la antigua China es vista como un símbolo de salud y fuerza y utilizada por algunos pueblos como una herramienta para predecir la salud y el éxito futuros. El diámetro de la rueda es indicador de la salud futura.

La rueda alada es un símbolo de progreso, visto en muchos contextos, incluyendo el escudo de armas de Panamá, el logotipo de la «Patrulla de Carreteras del Estado de Ohio» y el del «Ferrocarril Estatal de Tailandia». La rueda es también la figura prominente en la bandera de la India. La rueda en este caso representa la ley (dharma). También aparece en la bandera gitana, aludiendo a su historia nómada y a sus orígenes indios.


La introducción de las ruedas de radios (carro) en la Edad de Bronce Media parece haber conllevado cierto prestigio. La cruz solar parece tener un significado en la religión de la Edad del Bronce, sustituyendo el concepto anterior de barco solar por el más "moderno" y tecnológicamente avanzado  carro solar. La rueda también era un símbolo solar para los Antiguo Egiptoianos.[32]​
 Reunión (Francia)
 Mayotte (Francia)
 Santa Elena, Ascensión y Tristán de Acuña
Madeira
 Pantelaria
 Islas Canarias
Ceuta
Melilla

África es el tercer continente más extenso, tras Asia y América. Está situado entre los océanos Atlántico, al oeste, e Índico, al este. El mar Mediterráneo lo separa al norte del continente europeo; el punto en el que los dos continentes se hallan más cercanos es el estrecho de Gibraltar de 14.4 km de ancho. El mar Rojo lo separa al este de la península arábiga y queda unido a Asia a través del istmo de Suez, en territorio egipcio. Posee una superficie total de 30 272 922 km² (621 600 km² en masa insular), que representa el 20,4 % del total de las tierras emergidas del planeta. La población supera los mil trescientos millones de habitantes, un 15 % del total mundial. El continente se divide en 54 estados soberanos siendo uno de ellos, Egipto, transcontinental, además de dos estados con reconocimiento limitado y dos territorios dependientes.

El nombre del continente proviene del latín. Desde el siglo II a. C., los romanos llamaban África a las tierras que los griegos conocían como Libia, al oeste del Nilo y al este de los montes Atlas. El topónimo se formó con el nombre de un pueblo local, los Afri y el sufijo -ica usado para indicar un país en función de sus habitantes (como Céltica de "los celtas").[1]​[2]​[3]​

Los afri eran una tribu autóctona cuyo nombre es de origen bereber ; ⵉⴼⵔⵉ ifri (plural ifran)[4]​ que significa "caverna", en referencia a los pueblos que los griegos llamaron Τρωγλοδύται / troglodytai, "los moradores de cavernas".[5]​ La misma palabra aparece en la tribu argelina de los Banu Ifran originarios de Ifrane / Yafran.[6]​ 

Una hipótesis antigua, y menos probable, relaciona a los afri con el púnico 𐤏𐤐𐤓 ʿpr /ʿafar "polvo".[7]​ 

Después de 146 a. C., África fue el nombre de una provincia romana en el noroeste del continente. La Geografía de Ptolomeo marca el límite oriental en el istmo de Suez, que la separa de Europa, el occidental en el Océano y el meridional en el Sahara. 

A partir de la era de los Descubrimientos y a medida que el conocimiento del continente se extendía en Europa, el nombre también lo hizo.

Etimologías populares o míticas del nombre lo derivan de la palabra latina aprica, "soleado", como dice Isidoro de Sevilla,[8]​ o del griego: a-phrike: "sin frío", como postuló León el Africano, si bien esta combinación de palabras no existe.[9]​[10]​

Michèle Fruyt propuso derivar el nombre de africus "viento del sur"[11]​ asignando a la palabra un origen itálico, aunque la mayoría considera que el viento tomó su nombre por la provincia romana, y Gerald Massey, en 1881, inventó una etimología egipcia: af-rui-ka es decir, según su propuesta:  "girar hacia la apertura del Ka", es decir, hacia el vientre materno,  con lo cual los egipcios se referían a África como "lugar de nacimiento".[12]​

Entre las etimologías fundadas en personajes epónimos, Flavio Josefo  relaciona África con un nieto de Abraham de nombre Efer[13]​ mientras que Ibn Khallikan lo vincula con el rey himyarita "Afriko hijo de Abraha", su conquistador.[14]​[15]​[16]​

Se cree que África es la cuna de la humanidad y que de allí proceden las sucesivas especies de homínidos y antropoides que dieron lugar a los seres humanos. La teoría explica que allí se originó el Homo sapiens hace cerca de 300 000 años para luego expandirse por el resto de los continentes.
Según el historiador griego Heródoto (484 a. C.), una expedición fenicia auspiciada por el faraón Necao II (616 a. C.) circunnavegó el continente africano por primera vez.
Los orígenes del tráfico comercial entre el oeste y el centro de África y la cuenca mediterránea se pierden en la prehistoria. Los primeros relatos históricos datan de la antigüedad y versan sobre los nómadas que organizaban el comercio entre Leptis Magna y el Chad. Este comercio vivió su primer auge en el siglo I a. C. con el ascenso del Imperio romano. Sobre todo se comerciaba con oro, esclavos, marfil y animales exóticos para los juegos de circo en Roma a cambio de bienes de lujo romanos. De hecho es en esta época en la que se gesta el propio nombre de África. Tras la derrota de Cartago por Roma en la tercera guerra púnica se establece la provincia romana de África que abarcaría aproximadamente el Túnez actual. Fue una generalización territorial de la provincia lo que dio nombre a todo el continente. Una importancia crucial tuvo también la mayor utilización del camello a partir del siglo I en el norte de África.

A partir del siglo VII los árabes invaden el África del norte. El comercio caravanero y la expansión islámica alimentan el establecimiento de nuevas relaciones entre las «dos Áfricas».

El Imperio Kanem-Bornu existió en África entre el siglo XIII y la década de 1840. En su momento de mayor esplendor abarcó el área de lo que actualmente es el sur de Libia, Chad, noreste de Nigeria, este de Níger y norte de Camerún.

El Reino del Congo fue un estado situado en lo que actualmente constituye la zona norte de Angola, el enclave de Cabinda, Congo-Brazzaville y la parte occidental de Congo-Kinsasa. Su área de influencia abarcaba también los estados vecinos.

La repartición colonial de África por las potencias europeas, iniciada a partir del siglo XVII, tuvo lugar aproximadamente en 1885, con la conferencia de Berlín y el comienzo de la Primera Guerra Mundial, época en la que los imperios coloniales se extendieron más rápidamente en África que en cualquier otro lugar del mundo, si bien dos países, Liberia y Etiopía, consiguieron mantener su independencia. Es un ejemplo del Nuevo Imperialismo generado por la necesidad de los países europeos de obtener materias primas para el rápido crecimiento de su producción manufacturera después de la Revolución Industrial, iniciada en Inglaterra a fines del siglo XVIII.

Al final de la Segunda Guerra Mundial los aliados no logran ponerse de acuerdo sobre el futuro de la antigua colonia italiana de Libia. En ese momento era un territorio más de cinco veces mayor que la propia Italia. Sin embargo, la población no sobrepasaba el millón de habitantes, por lo que representaba un destino apropiado para la población desplazada de Italia por la guerra, que empezó a buscar lugares a donde emigrar. Los recelos entre Occidente y la Unión de Repúblicas Socialistas Soviéticas hacen que finalmente la Organización de las Naciones Unidas decida dar la independencia al país dejándolo en manos del rey Idris.

Aunque ya había cuatro países independientes en África (Liberia en 1847, Sudáfrica en 1910, Egipto en 1922 y Etiopía en 1941) Libia se convierte así en la primera colonia africana en lograr su independencia en 1951, a la que seguirá la de Ghana en 1957. Más adelante las potencias europeas lamentarían este hecho, pues contribuyó a desencadenar las diferentes luchas por la independencia africana.

En su mayor parte, África es una enorme y antigua plataforma continental maciza y compacta, elevada entre 600 y 800 msnm, surcada por grandes ríos (aunque pocos) y escasa en penínsulas. Destaca por su regularidad orográfica y considerable altitud media.

Tres franjas climáticas sucesivas se repiten al norte y al sur del ecuador, abarcando los climas mediterráneo, desértico, subtropical e intertropical lluvioso, este último, en sus dos tipos principales, tanto de sabana como de selva. África es el continente con mayor índice de insolación anual, lo cual podría haber dado origen a su nombre (África, del griego "a-phrike", ‘sin frío’).

Los suelos son excepcionalmente ricos en minerales y muy aptos para pastos. Debido al clima es allí donde evolucionó la mosca tsetsé y donde prolifera actualmente. Las principales áreas cultivadas se encuentran en las tierras altas orientales y la zona de los Grandes Lagos, algunos deltas y riberas e incluso en el Sahel.- Situación Astronómica Continental: Norte: Cabo Blanco, Túnez (37°20′ Norte) Sur: Cabo de las agujas, Rep. Sudafricana (35° Sur) Este: Cabo Hafún, Somalia (51°24′ Este) Oeste: Cabo Verde, Senegal (18° Oeste)

El cambio climático en África es una amenaza cada vez más grave para el continente, ya que África es uno de los continentes más vulnerables al cambio climático.[29]​[30]​ El cambio climático antropogénico ya afecta significativamente a África. Según el Panel Intergubernamental sobre Cambio Climático, la vulnerabilidad de África al cambio climático está impulsada por una serie de factores que incluyen una capacidad de adaptación débil, una alta dependencia de los bienes de los ecosistemas para los medios de vida y sistemas de producción agrícola poco desarrollados.[31]​ Los riesgos del cambio climático sobre la producción agrícola, la seguridad alimentaria, los recursos hídricos y los servicios de los ecosistemas probablemente tendrán consecuencias cada vez más graves en la vida y las perspectivas de desarrollo sostenible en África.[30]​ La gestión de este riesgo requiere la integración de estrategias de mitigación y adaptación en la gestión de los bienes y servicios de los ecosistemas y los sistemas de producción agrícola en África.[31]​

En las próximas décadas, se espera un calentamiento debido al cambio climático en casi toda la superficie de la Tierra, y la precipitación media global aumentará.[32]​ Se espera que los efectos regionales sobre las precipitaciones en los trópicos sean mucho más variables espacialmente y hay menos seguridad respecto del aumento o disminución de las precipitaciones, pero hay certeza de que habrá cambios. Las temperaturas superficiales observadas han aumentado en África desde finales del siglo XIX hasta principios del siglo XXI en aproximadamente 1 ° C. En ciertos lugares, la temperatura ha aumentado todavía más, por ejemplo en el Sahel la temperatura mínima ha aumentado hasta 3 ° C al final de la estación seca.[33]​ Las tendencias de precipitación observadas indican discrepancias espaciales y temporales.[34]​[30]​ Los cambios observados en temperatura y precipitación varían regionalmente.[35]​[34]​

En términos de esfuerzos de adaptación, los actores a nivel regional están logrando algunos avances. Esto incluye el desarrollo y adopción de varias estrategias regionales de adaptación al cambio climático,[36]​ por ejemplo, el Reporte de Políticas sobe Cambio Climático de la Comunidad de Desarrollo de África Austral (SADC, por sus siglas en inglés)[37]​ y la estrategia de adaptación para el sector del agua.[38]​ Además, se han realizado otros esfuerzos para mejorar la adaptación al cambio climático, como el Programa tripatito de Adaptación y Mitigación del Cambio Climático en África Oriental y Meridional (COMESA-EAC-SADC).[39]​

El continente africano está compuesto de 54 Estados soberanos, tres territorios dependientes y varios territorios integrados en Estados no africanos como Francia, España o Portugal. Las entidades políticas africanas anteriores al colonialismo desaparecieron con la expansión europea por el continente a finales del siglo XIX. Solo Abisinia, que se mantuvo independiente gracias a su victoria sobre los italianos en 1896 en la batalla de Adua, y Liberia, que fundada por el Gobierno estadounidense con esclavos liberados de su país en 1847, se mantuvieron independientes. La mayoría de los países africanos lograron la independencia en el siglo XX a partir del proceso de descolonización tras la Segunda Guerra Mundial y que alcanzó su plenitud en los años 1960. En 2011 surgió, hasta el momento, el último Estado en el continente, Sudán del Sur, tras conseguir la independencia de Sudán tras dos largas guerras civiles (1955-1972 y 1983-2005).

División política de África

Regiones de África según la ONU

Todos los Estados africanos soberanos son miembros de pleno derecho de la ONU, contando entre ellos con cuatro Estados fundadores como fueron Egipto, Sudáfrica, Liberia y Etiopía.

En materia económica 52 Estados son miembros de la Organización Mundial del Comercio (ocho son observadores) mientras que Sudán del Sur y Eritrea no pertenecen a ella. Asimismo, la totalidad del continente se incluye en el Fondo Monetario Internacional aunque ocho Estados no cumplen el artículo VIII de la organización.

Es importante destacar la presencia del continente en la OPEP, ya que Argelia, Angola, Gabón, Libia y Nigeria son productores de petróleo.

En materia de justicia y seguridad todos los países africanos están integrados en la Interpol, sin embargo en el caso de la Corte Penal Internacional nueve países no han firmado ni ratificado el Estatuto de Roma, mientras que son diez los firmantes que aún no lo han ratificado. El resto de países acepta la jurisdicción del Corte Penal Internacional para juzgar casos de crímenes contra la humanidad.

También está presente la Liga Árabe, que engloba a los países musulmanes del continente: Marruecos, Argelia, Túnez, Libia, Egipto, Sudán, Mauritania, Somalia y Yibuti.

Los Estados africanos (salvo Sudán del Sur) están adscritos al Movimiento de Países No Alineados.

En cuanto a las organizaciones transcontinentales, el continente africano está presente en la Asociación ribereña del Océano Índico para la cooperación regional (1995) de cooperación entre países asiáticos, Australia y nueve Estados africanos (Somalia, Tanzania, Madagascar, Seychelles, Mauricio, Mozambique, Kenia, Sudáfrica y Comoras). Durante la Guerra Fría (1986) se creó la Zona de Paz y Cooperación del Atlántico Sur, bajo el auspicio de las Naciones Unidas, con el objetivo de mantener la seguridad y la paz en el Atlántico Sur. Por iniciativa de Brasil se unieron 21 Estados africanos más tres sudamericanos.

En 1975, se creó Estados de África, del Caribe y del Pacífico (ACP) para, a través de varios acuerdos (el más reciente Acuerdo de Cotonú del año 2000) luchar contra la pobreza junto a la Unión Europea, que trabaja por medio del Fondo Europeo de Desarrollo. Forman parte de esta organización los 47 Estados africanos. La Unión Europea trabaja a través de la firma de acuerdos económicos con los cinco bloques regionales.

La principal organización política regional del continente es la Unión Africana (UA), heredera de varios intentos previos de unir políticamente al continente, a semejanza de la Unión Europea en Europa. Sus predecesoras son la Unión de Estados Africanos, creada por el ghanés Kwame Nkrumah en 1958, y la Organización para la Unidad Africana de 1963. Desde 1984 hasta el año 2017 Marruecos no formó parte de la UA como protesta por la admisión de la República Árabe Saharaui Democrática, con la que mantiene un contencioso por el Sáhara Occidental.

La principal organización económica es la Comunidad Económica Africana (CEA), fundada en 1981. El objetivo de la CEA es fomentar la integración y el desarrollo a través de la cooperación entre los estados africanos. Para ello utiliza un sistema de agrupaciones regionales como pilares básicos:

Fuera del paraguas de la CEA, existen otras organizaciones de tipo económico, como la Comisión del Océano Índico, Autoridad de Liptako-Gourma, Unión del Río Mano o la Comunidad Económica de los Países de los Grandes Lagos.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 de junio de 2016.

De los 54 estados soberanos de África, 51 de ellos están constituidos como república, y tan solo 3 tienen forma de monarquía: Lesoto, Marruecos y Suazilandia.

Varios estados alcanzaron su independencia como monarquías pero a lo largo de los años se han ido convirtiendo en repúblicas: Egipto (1952), Ghana (1960), Kenia (1964), Burundi (1966), Malawi (1966), Libia (1969), Gambia (1970), Etiopía (1975) y Mauricio (1992).

Uno de los grandes males del continente africano es la proliferación de regímenes políticos autoritarios desde que los distintos estados fueron obteniendo su independencia. Atendiendo al Índice de democracia publicado anualmente por la Economist Intelligence Unit, en 2018 solamente una pequeña parte de los países africanos podían ser considerados como democráticos, mientras que más de la mitad se mantenían bajo formas políticas autoritarias o dictatoriales:

Desde que comenzó a publicarse este índice de democracia en 2006, la situación ha mejorado muy ligeramente. Han pasado a tener la consideración de países democráticos Ghana (2010), Senegal (2012) y Túnez (2014); en cambio la han perdido Malí (2012) y Benín (2013). Temporalmente fueron considerados democráticos los regímenes de Zambia (2011-2015) y Malawi (2012-2013).

Han pasado de estar calificados como regímenes autoritarios a ser considerados como híbridos Sierra Leona (2008), Marruecos (2012), Burkina Faso (2013), Nigeria (2016) y Costa de Marfil (2018). En el sentido contrario, Burundi (2012) y Mozambique (2018) han pasado a ser catalogados como regímenes autoritarios. Temporalmente fueron considerados híbridos los regímenes de Níger (2011-2015), Mauritania (2011-2014), Egipto (2011-2012) y Libia (2012-2013), regresando todos ellos a la condición de autoritarios.

Dos territorios africanos aún tienen un reconocimiento limitado debido a conflictos territoriales. Así, la República Árabe Saharaui Democrática surgió como resultado del proceso de descolonización del Sáhara Español en 1976, siendo reconocido por 48 estados soberanos. El  resto del territorio está ocupado por Marruecos que no reconoce la independencia y lo reclama como territorio propio. Somalilandia se autoproclamó estado durante la crisis política somalí que desembocó en una guerra civil todavía latente. Somalilandia no es reconocida por ningún estado.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 de junio de 2016.

La dependencia francesa está compuesta por una serie de islas conocidas como Islas Dispersas del Océano Índico de las que la mayoría están deshabitadas y son reclamadas por países soberanos como Mauricio, Madagascar o Seychelles. Por su parte el territorio de ultramar de Santa Helena está incluido en el Comité de Descolonización de la ONU. Ambos territorios forman parte de los países y territorios de ultramar (o PTU) son las dependencias y territorios de ultramar de los Estados miembros de la Unión Europea que no forman parte de la Unión, sino que tiene un estatuto de asociados a los Estados miembros desde el Tratado de Lisboa.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 de junio de 2016.

Está formada por territorios integrados como parte de otros estados. Los territorios de las Islas Canarias, Reunión, Mayotte y Madeira forman parte de la Región Ultraperiférica de la Unión Europea, que, aun estando geográficamente alejados del continente europeo, forman parte indivisible de alguno de los veintisiete Estados miembros de la Unión.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). actualizados 1 de junio de 2016.

En su condición de excolonias, la mayoría de los países africanos mantienen estrechas relaciones económicas con la Unión Europea (UE).

Existe una organización supranacional, tomando como referencia a la Unión Europea, llamada Unión Africana (UA), de la que forman parte todos los países del continente, incluida la República Árabe Saharaui Democrática. La mayor parte de los países africanos están subdesarrollados o en vías de desarrollo.

El 36,2% de la población, unos 350 millones de personas, viven con menos de un dólar al día. África paga cerca de 20 000 millones de dólares en pagos de deuda cada año, aun pese a las condonaciones de deuda de los años 1990.[44]​

Durante el régimen colonial los europeos explotaron los productos más fáciles y más provechosos de extraer, como el oro, el marfil, maderas y fibras textiles. Tras la emancipación de las colonias los más codiciados pasaron a ser el petróleo, los diamantes y la minería en general, pero estos productos mencionados se hallan en pocos países. El petróleo y el gas natural se explotan principalmente en Nigeria, Angola, Argelia, Egipto y Libia.[45]​ 

El subsuelo africano proporciona los siguientes minerales en abundancia: oro (principalmente Ghana, Sudáfrica, Sudán y Malí),[46]​ bauxita (Guinea),[47]​ cromo (Sudáfrica),[48]​ cobre (Congo y Zambia),[49]​ cobalto (Congo, Madagascar, Marruecos y Sudáfrica),[50]​ mineral de hierro (Sudáfrica),[51]​ litio (Zimbabue),[52]​ manganeso (Sudáfrica, Gabón y Costa de Marfil),[53]​ fosfato (Marruecos, Egipto, Túnez, Senegal, Argelia y Togo),[54]​ platino (Sudáfrica y Zimbabue),[55]​ estaño (Congo, Nigeria y Ruanda),[56]​ tungsteno (Ruanda)[57]​ tántalo (Congo, Ruanda, Nigeria, Etiopía y Burundi) [58]​ y uranio (Namibia y Níger).[59]​ 

La agricultura del continente africano se presenta en dos formas: agricultura de subsistencia y agricultura comercial. Los principales productos de cultivo para el consumo de la población local incluyen ñame, mandioca, batata, sorgo, maní, taro, frijoles, maíz y arroz. Se incluyen en los productos que África cultiva normalmente para la exportación: algodón, té, tabaco, cacao, café, anacardo y plátano. También hay cultivos típicos de determinadas regiones de África, como aceituna en el norte del continente; a nuez de cola, semilla de calabaza (egusi) a  nuez de karité en África Occidental; y sésamo en todo el continente. [60]​

La carencia de buena tecnología y de medios de comunicación eficientes dificultan la explotación de dichas materias primas. El 60 % de los trabajadores africanos realiza actividades rurales, y el 80 % de lo que África exporta son materias primas, siendo a su vez los productos industrializados los que representan la casi totalidad de sus importaciones. Solo el 15 % está empleado en el sector industrial, siendo Egipto, Sudáfrica, Túnez y Marruecos los que poseen casi el total de dicha actividad. El resultado es que África es el continente más pobre del planeta: su PBI representa tan solo el 2,6 % del total mundial.[61]​

La ayuda exterior llega a los cincuenta millones de dólares cada año, y en los últimos 60 años esa ayuda ha sido de al menos mil millones. Sin embargo, esto ha empobrecido más a los países, ha ralentizado el crecimiento, los ha endeudado más, los ha hecho más propensos a la inflación y vulnerables a los vaivenes de las divisas, ha reducido el atractivo para la inversión y ha aumentado el riesgo de conflictos civiles. La ayuda exterior se transforma en deuda, que se paga a expensas de la educación y los servicios médicos africanos. Aun cuando se termina de pagar una deuda, los países vuelven a pedir más ayuda. A fin de paliar este círculo vicioso, la tendencia actual consiste en condonar la deuda externa a los países que demuestran un compromiso con el sistema democrático y con el desarrollo.

La asistencia ha estado afectada por corrupción, y los flujos han acabado beneficiando a las burocracias gubernamentales y ciertas ONG financiadas por algunos gobiernos.[62]​ La corrupción le cuesta a África 150 millones de dólares al año. No existen incentivos para que los gobiernos busquen formas más transparentes para recaudar fondos para el desarrollo, solo se les pide a las agencias de donación una infusión de capital.

En contraposición, en otros países la ayuda ha servido para resolver problemas como las epidemias que diezman la salud y las vidas de la población activa (sida, malaria), la falta de infraestructuras básicas, el rendimiento agrícola, el analfabetismo y la carencia de educación primaria universal. Existen ejemplos de países, como Ghana, que demuestran emplear correctamente la ayuda.

El flujo de capital ayuda a que los gobiernos ineficientes sigan en el poder, ya que el presidente no tiene que hacer nada pues la ayuda sigue llegando, siempre y cuando pague al ejército. No tiene que subir los impuestos, ni preocuparse del descontento de los ciudadanos ni de la representación de estos. Los choques civiles a menudo son motivados por el conocimiento de que al hacerse con el poder, el ganador obtiene un acceso virtualmente completo al paquete de ayuda.

La ayuda hace que la burocracia se vuelva clientelista y envuelva a los ciudadanos con trámites innecesarios. En Camerún se tardan 426 días en hacer un procedimiento comercial y 119 días en Angola.

La ayuda alimentaria que compra comida cultivada en Estados Unidos quiebra a los agricultores locales. Se ha hecho poco para ayudar a los agricultores y se gastan millones de dólares en el programa.

La gran cantidad de dinero crea la "enfermedad neerlandesa": los grandes flujos de dinero hacen que la moneda local se fortalezca incrementando además los precios internos. Esto crea además inflación, por lo que los países deben emitir bonos. Uganda fue obligada a emitirlos en 2005, pagando intereses de $110 millones anuales.[44]​

China está presente en países con grandes recursos, por ejemplo petróleo, en Angola, que es su principal proveedor, y en otros países como son Guinea Ecuatorial, Nigeria, Chad, Sudán, Gabón, Zambia y República Democrática del Congo, estos dos últimos países productores de minerales.

Después de Estados Unidos y de la Unión Europea, China es el tercer socio más importante del continente, con inversiones en industrias de la construcción que están haciendo carreteras, embalses, viviendas, hospitales, y en la explotación de hidrocarburos y minerales.[63]​ China tiene estrecha relación con Zimbabue, y Sudán, cuyos gobiernos son cuestionados.

Estados Unidos tiene interés en África por el petróleo.[63]​

Según el Programa de las Naciones Unidas para el Desarrollo (PNUD), África cuenta con ciudades subdesarrolladas, la mayoría de ciudades de África que poseen mayor desarrollo están en Sudáfrica y Egipto.

Hay aproximadamente 41 monedas oficiales distintas. La moneda oficial más extendida es el franco CFA en sus dos versiones: el Franco CFA de África Occidental moneda nacional en ocho países y el Franco CFA de África Central en seis. También está presente el euro a través de los territorios españoles y franceses en el continente, así como la libra de Santa Elena, par a la libra esterlina, pero con sus propios modelos de monedas y billetes con respecto a la moneda británica.

Las estimaciones sobre la población no son precisas debido a lo obsoleto de gran número de censos nacionales. Se calcula, sin embargo, que viven en África no menos de mil millones de personas.

En África predomina la etnia negra, cerca de un 80 % del total de la población, a excepción de la franja costera mediterránea donde son mayoritarios, aunque no exclusivos, tipos humanos arabo-bereberes y caucasoides-mediterráneos. Entre el trópico de Capricornio y el trópico de Cáncer la población es casi en su totalidad negra, y suele ser subdividida en cuatro grupos principales, aunque siempre han existido en las zonas limítrofes entre estos grandes grupos pueblos más o menos mixtos en todas sus combinaciones. Tales grupos principales son sudanés, (Sahel y países del golfo de Guinea), nilótico, (Nilo, desde Sudán hasta los Grandes Lagos), cusita (Macizo etíope y Cuerno de África) y bantú, siendo este el más extendido, ya que ocupa toda el área a partir del cinturón selvático ecuatorial. Es además un tipo mixto relacionado con dos tipos antaño muy extendidos (hoy en día minoritarios), los twa y otros grupos mal denominados pigmeos, habitantes de los bosques, y los kung-san, mal denominados bosquimanos, de las zonas áridas del extremo sur.

Migrantes de origen francés se hallan establecidos en el Magreb y escasamente en las grandes ciudades de África Occidental, los de origen español habitan Marruecos y el Sáhara Occidental, mientras que en Angola y algunas ciudades costeras de África Occidental hay un número minoritario de grupos mixtos de origen africano-portugués. En el sur de África hay una significante cantidad (seis millones) de africanos blancos o afrikáneres, descendientes de neerlandeses y británicos.

La mayoría de los africanos mantienen un estilo de vida rural, pero la urbanización aumenta, ya que la gente abandona el campo para buscar trabajo en las ciudades. Las mayores densidades de población se encuentran donde el agua es más accesible, como en el valle del Nilo, las costas del norte y oeste, a lo largo del Níger, en las regiones montañosas del este y en Sudáfrica.

Aumento de la población desde el año 1 d. C. hasta el 2010 y proyecciones de población para los años 2050 y 2100.[64]​

En África las características de la población y su esperanza de vida varían según las condiciones. En África del Norte o en el desierto del Sahara, la mayor parte de sus habitantes son adultos y superan a la población juvenil, aunque no se da tampoco un envejecimiento progresivo. En el África subsahariana la mayor parte de sus habitantes son jóvenes, aunque en las últimas décadas se ha experimentado un crecimiento en la población adulta y un progresivo envejecimiento. Esto se da principalmente en países como Etiopía y Somalia, aunque en Sudáfrica también se experimenta un crecimiento de población adulta pero no es tan común el envejecimiento.

La población por sexo varía en el continente; al sur del Sahara, conocido también como el África negra, predominan las personas de sexo femenino, excepto en países como Angola, Mozambique, Etiopía, Somalia y Yibuti, entre otros. En cambio, en la mayor parte de los países del África del Norte predominan las personas de sexo masculino, excepto Marruecos, Sáhara Occidental, Mauritania y Chad.[cita requerida]

Además de lenguas alóctonas como el árabe, el francés o el inglés (entre otras) cuya presencia en África se debe a procesos de conquista y dominación política. Se estima que en África actualmente existen unas 1700 lenguas autóctonas.

Demográficamente, el árabe y el francés son las lenguas con más hablantes potenciales y las más extendidas en el continente. Las lenguas autóctonas de África con el mayor número de hablantes son el suahili (90 millones de hablantes), el oromo (70 millones), el hausa (40 millones) y el amhárico, todas ellas con un buen número de hablantes para los cuales es su segunda lengua y no su lengua materna (estas cuatro lenguas se usan ampliamente como lingua franca en sus respectivas áreas de influencia). Las lenguas europeas más extendidas son el francés, el inglés y el portugués, generalmente utilizados por las administraciones postcoloniales y las clases urbanas. A continuación existe un grupo de cerca de 20 idiomas étnicos con entre 1 y 20 millones de hablantes como: (de norte a sur) el wólof, manding (mandé), ewe, fon, yoruba, igbo, lingala, shona, setsuana, xhosa, malgache, etc. Otros idiomas minoritarios son el afrikáans y el español, de origen europeo, y otros autóctonos como el bereber. Los idiomas africanos y oficiales en sus respectivos estados son: el amárico hablado en Etiopía, el somalí en Somalia, el suajili en Kenia y Tanzania, el setsuana en Botsuana, el afrikáans en Sudáfrica y Namibia (junto con el inglés), y el malgache en la República de Madagascar (junto con el francés).

Las lenguas africanas autóctonas pertenecen a cuatro grandes grupos:

La mayor parte del continente profesa religiones tradicionales africanas, englobadas dentro del impreciso grupo conocido como animista. Esto significa que creen que los espíritus habitan objetos animados o inanimados. Dicho así mismo suele persistir bajo la apariencia de religiones universalistas como el islam o el cristianismo. También hay creyentes del rastafarismo.

El islam tiene una presencia dominante en el norte y destacada en el Sáhara, el Sahel, África Occidental y África Oriental. El cristianismo monofisita, aunque más antiguo que el islam, quedó confinado a Etiopía. A partir del siglo XX adquirirán una creciente importancia el catolicismo y protestantismo.

Sin embargo, tanto islam como el cristianismo se encuentran en África con sincretismos más o menos sectarizados como el kimbanguismo o la Iglesia "Cita con la Vida", que persisten y se reproducen gracias a la fortaleza implícita de los conceptos de las religiones tradicionales. Las religiones tradicionales africanas tienen una presencia destacada en América, especialmente el vudú en Haití, la religión yoruba y las religiones del antiguo Reino del Congo en el Caribe y en Brasil principalmente.

Existen asimismo minorías hinduistas.

Los antiguos la personificaban bajo la figura de una mujer y con la de un escorpión. En una medalla del emperador Adriano lleva por casco o morrión la cabeza de un elefante. En otras varias medallas se observa que tiene en la mano derecha un escorpión y en la izquierda el cuerno de la abundancia; a sus pies un cesto lleno de flores y de frutos. El caballo y la palma son los símbolos de la parte de África vecina a Cartago.

En una medalla de la reina Cristina se ve una alegoría menos conocida: Atlas cubierto con la piel de la cabeza de un elefante guarnecida con su trompa y sus colmillos, contemplando los signos del zodíaco, para indicar que este rey, considerado por algunos como el inventor de la astronomía, reinó en África. Los modernos aprovechando de todas estas ideas han representado el África bajo los rasgos de una mujer mora, casi desnuda teniendo los cabellos rizados, llevando por casco una cabeza de elefante, un collar de coral, un cuerno lleno de espigas en una mano, un escorpión en la otra o un colmillo de elefante y acompañada de un león y de varias serpientes. Lebrun la ha pintado bajo la figura de una mora desnuda hasta la cintura, sentada sobre un elefante y en la cabeza un parasol que la pone enteramente a la sombra. Sus cabellos son negros, cortos y rizados: lleva por pendientes dos grandes perlas y sus brazos adornados con ricos brazaletes.[67]​

El teatro africano, entre tradición e historia, se está encauzando actualmente por nuevas vías. Todo predispone en África al teatro. El sentido del ritmo y de la mímica, la afición por la palabra y la verborrea son cualidades que todos los africanos comparten en mayor o menor medida y que hacen de ellos actores natos. La vida cotidiana de los africanos transcurre al ritmo de variadas ceremonias, rituales o religiosas, concebidas y vividas generalmente como verdaderos espectáculos. No obstante, aunque África ha conocido desde siempre este tipo de ceremonias, cabe preguntarse si se trataba realmente de teatro; a los ojos de muchos, estos espectáculos están demasiado cargados de significado religioso para que puedan considerarse como tal. Otros estiman que los tipos de teatro africanos guardan cierto parecido, como en otros tiempos la tragedia griega, como un preteatro que nunca llegará totalmente a ser teatro si no se desacraliza. La fuerza y las posibilidades de supervivencia del teatro negro residirán, por lo tanto, en su capacidad para conservar su especificidad. en el África independiente está tomando forma un nuevo teatro.

Nuevo Teatro: Se trata de un teatro comprometido, incluso militante, concebido para defender la identidad de un pueblo que ha logrado su independencia

Teatro de Vanguardia: Se orienta actualmente hacia una investigación sobre el papel de actor, próxima a la de Jerzy Grotowski y su teatro laboratorio. Así, en Libreville (Gabón), se formó en 1970 un teatro vanguardista que realizó dos espectáculos que dejaron una huella perdurable en las jóvenes generaciones de comediantes. Otra vía de investigación es el teatro de silencio, creado por François Rosira, cuyo fin era realizar espectáculos en los que el canto, el recitado, la música y el baile se complementen en perfecta armonía.

Asociaciones como Ndjembé promovían el carácter teatral en África.

Enrique VIII (28 de junio de 1491-28 de enero de 1547) fue rey de Inglaterra y señor de Irlanda desde el 22 de abril de 1509 hasta su muerte. Fue el segundo monarca de la casa Tudor, heredero de su padre, Enrique VII. Se casó seis veces y ejerció el poder más absoluto entre todos los monarcas ingleses.[2]​[3]​ Entre los hechos más notables de su reinado se incluyen la ruptura con la Iglesia católica y el establecimiento del monarca como jefe supremo de la Iglesia de Inglaterra (Iglesia anglicana), la disolución de los monasterios y la unión de Inglaterra con Gales.

También promulgó legislaciones importantes, como las varias actas de separación con la Iglesia de Roma, de su designación como cabeza suprema de la Iglesia de Inglaterra, las Union Acts de 1535 y 1542, que unificaron a Inglaterra y Gales como una sola nación, la Buggery Act de 1533, primera legislación contra la sodomía en Inglaterra y la Witchcraft Act de 1542, que castigaba la brujería con la pena de muerte.[4]​

La protección que dispensó al pintor alemán Hans Holbein se tradujo en una formidable serie de retratos y dibujos en color, que efigian a muchos personajes de la corte de aquella época, destacando varios al propio rey.

Enrique nació en el palacio de Placentia en Greenwich el 28 de junio de 1491. Fue el tercer hijo de Enrique VII e Isabel de York. Solo tres de sus seis hermanos sobrevivieron a la infancia: Arturo, príncipe de Gales, Margarita y María, futura reina consorte de Francia. Su padre, miembro de la Casa de Lancaster, había adquirido el trono por derecho de conquista, ya que su ejército derrotó al último Plantagenet, Ricardo III, y posteriormente completó sus derechos desposando a Isabel, hija de Eduardo IV de Inglaterra. En 1493, Enrique fue designado condestable[5]​ del castillo de Dover y lord Warden de los cinco puertos. En 1494 fue nombrado duque de York y posteriormente conde mariscal de Inglaterra y lord teniente de Irlanda. 

Enrique recibió una buena educación, contando con unos buenos tutores, lo que le permitió adquirir fluidez en latín, francés y español. Su madre falleció cuando él tenía once años.

Durante su juventud fue un ávido apostador y jugador de dados, y también practicó justas, caza y royal tennis, antepasado del actual tenis. Fue, además, un músico completo, escritor y poeta. Se involucró en la reconstrucción y mejoramiento de varios edificios importantes, como el palacio de Nonsuch, la capilla del King's College, en Cambridge, y la abadía de Westminster, en Londres. En muchos casos se trataba de edificios confiscados, por ejemplo al cardenal Thomas Wolsey; entre ellos, la Christ Church en Oxford, el palacio de Hampton Court, el palacio de Whitehall y el Trinity College en Cambridge.

En 1501, Arturo, heredero de la corona inglesa, se casó con Catalina de Aragón, hija menor de los Reyes Católicos, en la catedral de San Pablo, en Londres. La pareja, que por entonces tenía quince y dieciséis años, respectivamente, fue enviada por un tiempo a Gales, como se acostumbraba con el heredero del trono y su esposa. Al año siguiente, tras solo veinte semanas de matrimonio, Arturo murió de una infección, por lo que Enrique se convirtió en príncipe de Gales y heredero al trono. Enrique VII, aún interesado en sellar una alianza matrimonial entre Inglaterra y España, ofreció a su hijo Enrique en matrimonio a Catalina de Aragón.

Para lograr el matrimonio entre su hijo y Catalina de Aragón, Enrique VII debía primero obtener una dispensa papal. Catalina manifestaba que su primer matrimonio no había sido consumado; de ser así no se requería dispensa alguna, sino una simple disolución de un matrimonio meramente formal. Sin embargo, tanto la corte española como la inglesa insistieron en la necesidad de una dispensa papal para eliminar todas las dudas concernientes a la legitimidad del casamiento. Debido a la impaciencia de Isabel I de Castilla, el papa otorgó apresuradamente la dispensa mediante una bula. De esta manera, catorce meses después de la muerte de su primer marido, Catalina se encontró comprometida con el hermano de aquel. En 1505 Enrique VII perdió su interés en mantener la alianza con España y el príncipe de Gales fue obligado a declarar que el compromiso había sido arreglado sin su consentimiento.

Enrique VIII ascendió al trono en 1509, tras la muerte de su padre. Fernando el Católico organizó el casamiento de su hija Catalina de Aragón con el nuevo rey. Enrique VIII desposó a Catalina en Greenwich, el 11 de junio de 1509, dejando de lado los consejos del papa Julio II y de William Warham, arzobispo de Canterbury, en cuanto a la validez de tal unión. Fueron coronados juntos en la abadía de Westminster el 24 de junio de 1509. 

El primer embarazo de Catalina terminó en un aborto en 1510. Luego dio a luz a un hijo, Enrique, el 1 de enero de 1511, pero el bebé solo vivió hasta el 22 de febrero de ese mismo año.

Con su coronación, Enrique VIII debió enfrentarse a las problemáticas consecuencias de los impuestos nobiliarios establecidos por Richard Empson y Edmund Dudley, miembros del gabinete de su padre. Dos días después de su nombramiento los hizo detener en la Torre de Londres, fueron acusados de alta traición y decapitados en 1510. A diferencia de Enrique VII, que favorecía las políticas pacíficas, Enrique VIII manifestó una inclinación bélica durante todo su reinado.

Durante los dos años posteriores a la ascensión de Enrique VIII, Richard Fox, obispo de Winchester, y William Warham controlaron los asuntos de Estado. A partir de 1511, sin embargo, el poder real fue ostentado por el cardenal Thomas Wolsey. En ese mismo año, el papa Julio II proclamó una Liga Santa contra Francia. La nueva alianza se forjó rápidamente, incluyendo a Inglaterra, España, regida por los Reyes Católicos, y el Sacro Imperio Romano, gobernado por el emperador Maximiliano I. Enrique VIII firmó el Tratado de Westminster, en el que prometía ayuda mutua a España contra Francia. En 1513 invadió este país y derrotó a sus ejércitos en la batalla de las Espuelas. Por su parte, Jacobo IV de Escocia, aliado de Francia, invadió Inglaterra por el norte, pero fue derrotado y muerto en Flodden el 9 de septiembre de 1513, por lo que el conflicto se vio terminado.

En 1514, Fernando abandonó la alianza, y las otras partes hicieron la paz con Francia. La consecuente irritación con España inició la discusión sobre un divorcio entre Enrique VIII y Catalina. Sin embargo, con la ascensión en 1515 de Francisco I al trono francés, aumentó nuevamente el antagonismo entre Inglaterra y Francia, y Enrique se reconcilió con los reyes de España.

En 1516 Catalina pudo concebir a una niña, María, lo que renovó las esperanzas de Enrique de lograr un heredero varón a pesar de los previos embarazos fallidos de su esposa.[6]​ Fue un matrimonio de larga data, pero la paciencia de Enrique VIII por un hijo varón que Catalina no le pudo dar, lo llevó al final de la relación.[7]​

Fernando II murió en 1516 y fue sucedido por su nieto Carlos, sobrino de Catalina. Para octubre de 1518, Thomas Wolsey había diseñado el Tratado de Londres con el papado, con la idea de conseguir un triunfo para la diplomacia inglesa, lo que ubicaba al reino en el centro de una nueva alianza europea con el ostensible objeto de repeler las invasiones moriscas a España, tal como había solicitado el papa.

En 1519 murió Maximiliano, y Wolsey propuso secretamente a Enrique como candidato para el puesto de emperador del Sacro Imperio Romano, a pesar de que públicamente parecía apoyar al rey francés, Francisco I. Finalmente, los príncipes electores eligieron a Carlos I de España. La subsecuente rivalidad entre Francia y España permitió a Enrique actuar como mediador. Así empezó a manejar el equilibrio del poder europeo.
Tanto Francisco I como Carlos I intentaron gozar del favor de Enrique VIII, Francisco en forma espectacular y deslumbrante, con el encuentro en el Campo del paño de oro,[8]​ y Carlos I con toda solemnidad en los encuentros de Kent. Después de 1521, sin embargo, la influencia inglesa sobre Europa comenzó a menguar. Enrique entró en una alianza con Carlos I a través del tratado de Brujas, y Francisco I de Francia fue derrotado por el ejército imperial de Carlos I en la Batalla de Pavía, en febrero de 1525. La confianza del emperador en Enrique disminuyó al mismo ritmo que el poder inglés sobre el continente. Enrique VIII se mostró reacio en ayudarlo a conquistar Francia, a pesar de las garantías de Carlos I. Esto terminó con el Tratado de Westminster de 1527.

El interés de Enrique en los asuntos continentales se extendió hasta el ataque contra la revolución alemana de Lutero. En 1521 le dedicó su "Defensa de los siete sacramentos", que le valió el título de Fidei defensor ("Defensor de la Fe").[9]​ Con base en esto, se lo reconoció con el título de inclitissimus.[10]​ Este honor lo mantuvo aun después de romper con Roma, y es todavía usado por la monarquía británica.

La de Enrique VIII fue la primera coronación pacífica en Inglaterra en muchos años; sin embargo, todavía tenía que ponerse a prueba la legitimidad de la dinastía Tudor. Esta se dio gracias al fallecimiento de su hermano mayor Arturo, a los quince años. 

El pueblo inglés parecía disconforme con las reglas de sucesión femenina, y Enrique sintió que solo un heredero varón podría asegurar el trono. Aunque Catalina quedó embarazada al menos siete veces (por última vez en 1518), solo uno de los hijos, María, sobrevivió a la infancia. Enrique había frecuentado amantes, entre ellas María Bolena e Isabel Blount, con quien tuvo un hijo ilegítimo, Henry Fitzroy, primer duque de Richmond y Somerset. En 1526, cuando estuvo claro que Catalina no podría tener más niños, Enrique comenzó a interesarse en la hermana de María Bolena, Ana.

Aunque la motivación principal para solicitar la declaración de nulidad de Catalina era su deseo de tener un heredero varón, Enrique se fue encaprichando con Ana hasta tal punto que terminó enamorándose de ella. El largo intento del rey para terminar su matrimonio fue denominado «La cuestión real».[11]​ El cardenal Wolsey y William Warham comenzaron secretamente a investigar la validez del matrimonio con Catalina. La reina había testificado que su primer matrimonio no había sido consumado y que, en consecuencia, no había impedimento para el posterior casamiento con Enrique. La investigación no pudo ir más allá, y se desestimó.

Sin informar a Wolsey, Enrique apeló directamente a la Santa Sede. Envió a su secretario William Knight a Roma para argüir que la bula de Julio II, por la que se permitió el matrimonio entre Enrique VIII y Catalina de Aragón, había sido obtenida mediante engaños y era en consecuencia nula. Además, pedía al papa Clemente VII que le otorgase una dispensa para permitirle desposar a cualquier mujer, incluso en el primer grado de afinidad. Esta dispensa era necesaria, ya que Enrique había tenido previamente relaciones con María Bolena.

Knight se encontró con que Clemente VII era prácticamente prisionero del emperador Carlos V, sobrino de Catalina. Tuvo dificultades hasta para entrevistarse con el papa y, cuando finalmente lo logró, no consiguió los resultados que buscaba. Aunque no estaba de acuerdo en declarar nulo el matrimonio, Clemente VII otorgó la dispensa, presumiendo que esta no tendría mucho efecto mientras Enrique permaneciera casado con Catalina.

Informado de lo obtenido por el representante del rey, Wolsey envió a Stephen Gardiner y a Edward Fox a Roma. Quizá por temor a Carlos V, el papa inicialmente evitó atender sus reclamaciones. Fox fue enviado de regreso con una comisión autorizando el inicio de un proceso, pero las restricciones impuestas la tornaban prácticamente insignificante.

Gardiner procuró formar una comisión ejecutiva que decidiera con antelación los puntos legales a discutir. Clemente VII fue persuadido para aceptar tal propuesta, y permitió a Wolsey y al cardenal Lorenzo Campeggio llevar el caso juntos. La comisión actuó en secreto; sus conclusiones no debían ser mostradas a nadie, y debían permanecer siempre en poder de Campeggio.

La comisión estableció que la bula papal que había autorizado el casamiento de Enrique con Catalina sería declarada nula si los alegatos en que se basó se demostraban falsos. Por ejemplo, la bula sería nula si resultaba falso que el matrimonio había sido absolutamente necesario para mantener la alianza anglo-hispana.

El cardenal Campeggio llegó a Inglaterra en 1528. Los procedimientos, sin embargo, se paralizaron cuando los españoles emitieron un segundo documento que presumía el otorgamiento de la necesaria dispensa. Se aseguraba que, unos pocos meses antes de otorgarle la dispensa en una bula pública, el papa Julio II había otorgado lo mismo en una nota privada enviada a España.

La comisión, sin embargo, solo hizo mención de la bula; no autorizó a los cardenales Wolsey y Campeggio a determinar la validez de la nota y durante ocho meses las partes litigaron sobre su autenticidad. Durante la primavera de 1529, el equipo legal de Enrique VIII completó el «libelo», sumario de los argumentos reales, incluyendo Levítico 20, 21, que fue presentado ante los delegados papales, y donde se observa, por ejemplo, lo siguiente:


Enojado por la demora, Enrique despojó a Wolsey de su poder y riqueza. Lo acusó de «præmunire»,[12]​ pero Wolsey murió al poco tiempo. Con Wolsey cayeron otros poderosos miembros de la Iglesia en Inglaterra; en las oficinas del lord canciller y del guardián de sellos fueron nombrados laicos en cargos antes reservados únicamente a clérigos.

El poder pasó en primer término a Tomás Moro, quien asumió como nuevo lord canciller de Inglaterra el 26 de octubre de 1529.[13]​ John Stokesley, quien había sido miembro del Consejo real, capellán y asistente de Enrique VIII, sirviéndolo en el Campo del Paño de Oro en 1520, fue enviado en 1529 a Francia —como embajador ante Francisco I— y a Italia buscando obtener nuevas opiniones favorables al divorcio del rey y de Catalina de Aragón.[14]​ Se lo designó obispo de Londres el 28 de marzo de 1530, en concomitancia con la creciente caída en desgracia de Tomás Moro, quien terminaría por renunciar a su cargo el 16 de mayo de 1532, un día después de que el clero inglés se sometiera definitivamente a la supremacía del rey sobre la Iglesia.[13]​ Ya en 1531 la influencia de Moro había mermado y distintos personajes que respaldaban las intenciones del rey mejoraron rápidamente sus posiciones. Así, el incremento de la influencia política de Thomas Cromwell se puso de manifiesto a través de la serie de cargos que asumió entre 1532 y 1533, que terminaron por incluir el de ministro de Hacienda y secretario de Estado.[15]​ Por su parte, Thomas Cranmer fue consagrado como arzobispo de Canterbury el 30 de marzo de 1533.[16]​

El 25 de enero de 1533, Cranmer participó de la boda entre Enrique y Ana Bolena. En mayo anunció la anulación del matrimonio con Catalina y poco después declaró válido el matrimonio con Ana. Catalina perdió el título de reina y se convirtió en la princesa viuda de Gales. Su hija María, ahora considerada, ilegítima, perdió el título de princesa de Gales y pasó a ser, simplemente, lady. La hija de Ana, Isabel, se convirtió en heredera presuntiva. Catalina de Aragón murió de cáncer en 1536. Tomás Moro aceptó que el Parlamento hiciera reina a Ana, pues del Parlamento emanaban las leyes y no se pronunció sobre que Enrique VIII fuese cabeza de la Iglesia de Inglaterra, llegando a dimitir como lord canciller para no tener que pronunciarse. Sabía que la vida le iba en ello. Durante un tiempo Enrique VIII lo dejó tranquilo, pero su silencio era tan atronador para toda Inglaterra que al final le quiso hacer hablar. Muchas veces fue interrogado. Fue encerrado en la Torre de Londres y llevado a un juicio que incluyó falsos testimonios. La función de acusación fue ejercida por Thomas Cromwell. Hallado culpable de alta traición, debido al falso testimonio, fue condenado a muerte. Una vez dictada la sentencia y al solicitársele por los jueces unas últimas palabras, por fin habló, diciendo que el juicio había sido una patraña y negando que Enrique VIII pudiera ser cabeza de la Iglesia. Fue ejecutado en 1535. La Iglesia católica lo consideró un mártir de la fe, y lo canonizó cuatro siglos después de su muerte.

El papa respondió a estos acontecimientos excomulgando a Enrique VIII en julio de 1533. Siguió una considerable agitación religiosa. Urgido por Thomas Cromwell, el parlamento aprobó varias leyes que sellaron la brecha con Roma en la primavera de 1534. La Ley de restricción de apelaciones[17]​ prohibió las apelaciones de las cortes eclesiásticas al papa. También previno que la Iglesia decretara cualquier tipo de regulación sin previo consentimiento del rey. La Ley de designaciones eclesiásticas[18]​ de 1534 decretó que los clérigos elegidos para obispos debían ser nominados por el soberano. La Ley de Supremacía,[19]​ del mismo año, declaró que «el rey es la única cabeza suprema en la tierra de la Iglesia de Inglaterra». La Ley de traiciones,[20]​ también de 1534, convirtió en alta traición castigada con la muerte desconocer la autoridad del rey, entre otros casos. Al papa se le negaron todas las fuentes de ingresos monetarios, como el Óbolo de San Pedro.

Rechazando las decisiones del papa, el Parlamento validó el matrimonio entre Enrique y Ana Bolena con la Ley de Sucesión[21]​ de 1534. La hija de Catalina, María, fue declarada ilegítima, y los descendientes de Ana pasaron a estar en la línea de sucesión real. Todos los adultos fueron obligados a reconocer las previsiones de esta acta; quienes la rechazaban eran condenados a prisión de por vida. La publicación de cualquier escrito alegando que el matrimonio de Enrique con Ana era inválido sería considerado alta traición.

La oposición a las políticas religiosas de Enrique fue rápidamente suprimida. Varios monjes disidentes fueron torturados y ejecutados. Cromwell, por quien fue creado el puesto de viceregente espiritual, fue autorizado a visitar monasterios, supuestamente para asegurarse de que seguían las instrucciones reales, pero en la práctica para hacerse con sus riquezas. En 1536, una ley del Parlamento permitió a Enrique confiscar las posesiones de los monasterios deficitarios (aquellos con ingresos anuales de 200 libras o menos).

En 1536, Ana comenzó a perder el favor de Enrique. Después del nacimiento de su hija Isabel, Ana tuvo dos embarazos que terminaron en aborto o muerte del niño. [22]​ Mientras tanto, Enrique empezaba a prestar atención a otra doncella de su corte, Juana Seymour. Quizá animado por Thomas Cromwell, Enrique hizo arrestar a Ana bajo cargos de usar brujería para convertirlo en su esposo, de tener relaciones adúlteras con cinco hombres, de incesto con su hermano Jorge Bolena, vizconde de Rochford, de injuriar al rey y conspirar para asesinarlo, con el agravante de traición. Los cargos eran enteramente fabricados. La Corte que trató el caso fue presidida por el propio tío de Ana, Thomas Howard, III duque de Norfolk. En mayo de 1536, se condenó a Ana y a su hermano a muerte por la hoguera o por decapitación, lo que el rey eligiera. Los otros cuatro hombres sobre los que se alegó tener relaciones con Ana fueron condenados a ser colgados, ahogados y descuartizados.

Lord Rochford fue decapitado al término del juicio de forma inmediata; a los otros cuatro implicados les fueron conmutadas sus diversas sentencias de muerte por la de decapitación. Ana también fue decapitada al poco tiempo.

En 1536, pocos días después de la ejecución de Ana, Enrique VIII se desposó con Juana Seymour. El Acta de Sucesión de 1536 declaró a los hijos de Juana dentro de la línea sucesoria, excluyendo a las otras hijas de Enrique, María e Isabel. El rey fue habilitado para determinar por sí en lo sucesivo la línea sucesoria. Juana dio a luz a un único hijo varón que tuvo en vida Enrique VIII, el príncipe Eduardo en 1537. Eduardo murió en el Palacio de Greenwich el 6 de julio de 1553, a los quince años de edad, y fue sepultado en la Abadía de Westminster. El 10 de julio de ese año subió al trono Juana Grey.

Luego de la muerte de Juana, la corte entera guardó luto con Enrique por algún tiempo. El rey la consideró siempre su «verdadera» esposa, por ser la única que le dio el heredero varón, que tan desesperadamente anhelaba para asegurar a la Dinastía Tudor.

De acuerdo con una investigación realizada en marzo de 2011, el patrón de embarazos de sus esposas y su deterioro mental sugieren que Enrique VIII tenía el antígeno sanguíneo Kell positivo, que ocasiona abortos en mujeres con antígeno Kell negativo y mortalidad neonatal y el síndrome de McLeod.[23]​[24]​[25]​ Enrique VIII nunca reconoció mal alguno en sí mismo por las pérdidas recurrentes de los embarazos de sus respectivas 6 esposas. [26]​

Para la época de su casamiento con Juana Seymour, Enrique concedió su aprobación a la Constitución de Gales[27]​ (1535–1542), que lo anexó legalmente con Inglaterra, haciendo de ambos un solo país. La ley decretó el uso exclusivo del inglés para los procedimientos oficiales en Gales, contrariando a los numerosos hablantes del idioma galés.

Enrique continuó la persecución de sus oponentes religiosos. En 1536 se desató en el norte de Inglaterra una revuelta conocida como la «peregrinación de Gracia» (en inglés, Pilgrimage of Grace).[28]​ Para aplastar a los católicos rebeldes, Enrique concedió poderes al Parlamento, y decretó un perdón general a todos los involucrados. No cumplió ninguna de sus promesas, y una segunda revuelta se inició en 1537. Los líderes de la rebelión fueron acusados de traición y ejecutados.[29]​ En 1538 Enrique ordenó la destrucción de los santuarios de todos los santos de la Iglesia católica, y para 1538, todos los monasterios existentes habían sido disueltos, y sus propiedades transferidas a la corona. Como recompensa por su eficiencia, Thomas Cromwell fue nombrado Conde de Essex. Abades y priores perdieron sus escaños en la cámara de los lores, y solo los arzobispos y obispos formaron la representación eclesiástica del cuerpo. Los «lores espirituales», como se conocía a los miembros del clero con lugares en la Cámara de los Lores, fueron por primera vez superados en número por los lores temporales.

Como su hijo Eduardo, duque de Cornualles, no era un niño sano, Enrique decidió casarse una vez más para asegurarse un heredero varón. Existían varias candidatas, entre ellas la duquesa viuda de Milán Cristina de Dinamarca o la hermana del duque de Cleves Ana de Cleves, esta última protestante y por tanto apoyada por Thomas Cromwell al ser la alianza luterana una importante aliada en el caso de que Roma atacara a Inglaterra.

Hans Holbein el Joven fue enviado a Cléveris para retratar a Ana. Tras el rechazo al matrimonio de Cristina de Dinamarca y después de observar el favorecedor retrato de Ana de Cleves, en el que aparecía sin ninguno de sus rastros de viruela, y urgido por las cumplidas descripciones que sus cortesanos hacían de Ana, Enrique decidió casarse con ella. Sin embargo, se dice que no la encontró nada atractiva cuando llegó a Inglaterra, y la llamaba en privado «la yegua de Flandes». No obstante, Enrique la desposó el 6 de enero de 1540.

Poco después, Enrique deseó terminar el matrimonio, no solo por sus sentimientos personales, sino también por consideraciones políticas. El duque de Cleves se hallaba envuelto en una disputa con Carlos V, Emperador del Sacro Imperio Romano Germánico, con quien Enrique no quería tener disputas. Ana fue lo suficientemente inteligente para no impedir la búsqueda de una anulación. Testificó que el casamiento nunca había sido consumado, diciendo que Enrique había ingresado cada noche en su habitación para meramente besarla en la frente antes de dormir. El casamiento fue consecuentemente anulado basándose en que Ana había realizado previamente contratos nupciales con otros nobles europeos.

Ana recibió el título de «Hermana del rey» y se le otorgó el castillo de Haver, la antigua residencia de la familia de Ana Bolena. Thomas Cromwell, mientras tanto, por haber impulsado el fallido matrimonio, perdió el favor real, cayó en desgracia y fue decapitado. El puesto de «vicegerente espiritual», creado para él, no fue cubierto y permanece vacante hasta hoy.

El 28 de julio de 1540, el mismo día en que Cromwell fue ejecutado, Enrique se casó con la joven Catalina Howard, prima de Ana Bolena. Poco después del casamiento, Catalina tuvo un romance con el cortesano Thomas Culpeper. También había empleado a Francis Derham como secretario, con quien había estado informalmente relacionada antes del casamiento real. Thomas Cranmer, enemigo de la poderosa y católica familia Howard, obtuvo evidencias de las actividades de la reina e informó a Enrique de ello. Aunque en principio el rey no creyó tales denuncias, autorizó a Cranmer a efectuar una investigación, que confirmó las acusaciones. Al ser interrogada, Catalina pudo haber admitido un compromiso previo con Derham, lo que por sí mismo habría convertido en inválido el posterior matrimonio con Enrique pero, en lugar de esto, sostuvo que Derham la obligó a establecer una relación adúltera. Derham, a su vez, expuso la relación entre la reina y Culpeper.

En diciembre de 1541, Culpeper y Derham fueron ejecutados. Catalina no fue condenada en juicio sino por un decreto de deshonra aprobado por el Parlamento. El decreto detallaba la evidencia contra la reina, con una cláusula especial que permitía la aprobación real a través de comisionados, para evitar que el rey volviera a escuchar el relato de los crímenes. Nunca se había utilizado este método de aprobación real, pero se usó en reinados posteriores para reemplazar la presencia real en el parlamento.

El casamiento con Catalina fue anulado poco antes de su ejecución. Igual que en el caso de Ana Bolena, Catalina no podría ser culpada técnicamente de adulterio ya que el matrimonio resultó oficialmente nulo desde el origen. Nuevamente esta cuestión fue ignorada y Catalina fue ejecutada el 13 de febrero de 1542.

Enrique se casó en 1543 con su última esposa, la rica viuda Catalina Parr. La nueva reina discutía con Enrique sobre religión, ya que era calvinista mientras que el rey permanecía anglicano. Esta conducta podría haberle resultado peligrosa si no hubiera sido por sus muestras de sumisión. Ayudó a reconciliar a Enrique con sus dos primeras hijas, María e Isabel. En 1544, un decreto parlamentario puso a ambas en la línea de sucesión tras el príncipe Eduardo, a pesar de ser consideradas ilegítimas. El mismo decreto permitía a Enrique determinar la siguiente sucesión al trono a su arbitrio.

En sus últimos años, Enrique engordó notablemente y su cintura llegó a medir 137 centímetros. El inicio de la obesidad data de un accidente de justa en 1536, en el que sufrió una herida en el muslo que no solo le impidió realizar actividad física sino que gradualmente derivó en una úlcera que indirectamente pudo haberlo llevado a la muerte. La hipótesis de que tenía sífilis fue difundida por primera vez unos cien años después de su muerte. Argumentos más recientes sobre esta posibilidad provienen de un mayor conocimiento de la enfermedad, que permiten suponer que Eduardo VI, María I de Inglaterra e Isabel I mostraron síntomas característicos de sífilis congénita.

Enrique VIII falleció el 28 de enero de 1547 en el palacio de Whitehall, el día en que su padre habría cumplido noventa años. Fue sepultado en la Capilla de San Jorge en el castillo de Windsor, al lado de su tercera esposa, Juana Seymour. 

En el transcurso de la década posterior a su muerte sus tres hijos se sentaron sucesivamente en el trono de Inglaterra. En virtud de la Ley de Sucesión de 1544, la corona fue heredada por el único hijo varón, Eduardo, que se convirtió en Eduardo VI, como primer monarca protestante de Inglaterra. Con solo nueve años de edad, no podía ejercer por sí el poder, que recayó en un consejo de regencia formado por dieciséis miembros elegidos según el testamento de Enrique VIII. El consejo eligió a Edward Seymour, Duque de Somerset y hermano mayor de Juana, como lord protector del reino.

En la eventualidad de que Eduardo no tuviera hijos, sería sucedido por María, hija de Catalina de Aragón. Si esta, a su vez, no tenía descendencia, la corona real la heredaría la hija de Ana Bolena, Isabel. Finalmente, si Isabel moría sin descendencia sería sucedida por los descendientes de María Estuardo, sobrina de Enrique VIII.

Junto con Alfredo el Grande, Enrique VIII es tradicionalmente recordado como uno de los fundadores de la Armada Real británica. Durante su reinado se desarrollaron varias batallas navales, y fundamentalmente se invirtieron importantes recursos en la construcción de barcos, incluyendo grandes navíos como el Mary Rose, y en la innovación tecnológica, como el uso de artillería a bordo. A pesar de esto, Enrique VIII no legó a sus sucesores una armada orgánica, con estructuras, rangos, etcétera. Isabel I tuvo que improvisar sobre la base de navíos privados para luchar contra la armada española, y en realidad, en un sentido completo, la armada británica recién se constituyó como producto de la rivalidad anglo-holandesa en el siglo XVII. Por su ruptura con la Iglesia católica, Enrique VIII inició el escenario de grandes invasiones españolas o francesas. Para proteger las costas mejoró numerosas defensas, como el castillo de Dover y otras fortificaciones y guarniciones de artillería, desde East Anglia hasta Cornualles. Muchas de estas construcciones se efectuaron con material obtenido durante la disolución de los monasterios entre 1536 y 1541.

Enrique VIII compuso obras breves entre las que pueden citarse:

Enrique VIII aparece como personaje en las siguientes películas:

Nota: De los hijos ilegítimos de Enrique VIII, sólo el duque de Richmond y Somerset fue formalmente reconocido por el rey. El parentesco de los otros hijos ilegítimos no está establecido. También es posible que Enrique tuviese más hijos con otras amantes no conocidas.



El saxofón, también conocido como saxófono o simplemente saxo, es un instrumento musical cónico, de la familia de los instrumentos de viento-madera, generalmente hecho de latón, que consta de una boquilla con una caña simple al igual que el clarinete. Fue inventado por Adolphe Sax a principios de los años 1840. El saxofón se asocia comúnmente con la música popular, la música de big band y el jazz. A los intérpretes del instrumento se les llama saxofonistas[1]​ o saxos, aunque esta última se emplea para denominar al propio instrumento de viento.[2]​ Pueden ser de ocho tamaños distintos en función de su afinación: sopranino en fa o mi bemol, soprano en do o si bemol, contralto o alto en fa o mi bemol, tenor en do o si bemol, barítono en fa o mi bemol, bajo en do o si bemol, contrabajo y subcontrabajo en do o si bemol.[3]​

Se desconoce el origen de la inspiración que llevó a Sax a crear el instrumento, pero la teoría más extendida es que, basándose en el clarinete, instrumento que él tocaba, empezó a concebir la idea de construir un instrumento que tuviera la fuerza de uno de metal y las cualidades acústicas de uno de madera, una especie de "clarinete de metal". Pero después de un intenso trabajo de pruebas y experimentos sobre modificaciones para lograr una mayor sonoridad y un sonido más metálico, Sax se dio cuenta de que había construido un nuevo instrumento: el saxofón.

El cuerpo del saxofón está compuesto por un tubo cónico y delgado, comúnmente de latón, que se ensancha en su extremo para formar una campana. A lo largo del tubo existen entre 20 y 23 agujeros de tono de tamaño variable, incluyendo dos agujeros muy pequeños de octava para ayudar a la interpretación del registro superior, aunque estos no sean esencialmente necesarios para interpretar dicho registro. Estos agujeros están cubiertos por almohadillas, que presionan los agujeros para producir un sello hermético. En reposo, algunos agujeros están abiertos y otros están cerrados por las almohadillas, que se controlan mediante varias llaves con los dedos de ambas manos, mientras que el pulgar derecho se sitúa debajo de un soporte que ayuda a mantener el saxofón equilibrado. La digitación del saxofón es una combinación entre la digitación del oboe y el sistema Boehm, siendo muy similar a la digitación de la flauta travesera o el registro superior del clarinete. En los instrumentos más grandes, la palanca requerida para interpretar las notas más bajas (que habitualmente se tocan con los meñiques de ambas manos) es bastante grande, por lo que se introduce un conjunto de llaves adicional para permitir interpretar dichas notas con los pulgares.

El cuerpo cónico del saxofón le otorga propiedades más similares a las del oboe que al clarinete. El diseño más simple del saxofón es un tubo recto troncocónico y los saxofones sopranino y soprano tienen, por lo general, este diseño recto. Sin embargo, como los instrumentos con notas graves serían inaceptablemente largos si fueran totalmente rectos, por motivos ergonómicos los instrumentos más grandes, por lo general, incorporan un recodo en forma de U en el tercer agujero de tono más grave o ligeramente encima de él. Como esto causaría que la campana del instrumento señalara casi directamente hacia arriba, el final del instrumento es o biselado o inclinado ligeramente hacia adelante. Este recodo se ha convertido en un rasgo icónico de la familia del saxofón, hasta el punto de que el saxofón soprano, e incluso el sopranino, a veces están fabricados en el estilo curvo aun cuando no sea estrictamente necesario. En cambio, aunque los altos y tenores rectos también existen, son más raros.[4]​[5]​ Sin embargo, lo más común es que los saxofones alto y tenor incorporen un recodo curvo encima del agujero de tono más alto, pero debajo de la llave de octava superior, inclinando la boquilla formando un ángulo recto. El barítono, el bajo y el contrabajo amplían la longitud del calibre principalmente por el plegado doble de esta sección.

Con una digitación sencilla, el saxofón moderno es generalmente considerado un instrumento fácil de aprender, especialmente cuando se procede de otros instrumentos de viento madera, aunque a pesar de esto se requiere una cantidad considerable de práctica y trabajo para alcanzar un sonido con color y correctamente afinado.

El saxofón usa una boquilla con una sola caña similar a la del clarinete, aunque es mayor la del saxofón y posee una cámara interior hueca redonda o cuadrada y es más amplia que la del clarinete. La boquilla del saxofón también carece de la ensambladura cubierta por corcho que tiene la boquilla del clarinete porque el tudel del saxofón se inserta directamente en la boquilla mientras que esa parte de la boquilla del clarinete es insertada en la parte superior del instrumento. La diferencia más importante entre una boquilla de saxofón y una boquilla de clarinete es que la boquilla de saxofón debería entrar en la boca en un ángulo mucho más inferior o plano que la del clarinete.

Las boquillas están fabricadas en una amplia variedad de materiales, las hay tanto metálicas como no metálicas. Las boquillas no metálicas son normalmente de ebonita, de plástico o de caucho duro, a veces de madera, y raras veces de cristal, de porcelana e incluso hueso. A las boquillas de metal algunos le atribuyen un sonido distintivo, descrito a menudo como "más brillante" que las no metálicas. Algunos músicos creen que las de plástico no producen un buen timbre. Otros saxofonistas, como el profesor Larry Teal, afirman que el material tiene poca repercusión en el sonido, si es que tiene alguna, y que son las dimensiones físicas las que le dan a la boquilla su color tímbrico.[6]​

Las boquillas con una cámara cóncava son las más cercanas al diseño original de Adolphe Sax y funcionan muy bien en la interpretación clásica, ya que producen un sonido más suave o menos desgarrador. Por el contrario, en el jazz y la música popular los saxofonistas tocan a menudo con cañas normalmente flojas y con boquillas abiertas. Están adecuadas de manera que el bafle, o "techo", de la misma esté más cercano a la caña. Por esa razón se crea un flujo de aire más rápido. Esto produce un sonido más claro que acorta fácilmente las distancias existentes en una big band o entre instrumentos amplificados. Aunque las aberturas grandes, y el sonido resultante, están comúnmente asociadas con las boquillas metálicas, cualquier boquilla puede tener una. De esta manera se permite una mayor flexibilidad en la afinación, dando cabida a efectos como el bending, común en el jazz y el rock. Los intérpretes clásicos por lo general suelen optar por cañas más duras y por una boquilla con una abertura estrecha y un cámara más baja, produciendo un sonido más oscuro y estable (y más aviolinado).

Al igual que los clarinetes, los saxofones usan una única caña o lengüeta que son generalmente más anchas y más cortas que las del clarinete. Habitualmente, las lengüetas están fabricadas con caña común, pero desde el siglo XX también se han fabricado cañas de fibra de vidrio. Estas cañas son más duraderas pero generalmente se considera que tienen una menor calidad sonora. El tamaño de la caña también depende del tipo de saxofón (soprano, alto, tenor, barítono, bajo, contrabajo, etc.) al que está destinada.

Las cañas distribuidas comercialmente dependen de una gran serie de marcas, estilos y durezas. Cada saxofonista experimenta con cañas de dureza y material diferente para encontrar la adecuada a su boquilla, embocadura y estilo de interpretación. La dureza se mide habitualmente usando una escala numérica que va del 1 al 4 (con grados intermedios), siendo la 4 la más dura y 1 la más blanda (excepto en el saxofón barítono cuya numeración llega al 5). Normalmente a los principiantes se les recomienda la central, es decir la caña de 2 1/2. Las cañas están sujetas a la boquilla gracias a una abrazadera, que sujeta la caña en la boquilla, evitando el desplazamiento de esta a la hora de tocar.

La mayor parte de los saxofones, tanto los antiguos como los modernos, están fabricados en latón. A pesar de ello, son clasificados como instrumentos de viento-madera más que como instrumentos de viento-metal porque las ondas sonoras son producidas por una caña oscilante, no por los labios del intérprete contra una boquilla, como ocurre en los metales, y porque se producen notas diferentes abriendo y cerrando llaves. El latón es usado para fabricar el cuerpo del instrumento, el soporte de las almohadillas, las barras que unen las almohadillas a las llaves, las propias llaves y los soportes que sostienen las barras y llaves al cuerpo del instrumento. Los tornillos que unen las barras a los soportes y los muelles que hacen que las llaves vuelvan a su posición inicial después de ser liberadas, generalmente están fabricados de acero inoxidable. Desde 1920, la mayor parte de los saxofones tienen llaves de tacto (que son piezas decorativas lisas colocadas donde los dedos tocan el instrumento) fabricadas de plástico o de nácar.

Se ha intentado fabricar saxofones con otros materiales, con distintos grados de éxito, saxofón Grafton de plástico fabricado en los años 1950. Unas empresas, como Yanagisawa[7]​ y Bauhaus Walstein,[8]​ han construido modelos de saxofón de bronce al fósforo (una aleación de 3,5 al 10 % de cobre con un alto porcentaje de fósforo, superior al 1 %) debido a sus calidades tonales ligeramente diferentes de este material.[9]​ Por ejemplo, aunque sus diseños sean idénticos dejando aparte el metal usado, el Yanagisawa A992 y el T992, fabricados con bronce al fósforo, suenan perceptiblemente "más oscuros" que el A991 y el T991, fabricados de latón. Un saxofón construido con bronce al fósforo es más pesado que uno de latón, debido a su contenido más alto de cobre, que le da una masa mayor. Tanto Yanagisawa como algunos otros fabricantes han fabricado cuellos de saxofón o instrumentos enteros con plata Sterling (plata de ley, aleación de 95% plata con cobre),[10]​ cobre, alpaca o materiales sintéticos. Los saxofones de Canonball de Salt Lake City (Utah) usan principalmente metales sin cobre en sus procesos de fabricación; como por ejemplo el revestimiento de níquel negro anodizado.[11]​ Julius Keilwerth desarrolló un saxofón con un cuerpo de alpaca como el de una flauta travesera, con un revestimiento de níquel negro.[12]​

Después de completar el instrumento, los fabricantes por lo general aplican un revestimiento fino de laca acrílica o son chapados en plata sobre el latón desnudo. La laca o el chapado sirven para proteger el metal de la oxidación y mantienen su aspecto brillante. A lo largo de los años se han usado diferentes tipos y colores para la superficie del instrumento. También es posible chapar el instrumento con níquel u oro.[13]​ El chapado con oro de los saxofones es un proceso caro porque el oro no se adhiere directamente al metal. Por consiguiente, el metal es cubierto en primer lugar de plata (que se adhiere al instrumento) y luego chapado de oro sobre la capa de plata.

Hay quienes argumentan que el tipo de lacado o chapado, o su ausencia, puede realzar la calidad sonora de un instrumento. Los posibles efectos de los diferentes acabados sobre el tono son un asunto fuertemente discutido, no menor porque otras variables pueden afectar los timbres de un instrumento, como por ejemplo el diseño de la boquilla y las características físicas del intérprete. En cualquier caso, el hecho de que constituya un tono agradable es un asunto de preferencia personal y los gustos varían.[14]​[15]​

El saxofón fue creado a mitad de la década de 1840 por Adolphe Sax, e introducido en la música orquestal por Jules Massenet en sus óperas Manon y Werther.[16]​

Adolphe Sax era un fabricante de instrumentos, flautista y clarinetista nacido en Dinant (Bélgica) que trabajaba en París. Mientras estaba trabajando en la tienda de instrumentos de su padre en Bruselas, Sax comenzó a desarrollar un instrumento que tenía la proyección de un instrumento de viento-metal con la movilidad de un instrumento de viento-madera. Otra prioridad era crear un instrumento que, aunque similar al clarinete, pero transitado una octava, a diferencia del clarinete, que eleva su altura doce tonos cuando transita. Un instrumento que transita una octava tendría idéntica digitación para ambos registros. Esto también permite a los saxofonistas hacer un mejor uso de los sobretonos.

Antes de su trabajo en el saxofón, Sax hizo varias mejoras al clarinete bajo, mejorando sus llaves y acústica y ampliando su registro inferior. Sax era también un fabricante de figles, en aquella época muy populares y que eran instrumentos de metal grandes y cónicos de registro grave con llaves similares a las de instrumento de viento madera. Su experiencia con estos dos instrumentos le permitió desarrollar las habilidades y las tecnologías necesarias para fabricar los primeros saxofones. Adolphe Sax creó un instrumento con una boquilla de caña sola como un clarinete, de cuerpo cónico de cobre como un figle y las propiedades acústicas de la flauta.

A principios de los años 1840, Sax había construido varios saxofones de diferente tamaño y recibió una patente de 15 años para el instrumento el 28 de junio de 1846.[17]​ La patente abarcó a 14 versiones del diseño fundamental, dividido en dos categorías de siete instrumentos cada una, desde el sopranino al contrabajo. En el grupo previsto por Sax para obras orquestales, los instrumentos transpuestos afinados en fa o en do, mientras que para "banda militar" el grupo incluyó instrumentos que alternaban entre mi♭ y si♭. El saxofón soprano orquestal era el único instrumento que sonaba a una altura normal. Sax dio a todos los instrumentos un registro escrito inicial de si, por debajo del fa agudo tres líneas adicionales por encima del pentagrama, dando a cada saxofón un registro de dos octavas y media.

La patente de Sax expiró en 1866.[18]​ A partir de entonces numerosos saxofonistas y fabricantes de instrumentos pusieron en práctica sus propias mejoras al diseño y llaves del instrumento. La primera modificación sustancial hecha por un fabricante francés consistió en una ligera ampliación de la campana y añadió una llave suplementaria para ampliar el registro hacia abajo en un semitono al si♭. Se sospecha que el propio Sax puede haber intentado esta modificación. Este suplemento fue adoptado en casi todos los diseños modernos.

El conjunto original de llaves de Sax era muy simplista y dificultó la interpretación de algunos pasajes legato y amplios intervalos sumamente difíciles de tocar. Entonces numerosos fabricantes añadieron llaves suplementarias y digitación alterna para hacer más sencilla la interpretación cromática. Mientras los primeros saxofones tenían dos aberturas de ventilación de octava separadas para facilitar la interpretación de los registros superiores, tal como hacen los instrumentos modernos, los intérpretes del diseño original de Sax tuvieron que manejar estas dos llaves de octava separadas con el pulgar izquierdo. Un avance sustancial en el conjunto de llaves del saxofón fue el desarrollo de un método por el cual ambos agujeros de tono son manejados por una llave de octava sola con el pulgar izquierdo, y que es ahora universal en todos los saxofones modernos.

Uno de los cambios más radicales efectuados al instrumento, aunque temporal, fueron las revisiones del conjunto de llaves del saxofón realizado en los años 1950 por M. Houvenaghel de París, que renovó completamente la mecánica del sistema para permitir una serie de notas (do♯, si, la, sol, fa y mi♭) para reducir un bemol con un semitono simplemente bajando el dedo corazón de la mano derecha. Esto permitía interpretar una escala cromática más de dos octavas simplemente interpretando la escala diatónica combinando alternativamente la subida y bajada de dicho dedo.[19]​ Sin embargo, este conjunto de llaves nunca tuvo mucha popularidad y ya no está en uso.

El saxofón fue originalmente patentado como dos familias, cada una de siete instrumentos. La familia orquestal consistía en instrumentos afinados en do y fa, y la familia de banda militar en mi♭ y si♭. Cada familia constaba de un sopranino, un soprano, un alto, un tenor, un barítono, un bajo y un contrabajo. Adolphe Sax también proyectó un subcontrabajo, pero nunca llegó a realizarlo.

En la familia de banda de música, solo el soprano, alto, tenor y el barítono son de uso corriente (estos forman las típicas secciones de saxofón de ambas bandas, las militares y las big band). El saxofón bajo es a veces usado en bandas de música (especialmente música compuesta por Percy Grainger).

Casi todos los músicos empiezan aprendiendo con el saxofón alto, pasándose más tarde al tenor o al barítono una vez que hayan desarrollado ciertas aptitudes. El saxofón alto es el más popular entre los compositores clásicos y artistas; la mayoría de los saxofonistas clásicos se centraron en primer lugar en el alto. El soprano ha ido ganando cierta popularidad en las últimas décadas, gracias sobre todo a los trabajos del saxofonista de jazz John Coltrane en 1960. El soprano se tiene a menudo como más complicado de tocar o de mantener afinado. Aún se hacen unos pocos bajos, sopraninos y contrabajos; dirigidos principalmente a coleccionistas o para usos innovadores o de vanguardia, aunque son usados en raras ocasiones exceptuando los grandes conjuntos de saxofones.

Los saxofones de la familia orquestal, no han tenido tanto éxito popular con la familia de banda de música. Adolphe Sax tuvo una rivalidad personal con el compositor alemán Wilhelm Wieprecht a quien hacía responsable en parte del completo fracaso del saxofón en la música orquestal. De esta familia, sólo el saxofón tenor y el soprano, ambos afinados en do y por tanto habilitados para poder interpretar fácilmente música compuesta para instrumentos de cuerda o canto, han llegado a alcanzar popularidad. El tenor en do, habitualmente conocido como saxofón melódico en do, fue muy popular entre los principiantes entre los años 1920 y principios de la década de 1930, porque los que tocaban no tenían que transportar. Aunque el instrumento fue popularizado por músicos como Rudy Wiedoeft y Frank Trumbauer, esto no le aseguró un lugar en el jazz o la música clásica. Los melódicos en do se siguieron fabricando a lo largo de 1930 después de que su popularidad inicial decayera, aunque se convirtieron en un artículo especial de encargo en los catálogos de algunos fabricantes. Estos instrumentos son tratados habitualmente como objetos de colección, si bien desde 1980 sólo unos pocos saxofonistas contemporáneos han comenzado a utilizar este instrumento de nuevo.

También a principios del siglo XX, el soprano en do (que afinaba un tono por encima del soprano en si♭) fue comercializado para aquellos que deseaban ejecutar partes de oboe en bandas militares y arreglos de vodevil e himnos litúrgicos. Los sopranos en 'do son fáciles de confundir con los sopranos corrientes (en si♭), ya que son solo aproximadamente dos centímetros más cortos. No se ha seguido produciendo ninguno desde finales de los años 1920. El saxofón mezzosoprano en fa (similar al alto moderno y producido por la firma estadounidense C. G. Conn durante el periodo de 1928-1929) es extremadamente raro; la mayoría de los ejemplares que aún quedan están en posesión de importantes coleccionistas. Adolphe Sax realizó algún prototipo de saxofón barítono en fa pero no se han llegado a construir. No se conocen otros ejemplares de saxofón bajo afinado en do, al margen del primer saxofón construido y exhibido por Sax a principios de los años 1840, o el sopranino en fa, a pesar de que Maurice Ravel realizó la instrumentación para dicho instrumento en su Bolero. El único saxofón alto conocido en fa fue fabricado por el mismo Sax y se sabe su existencia por el saxofonista canadiense Paul Brodie.

El saxofón contralto fue desarrollado a finales del siglo XX por el luthier Jim Schmidt. Este instrumento es más grande y tiene un sistema nuevo de digitación así que no se parece al saxofón melódico en do exceptuando la clave en la cual está afinado y su registro.[20]​ Las dificultades para construir un verdadero saxofón sopranissimo, también llamado soprillo, han quedado patentes ya que solo recientemente han podido empezar a construirse. Este saxofón del tamaño de un flautín está una octava por encima del soprano y su diminuto tamaño hace necesaria una llave de octava en su boquilla. El instrumento, que amplía la familia original de Sax elevando una octava completa más aguda que el saxofón soprano en si♭, fue fabricado por Benedikt Eppelsheim, de Múnich (Alemania). Existe un prototipo raro de saxofón tenor de varas, pero se llegaron a construir pocos. Una de las empresas que fabricó un saxofón soprano de varas fue Reiffel & Husted, de Chicago (Estados Unidos), alrededor del año 1922 (catalogado como NMM 5385).[21]​[22]​[23]​

Han aparecido numerosos instrumentos relacionados con el saxofón desde que Adolphe Sax presentara su trabajo original, la mayoría sin ningún éxito significativo. Se incluyen entre estos el saxello, similar al soprano recto pero con una ligera curvatura en el tudel y una campana inclinada; el alto recto; y el tenor recto (actualmente solo fabricados por L.A. Sax Company).[24]​ Teniendo en cuenta que el tenor recto mide aproximadamente 1,5 metros de largo, el engorroso tamaño de semejante diseño dificulta dos cosas: tocarlo (especialmente cuando uno está sentado) y transportarlo. Los saxellos "King" (Rey), fabricados por H. N. White Company en la década de 1920, alcanzan en la actualidad un precio de 4000 dólares. Muchas compañías, incluyendo Rampone & Cazzani y Woodwind and Brasswind, están comerciando sopranos rectos con campana inclinada como saxellos (o "sopranos saxellos"). Dos de estas variantes fueron auspiciadas por el músico de jazz Rahsaan Roland Kirk, que llamó a su alto recto stritch y a su saxello modificado manzello. Este instrumento único tenía una campana más grande de lo habitual y un teclado modificado.

El tubax, desarrollado en 1999 por el fabricante de instrumentos alemán Benedikt Eppelsheim, reproduce el mismo registro y con la misma digitación que un saxofón contrabajo afinado en mi♭. Sin embargo, aunque es del mismo calibre es más estrecho que el saxofón contrabajo, haciéndolo un instrumento más compacto y con un sonido más "juncoso" (comparable al contrabajo de doble caña llamado sarrusofón). Puede ser interpretado con la boquilla y caña más pequeña (y más comúnmente disponible) del saxofón barítono. Eppelsheim también ha fabricado la tubax subcontrabajo afinada en do y en si♭, siendo este el saxofón más bajo que se haya construido. Entre los desarrollos más recientes se encuentra el aulocromo, un doble saxofón soprano inventado por el fabricante de instrumentos belga François Louis en 2001.[25]​

Otra variante inusual del saxofón fue el Conn-O-Sax, un instrumento recto afinado en fa (un tono por encima del alto en mi♭) con un tudel ligeramente curvado y una campana esférica. El instrumento, que combinaba el calibre y teclado del saxofón con una campana similar a la del heckelfón, intentaba imitar el timbre del corno inglés y fue producido solo entre 1928 y 1930. El instrumento tenía un registro que iba desde la bajo hasta sol alto.

Aunque no son verdaderos saxofones, en el siglo XX se crearon versiones baratas para música tradicional fabricadas de bambú y sin llaves, construidas por fabricantes de Hawái, Jamaica, Argentina, Chile, Tailandia e Indonesia. En los países latinoamericanos se les llama "Saxofón Andino", aunque no tienen ninguna relación histórica, antropológica o tradicional con instrumentos del área como la Zampoña o la Quena. Lo único que tienen en común, es el material con que están construidos (aunque de hecho es común encontrarlos con afinaciones en Sol, al igual que la Quena). El nombre "saxo andino" parece deberse más a razones comerciales destinadas a los turistas.
El instrumento hawaiano, llamado xaphoon, es comercializado como el "saxo de bambú", aunque su forma taladro cilíndrico corresponde al de un clarinete (presenta sólo armónicos impares) y su ausencia de llaves a una flauta dulce. El exponente jamaicano más conocido y con un "saxofón" casero de bambú fue para el mento y realizado por el fabricante de instrumentos "Sugar Belly" (William Walker).[26]​
En Argentina se desarrollaron saxofones de bambú de cuerpo cónico escalonado desde 1985, con una o más llaves, y que utilizan las mismas lengüetas que los saxos convencionales.[27]​
En la región de Minahasa de la isla indonesia de Célebes, existen bandas enteras constituidas por "saxofones"[28]​ de bambú e instrumentos de viento-metal de varios tamaños. Esos instrumentos son ingeniosas imitaciones de los instrumentos europeos fabricados con materiales locales. En Tailandia también se construyen instrumentos muy similares.
[29]​[30]​

El saxofón ganó popularidad en primer lugar en el ámbito para el que fue diseñado: la banda militar. Aunque el instrumento fue ignorado por los académicos en Alemania, las bandas militares francesas y belgas aprovecharon plenamente las ventajas del instrumento que Sax había diseñado expresamente para ellos. La mayoría de las bandas militares francesas y belgas incorporan al menos un cuarteto de saxofones que comprenden al menos un barítono en mi♭, un tenor en si♭, un alto en mi♭ y un soprano en si♭. Estos cuatro instrumentos han demostrado ser los más populares de todas las creaciones de Sax. El resto de creaciones de Sax, como el contrabajo en mi♭ y el bajo en si♭ por lo general son considerados poco prácticos debido a su gran tamaño y el sopranino en mi♭ insuficientemente poderoso. Las bandas militares británicas tienden a incluir como mínimo un saxofón alto y otro tenor.

Más recientemente, el saxofón ha encontrado un lugar tanto en la banda sinfónica como en la música de big band, que a menudo requiere el uso del barítono en mi♭, el tenor en si♭ y el alto en mi♭. El soprano en si♭ también es utilizado de vez en cuando y suele ser interpretado por el primer saxofonista alto. El saxofón bajo en si♭ es a veces usado en bandas militares (especialmente música compuesta por Percy Grainger) y orquestaciones de big band, sobre todo la música compuesta por Stan Kenton para la Mellophonium Orchestra. En los años 1920 el saxofón bajo fue usado a menudo en grabaciones de jazz clásico, ya que en aquel tiempo era más fácil poder grabarlo que una tuba o un contrabajo.[cita requerida] También fue usado en la partitura original (y la película) de Leonard Bernstein West Side Story. Tras la inclusión del saxofón en la orquesta sinfónica, el instrumento ha aumentado su popularidad. En cualquiera de sus tamaños, ha sido considerado un acompañamiento útil en géneros tan amplios como la ópera, la música coral y la música de cámara. Numerosos musicales incluyen partes para el saxofón, por lo general duplicando otro instrumento de viento-madera o de viento-metal. De este modo el saxofón sirve como un punto medio entre ambas familias, ayudando a fusionarlas.

El saxofón es mucho más conocido, e icónico, por su papel en la música jazz moderna. Por lo general, suele ser interpretado por un saxofonista solista acompañado por una sección de ritmo, aunque a veces puede formar un cuarteto o ser parte de una big band. Pero cabe resaltar que en Perú, en el valle del mantaro es ejecutada por orquestas y tomadas como patrón musical por autores y compositores.

El cuarteto de saxofón por lo general está compuesto por un saxofón soprano y un tenor en si♭ y un alto y un barítono en mi♭ (conocidos por las siglas SATB), aunque en ocasiones el soprano es sustituido por un segundo alto (conocidos por las siglas AATB). Algunos cuartetos profesionales de saxofón han destacado por tener una instrumentación no estándar, como el Alto Quartet de James Fei, compuesto por cuatro altos,[31]​ y la Bluiett Baritone Nation de Hamiet Bluiett, con cuatro barítonos.

Existe un repertorio de composiciones clásicas y arreglos para la instrumentación SATB que remonta al siglo XIX, en particular de los compositores franceses que conocían a Adolphe Sax. Los cuartetos de saxofones Raschèr,[32]​ Amherst,[33]​ Aurelia,[34]​ Amstel y Rova están entre los mejores grupos conocidos. Históricamente, los cuartetos dirigidos por Marcel Mule y Daniel Deffayet, profesores de saxofón en el Conservatorio de París, que comenzaron en 1928 y 1953, respectivamente, han sido muy recordados. El cuarteto de Mule es a menudo considerado el prototipo de todos los cuartetos futuros debido al alto nivel de virtuosismo demostrado por sus miembros y su papel principal en el desarrollo del repertorio para cuarteto. Sin embargo, ya existían cuartetos organizados antes del conjunto de Mule, como por ejemplo el primer cuarteto encabezado por Eduard Lefebre, antiguo saxofón solista de la banda de John Philip Sousa en Estados Unidos desde 1904 hasta 1911. Probablemente, también existieron otros conjuntos en este época como parte de las secciones de saxofón de muchas bandas turísticas "de negocio" que existieron a finales del siglo XIX y comienzos del XX. A finales de la década de 1970, el World Saxophone Quartet se ha hecho conocido como un cuarteto de saxofón de jazz preeminente. El Rova Saxophone Quartet, ubicado en San Francisco, es destacado por su trabajo en los campos de la música clásica contemporánea y la música improvisada.

Existen algunos conjuntos con todos los saxofones más grandes, entre los que destaca la SaxAssault de nueve miembros,[35]​ y Urban Sax, que incluye al menos a 52 saxofonistas. La Nuclear Whales Saxophone Orchestra de seis miembros posee uno de los pocos saxofones contrabajo en mi♭ e interpretan una variedad de piezas para conjunto, incluyendo "Casbah Shuffle"", un dúo para sopranino y contrabajo.[36]​ En ocasiones, se realizan interpretaciones de grupos de más de 100 saxofonistas en convenciones de saxofones.[37]​

La técnica para tocar el saxofón es subjetiva y está basada en el estilo que se pretenda tocar (música clásica, jazz, rock, ska, funk, etc.) además del sonido que el músico tenga idealizado y pretenda alcanzar. El diseño del saxofón permite una increíble variedad de producción tonal y el sonido "ideal" y las llaves para producirlo son temas que alimentan acalorados debates. Sin embargo, hay una estructura básica subyacente que sustenta la mayoría de las técnicas. El instrumento también tiene una estructura de digitación que lejos de ser fija se acerca más a una amplia variedad de alternativas que en variados casos pueden producir el mismo tono utilizando digitaciones totalmente diferentes. Esta versatilidad única le permite al intérprete utilizar la digitación más conveniente dependiendo de la escala que esté utilizando o el tipo de música que en ese momento interpreta.

La música para la mayor parte de saxofones está compuesta, por lo general, utilizando la clave de sol. El registro estándar escrito se extiende desde si♭3 (un tono por debajo del do central del piano) hasta fa o fa♯5 (tres líneas adicionales por encima del pentagrama). Hay unos modelos de saxofón soprano que tienen una llave para ejecutar el sol5 y varios modelos de saxofón barítono tienen un taladro ampliado y una llave para producir la2. También es posible interpretar la2 en cualquier saxofón tapando el final de la campana, por lo general con el pie o con la parte interna del muslo izquierdo. Las notas por encima de fa5 son consideradas parte del registro sobreagudo de cualquier saxofón y pueden ser producidas modificando la cavidad bucal y ayudándose de digitaciones especiales. El propio Adolphe Sax dominaba estas técnicas y demostró que el instrumento un registro escrito superior a las tres octavas, llegando a si2. Saxofonistas modernos como Lenny Pickett y John Zorn han ampliado este registro a más de 4 octavas en el saxofón tenor y alto respectivamente.

Como todos los saxofones usan el mismo sistema de llaves y tienen la misma digitación para producir una nota determinada, no es difícil para un intérprete experimentado cambiar entre varios tamaños de saxo cuando la música ha sido adecuadamente transpuesta. Como el barítono y el alto están afinados en lanzados en mi♭, los saxofonistas pueden leer la música escrita en clave de fa simplemente interpretándola como si estuviera escrita en clave de sol y añadiendo tres sostenidos a la armadura de clave. Este proceso, denominado "sustitución de clave", hace posible que el barítono o el alto puedan interpretar las partes escritas para el fagot, la tuba, el trombón o el contrabajo. Esto puede ser útil si una banda de música u orquesta carece de alguno de dichos instrumentos.

Un libro (del latín liber, libri) es una obra impresa, manuscrita o pintada en una serie de hojas de papel, pergamino, vitela u otro material, unidas por un lado (es decir, encuadernadas) y protegidas con tapas, también llamadas cubiertas. Un libro puede tratar sobre cualquier tema. Según la definición de la Unesco,[1]​[2]​ un libro debe poseer veinticinco hojas mínimo (49 páginas), pues de veinticuatro hojas o menos sería un folleto; y de una hasta cuatro páginas se consideran hojas sueltas (en una o dos hojas).[2]​

También se llama «libro» a una obra de gran extensión publicada en varias unidades independientes, llamados tomos o volúmenes. Otras veces se llama también «libro» a cada una de las partes de una obra, aunque físicamente se publiquen todas en un mismo volumen (ejemplo: Libros de la Biblia).

No obstante, esta definición no queda circunscrita al mundo impreso o de los soportes físicos, dada la aparición y auge de los nuevos formatos documentales y especialmente de la World Wide Web. El libro digital o libro electrónico, conocido como e-book, está viendo incrementado su uso en el mundo del libro y en la práctica profesional bibliotecaria y documental. Además, el libro también puede encontrarse en formato audio, en cuyo caso se denomina audiolibro.

Desde los orígenes la humanidad ha tenido que hacer frente a una cuestión fundamental: la forma de preservar y transmitir su cultura, es decir, sus creencias y conocimientos, tanto en el espacio como en el tiempo.

El planteamiento de esta cuestión supone, por un lado, determinar la forma de garantizar la integridad intelectual del contenido de la obra y la conservación del soporte en el que fue plasmada y, por otro, encontrar el medio por el cual se mantendrá inalterada la intención o finalidad para la cual se concibió.

Los orígenes de la historia del libro se remontan a las primeras manifestaciones pictóricas de nuestros antepasados, la pintura rupestre del hombre del paleolítico. Con un simbolismo, posiblemente cargado de significados mágicos, estas pinturas muestran animales, cacerías y otras escenas cotidianas del entorno natural del hombre antiguo que trataba de dominar las fuerzas adversas de la naturaleza, capturando su esencia mediante su representación.

Las señales gestuales fueron la primera forma de expresar y transmitir mensajes. La palabra hablada es la manera más antigua de contar historias. Mediante fórmulas de valor mnemotécnico se estructuraban narraciones, que pasaban de generación en generación como valiosa herencia cultural de los más diversos grupos humanos. Dichas reglas mnemotécnicas ayudaban tanto a la memorización como a la difusión de los relatos. Es el caso de los poemas homéricos, que han merecido valiosos estudios sobre el particular. Posiblemente, gran parte de las tradiciones y leyendas han tenido semejante inicio. Esta transmisión oral tenía el inconveniente de los «ruidos» que deformaban el mensaje. La mayoría de las veces era el narrador (rapsoda, aeda, juglar) quien en función de sus intereses la deformaba de una u otra forma.

Cuando los sistemas de escritura fueron inventados en las antiguas civilizaciones, el hombre utilizó diversos soportes de escritura: tablillas de arcilla, ostracon, placas de hueso o marfil, tablas de madera, papiros, tablillas enceradas, planchas de plomo, pieles curtidas, etc.

La escritura fue el resultado de un proceso lento de evolución con diversos pasos: imágenes que reproducían objetos cotidianos (pictografía); representación mediante símbolos (ideografía); y la reproducción de sílabas y letras.

Los más antiguos vestigios de escritura se encuentran, hacia finales del IV milenio a. C., en el Antiguo Egipto, con jeroglíficos, y la antigua Mesopotamia, mediante signos cuneiformes (escritura cuneiforme; utilizaban una varilla con sección triangular, que al hendir en placas de arcilla, dejaba una marca en forma de cuña). La usaron los sumerios, acadios, asirios, hititas, persas, babilonios, etc. La escritura egipcia, que perduró más de tres milenios, mediante jeroglíficos, representaba ideas abstractas, objetos, palabras, sílabas, letras y números. Evolucionó en las escrituras hierática y demótica. Otros pueblos, como los hititas y los aztecas también tuvieron tipos propios de escritura.

La escritura china más antigua que se conoce son 50000 inscripciones sobre conchas de tortuga que incorporan 4500 caracteres distintos, y data del 1400 a. C. en el yacimiento de Xiaotun, en la provincia de Henan. Pero los primeros libros reconocibles de China corresponden al siglo VI a. C., los jiance o jiandu, rollos de finas tiras de bambú o madera grabados con tinta indeleble y atados con cordel. Estos textos servían principalmente a causas institucionales, era la obra de funcionarios civiles o militares.[3]​

Desde Confucio en adelante (551-479 a. C.) los libros se convirtieron en importantes instrumentos de aprendizaje, se escribieron tratados de filosofía, medicina, astronomía y cartografía.

En el período de los reinos combatientes (475-221 a. C.) La seda se usó mucho como soporte para escribir. La tela era ligera, resistente al clima húmedo, absorbía bien la tinta y proporcionaba al texto un fondo blanco, sin embargo era mucho más cara que el bambú, es por esto que en ocasiones se hacía una copia en bambú antes de grabarse en seda los textos importantes.

La invención del papel según la tradición china, se atribuye a un eunuco de la corte imperial llamado Cai Lin en el 105 d. C. Usando nuevos ingredientes (trapos viejos, cáñamo, corteza de árbol y redes de pescar) creó un método de fabricación de papel muy similar al que se usa hoy en día. Pero el papel tardó cientos de años en reemplazar al bambú y la seda, fue hasta finales del siglo II d. C. que la corte imperial lo usó en cantidades importantes. Esta innovación no se propagó fuera de China hasta el 610 d. C. aproximadamente, y alcanzó Europa a través de España hasta el siglo XII.

A mediados del siglo VIII los chinos inventaron la impresión xilográfica, o el grabado en madera, y la necesidad de reproducir un gran número de textos e imágenes budistas, calendarios, manuales de adivinación y diccionarios promovió una rápida y temprana propagación de la xilografía. El primer libro impreso chino que se ha encontrado es el Sutra del diamante del 868 d. C.

Los impresores chinos crearon los tipos móviles hacia el siglo XI, el escritor chino Ch'en Kua (1030-1095) narra la historia de esta invención en su libro de cosas vistas y oídas (Mengshi Pitan), según el escritor el herrero JenTsung de la dinastía de los Song del norte entre 1041-1049 logró crear caracteres móviles, para esto utilizó arcilla endurecida al fuego sobre la cual había grabado unos caracteres móviles que fijo sobre una plancha de hierro impregnada de resina de pino, cera y cenizas. También se le atribuye la creación de una mesa giratoria para guardar los caracteres, esta técnica se llamaba tipografía tablearia. Hacia el 1300 Wang- Tcheng, un técnico agrónomo, emplazó la arcilla por madera de azufaifo, que era mucho más dura. Pero este avance no revolucionó la imprenta hasta el punto que lo hizo Gutenberg en Europa 400 años después. A diferencia de las lenguas europeas, el chino escrito requiere miles de caracteres únicos, lo que hace mucho más eficaz los bloques de madera individuales que los enormes conjuntos de tipos reutilizables. En contraste con el declive de las artes de los escribas en occidente en los siglos que siguieron a la creación de la imprenta de tipos móviles, la caligrafía china conservó su prestigio, era un arte. No obstante, a finales del siglo XV, China había producido más libros que el resto del mundo junto.

Los árabes aprendieron la técnica para fabricar papel de sus contactos con China en el siglo VIII, y este se introdujo en Europa en el siglo XII a través de la España musulmana.[3]​

La obra xilográfica más antigua encontrada hasta nuestros días es el Dharani Sutra de Corea, datado en el 751 a. C., aunque no se sabe quién fue el inventor de la xilografía los chinos y coreanos fueron los que impulsaron la impresión xilográfica, principalmente para editar textos religiosos. El budismo chino y coreano fue el vehículo que trasmitió la xilografía a Japón. Pero Corea realizó muchos otros avances que revolucionaron la manera de imprimir y en consecuencia el libro.

Entre 1234 y 1239, los coreanos que se habían refugiado en la isla de Gwanghwa, debido a la invasión mongol, no disponían de madera dura. Fue entonces cuando imprimieron 28 ejemplares de los 50 volúmenes del Go geum sang jeong ye mun con caracteres móviles metálicos. La obra del año 1239 describe el método utilizado y termina diciendo: impreso para la eternidad con caracteres de nueva fabricación. Más tarde el rey Taejong puso en funcionamiento un taller que contribuía a la difusión de la escritura y en 1403, el tercer año de su reinado, se restableció la fundición nacional, el Jujaso, donde se fabricaban caracteres móviles de imprenta, realizó la primera fundición de tipos móviles en bronce. Cabe señalar que la invención de la tipografía coreana es de primordial importancia para la religión, particularmente el budismo, el confucionismo, y el taoísmo.[4]​

Durante el reinado del tercer hijo de Taejong, Sejong aumentó el número de centros dedicados a la enseñanza. En la capital existían cuatro escuelas, un colegio para el pueblo y una escuela para la familia real y sus parientes. El libro se convirtió en la herramienta primordial de los esfuerzos de alfabetización que, incluso llegaron a las provincias y pueblos lejanos. Los niños varones tenían que seguir las clases que les inculcaban las nociones básicas como la escritura y la lectura.

Los caracteres fueron mejorando con el tiempo, buscaban una forma más cuadrada y más regular que los precedentes, facilitando así la composición. Durante la invasión japonesa (1592-1598) un general japonés llevó caracteres móviles y libros a Japón, así Japón pudo desarrollar su imprenta, en cambio, la imprenta coreana retrocedió a partir de ese momento, se volvió a la madera para la fabricación de tipos móviles y cada la producción de libros decayó.[5]​

Sin duda alguna la dinastía Joseon fue el gran periodo para los libros coreanos, se sabe de 32 fundiciones de caracteres móviles metálicos y más de 350 modelos diferentes. A pesar de las dificultades Corea supo desarrollar e incluso exportar sus técnicas de imprenta. China no utilizó caracteres móviles hasta finales del siglo XV, en 1490, por su parte, Japón adoptó la técnica tipográfica coreana a finales del siglo XVI en 1592.

Egipto creó el papiro y lo exportó a todo el Mediterráneo, se usaba para plasmar textos en Egipto, Grecia y Roma. La fabricación del papiro era complicada y dado que las láminas de papiro estaban hechas de dos capas superpuestas, por cada cara discurría una veta distinta, de ahí que se denomine recto donde el grano discurría de forma horizontal y verso en donde el grano discurría en vertical, sin embargo solo se escribía en la cara interna que era la más lisa. Las láminas se pegaban para hacer un rollo.

El arte en el perfeccionamiento de la técnica de escritura tiene que ver con la sustitución del papiro por el pergamino. Los faraones y gobernantes egipcios ejercieron un monopolio sobre la fabricación del papiro hasta el siglo XII (ya con los musulmanes). Decidían el precio de las ocho variedades de papiro del mercado, y aplicaban a su gusto medidas de presión o sabotaje. A principios del siglo II a.C el rey Ptolomeo V, corroído por la envidia, intentaba perjudicar a una biblioteca rival situada en la ciudad de Pérgamo (actual Turquía) que, al igual que la Biblioteca de Alejandría, intentaba atraer a los sabios de la época. Hizo encarcelar a su bibliotecario, Aristófanes de Bizancio, cuando supo que éste planeaba instalarse en Pérgamo bajo la protección del rey Eumenes II. Furioso por la posible competencia, Ptolomeo interrumpió el suministro de papiro al reino de Eumenes para doblegar a la biblioteca enemiga privándola de la materia prima que hasta entonces era el mejor material de escritura existente. El embargo impulsó un gran avance que llevaría el nombre de la ciudad. En Pérgamo perfeccionaron la antigua técnica oriental de escribir sobre cuero. En recuerdo de la ciudad que lo universalizó el producto mejorado se llamó “pergamino”. Unos cuantos siglos más tarde, este hallazgo cambió la fisonomía y el futuro de los libros.[6]​

A partir del siglo I d. C. el pergamino comenzó a competir con el papiro. Se cree que aquel surgió en Pérgamo, en la actual Turquía. El pergamino tenía la ventaja de resistir condiciones de humedad, era más duradero y podía doblarse sin romperse, también podía rasparse para limpiarlo y ser reutilizado.

Es muy poco lo que se conoce de las bibliotecas egipcias, un pequeño testimonio es el templo de Horus, donde en uno de los muros están los títulos de 37 libros que eran parte de las bibliotecas.[7]​

La escritura alfabética hizo más accesible la lectura y la escritura. El alfabeto griego se desarrolló en el siglo VI y V a. C., era puramente fonético a diferencia de los ideogramas chinos, un erudito chino podía dedicar toda su vida a dominar miles de caracteres, en comparación, el alfabeto griego podía aprenderse en unos días. El uso de la escritura se incrementó en Atenas hacia el siglo V a. C.[8]​

En relación con el uso de la escritura y de los libros, se conocían entre los griegos los oficios siguientes:

Estos entre los griegos no se vendían encuadernados sino enrollados en dos varillas, una en el principio y otra en el final, de modo que se desenrollaban para leerlos con la mano izquierda arriba cogiendo el extremo de la varilla del principio y se recogía con la mano derecha en la del final. Así, una obra solía constar de varios rollos o libros de papiros cosidos de una extensión más o menos definida.

En Atenas, los libreros tenían tiendas públicas y en ellas se reunían ordinariamente los literatos para leer los libros nuevos que se escribían.[9]​

Entre los romanos se conocían las siguientes profesiones relacionadas con los libros: 

En tiempo de la república las personas acomodadas tenían en sus casas muchos copistas o secretarios, la mayor parte esclavos o libertos, para copiar los manuscritos nuevos. Pero en tiempo de Augusto los vendedores de libros, bibliopolæ, se introdujeron en Roma y comenzaron a verse tiendas de libros, que solían estar cerca de la entrada de los templos y de los edificios públicos, y en particular en el foro romano. Los libreros fijaban en sus puertas los títulos de las obras que tenían en venta para que con un golpe de vista pudiese cualquiera enterarse de lo que había en ellas.[9]​

En la Roma imperial los escritos podían encontrarse en todas partes. La administración cotidiana produjo un flujo constante de documentos, la alfabetización rudimentario era habitual, incluso en las clases bajas, lo que provocó que en el siglo I d. C. hubiera un crecimiento del público lector, ya no se escribía para un círculo de amigo íntimos, sino para un público anónimo, pero la clase alta siguió conservando la cultura literaria oral tradicional.

En el siglo III d. C. empezó el declive del imperio romano y las invasiones bárbaras causaron una contracción de la cultura escrita. Muchas instituciones escolásticas cayeron, a excepción de las mantenidas por la iglesia cristiana.

Durante los primeros siglos de la era cristiana apareció el códice, una de las más importantes y perdurables revoluciones de la historia del libro. Era más compacto y fácil de manejar que los rollos, podía utilizarse ambas caras del papel, lo que le permitía contener más texto. Aunque el códice tenía claras ventajas, el rollo siguió en uso durante varios siglos. La monarquía inglesa continuó usando rollos para registrar sus leyes hasta la edad media.[10]​

Con el advenimiento de la imprenta, se inicia la época de expansión bibliográfica, de la modernidad y del pensamiento crítico, facilitado en la actualidad con el acceso a la información en otro tipo de fuentes, tales como periódicos, revistas, Internet, etc. No obstante, el valor del libro es perdurable a través del tiempo.

Antes de la invención de la imprenta era muy costosa la adquisición de una obra importante y se vendía lo mismo que una heredad o casa, por medio de escritura pública y bajo condiciones particulares. Los historiadores citan muchos ejemplos de lo escasos que eran en la edad media los libros y de lo caros que se vendían en Europa. Saint-Loup, abad de Ferrleres, envió dos de sus monjes a Italia el año 855, con el solo objeto de sacar una copia del Tratado de la Oratoria de Cicerón y de algunos otros libros latinos, de los cuales no poseía sino algunos fragmentos. En el siglo XII ejemplar de la Biblia y otro de las cartas de San Jerónimo eran poseídos en común por varios monasterios de España, que se servían de ellos simultáneamente. El abate Lebeuf menciona una colección de homilías por las cuales se dieron en Bretaña, en el siglo XI, 2000 carneros y tres moyos de grano. La copia de los manuscritos se hacia entonces con tanta pausa y lentitud, que una copia de la Biblia sacada en cinco meses se consideró como un prodigio de velocidad. Habiendo legado un particular en 1406 a una iglesia de Parts, un breviario para el uso de sus capellanes y para los sacerdotes pobres, se resolvió a fin de conservar tan preciosa alhaja y de cumplir al mismo tiempo los deseos del testador, encerrarlo en una caja de hierro. En el siglo XV todavía no se prestaban los libros sino con muchas garantías y seguridades.

Con el fin de que las obras se conservaran y reprodujeran, se acostumbraba en algunos monasterios a que cada novicio copiara antes de profesar el libro que el superior le señalaba a cuya costumbre debemos muchos libros preciosos de la antigüedad, que sin esta medida no habrían llegado hasta nosotros. Los monasterios contribuyeron con este y otros medios a la conservación de muchos escritos y documentos preciosos que se salvaron, en medio de la borrasca universal de la Edad Media, en aquellos monasterios donde se refugiaron y encontraron acogida las ciencias y las letras.[9]​



El libro comprendido como una unidad de hojas impresas que se encuentran encuadernadas en determinado material que forman un volumen ordenado, puede dividir su producción en dos grandes períodos: desde la invención de la imprenta de tipos móviles hasta 1801, y el periodo de producción industrializada.

El libro antiguo es aquel que fue producido en el período manual de la imprenta, es decir que fue impreso con tipos móviles metálicos, estos libros fueron publicados desde la creación de la imprenta en el siglo XV hasta el siglo XIX.

La aparición de la imprenta de tipos móviles en 1444, revolucionó el proceso de producción del libro, aunque algunos procesos de la fabricación se mantuvieron igual que en la época de los scriptoria, la imprenta hizo relativamente más sencilla la producción de libros.[11]​

La coexistencia del desarrollo de la imprenta con el comienzo del movimiento humanista y la reforma luterana impulsaron el crecimiento de la industria del libro, puesto que vieron en él un medio de difusión masivo. Pero también existían otras circunstancias que ayudaron a la propagación del libro impreso, el auge de las universidades desarrolló un mercado más amplio para los libros entre las élites intelectuales laicas y religiosas. En medio siglo, la segunda mitad del siglo XV, el libro impreso se convirtió en un importante negocio internacional, los libreros e impresores fueron ante todo empresarios. Pero el libro también debe su expansión a la atención que algunos monarcas y religiosos pusieron en la imprenta, en 1468 el papa Paulo II ordenó imprimir las epístolas de san Jerónimo, por su parte el rey de Francia Carlos VII mandó a Nicolás Jenson a Alemania para aprender la técnica de impresión, con el tiempo los más importantes soberanos en Europa protegieron el desarrollo de la imprenta.

La superioridad de la imprenta sobre la xilografía fue incuestionable, la escritura era regular, impresión a ambas caras, rapidez de impresión y la posibilidad de volver a utilizar los caracteres para imprimir otros textos.[12]​

Se puede establecer una cronología del libro antiguo dividida en siglos, tomando como base ciertas características comunes en un siglo determinado:[11]​

No es sino hasta mediados del siglo XVIII, una vez que el libro ha superado las dificultades tecnológicas que le impedían convertirse en una mercancía, que este inicia su rápido ascenso dentro del gusto de las minorías ilustradas de la sociedad.

La invención de la imprenta y el desarrollo del papel, así como la aparición de centros de divulgación de las ideas, permitieron la aparición del escritor profesional que depende de editores y libreros principalmente y ya no del subsidio público o del mecenazgo de los nobles o de los hombres acaudalados.

Además, surge una innovación comercial que convierte al libro en una mercancía de fácil acceso a los plebeyos y los pobres, que consiste en las librerías ambulantes, donde el librero cobra una cantidad mensual para prestar libros, que al ser devueltos le permiten al lector-usuario recibir otro a cambio.

El mismo libro, se convierte en un avance que da distinción a los lectores como progresistas en un siglo en que el progreso es una meta social ampliamente deseada y a la que pueden acceder por igual nobles y plebeyos, creando una meritocracia de nuevo cuño.

A pesar de lo anterior, la minoría que cultiva el gusto por el libro se encuentra entre los nobles y las clases altas y cultivadas de los plebeyos, pues solo estos grupos sociales saben leer y escribir, lo que representa el factor cultural adicional para el inevitable auge del libro.

Otro importante factor que fomentó el aprecio por los libros fue la censura, que si bien solía ejercerse también en periodos anteriores a los siglos XVII y XVIII, es precisamente en esta época cuando adquiere mayor relevancia, puesto que los libros se producen por millares, multiplicando en esa proporción la posibilidad de difundir ideas que el Estado y la Iglesia no desean que se divulguen.

En 1757 se publicó en París un decreto que condenaba a muerte a los editores, impresores y a los autores de libros no autorizados que se editarán, a pesar de carecer de dicha autorización. La draconiana medida fue complementada con un decreto que prohibía a cualquiera que no estuviera autorizado a publicar libros de tema religioso. En 1774, otro decreto obligaba a los editores a obtener autorizaciones antes y después de publicar cada libro y en 1787, se ordenó vigilar incluso los lugares libres de censura.

Estas medidas lo único que lograron fue aumentar el precio de los libros y obligar a los libreros ambulantes a no incluirlos en su catálogo, con lo cual incrementaron el negocio de los libros prohibidos, que de esta manera tenían un mayor precio y despertaban un mayor interés entre la clase alta que podía pagar el sobrevalor, con lo cual se fomentaron en el exterior, en Londres, Ámsterdam, Ginebra y en toda Alemania, las imprentas que publicaban libros en francés. Así fueron editados hasta la saciedad Voltaire, Rousseau, Holbach, Morell y muchos más, cuyos libros eran transportados en buques que anclaban en El Havre, Boulogne y Burdeos, desde donde los propios nobles los transportaban en sus coches para revenderlos en París.

En tanto la censura se volvió inefectiva e incluso los censores utilizaron dicha censura como medio para promover a astutos escritores y editores. Así, por ejemplo, cuando el todopoderoso ministro Guillaume-Chrétien de Lamoignon de Malesherbes revocó la autorización para publicar L'Encyclopédie, fue él mismo quien protegió a la obra cumbre de la Ilustración para después distribuirla de manera más libre, lo mismo hizo para proteger Emile y La nouvelle Éloise.

Normalmente, un libro es impreso en grandes hojas de papel, donde se alojan 8 páginas a cada lado. Cada una de estas grandes hojas es doblada hasta convertirla en una signatura de 16 páginas. Las signaturas se ordenan y se cosen por el lomo. Luego este lomo es redondeado y se le pega una malla de tela para asegurar las partes. Finalmente las páginas son alisadas por tres lados con una guillotina y el lomo pegado a una tapa de cartón. Toda esta tarea se realiza en serie, inclusive la encuadernación.

En el caso de que las hojas no sean alisadas mediante un proceso de corte, se habla de un libro intonso.

Las imprentas más modernas pueden imprimir 16, 32 y hasta 64 páginas por cara de grandes hojas, luego, como se mencionara más arriba, se las corta y se las dobla. Muchas veces el texto de la obra no alcanza a cubrir las últimas páginas, lo que provoca que algunos libros tengan páginas vacías al final del mismo, aunque muchas veces son cubiertas con propaganda de la editorial sobre textos del mismo autor o inclusive otros de su plantilla.

Los importantes avances en desarrollo de software y las tecnologías de impresión digital han permitido la aplicación de la producción bajo demanda (en inglés el acrónimo P.O.D.) al mundo del libro. Esto está permitiendo eliminar el concepto de "Libro Agotado" al poder reimprimirse títulos desde un solo ejemplar, y se está fomentando la edición de libros en tiradas muy cortas que antes no eran rentables por los medios tradicionales.

Como aplicación más innovadora, las librerías electrónicas más reconocidas están además ofertando a todo el mundo libros que no son fabricados hasta que son vendidos. Esto es posible solo por estar dados de alta en los sistemas de producción de compañías internacionales como Lightning Source, Publidisa, Booksurge, Anthony Rowe, etc.

A finales de 1971 comenzó a desarrollarse lo que hoy denominamos libro digital o electrónico. Michael Hart fue el impulsor del Proyecto Gutenberg, (que consistía en la creación de una biblioteca digital totalmente gratis), donde podíamos encontrar obras de autores como Shakespeare, Poe y Dante entre otros, todas ellas obras de dominio público. En 1981 se produce un importante avance, ya que sale a la venta el primer libro electrónico: Random House's Electronic Dictionary. Sin embargo, fue en marzo de 2001 cuando el libro digital (también conocido como eBook) experimentó su máxima expansión gracias al novelista Stephen King, quien lanzó al mercado a través de la red su novela Riding the Bullet. La obra, en apenas 48 horas, vendió 400 000 copias, al precio de dos dólares y medio la copia.[14]​ El mes siguiente Vladímir Putin también sacó a través de Internet sus memorias.

Desde este momento comenzaron a aparecer varias editoriales electrónicas y muchas tiendas virtuales empezaron a incorporar libros electrónicos en sus catálogos.

En el año 2000 se recogían los siguientes datos:
«Si la celebridad de un individuo consiste en que se escriba un libro sobre él, […] Jesucristo es aún el personaje que goza de más fama en el mundo actual», dice el periódico británico The Guardian. Una investigación que tomó como base los libros de la Biblioteca del Congreso de Estados Unidos, con sede en Washington, D. C., reveló la existencia de 17 239 obras acerca de Jesús, casi el doble que de William Shakespeare, quien alcanza el segundo lugar, con 9801. Vladimir Lenin resulta el tercero, con 4492, seguido de Abraham Lincoln, con 4378, y de Napoleón I, con 4007. El séptimo puesto, con 3595, lo ocupa María, la madre de Jesús, quien es la única mujer entre los treinta principales. La siguiente es Juana de Arco, con 545. Encabeza la nómina de compositores Richard Wagner, tras quien vienen Mozart, Beethoven y Bach. Picasso es el número uno de los pintores, seguido de Leonardo da Vinci y Miguel Ángel. Da Vinci, sin embargo, se lleva la palma en la lista de científicos e inventores, superando a Charles Darwin, Albert Einstein y Galileo Galilei. «No figura ningún personaje vivo en los treinta primeros lugares», agrega el rotativo.[15]​

De acuerdo con el contenido los libros se pueden clasificar en:

La ciencia (del latín scientĭa, 'conocimiento') es un sistema que organiza y construye el conocimiento a través de preguntas comprobables y un método estructurado que estudia e interpreta los fenómenos naturales, sociales y artificiales.[1]​ El conocimiento científico se obtiene mediante observación y experimentación en ámbitos específicos. Dicho conocimiento es organizado y clasificado sobre la base de principios explicativos, ya sean de forma teórica o práctica. A partir de estos se generan preguntas y razonamientos, se formulan hipótesis, se deducen principios y leyes científicas, y se construyen modelos científicos, teorías científicas y sistemas de conocimientos por medio de un método científico.[2]​

La ciencia considera y tiene como fundamento la observación experimental. Este tipo de observación se organiza por medio de métodos, modelos y teorías con el fin de generar nuevo conocimiento. Para ello se establecen previamente unos criterios de verdad y un método de investigación. La aplicación de esos métodos y conocimientos conduce a la generación de nuevos conocimientos en forma de predicciones concretas, cuantitativas y comprobables referidas a observaciones pasadas, presentes y futuras. Con frecuencia esas predicciones se pueden formular mediante razonamientos y estructurar como reglas o leyes generales, que dan cuenta del comportamiento de un sistema y predicen cómo actuará dicho sistema en determinadas circunstancias.

Desde la revolución científica, el conocimiento científico ha aumentado tanto que los científicos se han vuelto especialistas y sus publicaciones se han vuelto muy difíciles de leer para los no especialistas.[3]​ Esto ha dado lugar a diversos esfuerzos de divulgación científica, tanto para acercar la ciencia al gran público, como para facilitar la compresión y colaboración entre científicos de distintos campos.[3]​

La historia de la ciencia abarca el desarrollo de la ciencia desde la antigüedad hasta el presente. La ciencia es un conocimiento empírico, teórico y de procedimiento sobre el universo, producido por científicos que formulan explicaciones y predicciones comprobables basadas en sus observaciones.[4]​ Hay tres ramas principales de la ciencia: natural, social y formal.[5]​

Las primeras raíces de la ciencia se remontan al Antiguo Egipto y Mesopotamia alrededor de 3000 a 1200 A.C.[6]​[7]​ Sus contribuciones a las matemáticas, la astronomía y la medicina entraron y dieron forma a la filosofía natural griega de la antigüedad clásica, mediante la cual se hicieron intentos formales para proporcionar explicaciones de eventos en el mundo físico basadas en causas naturales.[6]​[7]​ Después de la caída del Imperio romano occidental, el conocimiento de las concepciones griegas del mundo se deterioró en Europa occidental de habla latina durante los primeros siglos (400 a 1000 EC) de la Edad Media,[8]​ pero continuó prosperando en el Imperio Romano Oriental (o Bizantino) de habla griega. Con la ayuda de traducciones de textos griegos, la cosmovisión helenística se conservó y se absorbió en el mundo musulmán de habla árabe durante la Edad de Oro islámica.[9]​ La recuperación y asimilación de obras griegas y las investigaciones islámicas en Europa occidental desde el siglo X al XIII revivieron el aprendizaje de la filosofía natural en Occidente.[8]​[10]​

La filosofía natural se transformó durante la Revolución Científica en la Europa de los siglos XVI al XVII,[11]​[12]​ a medida que nuevas ideas y descubrimientos se apartaron de las concepciones y tradiciones griegas anteriores.[13]​[14]​[15]​[16]​ La Nueva Ciencia que surgió era más mecanicista en su cosmovisión, más integrada con las matemáticas y más confiable y abierta ya que su conocimiento se basaba en un método científico recién definido.[14]​[17]​[18]​ Pronto siguieron más "revoluciones" en los siglos siguientes. La revolución químicadel siglo XVIII, por ejemplo, introdujo nuevos métodos cuantitativos y medidas para la química. En el siglo XIX, se enfocaron nuevas perspectivas con respecto a la conservación de la energía, la edad de la Tierra y la evolución.[19]​[20]​[21]​[22]​[23]​[24]​ Y en el siglo XX, nuevos descubrimientos en genética y física sentaron las bases para nuevas subdisciplinas como la biología molecular y la física de partículas.[25]​[26]​Además, las preocupaciones industriales y militares, así como la creciente complejidad de los nuevos esfuerzos de investigación, pronto marcaron el comienzo de la era de la " gran ciencia ", particularmente después de la Segunda Guerra Mundial.[25]​[26]​[27]​

Las primeras raíces de la ciencia se remontan al Antiguo Egipto y a la Mesopotamia en torno a los años 3000 a 1200 a.C.[28]​ Aunque las palabras y los conceptos de "ciencia" y "naturaleza" no formaban parte del paisaje conceptual de la época, los antiguos egipcios y mesopotámicos hicieron aportaciones que más tarde encontrarían un lugar en la ciencia griega y medieval: las matemáticas, la astronomía y la medicina.[29]​[28]​A partir de alrededor del año 3000 a.C., los antiguos egipcios desarrollaron un sistema de numeración de carácter decimal y orientaron sus conocimientos de geometría a la resolución de problemas prácticos, como los de los topógrafos y constructores.[28]​Incluso desarrollaron un calendario oficial que contenía doce meses, de treinta días cada uno, y cinco días al final del año.[28]​Los antiguos pueblos de Mesopotamia utilizaban los conocimientos sobre las propiedades de diversos productos químicos naturales para la fabricación de cerámica, loza, vidrio, jabón, metales, yeso de cal e impermeabilización;[30]​ también estudiaban la fisiología animal, la anatomía y el comportamiento con fines divinatorios[30]​y realizaban amplios registros de los movimientos de los objetos astronómicos para su estudio de la astrología.[31]​ Los mesopotámicos tenían intenso interés por la medicina[30]​y las primeras prescripciones médicas aparecen en sumeria durante la Tercera Dinastía de Ur (c. 2112 a.C. - c. 2004 a.C.).[32]​ No obstante, los mesopotámicos parecen haber tenido poco interés en recopilar información sobre el mundo natural por el mero hecho de recopilar información[30]​y principalmente sólo estudiaron temas científicos que tenían aplicaciones prácticas obvias o relevancia inmediata para su sistema religioso.[30]​

En la antigüedad clásica, no existe un verdadero análogo antiguo de un científico moderno. En su lugar, individuos bien educados, generalmente de clase alta, y casi universalmente varones, realizaban diversas investigaciones sobre la naturaleza siempre que podían disponer de tiempo.[33]​Antes de la invención o descubrimiento del concepto de "naturaleza" (griego antiguo physis) por parte de los filósofos presocráticos, las mismas palabras solían utilizarse para describir la forma natural en que crece una planta,[34]​ y la "manera" en que, por ejemplo, una tribu adora a un dios determinado. Por esta razón, se afirma que estos hombres fueron los primeros filósofos en sentido estricto, y también los primeros en distinguir claramente "naturaleza" y "convención"[35]​:209 La filosofía natural, precursora de la ciencia natural, se distinguía así como el conocimiento de la naturaleza y de las cosas que son verdaderas para toda comunidad, y el nombre de la búsqueda especializada de tal conocimiento era filosofía, el reino de los primeros filósofos-físicos. Eran principalmente especuladores o teóricos, particularmente interesados en la astronomía. En cambio, tratar de utilizar el conocimiento de la naturaleza para imitarla (artificio o tecnología, griego technē) era visto por los científicos clásicos como un interés más apropiado para los artesanos de clase social inferior.[36]​

Los primeros filósofos griegos de la Escuela Milesiana, fundada por Tales de Mileto y continuada posteriormente por sus sucesores Anaximandro y Anaximenes, fueron los primeros en intentar explicar la fenómenos naturales sin apoyarse en lo sobrenatural.[38]​ El Pitagóricos desarrolló una filosofía de números complejos[39]​:467-68 y contribuyó significativamente al desarrollo de la ciencia matemática.[39]​{rp|465}} El teoría de los átomos fue desarrollado por el filósofo griego Leucipo y su alumno Demócrito.[40]​[41]​ El médico griego Hipócrates estableció la tradición de la ciencia médica sistemática[42]​[43]​ y es conocido como "El padre de la medicina"[44]​

Un punto de inflexión en la historia de la ciencia filosófica primitiva fue el ejemplo de Sócrates de aplicar la filosofía al estudio de los asuntos humanos, incluyendo la naturaleza humana, la naturaleza de las comunidades políticas y el propio conocimiento humano. El método socrático, tal y como se documenta en los diálogos de Platón, es un método dialéctico de eliminación de hipótesis: se encuentran mejores hipótesis identificando y eliminando constantemente las que conducen a contradicciones. Se trata de una reacción al énfasis de los sofistas en la retórica. El método socrático busca verdades generales, comúnmente sostenidas, que dan forma a las creencias y las escudriña para determinar su consistencia con otras creencias.[45]​ Sócrates criticó el tipo de estudio más antiguo de la física por ser demasiado puramente especulativo y carente de autocrítica. Sócrates fue más tarde, en palabras de su Apología, acusado de corromper a la juventud de Atenas porque "no creía en los dioses en los que cree el Estado, sino en otros nuevos seres espirituales". Sócrates refutó estas afirmaciones,[46]​ pero fue condenado a muerte.[47]​: 30e

Aristóteles creó posteriormente un programa sistemático de filosofía teleológica: El movimiento y el cambio se describen como la actualización de los potenciales que ya están en las cosas, según el tipo de cosas que sean. En su física, el Sol gira alrededor de la Tierra, y muchas cosas tienen como parte de su naturaleza que son para los humanos. Cada cosa tiene una causa formal, una causa final, y un papel en un orden cósmico con un impulsor inmóvil. Los socráticos también insistieron en que la filosofía debería utilizarse para considerar la cuestión práctica de la mejor manera de vivir para un ser humano (un estudio que Aristóteles dividió en ética y filosofía política). Aristóteles sostenía que el hombre conoce una cosa científicamente "cuando posee una convicción a la que ha llegado de una manera determinada, y cuando los primeros principios sobre los que descansa esa convicción le son conocidos con certeza".[48]​

El astrónomo griego Aristarco de Samos (310-230 a.C.) fue el primero en proponer un modelo heliocéntrico del universo, con el Sol en el centro y todos los planetas orbitando alrededor de él.[49]​ El modelo de Aristarco fue ampliamente rechazado porque se creía que violaba las leyes de la física.[49]​El inventor y matemático Archimedes de Siracusa hizo importantes contribuciones a los inicios del cálculo[50]​y a veces se le ha atribuido como su inventor,[50]​ aunque su protocálculo carecía de varias características definitorias.[50]​Plinio el Viejo fue un escritor y polímata romano, que escribió la enciclopedia seminal Historia Natural,[51]​[52]​[53]​ que se ocupan de la historia, la geografía, la medicina, la astronomía, las ciencias de la tierra, la botánica y la zoología.[51]​Otros científicos o protocientíficos de la Antigüedad fueron Teofrasto, Euclides, Herófilo, Hiparco, Ptolomeo y Galeno.

Las ramas de la ciencia, disciplinas científicas, o simplemente ciencias, se suelen dividir en tres grupos: ciencias formales, ciencias naturales, y ciencias humanas o ciencias sociales. Estas conforman las ciencias básicas, sobre las que se apoyan las ciencias aplicadas como la ingeniería y la medicina.

En filosofía de la ciencia, la unidad de la ciencia es la idea de que todas las ciencias forman una integralidad o un todo unificado, que no puede ser separado o desmembrado a riesgo de perder la visión de conjunto.[55]​[56]​

A pesar de esta afirmación, por ejemplo, es claro que física y sociología son dos disciplinas bien distintas y diferenciadas, y casi podríamos decir de una cualidad diferente, aunque la tesis de la unidad o unicidad de la ciencia afirmaría que, en principio, ambas deberían formar parte de un universo intelectual unificado de difícil o inconducente desmembramiento.

La tesis de la unidad de la ciencia[57]​ está usualmente asociada con una visión de diferentes niveles de organización en la naturaleza, donde la física es la más básica o fundamental, y donde la química es la que le sigue en jerarquía, y sobre esta última sigue la biología, y sobre la biología sigue la sociología. Según esta concepción, y partiendo desde la física, se reconocería así que las células, los organismos, y las culturas, tienen todos una base o un origen biológico, pero representando tres diferentes niveles jerárquicos de la organización biológica.[58]​

A pesar de lo expresado, también se ha sugerido (por ejemplo por Jean Piaget, 1950),[59]​ que la unicidad de la ciencia podría ser considerada en términos de un círculo de ciencias o de disciplinas, donde la física provee la base para la química, y donde a su vez la química es la base para la biología, y la biología la base para la psicología, y esta la base para la lógica y la matemática, y a su vez la lógica y la matemática serviría de base y de comprensión para la física.

La tesis de la unidad de la ciencia[60]​ simplemente expresa que hay leyes científicas comunes aplicables a cualquier cosa y en cualquier nivel de organización. Pero en un determinado nivel de organización, los científicos llaman a esas leyes con nombres particulares, y visualizan la aplicación y expresión de esas leyes en ese nivel de una manera adaptada y simplificada, enfatizando por ejemplo la importancia de alguna de ellas sobre las otras. Es así como la termodinámica o las leyes de la energía, parecerían ser universales para cierto número de diferentes disciplinas, ya que por cierto, todos los sistemas en la naturaleza operan o parecen operar sobre la base de transacciones de energía. Claro, esto no excluye la posibilidad de algunas leyes particulares aplicables específicamente a dominios quizás caracterizados por una complejidad creciente, tal como lo sugerido por Gregg R. Henriques (2003, consultar 'Tree of Knowledge System'), quien precisamente propone cuatro grados de complejidad: Materia, Vida, Mente, y Cultura. Desde luego, este árbol igualmente podría ser circular, con la cultura enmarcando la comprensión y la percepción de la materia y de los sistemas por parte de la gente.

La ciencia es una creación humana, y forma parte de cultura humana. La ciencia es un todo unificado, en el sentido que es profundamente entendida cuando se la considera de una manera integral y holística, y no hay científicos que estudien realidades alternativas. Sin embargo, bien podría argumentarse que los científicos no actúan con un enfoque integral, pues por facilidad de análisis o por las razones que fueren, se hacen hipótesis simplificatorias, se aísla, se trata separadamente. Es posiblemente la percepción de una realidad sola, lo único que desemboca en la unidad de la ciencia.

Según la lógica proposicional, la ciencia parecería ser un camino hacia la simplificación, o en realidad hacia la universalización de teorías científicas discretas sobre la energía, y que los físicos llaman unificación. Esto ha conducido a la teoría de cuerdas y a sus concepciones derivadas, probablemente relacionadas con la noción que, en la base, sólo se encuentra la energía que no fue liberada en la Gran Explosión, y realmente nada más.

En filosofía de la ciencia, el problema de la demarcación es la cuestión de definir los límites que deben configurar el concepto «ciencia».[62]​ Las fronteras se suelen establecer entre lo que es conocimiento científico y no científico, entre ciencia y metafísica, entre ciencia y pseudociencia, y entre ciencia y religión. El planteamiento de este problema, conocido como problema generalizado de la demarcación, abarca estos casos. El problema generalizado, en último término, lo que intenta es encontrar criterios para poder decidir, entre dos teorías dadas, cuál de ellas es más «científica».

Tras más de un siglo de diálogo entre filósofos de la ciencia y científicos en diversos campos, y a pesar de un amplio consenso acerca de las bases generales del método científico,[63]​ los límites que demarcan lo que es ciencia, y lo que no lo es, continúan siendo debatidos.[64]​

La investigación es el trabajo creativo y sistemático realizado para aumentar el acervo de conocimientos.[68]​ Implica la recopilación, organización y análisis de información para aumentar la comprensión de un tema o problema. Un proyecto de investigación puede ser una expansión del trabajo anterior en el campo. Para probar la validez de instrumentos, procedimientos o experimentos, la investigación puede reproducir elementos de proyectos anteriores o del proyecto en su conjunto.

El método científico es una metodología para obtener nuevos conocimientos, que ha caracterizado históricamente a la ciencia, y que consiste en la observación sistemática, medición, experimentación y la formulación, análisis y modificación de hipótesis.[70]​ Las principales características de un método científico válido son la falsabilidad y la reproducibilidad y repetibilidad de los resultados, corroborada por revisión por pares. Algunos tipos de técnicas o metodologías utilizadas son la deducción,[71]​ la inducción, la abducción, y la predicción, entre otras. 

El método científico abarca las prácticas aceptadas por la comunidad científica como válidas a la hora de exponer y confirmar sus teorías. Las reglas y principios del método científico buscan minimizar la influencia de la subjetividad del científico en su trabajo, reforzando así la validez de los resultados, y por ende, del conocimiento obtenido. 

No todas las ciencias tienen los mismos requisitos. La experimentación, por ejemplo, no es posible en ciencias como la física teórica. El requisito de reproducibilidad y repetibilidad, fundamental en muchas ciencias, no se aplica a otras, como las ciencias humanas y sociales, donde los fenómenos no solo no se pueden repetir controlada y artificialmente (que es en lo que consiste un experimento), sino que son, por su esencia, irrepetibles, por ejemplo, la historia.

Así mismo, no existe un único modelo de método científico.[72]​ El científico puede usar métodos definitorios, clasificatorios, estadísticos, empírico-analíticos, hipotético-deductivos, procedimientos de medición, entre otros. Por esto, referirse a el método científico, es referirse a un conjunto de tácticas empleadas para construir conocimiento de forma válida. Estas tácticas pueden ser mejoradas, o reemplazadas por otras, en el futuro.[73]​ Cada ciencia, y aun cada investigación concreta, puede requerir un modelo propio de método científico.

Una ley científica es una proposición científica que afirma una relación constante entre dos o más variables o factores, cada uno de los cuales representa una propiedad o medición de sistemas concretos. También se define como regla y norma constantes e invariables de las cosas, surgida de su causa primera o de sus cualidades y condiciones. Por lo general se expresa matemáticamente o en lenguaje formalizado. Las leyes muy generales pueden tener una prueba indirecta verificando proposiciones particulares derivadas de ellas y que sean verificables. Los fenómenos inaccesibles reciben una prueba indirecta de su comportamiento a través del efecto que puedan producir sobre otros hechos que sí sean observables o experimentables.

En la arquitectura de la ciencia la formulación de una ley es un paso fundamental. Es la primera formulación científica como tal. En la ley se realiza el ideal de la descripción científica; se consolida el edificio entero del conocimiento científico: de la observación a la hipótesis teórica-formulación-observación-experimento (ley científica), teoría general, al sistema. El sistema de la ciencia es o tiende a ser, en su contenido más sólido, sistema de las leyes.[75]​

Diferentes dimensiones que se contienen en el concepto de ley:[76]​

Desde un punto de vista descriptivo la ley se muestra simplemente como una relación fija, entre ciertos datos fenoménicos. En términos lógicos supone un tipo de proposición, como afirmación que vincula varios conceptos relativos a los fenómenos como verdad.[77]​ En cuanto a la consideración ontológica la ley como proposición ha sido interpretada históricamente como representación de la esencia, propiedades o accidentes de una sustancia. Hoy día se entiende que esta situación ontológica se centra en la fijación de las constantes del acontecer natural, en la aprehensión de las regularidades percibidas como fenómeno e incorporadas en una forma de «ver y explicar el mundo».[78]​

El problema epistemológico consiste en la consideración de la ley como verdad y su formulación como lenguaje y en establecer su «conexión con lo real», donde hay que considerar dos aspectos:

Los científicos elaboran distintas teorías partiendo de hipótesis que hayan sido corroboradas por el método científico, luego recolectan pruebas para poner a prueba dichas teorías. Como en la mayoría de las formas del conocimiento científico, las teorías son inductivas por naturaleza y su finalidad es meramente explicativa y predictiva.

La fuerza de una teoría científica se relaciona con la cantidad de fenómenos que puede explicar, los cuales son medidos por la capacidad que tiene dicha teoría de hacer predicciones falsables respecto de dichos fenómenos que tiende a explicar. Las teorías son mejoradas constantemente dependiendo de la nueva prueba que se consiga, por eso las teorías mejoran con el tiempo. Los científicos utilizan las teorías como fundamentos para obtener conocimiento científico, pero también para motivos técnicos, tecnológicos o médicos.

La teoría científica es la forma más rigurosa, confiable y completa de conocimiento posible. Esto es significativamente distinto al uso común y coloquial de la palabra «teoría», que se refiere a algo sin sustento o una suposición.

La teoría científica representa el momento sistemático explicativo del saber propio de la ciencia natural; su culminación en sentido predictivo.

Los años 50 del siglo XX supusieron un cambio de paradigma en la consideración de las «teorías científicas».

Según Mario Bunge en aras de un inductivismo dominante,[81]​ con anterioridad se observaba, clasificaba y especulaba. Ahora en cambio:

En definitiva, concluye Bunge: «Empezamos a comprender que el fin de la investigación no es la acumulación de hechos sino su comprensión, y que ésta solo se obtiene arriesgando y desarrollando hipótesis precisas que tengan un contenido empírico más amplio que sus predecesoras.»[83]​

Existen dos formas de considerar las teorías:

Un modelo científico es una representación abstracta, conceptual, gráfica o visual (ver, por ejemplo: mapa conceptual), física de fenómenos, sistemas o procesos a fin de analizar, describir, explicar, simular (en general, explorar, controlar y predecir) esos fenómenos o procesos. Un modelo permite determinar un resultado final a partir de unos datos de entrada. Se considera que la creación de un modelo es una parte esencial de toda actividad científica.[85]​[86]​[87]​

Aun cuando hay pocos acuerdos generales acerca del uso de modelos, La ciencia moderna ofrece una colección creciente de métodos, técnicas y teorías acerca de los diversos tipos de modelos. Las teorías y/o propuestas sobre la construcción, empleo y validación de modelos se encuentran en disciplinas tales como la metodología, filosofía de la ciencia, teoría general de los sistemas y en el campo relativamente nuevo de visualización científica. En la práctica, diferentes ramas o disciplinas científicas tienen sus propias ideas y normas acerca de tipos específicos de modelos. Sin embargo, y en general, todos siguen los principios del modelado.

Debe distinguirse entre un modelo científico y una teoría, aun cuando ambos se hallan muy estrechamente relacionados, pues el modelo para una teoría equivale a una interpretación de esta teoría. Una teoría dada puede tener diversos modelos para poder ser explicada.[88]​

Para hacer un modelo es necesario plantear una serie de hipótesis, de manera que lo que se quiere estudiar esté suficientemente plasmado en la representación, aunque también se busca, normalmente, que sea lo bastante sencillo como para poder ser manipulado y estudiado.

Todo conocimiento de la realidad comienza con idealizaciones que consisten en abstraer y elaborar conceptos; es decir, construir un modelo acerca de la realidad. El proceso consiste en atribuir a lo percibido como real ciertas propiedades, que frecuentemente, no serán sensibles. Tal es el proceso de conceptualización y su traducción al lenguaje.

Eso es posible porque se suprimen ciertos detalles destacando otros que nos permiten establecer una forma de ver la realidad, aun sabiendo que no es exactamente la propia realidad. El proceso natural sigue lo que tradicionalmente se ha considerado bajo el concepto de analogía. Pero en la ciencia el contenido conceptual solo se considera preciso como modelo científico de lo real, cuando dicho modelo es interpretado como caso particular de un modelo teórico y se pueda concretar dicha analogía mediante observaciones o comprobaciones precisas y posibles.

El objeto modelo es cualquier representación esquemática de un objeto. Si el objeto representado es un objeto concreto entonces el modelo es una idealización del objeto, que puede ser pictórica (por ejemplo, un dibujo) o conceptual (una fórmula matemática); es decir, puede ser figurativa o simbólica. La informática ofrece herramientas para la elaboración de objetos-modelo a base del cálculo numérico.

La representación de una cadena polimérica con un collar de cuentas de colores es un modelo análogo o físico; un sociograma despliega los datos de algunas de las relaciones que pueden existir entre un grupo de individuos. En ambos casos, para que el modelo sea modelo teórico debe estar enmarcado en una estructura teórica. El objeto modelo así considerado deviene, en determinadas circunstancias y condiciones, en modelo teórico.

Un modelo teórico es un sistema hipotético-deductivo concerniente a un objeto modelo que es, a su vez, representación conceptual esquemática de una cosa o de una situación real o supuesta real.[89]​ El modelo teórico siempre será menos complejo que la realidad que intenta representar, pero más rico que el objeto modelo, que es solo una lista de rasgos del objeto modelizado. Bunge esquematiza estas relaciones de la siguiente forma:[90]​

El consenso científico es el juicio colectivo, la posición y la opinión de la comunidad científica en un campo particular de estudio. El consenso implica un acuerdo general, aunque no necesariamente unanimidad.[91]​ 

El consenso suele lograrse a través del debate científico.[92]​ La ética científica exige que las nuevas ideas, los hechos observados, las hipótesis, los experimentos y los descubrimientos se publiquen, justamente para garantizar la comunicación a través de conferencias, publicaciones (libros, revistas) y su revisión entre pares y, dado el caso, la controversia con los puntos de vista discrepantes.[93]​ La reproducibilidad de los experimentos y la falsación de las teorías científicas son un requisito indispensable para la buena práctica científica.

En ocasiones, las instituciones científicas emiten declaraciones con las que tratan de comunicar al "exterior" una síntesis del estado de la ciencia desde el "interior". El debate mediático o político sobre temas que son controvertidos dentro de la esfera pública pero no necesariamente para la comunidad científica puede invocar un consenso científico, como por ejemplo el tema de la evolución biológica[94]​[95]​ o el cambio climático.[96]​

El conocimiento científico adquiere el carácter de objetividad por medio de la comunidad y sus instituciones, con independencia de los individuos. D. Bloor, siguiendo a Popper y su teoría del mundo 3, convierte simétricamente el reino de lo social en un reino sin súbditos individuales, en particular reduce el ámbito del conocimiento al estado del conocimiento en un momento dado, esto es, a las creencias aceptadas por la comunidad relevante, con independencia de los individuos en concreto. El conocimiento científico es únicamente adscrito a la «comunidad científica».

Archivo:Https://images.app.goo.gl/Ckdhc9MkjwzkmowK8.

El avance científico es una etiqueta o una denominación, con frecuencia usada para señalar o evocar el desarrollo de los conocimientos científicos. El progreso técnico depende, en buena medida, del progreso científico.

La filosofía de la ciencia es la rama de la filosofía que investiga el conocimiento científico y la práctica científica. Se ocupa de saber, entre otras cosas, cómo se desarrollan, evalúan y cambian las teorías científicas, y de saber si la ciencia es capaz de revelar la verdad de las «entidades ocultas» (o sea, no observables) y los procesos de la naturaleza. Son filosóficas las diversas proposiciones básicas que permiten construir la ciencia. Por ejemplo:

Si bien estos supuestos metafísicos no son cuestionados por el realismo científico, muchos han planteado serias sospechas respecto del segundo de ellos[98]​ y numerosos filósofos han puesto en tela de juicio alguno de ellos o los tres.[99]​ De hecho, las principales sospechas con respecto a la validez de estos supuestos metafísicos son parte de la base para distinguir las diferentes corrientes epistemológicas históricas y actuales. De tal modo, aunque en términos generales el empirismo lógico defiende el segundo principio, opone reparos al tercero y asume una posición fenomenista, es decir, admite que el hombre puede comprender la naturaleza siempre que por naturaleza se entienda "los fenómenos" (el producto de la experiencia humana) y no la propia realidad.

En pocas palabras, lo que intenta la filosofía de la ciencia es explicar problemas tales como:

La filosofía de la ciencia comparte algunos problemas con la gnoseología —la teoría del conocimiento— que se ocupa de los límites y condiciones de posibilidad de todo conocimiento. Pero, a diferencia de esta, la filosofía de la ciencia restringe su campo de investigación a los problemas que plantea el conocimiento científico; el cual, tradicionalmente, se distingue de otros tipos de conocimiento, como el ético o estético, o las tradiciones culturales.

Algunos científicos han mostrado un vivo interés por la filosofía de la ciencia y algunos como Galileo Galilei, Isaac Newton y Albert Einstein, han hecho importantes contribuciones. Numerosos científicos, sin embargo, se han dado por satisfechos dejando la filosofía de la ciencia a los filósofos y han preferido seguir haciendo ciencia en vez de dedicar más tiempo a considerar cómo se hace la ciencia. Dentro de la tradición occidental, entre las figuras más importantes anteriores al siglo XX destacan entre muchos otros Platón, Aristóteles, Epicuro, Arquímedes, Boecio, Alcuino, Averroes, Nicolás de Oresme, Santo Tomas de Aquino, Jean Buridan, Leonardo da Vinci, Raimundo Lulio, Francis Bacon, René Descartes, John Locke, David Hume, Emmanuel Kant y John Stuart Mill.

La comunidad científica consta del cuerpo total de científicos junto a  sus relaciones e interacciones. Se divide normalmente en "subcomunidades", cada una trabajando en un campo particular de la ciencia (por ejemplo existe una comunidad de robótica dentro del campo de las ciencias de la computación).

Un científico (del latín scientificus,[100]​ y a su vez de scientia, 'conocimiento' y -fic, raíz apofónica de facis, 'hacer') es una persona que participa y realiza una actividad sistemática para generar[101]​ nuevos conocimientos en el campo de las ciencias (tanto naturales como sociales), es decir, que realiza investigación científica.[102]​[103]​[104]​ El término fue acuñado por el británico William Whewell en 1833.[105]​[106]​[107]​[108]​[109]​[110]​

Las mujeres han contribuido notablemente a la ciencia desde sus inicios. El estudio histórico, crítico y sociológico de este asunto se ha vuelto una disciplina académica en sí misma.

Involucrar a mujeres en el campo de la medicina ocurrió en varias civilizaciones antiguas y el estudio de la filosofía natural estaba abierto a las mujeres en la Antigua Grecia. Las mujeres también contribuyeron en la protociencia de la alquimia en el siglo I y II d. C. Durante la Edad Media, los conventos fueron un importante lugar para la educación femenina y algunas de estas instituciones proporcionaron oportunidades para que las mujeres pudiesen formar parte y contribuir en el campo de la investigación. Pero en el siglo XI se fundaron las primeras universidades y las mujeres fueron excluidas de la educación universitaria.[114]​ La actitud de educar a mujeres en el campo de la medicina era más liberal en Italia que en otros lugares.[114]​ La primera mujer conocida en completar los estudios universitarios en un campo de estudios científicos fue Laura Bassi en el siglo XVIII.

La divulgación científica es el conjunto de actividades que interpretan y hacen accesible el conocimiento científico a la sociedad, es decir, todas aquellas labores que llevan a cabo el conocimiento científico a las personas interesadas en entender o informarse sobre ese tipo de conocimiento. La divulgación pone su interés no solo en los descubrimientos científicos del momento (por ejemplo, la determinación de la masa del neutrino), sino también en teorías más o menos bien establecidas o aceptadas socialmente (por ejemplo, la teoría de la evolución) o incluso en campos enteros del conocimiento científico.[116]​

Los estudios sociales sobre ciencia y tecnología abarcan un campo interdisciplinario de estudios sobre los efectos culturales, éticos y políticos del conocimiento científico y la innovación tecnológica.[118]​ Colocan el énfasis en la interpretación sobre las utilidades, apropiaciones e impactos en la vida cotidiana de las personas, con el objetivo de romper las antiguas barreras de investigación científico-técnica.

En las regiones de habla hispana, este tipo de inquietudes y de reflexiones han llegado con el nombre común de estudios de/sobre Ciencia, Tecnología, y Sociedad (abreviado CTS), lo que en las regiones de habla inglesa se conoce como Science and Technology Studies (Estudios de Ciencia y Tecnología) o Science, Technology and Society (Ciencia, Tecnología y Sociedad), ambas con el acrónimo STS. En las regiones de lengua hispana, la multidisciplinariedad en CTS incluye desde el principio los ámbitos de la sociología, la filosofía, la historia y la antropología, así como incorpora desde sus orígenes en los movimientos en defensa de los derechos humanos, el movimiento feminista, las corrientes medioambientalistas, pacifistas y los primeros grupos de LGBT surgidos sobre todo tras la guerra del Vietnam. Por sus orígenes y naturaleza vemos cierto paralelismo entre este campo y otros tipos de estudios culturales.[119]​[120]​

Dado el carácter universal de la ciencia, su influencia se extiende a todos los campos de la sociedad, desde el desarrollo tecnológico a los modernos problemas de tipo jurídico relacionados con campos de la medicina o la genética. En ocasiones la investigación científica permite abordar temas de gran calado social como el Proyecto Genoma Humano y grandes implicaciones éticas como el desarrollo del armamento nuclear, la clonación, la eutanasia y el uso de las células madre.

La Divina comedia (en italiano moderno: Divina Commedia, en toscano: Divina Comedìa), también conocida simplemente como Comedia, es un poema escrito por Dante Alighieri. Se desconoce la fecha exacta en que fue redactado aunque las opiniones más reconocidas aseguran que el Infierno pudo ser compuesto entre 1304 y 1308, el Purgatorio de 1307 a 1314 y, por último, el Paraíso de 1313 a 1321, fecha del fallecimiento del poeta. Se considera por tanto que la redacción de la primera parte habría sido alternada con la redacción del Convivium y De vulgari eloquentia, mientras que De monarchia pertenecería a la época de la segunda o tercera etapa, a la última de las cuales hay que atribuir sin duda la de dos obras de menor empeño: la Cuestión de agua y La Tierra y las dos églogas escritas en respuesta a sendos poemas de Giovanni de Regina.

Es la creación más importante de su autor y una de las obras fundamentales de la transición del pensamiento medieval (teocentrista) al renacentista (antropocentrista). Es considerada la obra maestra de la literatura italiana y una de las cumbres de la literatura universal.

Dante Alighieri llamó sencillamente Commedia a su libro, pues, de acuerdo con el esquema clásico, no podía ser una tragedia, ya que su final es feliz. Fue el escritor y humanista Giovanni Boccaccio quien añadió el adjetivo "divina" durante la época en la que se encargó de leerla y comentarla públicamente por diferentes ciudades italianas, también por ser un poema que canta a la cristiandad. El libro suele presentarse actualmente con un gran cuerpo de notas que ayudan a entender quiénes eran los personajes mencionados. Estos comentarios incluyen interpretaciones de las alegorías cristianas o significados místicos que contendría el texto. Miguel Asín Palacios, por otra parte, destacó la importancia de la escatología musulmana en la estructura del Infierno dantesco.[1]​

La Divina comedia se considera una de las obras maestras de la literatura italiana y universal. Dante resume en ella todo el amplio conocimiento acumulado durante siglos, desde los antiguos clásicos hasta el mundo medieval; su fe religiosa y sus convicciones morales y filosóficas. El estilo de la obra posee un rico lenguaje lleno de símbolos y frecuentes referencias a personajes históricos y de la antigua mitología. Numerosos artistas de todos los tiempos crearon ilustraciones sobre ella; destacándose entre ellos las de Sandro Botticelli, Gustave Doré, Salvador Dalí, William Blake, William Adolphe Bouguereau y Miquel Barceló. Dante Alighieri la escribió en dialecto toscano, matriz del italiano actual, el cual se usó entre los siglos XI y XII. Dante finaliza cada una de las cánticas utilizando la palabra estrellas; conectándolas, a pesar de sus marcadas diferencias.

Aunque la Divina comedia es principalmente un poema religioso, que discute el pecado, la virtud y la teología, Dante también discute varios elementos de la ciencia de su época,[2]​ como por ejemplo: las implicaciones de una Tierra esférica y las estrellas visibles en el hemisferio sur o analiza la importancia del método experimental en la ciencia, en las líneas 94-105 del Canto II del "Paraíso".

Cada una de sus partes (Infierno, Purgatorio y Paraíso), está dividida en cantos, cada uno consta de treinta y tres cantos, y más el canto introductorio suman 100 cantos en total. Cada canto fue compuesto por estrofas de tres versos endecasílabos o terza rima, que se dice, él mismo inventó (tercetos).

El poema se ordena en función del simbolismo del número tres, que evoca la Santísima Trinidad (el Padre, el Hijo y el Espíritu Santo), el equilibrio, la estabilidad y el triángulo, las tres proposiciones que componen el silogismo, lo que se sumaba al cuatro, que representaba los cuatro elementos: Tierra, aire, fuego y agua, dando como resultado el número siete, como siete son los pecados capitales. Finalmente, el Infierno está dividido en nueve círculos, el Purgatorio en siete y el Paraíso queda formado por nueve esferas que giran como los planetas en torno al sol.

Toda la obra está llena de símbolos que remiten al conocimiento y al pensamiento medievales; religión, astronomía, filosofía, matemáticas, óptica, etcétera, que se encarnan en personajes, lugares y acciones.

El poema cuenta con un personaje principal: Dante, que personifica a la humanidad, y representa la tentación del pecado. Luego cuenta con dos personajes secundarios: Beatriz, que personifica la Fe y lleva a Dante a cada una de las esferas del paraíso, hasta el Empíreo, espacio inmóvil, donde contempla la Rosa mística formada por Dios y sus elegidos; y Virgilio, que hace otro tanto con la razón.

El Paraíso representa el saber y la ciencia divina. El Infierno representa al ser humano frente a sus pecados y sus funestas consecuencias. El Purgatorio, la lenta purificación de sus culpas hasta la liberación. En esta obra el autor narra con extraordinario realismo un maravilloso viaje durante el que se encuentra con las almas de grandes y terribles personajes de la historia; es un canto a la humanidad que sólo en la fe en Dios encuentra su felicidad.

La estrofa por su parte está compuesta por tres versos, y cada una de las cánticas cuenta con treinta y tres cantos, más el canto introductorio, de manera que la obra completa se compone de cien cantos. Se usa un tipo de rima original, la "terza rima"

Dante también utiliza el número diez tanto como cabalístico que como número pitagórico, que se aprecia en los cien cantos de la comedia, compuestos por los treinta y tres de cada reino, más el de introducción. También es notable la importancia decimal en los diez niveles del infierno, que son nueve círculos más el anteinfierno, donde se encuentran los ignavi, es decir, los indiferentes.

La estructura matemática de la Divina comedia, por otra parte, es mucho más compleja de lo que aquí se esboza[cita requerida]. El poema puede leerse según los cuatro significados que se atribuyen a los textos sagrados: literal, moral, alegórico y anagógico. En este poema, Dante hace gala además de un gran poder de síntesis que es característico de los grandes poetas.

La estructura también afecta a los registros lingüísticos: en el infierno se utiliza un lenguaje vulgar, el texto del "Purgatorio" está lleno de citas bíblicas y el del "Paraíso", de himnos y cantos litúrgicos.

Antes de emprender el viaje hacia los tres mundos, Dante despierta en una selva oscura sin saber por qué llegó ahí. Cuando se habla de selva oscura se hace referencia a algo malo, a lo contrario a Dios, ya que esa selva era oscura, era mala. En esa selva se describe un paisaje inicial en el cual se ve el sol, las estrellas, la playa y la colina. El sol en representación de la divinidad, de Dios, la colina es el camino que debe subir para llegar a Dios, y Dante no puede llegar a esa luz porque en el camino se encuentra con tres fieras que son obstáculos para él, cada fiera representando un pecado. Ahí se encuentra con Virgilio y después de una conversación con este, comienza su viaje.

La primera parte describe cómo Dante se halla perdido en un bosque oscuro en su mediana edad y entra en una cueva al pie del monte Sion, cerca de Jerusalén. Tres animales alegóricos le salen al paso: una pantera, una loba y un león. Estos animales representan los pecados que pueden atacar a Dante. La pantera representa la lujuria y la ciudad de Florencia que lo ha exiliado. La loba es, según los comentaristas, el pecado de la codicia, y más allá de esto el poder temporal del Papa en Roma. El león representa la soberbia, y el poder de Francia, que pretendía dominar Italia. Después se narra el descenso del autor al Infierno, acompañado por el poeta latino Virgilio, autor de la Eneida, a quien Dante admiraba, y que en la Edad Media tenía una curiosa fama de mago. Acompañado por su maestro y guía, desciende al Infierno, que tiene forma de cono con la punta hacia abajo y los nueve círculos que poseía, en los que los condenados son sometidos a castigo, según la gravedad de los pecados cometidos en vida. En las puertas se advierte: "Lasciate ogni speranza, voi ch'entrate / Quien entre aquí, abandone toda esperanza". Después pasan al círculo del limbo, donde se encuentran las almas inocentes de los que obraron bien, pero desconocieron el mensaje de Jesucristo porque nacieron antes que él. Esas almas no sufren, pero no pueden participar del Paraíso. Allí se encuentra a un grupo de cinco grandes poetas, entre los cuales es aceptado como el sexto.

Dante encuentra en el Infierno a muchos personajes antiguos, pero también de su época (muchos de ellos enemigos de Dante o que colaboraron en su destierro), y cada uno de ellos narra su historia brevemente a cambio de que Dante prometa mantener vivo su recuerdo en el mundo; cada castigo se ajusta a la naturaleza de su falta (contrapasso) y se repite eternamente. Jorge Luis Borges[4]​ cita como los mejores pasajes la historia de Paolo y Francesca, amantes adúlteros que se conocieron al leer en el libro de Lanzarote los amores de la reina Ginebra y esta persona, que fue motivo de inspiración y homenaje por poetas románticos y contemporáneos, así como la historia del conde Ugolino da Pisa, que se comió a sus propios hijos, y del último viaje de Ulises. Son también impresionantes el tránsito por el bosque de los suicidas, la travesía del desierto donde llueve el fuego, donde Dante se encuentra a su maestro Brunetto Latini, y la llanura de hielo de los traidores, estos últimos, considerados los peores pecadores entre todos. En el último círculo "judesco", el Canto trigésimo cuarto, Dante se encuentra en la cuarta zona del noveno círculo, en el hielo del Cocito, donde son castigados los traidores de los benefactores, durante la noche del 9 de abril de 1300 (Sábado Santo), o, según otros comentadores, del 26 de marzo de 1300. Los traidores están sumergidos en hielo y al llorar las lágrimas les cortan los ojos. Se describe a Lucifer como un demonio de tres cabezas dentro de cuya boca principal se hallaba Judas, al cual mordía con sus filosos colmillos como un juguete, mientras este gritaba de dolor; en las otras mordía a los asesinos de Julio César, Marco Junio Bruto y Casio.

Agarrados al pelaje de Lucifer van descendiendo, pero de repente se encuentran subiendo, porque han pasado el centro de la Tierra y se encaminan a las antípodas. Allí vuelven a ver las estrellas.

En esta segunda parte, Dante y Virgilio atraviesan el Purgatorio, una montaña de cumbre plana y laderas escalonadas y redondas, simétricamente al Infierno. En cada repisa o escalón se redime un pecado, pero los que allí habitan están contentos porque poseen esperanza y saben que su pena es finita y acabará. Dante se va purificando de sus pecados en cada nivel porque un ángel en cada uno le va borrando una letra de una escritura que le han puesto encima. Allí encuentra a famosos poetas, entre ellos a Publio Papinio Estacio, autor de la Tebaida.

Esta parte comienza propiamente con la salida del Infierno a través de la natural burella. Dante y Virgilio llegan así al hemisferio sur terrestre (que se creía por completo bajo las aguas), donde en medio de ellas se halla la montaña del Purgatorio, creada con la tierra utilizada para crear el abismo del Infierno, cuando Lucifer fue expulsado del Paraíso tras rebelarse contra Dios. Tras salir del túnel llegan a una playa, donde encuentran a Catón el Joven, que se desempeña como guardián del Purgatorio. Teniendo que emprender el ascenso de la empinada montaña, que resulta imposible escalar, es tan empinada que Dante tiene que preguntar a algunas almas cuál es el pasaje más cercano; pertenecen al grupo de los negligentes, los muertos en estado de excomunión, que viven en el Ante-purgatorio. Un personaje notable de este lugar es Manfredo de Sicilia. Junto a los que por pereza tardaron en arrepentirse, los muertos violentamente y a los principios negligentes, de hecho, esperan el tiempo de purificación necesario para poder acceder al Purgatorio propiamente dicho. En la entrada del valle donde se encuentran los principios negligentes, Dante, siguiendo las indicaciones de Virgilio, pide indicaciones a un alma que resulta ser el guardián del valle, un compatriota de Virgilio, Sordello, que será su guía hasta la puerta del Purgatorio.

Tras llegar al final del Antepurgatorio, tras un valle florecido, los dos cruzan la puerta del Purgatorio, que custodia un ángel con una espada de fuego, que parece tener vida propia. Está precedido por tres jardines, el primero de mármol blanco, el segundo de una piedra oscura y el tercero y último de pórfido rojo. El ángel, sentado en el solio de diamante y apoyando los pies en el escalón rojo, marca siete "p" en la frente de Dante y abre la puerta con dos llaves, una de plata y otra de oro, que San Pedro le dio, y los dos poetas se adentran en el segundo reino.

El Purgatorio se divide en siete cornisas, donde las almas expían sus pecados para purificarse antes de entrar al Paraíso. Al contrario del Infierno, donde los pecados se agravan a medida que se avanza en los círculos, en el Purgatorio la base de la montaña, es decir la cornisa I, alberga a quienes padecen las culpas más graves, mientras que en la cumbre, cerca del Edén, se encuentran los pecadores menos culpables. Las almas no son castigadas para siempre, ni por una sola culpa, como en el primer reino, pero expían una pena equivalente a los pecados durante la vida.

En la primera cornisa, Dante y Virgilio encuentran a los orgullosos, en la segunda a los envidiosos, en la tercera a los iracundos, en la cuarta a los perezosos, en la quinta a los avaros y a los pródigos. En esta encuentran el alma de Cecilio Estacio tras un terremoto (que se produce cada vez que se libera un alma) y un canto Gloria in excelsis Deo. En vida este personaje fue en exceso pródigo. Tras años de expiación siente el deseo de guiarlos hasta la cumbre, a través de la sexta cornisa, donde expían sus culpas los golosos, que lucen delgadísimos, y la séptima, donde se encuentran los lujuriosos, envueltos en llamas. Dante recuerda que Estacio se convirtió gracias a Virgilio y a sus obras, en particular la Eneida y las Bucólicas, que le mostraron la importancia de la fe cristiana y el error de su vicio. En ese sentido, Virgilio lo iluminó permaneciendo él en la oscuridad. Virgilio fue un profeta sin saberlo, pues llevó a Estacio a la fe pero él, pudiendo tan solo entreverla, no pudo salvarse, y deberá habitar hasta la eternidad en el Limbo. Es en esta repisa de los lujuriosos donde se encuentra el Alighieri a sus amigos poetas de lo que él mismo bautiza como dolce stil novo ("Purgatorio", canto XXIV, v. 57). En la séptima cornisa, los tres tienen que atravesar un muro de fuego, tras la cual hay una escalera, por la que se entra al Paraíso terrestre. Dante se muestra asustado y es confortado por Virgilio. Allí, donde vivieron Adán y Eva prima del pecado, Virgilio y Dante tienen que despedirse, porque el poeta latino no es digno de conducirlo en el Paraíso. Pero Beatriz sí.

Aquí Dante se encuentra con Santa Matilde, la personificación de la felicidad perfecta, precedente al pecado original, que le muestra los dos ríos, Lete, que hace olvidar los pecados, y Eunoe, que devuelve la memoria del bien realizado, y se ofrece a reunirlo con Beatriz, que pronto llegará. Beatriz le llama severamente la atención a Dante y después le propone verla sin el velo. El poeta, por su parte, busca a su maestro Virgilio, que ya no se encuentra con él. Tras beber las aguas del Lete y del Eunoe, que hacen olvidar las cosas malas y recordar las buenas, el poeta sigue a Beatriz hacia el tercer y último reino, el del Paraíso.

Libre de todo pecado, Dante puede ascender al Paraíso, lo que hace junto a Beatriz en condiciones que desafían las leyes físicas, encadenando milagros, lo cual es más bien natural dado el lugar en el cual se desarrolla el poema. Dentro del recorrido será de hecho de gran importancia que el nombre de Beatriz signifique "dadora de felicidad" y "beatificadora", pues en esta sección de la Comedia ella releva a Virgilio en la función de guía. En efecto, a través de este personaje, el autor expresa en los treinta y tres cantos de la sección varios razonamientos teológicos y filosóficos de gran sutileza.

Sin embargo, el poeta expresa desde un principio la gran dificultad que significa transmitir el recorrido emocional y físico de trashumanar, es decir ir más allá de las condiciones de la vida terrenal. Sin embargo, confía en el apoyo del Espíritu Santo (el buen Apolo) y en el hecho de que pese a sus falencias, su esfuerzo descriptivo será emulado y continuado por otros (canto I, 34). En la introducción del canto II, el autor reitera que para entender las alegorías de la obra es indispensable tener de antemano muy amplios conocimientos en las materias que se van a tratar (II, 1-15).

El Paraíso está compuesto por nueve círculos concéntricos correspondientes a los nueve órdenes angélicos de la Jerarquía celestial / Περὶ τῆς οὐρανίου ἱεραρχίας, obra compuesta por el Pseudo Dionisio Areopagita (ángeles, arcángeles, principados, potestades, virtudes, dominaciones, tronos, querubines y serafines), en cuyo centro se encuentra la Tierra. En cada uno de estos cielos, en donde se encuentra cada uno de los planetas, se encuentran los beatos, más cercanos a Dios en función de su grado de beatitud. Pero las almas del Paraíso no están mejor unas que otras y ninguna desea encontrarse en mejores condiciones que las que le corresponden, pues la caridad no permite desear más que lo que se tiene (II, 70-87). De hecho, a cada alma al nacer Dios le dio cierta cantidad de gracia según criterios insondables, en función de los cuales gozan aquellas de los diferentes grados de beatitud. Antes de llegar al primer cielo el poeta y Beatriz atraviesan la Esfera de fuego.

En el primer cielo, que es el de la Luna, se encuentran quienes no cumplieron con sus promesas (Angeli), como la madre de Federico II, Constanza I de Sicilia. En el segundo, el de Mercurio, residen quienes hicieron el bien para obtener gloria y fama, pero no dirigiéndose al bien divino (Arcangeli). En el tercero, de Venus, se encuentran las almas de los "espíritus amantes" (Principati). En el cuarto, del Sol, los "espíritus sabios" (Potestà). En el quinto, de Marte, los "espíritus militantes" de los combatientes por la fe (Virtù). En el sexto, de Júpiter, los "espíritus gobernantes justos" (Dominazioni).

En el séptimo cielo, de Saturno, de los "espíritus contemplativos" (Troni), Beatriz deja de sonreír, como lo había hecho hasta entonces. Desde ese punto en adelante su sonrisa desaparece, pues por la cercanía de Dios su luminosidad resultaría imposible de contemplar. En este último cielo residen los "espíritus contemplativos". Desde allí Beatriz eleva a Dante hasta el cielo de las estrellas fijas, donde no están más repartidos los beatos, sino las "almas triunfantes", que cantan en honor a Cristo y María, a quien Dante alcanza a ver. Desde ese cielo, además, el poeta observa el mundo debajo de sí, los siete planetas, sus movimientos y la Tierra, muy pequeña e insignificante en comparación con la grandeza de Dios (Cherubini). Antes de continuar Dante debe sostener una especie de "examen" de las tres virtudes teologales: Fe, Esperanza y Caridad, por parte de tres profesores particulares: San Pedro, Santiago y San Juan. Por lo tanto, después de un último vistazo al planeta, Dante y Beatriz ascendieron al cielo, el Primo Mobile o Cristallino, el cielo más externo, origen del movimiento y del tiempo universal (Serafini).

En este lugar, tras levantar la mirada, Dante ve un punto muy luminoso, rodeado por nueve círculos de fuego, girando alrededor de ella; el punto, explica Beatriz, es Dios, y a su alrededor se mueven los nueve coros angelicales, divididos por cantidad de virtud. Superado el último cielo, los dos ascienden al Empíreo, donde se encuentra la "rosa de los beatos", una estructura en forma de anfiteatro, en el cual, sobre la grada más alta está la Virgen María. Aquí, en la inmensa multitud de los beatos, están los más grandes de los santos y las figuras más importantes de la Biblia, como San Agustín, San Benito de Nursia, San Francisco, y también Eva, Raquel, Sara y Rebeca. Cada alma es un pétalo de esa rosa.

Desde aquí Dante observa finalmente la luz de Dios, gracias a la intervención de María a la cual San Bernardo (guía de Dante de la última parte del viaje) había pedido ayuda para que Dante pudiese ver a Dios y sostener la visión de lo divino, penetrándola con la mirada hasta que se une con Él, viendo así la perfecta unión de toda la realidad, la explicación de toda la grandeza. En el punto más central de esa gran luz Dante ve tres círculos, las tres personas de la Trinidad, el segundo del cual tiene imagen humana, signo de la naturaleza humana y divina al mismo tiempo, de Cristo. Cuando trata de penetrar aún más el misterio su intelecto flaquea, pero en un excessus mentis[5]​ su alma es tomada por la iluminación, la armonía que da la visión de Dios, en el canto XXXIII (145), del amor que mueve el sol y las otras estrellas (L'amor che move el sole e l'altre stelle). Por la grandiosa luz del último cielo, Dante queda ofuscado, concluyendo así la Divina Comedia.

Como clásico de la literatura universal la Divina Comedia ha tenido traducciones en varias épocas en 25 lenguas. Cabe destacar:

El poemario Siete caminos para Beatriz de Ernesto Pérez Zúñiga (Fundación José Manuel Lara-Vandalia) está basado en la figura de Beatriz de la Divina Comedia de Dante.[18]​
El poemario El Dante en Toledo, de Juan Antonio Villacanas, igualmente estructurado en Cantos, toma la Divina comedia y a Dante como punto de partida del recorrido lírico, histórico y social del poeta toledano por su ciudad natal.[19]​



El arte (del latín ars, artis, y este calco del griego τέχνη téchnē)[1]​ es entendido generalmente como cualquier actividad o producto realizado con una finalidad estética y también comunicativa, mediante la cual se expresan ideas, emociones y, en general, una visión del mundo, a través de diversos recursos, como los plásticos, lingüísticos, sonoros, corporales y mixtos.[2]​ El arte es un componente de la cultura, reflejando en su concepción las bases económicas y sociales, y la transmisión de ideas y valores, inherentes a cualquier cultura humana a lo largo del espacio y el tiempo. Se suele considerar que con la aparición del Homo sapiens el arte tuvo en principio una función ritual, mágica o religiosa (arte paleolítico), pero esa función cambió con la evolución del ser humano, adquiriendo un componente estético y una función social, pedagógica, mercantil o simplemente ornamental.

La noción de arte continúa sujeta a profundas disputas, dado que su definición está abierta a múltiples interpretaciones, que varían según la cultura, la época, el movimiento, o la sociedad para la cual el término tiene un determinado sentido. El vocablo ‘arte’ tiene una extensa acepción, pudiendo designar cualquier actividad humana hecha con esmero y dedicación, o cualquier conjunto de reglas necesarias para desarrollar de forma óptima una actividad: se habla así de “arte culinario”, “arte médico”, “artes marciales”, “artes de arrastre” en la pesca, etc. En ese sentido, arte es sinónimo de capacidad, habilidad, talento, experiencia. Sin embargo, más comúnmente se suele considerar al arte como una actividad creadora del ser humano, por la cual produce una serie de objetos (obras de arte) que son singulares, y cuya finalidad es principalmente estética. En ese contexto, arte sería la generalización de un concepto expresado desde antaño como “bellas artes”, actualmente algo en desuso y reducido a ámbitos académicos y administrativos. De igual forma, el empleo de la palabra arte para designar la realización de otras actividades ha venido siendo sustituido por términos como ‘técnica’ u ‘oficio’. En este artículo se trata de arte entendido como un medio de expresión humano de carácter creativo.

La definición de arte es abierta, subjetiva y discutible. No existe un acuerdo unánime entre historiadores, filósofos o artistas. A lo largo del tiempo se han dado numerosas definiciones de arte, entre ellas: «el arte es el recto ordenamiento de la razón» (Tomás de Aquino); «el arte es aquello que establece su propia regla» (Schiller); «el arte es el estilo» (Max Dvořák); «el arte es expresión de la sociedad» (John Ruskin); «el arte es la libertad del genio» (Adolf Loos); «el arte es la idea» (Marcel Duchamp); «el arte es la novedad» (Jean Dubuffet); «el arte es la acción, la vida» (Joseph Beuys); «arte es todo aquello que los hombres llaman arte» (Dino Formaggio); «el arte es la mentira que nos ayuda a ver la verdad» (Pablo Picasso); «arte es vida, vida es arte» (Wolf Vostell). El concepto ha ido variando con el paso del tiempo: hasta el Renacimiento, solo las artes liberales eran consideradas arte; la arquitectura, la escultura y la pintura eran consideradas “manualidades”. El arte ha sido desde siempre uno de los principales medios de expresión del ser humano, a través del cual manifiesta sus ideas y sentimientos, la forma como se relaciona con el mundo. Su función puede variar desde la más práctica hasta la más ornamental, puede tener un contenido religioso o simplemente estético, puede ser duradero o efímero. En el siglo XX se pierde incluso el sustrato material: decía Beuys que la vida es un medio de expresión artística, destacando el aspecto vital, la acción. Así, todo el mundo es capaz de ser artista.

El término arte procede del latín ars, y es el equivalente al término griego τέχνη (téchne, de donde proviene ‘técnica’). Originalmente se aplicaba a toda la producción realizada por el hombre y a las disciplinas del saber hacer. Así, artistas eran tanto el cocinero, el jardinero o el constructor, como el pintor o el poeta. Con el tiempo la derivación latina (ars -> arte) se utilizó para designar a las disciplinas relacionadas con las artes de lo estético y lo emotivo; y la derivación griega (téchne -> técnica), para aquellas disciplinas que tienen que ver con las producciones intelectuales y de artículos de uso.[3]​ En la actualidad es difícil encontrar que ambos términos (arte y técnica) se confundan o utilicen como sinónimos.

En la antigüedad clásica grecorromana, una de las principales cunas de la civilización occidental y primera cultura que reflexionó sobre el arte, se consideraba el arte como una habilidad del ser humano en cualquier terreno productivo, siendo prácticamente un sinónimo de ‘destreza’: destreza para construir un objeto, para comandar un ejército, para convencer al público en un debate, o para efectuar mediciones agronómicas. En definitiva, cualquier habilidad sujeta a reglas, a preceptos específicos que la hacen objeto de aprendizaje y de evolución y perfeccionamiento técnico. En cambio, la poesía, que venía de la inspiración, no estaba catalogada como arte. Así, Aristóteles, por ejemplo, definió el arte como aquella «permanente disposición a producir cosas de un modo racional», y Quintiliano estableció que era aquello «que está basado en un método y un orden» (via et ordine).[4]​ Platón, en el Protágoras, habló del arte, opinando que es la capacidad de hacer cosas por medio de la inteligencia, a través de un aprendizaje. Para Platón, el arte tiene un sentido general, es la capacidad creadora del ser humano.[5]​ Casiodoro destacó en el arte su aspecto productivo, conforme a reglas, señalando tres objetivos principales del arte: enseñar (doceat), conmover (moveat) y complacer (delectet).[6]​

Durante el Renacimiento se empezó a gestar un cambio de mentalidad, separando los oficios y las ciencias de las artes, donde se incluyó por primera vez a la poesía, considerada hasta entonces un tipo de filosofía o incluso de profecía –para lo que fue determinante la publicación en 1549 de la traducción italiana de la Poética de Aristóteles–. En este cambio intervino considerablemente la progresiva mejora en la situación social del artista, debida al interés que los nobles y ricos prohombres italianos empezaron a mostrar por la belleza. Los productos del artista adquirieron un nuevo estatus de objetos destinados al consumo estético y, por ello, el arte se convirtió en un medio de promoción social, incrementándose el mecenazgo artístico y fomentando el coleccionismo.[7]​ Surgieron en ese contexto varios tratados teóricos acerca del arte, como los de Leon Battista Alberti (De Pictura, 1436-1439; De re aedificatoria, 1450; y De Statua, 1460), o Los Comentarios (1447) de Lorenzo Ghiberti. Alberti recibió la influencia aristotélica, pretendiendo aportar una base científica al arte. Habló de decorum, el tratamiento del artista para adecuar los objetos y temas artísticos a un sentido mesurado, perfeccionista. Ghiberti fue el primero en periodificar la historia del arte, distinguiendo antigüedad clásica, periodo medieval y lo que llamó “renacer de las artes”.[8]​

Con el manierismo comenzó el arte moderno: las cosas ya no se representan tal como son, sino tal como las ve el artista. La belleza se relativiza, se pasa de la belleza única renacentista, basada en la ciencia, a las múltiples bellezas del manierismo, derivadas de la naturaleza. Apareció en el arte un nuevo componente de imaginación, reflejando tanto lo fantástico como lo grotesco, como se puede percibir en la obra de Brueghel o Arcimboldo. Giordano Bruno fue uno de los primeros pensadores que prefiguró las ideas modernas: decía que la creación es infinita, no hay centro ni límites –ni Dios ni hombre–, todo es movimiento, dinamismo. Para Bruno, hay tantos artes como artistas, introduciendo la idea de originalidad del artista. El arte no tiene normas, no se aprende, sino que viene de la inspiración.[9]​

Los siguientes avances se hicieron en el siglo XVIII con la Ilustración, donde comenzó a producirse cierta autonomía del hecho artístico: el arte se alejó de la religión y de la representación del poder para ser fiel reflejo de la voluntad del artista, centrándose más en las cualidades sensibles de la obra que no en su significado.[10]​ Jean-Baptiste Dubos, en Reflexiones críticas sobre la poesía y la pintura (1719), abrió el camino hacia la relatividad del gusto, razonando que la estética no viene dada por la razón, sino por los sentimientos. Así, para Dubos el arte conmueve, llega al espíritu de una forma más directa e inmediata que el conocimiento racional. Dubos hizo posible la popularización del gusto, oponiéndose a la reglamentación académica, e introdujo la figura del ‘genio’, como atributo dado por la naturaleza, que está más allá de las reglas. 

En el romanticismo, surgido en Alemania a finales del siglo XVIII con el movimiento denominado Sturm und Drang, triunfó la idea de un arte que surge espontáneamente del individuo, desarrollando la noción de genio –el arte es la expresión de las emociones del artista–, que comienza a ser mitificado.[11]​ Autores como Novalis y Friedrich von Schlegel reflexionaron sobre el arte: en la revista Athenäum, editada por ellos, surgieron las primeras manifestaciones de la autonomía del arte, ligado a la naturaleza. Para ellos, en la obra de arte se encuentran el interior del artista y su propio lenguaje natural.[12]​

Arthur Schopenhauer dedicó el tercer libro de El mundo como voluntad y representación a la teoría del arte: el arte es una vía para escapar del estado de infelicidad propio del hombre. Identificó conocimiento con creación artística, que es la forma más profunda de conocimiento. El arte es la reconciliación entre voluntad y conciencia, entre objeto y sujeto, alcanzando un estado de contemplación, de felicidad. La conciencia estética es un estado de contemplación desinteresada, donde las cosas se muestran en su pureza más profunda. El arte habla en el idioma de la intuición, no de la reflexión; es complementario de la filosofía, la ética y la religión. Influido por la filosofía oriental, manifestó que el hombre debe liberarse de la voluntad de vivir, del ‘querer’, que es origen de insatisfacción. El arte es una forma de librarse de la voluntad, de ir más allá del ‘yo’.[13]​

Richard Wagner recogió la ambivalencia entre lo sensible y lo espiritual de Schopenhauer: en Ópera y drama (1851), Wagner planteó la idea de la “obra de arte total” (Gesamtkunstwerk), donde se haría una síntesis de la poesía, la palabra –elemento masculino–, con la música –elemento femenino–. Opinaba que el lenguaje primitivo sería vocálico, mientras que la consonante fue un elemento racionalizador; así pues, la introducción de la música en la palabra sería un retorno a la inocencia primitiva del lenguaje.[14]​

A finales del siglo XIX surgió el esteticismo, que fue una reacción al utilitarismo imperante en la época y a la fealdad y el materialismo de la era industrial. Frente a ello, surgió una tendencia que otorgaba al arte y a la belleza una autonomía propia, sintetizada en la fórmula de Théophile Gautier “el arte por el arte” (l'art pour l'art), llegando incluso a hablarse de “religión estética”.[15]​ Esta postura pretendía aislar al artista de la sociedad, para que buscase de forma autónoma su propia inspiración y se dejase llevar únicamente por una búsqueda individual de la belleza.[16]​ Así, la belleza se aleja de cualquier componente moral, convirtiéndose en el fin último del artista, que llega a vivir su propia vida como una obra de arte –como se puede apreciar en la figura del dandi–.[17]​ Uno de los teóricos del movimiento fue Walter Pater, que influyó sobre el denominado decadentismo inglés, estableciendo en sus obras que el artista debe vivir la vida intensamente, siguiendo como ideal a la belleza. Para Pater, el arte es “el círculo mágico de la existencia”, un mundo aislado y autónomo puesto al servicio del placer, elaborando una auténtica metafísica de la belleza.[18]​

Por otro lado, Charles Baudelaire fue uno de los primeros autores que analizaron la relación del arte con la recién surgida era industrial, prefigurando la noción de “belleza moderna”: no existe la belleza eterna y absoluta, sino que cada concepto de lo bello tiene algo de eterno y algo de transitorio, algo de absoluto y algo de particular. La belleza viene de la pasión y, al tener cada individuo su pasión particular, también tiene su propio concepto de belleza. En su relación con el arte, la belleza expresa por un lado una idea “eternamente subsistente”, que sería el “alma del arte”, y por otro un componente relativo y circunstancial, que es el “cuerpo del arte”. Así, la dualidad del arte es expresión de la dualidad del hombre, de su aspiración a una felicidad ideal enfrentada a las pasiones que le mueven hacia ella. Frente a la mitad eterna, anclada en el arte clásico antiguo, Baudelaire vio en la mitad relativa el arte moderno, cuyos signos distintivos son lo transitorio, lo fugaz, lo efímero y cambiante –sintetizados en la moda–. Baudelaire tenía un concepto neoplatónico de belleza, que es la aspiración humana hacia un ideal superior, accesible a través del arte. El artista es el “héroe de la modernidad”, cuya principal cualidad es la melancolía, que es el anhelo de la belleza ideal.[19]​

En contraposición al esteticismo, Hippolyte-Adolphe Taine elaboró una teoría sociológica del arte: en su Filosofía del arte (1865-1869) aplicó al arte un determinismo basado en la raza, el contexto y la época (race, milieu, moment). Para Taine, la estética, la “ciencia del arte”, opera como cualquier otra disciplina científica, basándose en parámetros racionales y empíricos. Igualmente, Jean Marie Guyau, en Los problemas de la estética contemporánea (1884) y El arte desde el punto de vista sociológico (1888), planteó una visión evolucionista del arte, afirmando que el arte está en la vida, y que evoluciona como esta; y, al igual que la vida del ser humano está organizada socialmente, el arte debe ser reflejo de la sociedad.[20]​

La estética sociológica tuvo una gran vinculación con el realismo pictórico y con movimientos políticos de izquierdas, especialmente el socialismo utópico: autores como Henri de Saint-Simon, Charles Fourier y Pierre Joseph Proudhon defendieron la función social del arte, que contribuye al desarrollo de la sociedad, aunando belleza y utilidad en un conjunto armónico. Por otro lado, en el Reino Unido, la obra de teóricos como John Ruskin y William Morris aportó una visión funcionalista del arte: en Las piedras de Venecia (1851-1856) Ruskin denunció la destrucción de la belleza y la vulgarización del arte llevada a cabo por la sociedad industrial, así como la degradación de la clase obrera, defendiendo la función social del arte. En El arte del pueblo (1879) pidió cambios radicales en la economía y la sociedad, reclamando un arte “hecho por el pueblo y para el pueblo”. Por su parte, Morris –fundador del movimiento Arts & Crafts– defendió un arte funcional, práctico, que satisfaga necesidades materiales y no solo espirituales. En Escritos estéticos (1882-1884) y Los fines del arte (1887) planteó un concepto de arte utilitario pero alejado de sistemas de producción excesivamente tecnificados, próximo a un concepto del socialismo cercano al corporativismo medieval.[21]​

Por otro lado, la función del arte fue cuestionada por el escritor ruso Lev Tolstoi: en ¿Qué es el arte? (1898) se planteó la justificación social del arte, argumentando que siendo el arte una forma de comunicación solo puede ser válido si las emociones que transmite pueden ser compartidas por todos los hombres. Para Tolstoi, la única justificación válida es la contribución del arte a la fraternidad humana: una obra de arte solo puede tener valor social cuando transmite valores de fraternidad, es decir, emociones que impulsen a la unificación de los pueblos.[22]​

En esa época se empezó a abordar el estudio del arte desde el terreno de la psicología: Sigmund Freud aplicó el psicoanálisis al arte en Un recuerdo infantil de Leonardo da Vinci (1910), defendiendo que el arte sería una de las maneras de representar un deseo, una pulsión reprimida, de forma sublimada. Opinaba que el artista es una figura narcisista, cercana al niño, que refleja en el arte sus deseos, y afirmó que las obras artísticas pueden ser estudiadas como los sueños y las enfermedades mentales, con el psicoanálisis. Su método era semiótico, estudiando los símbolos, y opinaba que una obra de arte es un símbolo. Pero como el símbolo representa un determinado concepto simbolizado, hay que estudiar la obra de arte para llegar al origen creativo de la obra.[23]​ Igualmente, Carl Gustav Jung relacionó la psicología con diversas disciplinas como la filosofía, la sociología, la religión, la mitología, la literatura y el arte. En Contribuciones a la psicología analítica (1928), sugirió que los elementos simbólicos presentes en el arte son “imágenes primordiales” o “arquetipos”, que están presentes de forma innata en el “subconsciente colectivo” del ser humano.[24]​

Wilhelm Dilthey, desde la estética cultural, formuló una teoría acerca de la unidad entre arte y vida. Prefigurando el arte de vanguardia, Dilthey ya vislumbraba a finales del siglo XIX cómo el arte se alejaba de las reglas académicas, y cómo cobraba cada vez mayor importancia la función del público, que tiene el poder de ignorar o ensalzar la obra de un artista determinado. Encontró en todo ello una “anarquía del gusto”, que achacó a un cambio social de interpretación de la realidad, pero que percibió como transitorio, siendo necesario hallar «una relación sana entre el pensamiento estético y el arte». Así, ofreció como salvación del arte las “ciencias del espíritu”, especialmente la psicología: la creación artística debe poder analizarse bajo el prisma de la interpretación psicológica de la fantasía. En Vida y poesía (1905) presentó la poesía como expresión de la vida, como ‘vivencia’ (Erlebnis) que refleja la realidad externa de la vida. La creación artística tiene pues como función intensificar nuestra visión del mundo exterior, presentándolo como un conjunto coherente y pleno de sentido.[25]​

El siglo XX ha supuesto una radical transformación del concepto de arte: la superación de las ideas racionalistas de la Ilustración y el paso a conceptos más subjetivos e individuales, partiendo del movimiento romántico y cristalizando en la obra de autores como Kierkegaard y Nietzsche, suponen una ruptura con la tradición y un rechazo de la belleza clásica. El concepto de realidad fue cuestionado por las nuevas teorías científicas: la subjetividad del tiempo de Bergson, la Teoría de la relatividad de Einstein, la mecánica cuántica, la teoría del psicoanálisis de Freud, etc. Por otro lado, las nuevas tecnologías hacen que el arte cambie de función, debido a que la fotografía y el cine ya se encargan de plasmar la realidad. Todos estos factores producen la génesis del arte abstracto, el artista ya no intenta reflejar la realidad, sino su mundo interior, expresar sus sentimientos.[26]​ El arte actual tiene oscilaciones continuas del gusto, cambia simultáneamente junto a este: así como el arte clásico se sustentaba sobre una metafísica de ideas inmutables, el actual, de raíz kantiana, encuentra gusto en la conciencia social de placer (cultura de masas). También hay que valorar la progresiva disminución del analfabetismo, puesto que antiguamente, al no saber leer gran parte de la población, el arte gráfico era el mejor medio para la transmisión del conocimiento –sobre todo religioso–, función que ya no es necesaria en el siglo XX.

Una de las primeras formulaciones fue la del marxismo: de la obra de Marx se desprendía que el arte es una “superestructura” cultural determinada por las condiciones sociales y económicas del ser humano. Para los marxistas, el arte es reflejo de la realidad social, si bien el propio Marx no veía una correspondencia directa entre una sociedad determinada y el arte que produce. Georgi Plejánov, en Arte y vida social (1912), formuló una estética materialista que rechazaba el “arte por el arte”, así como la individualidad del artista ajeno a la sociedad que lo envuelve.[27]​ Walter Benjamin incidió de nuevo en el arte de vanguardia, que para él es «la culminación de la dialéctica de la modernidad», el final del intento totalizador del arte como expresión del mundo circundante. Intentó dilucidar el papel del arte en la sociedad moderna, realizando un análisis semiótico en el que el arte se explica a través de signos que el hombre intenta descifrar sin un resultado aparentemente satisfactorio. En La obra de arte en la época de la reproductibilidad técnica (1936) analizó la forma cómo las nuevas técnicas de reproducción industrial del arte pueden hacer variar el concepto de este, al perder su carácter de objeto único y, por tanto, su halo de reverencia mítica; esto abre nuevas vías de concebir el arte –inexploradas aún para Benjamin– pero que supondrán una relación más libre y abierta con la obra de arte.[28]​

Theodor W. Adorno, como Benjamin perteneciente a la Escuela de Fráncfort, defendió el arte de vanguardia como reacción a la excesiva tecnificación de la sociedad moderna. En su Teoría estética (1970) afirmó que el arte es reflejo de las tendencias culturales de la sociedad, pero sin llegar a ser fiel reflejo de esta, ya que el arte representa lo inexistente, lo irreal; o, en todo caso, representa lo que existe pero como posibilidad de ser otra cosa, de trascender. El arte es la “negación de la cosa”, que a través de esta negación la trasciende, muestra lo que no hay en ella de forma primigenia. Es apariencia, mentira, presentando lo inexistente como existente, prometiendo que lo imposible es posible.[29]​

Representante del pragmatismo, John Dewey, en Arte como experiencia (1934), definió el arte como “culminación de la naturaleza”, defendiendo que la base de la estética es la experiencia sensorial. La actividad artística es una consecuencia más de la actividad natural del ser humano, cuya forma organizativa depende de los condicionamientos ambientales en que se desenvuelve. Así, el arte es “expresión”, donde fines y medios se fusionan en una experiencia agradable. Para Dewey, el arte, como cualquier actividad humana, implica iniciativa y creatividad, así como una interacción entre sujeto y objeto, entre el hombre y las condiciones materiales en las que desarrolla su labor.[30]​

José Ortega y Gasset analizó en La deshumanización del arte (1925) el arte de vanguardia desde el concepto de “sociedad de masas”, donde el carácter minoritario del arte vanguardista produce una elitización del público consumidor de arte. Ortega aprecia en el arte una “deshumanización” debida a la pérdida de perspectiva histórica, es decir, de no poder analizar con suficiente distancia crítica el sustrato socio-cultural que conlleva el arte de vanguardia. La pérdida del elemento realista, imitativo, que Ortega aprecia en el arte de vanguardia, supone una eliminación del elemento humano que estaba presente en el arte naturalista. Asimismo, esta pérdida de lo humano hace desaparecer los referentes en que estaba basado el arte clásico, suponiendo una ruptura entre el arte y el público, y generando una nueva forma de comprender el arte que solo podrán entender los iniciados. La percepción estética del arte deshumanizado es la de una nueva sensibilidad basada no en la afinidad sentimental –como se producía con el arte romántico–, sino en un cierto distanciamiento, una apreciación de matices. Esa separación entre arte y humanidad supone un intento de volver al hombre a la vida, de rebajar el concepto de arte como una actividad secundaria de la experiencia humana.[31]​

En la escuela semiótica, Luigi Pareyson elaboró en Estética. Teoría de la formatividad (1954) una estética hermenéutica, donde el arte es interpretación de la verdad. Para Pareyson, el arte es “formativo”, es decir, expresa una forma de hacer que, «a la vez que hace, inventa el modo de hacer». En otras palabras, no se basa en reglas fijas, sino que las define conforme se elabora la obra y las proyecta en el momento de realizarla. Así, en la formatividad la obra de arte no es un “resultado”, sino un “logro”, donde la obra ha encontrado la regla que la define específicamente. El arte es toda aquella actividad que busca un fin sin medios específicos, debiendo hallar para su realización un proceso creativo e innovador que dé resultados originales de carácter inventivo.[32]​ Pareyson influyó en la denominada Escuela de Turín, que desarrollará su concepto ontológico del arte: Umberto Eco, en Obra abierta (1962), afirmó que la obra de arte solo existe en su interpretación, en la apertura de múltiples significados que puede tener para el espectador; Gianni Vattimo, en Poesía y ontología (1968), relacionó el arte con el ser, y por tanto con la verdad, ya que es en el arte donde la verdad se muestra de forma más pura y reveladora.[33]​

Una de las últimas derivaciones de la filosofía y el arte es la postmodernidad, teoría socio-cultural que postula la actual vigencia de un periodo histórico que habría superado el proyecto moderno, es decir, la raíz cultural, política y económica propia de la Edad Contemporánea, marcada en lo cultural por la Ilustración, en lo político por la Revolución francesa y en lo económico por la Revolución industrial. Frente a las propuestas del arte de vanguardia, los postmodernos no plantean nuevas ideas, ni éticas ni estéticas; tan solo reinterpretan la realidad que les envuelve, mediante la repetición de imágenes anteriores, que pierden así su sentido. La repetición encierra el marco del arte en el arte mismo, se asume el fracaso del compromiso artístico, la incapacidad del arte para transformar la vida cotidiana. El arte postmoderno vuelve sin pudor al sustrato material , a la obra de arte-objeto, al “arte por el arte”, sin pretender hacer ninguna evolución, ninguna ruptura. Algunos de sus más importantes teóricos han sido Jacques Derrida y Michel Foucault.[34]​

Como conclusión, cabría decir que las viejas fórmulas que basaban el arte en la creación de belleza o en la imitación de la naturaleza han quedado obsoletas, y hoy día el arte es una cualidad dinámica, en constante transformación, inmersa además en los medios de comunicación de masas, en los canales de consumo, con un aspecto muchas veces efímero, de percepción instantánea, presente con igual validez en la idea y en el objeto, en su génesis conceptual y en su realización material.[35]​ Morris Weitz, representante de la estética analítica, opinaba en El papel de la teoría en la estética (1957) que «es imposible establecer cualquier tipo de criterios del arte que sean necesarios y suficientes; por lo tanto, cualquier teoría del arte es una imposibilidad lógica, y no simplemente algo que sea difícil de obtener en la práctica». Según Weitz, una cualidad intrínseca de la creatividad artística es que siempre produce nuevas formas y objetos, por lo que «las condiciones del arte no pueden establecerse nunca de antemano». Así, «el supuesto básico de que el arte pueda ser tema de cualquier definición realista o verdadera es falso».[36]​

En el fondo, la indefinición del arte estriba en su reducción a determinadas categorías –como imitación, como recreación, como expresión–; el arte es un concepto global, que incluye todas estas formulaciones y muchas más, un concepto en evolución y abierto a nuevas interpretaciones, que no se puede fijar de forma convencional, sino que debe aglutinar todos los intentos de expresarlo y formularlo, siendo una síntesis amplia y subjetiva de todos ellos.

La clasificación del arte, o de las distintas facetas o categorías que pueden considerarse artísticas, ha tenido una evolución paralela al concepto mismo de arte: como se ha visto anteriormente, durante la antigüedad clásica se consideraba arte todo tipo de habilidad manual y destreza, de tipo racional y sujeta a reglas; así, entraban en esa denominación tanto las actuales bellas artes como la artesanía y las ciencias, mientras que quedaban excluidas la música y la poesía. Una de las primeras clasificaciones que se hicieron de las artes fue la de los filósofos sofistas presocráticos, que distinguieron entre “artes útiles” y “artes placenteras”, es decir, entre las que producen objetos de cierta utilidad y las que sirven para el entretenimiento. Plutarco introdujo, junto a estas dos, las “artes perfectas”, que serían lo que hoy consideramos ciencias. Platón, por su parte, estableció la diferencia entre “artes productivas” y “artes imitativas”, según si producían objetos nuevos o imitaban a otros.[38]​

Durante la era romana hubo diversos intentos de clasificar las artes: Quintiliano dividió el arte en tres esferas: “artes teóricas”, basadas en el estudio (principalmente, las ciencias); “artes prácticas”, basadas en una actividad, pero sin producir nada (como la danza); y “artes poéticas” –según la etimología griega, donde ποίησις (poíêsis) quiere decir ‘producción’–, que son las que producen objetos. Cicerón catalogó las artes según su importancia: “artes mayores” (política y estrategia militar), “artes medianas” (ciencias, poesía y retórica) y “artes menores” (pintura, escultura, música, interpretación y atletismo). Plotino clasificó las artes en cinco grupos: las que producen objetos físicos (arquitectura), las que ayudan a la naturaleza (medicina y agricultura), las que imitan a la naturaleza (pintura), las que mejoran la acción humana (política y retórica) y las intelectuales (geometría).[39]​

Sin embargo, la clasificación que tuvo más fortuna –llegando hasta la era moderna– fue la de Galeno en el siglo II, que dividió el arte en “artes liberales” y “artes vulgares”, según si tenían un origen intelectual o manual. Entre las liberales se encontraban: la gramática, la retórica y la dialéctica –que formaban el trivium–, y la aritmética, la geometría, la astronomía y la música –que formaban el quadrivium–; las vulgares incluían la arquitectura, la escultura y la pintura, pero también otras actividades que hoy consideramos artesanía.[40]​

Durante la Edad Media continuó la división del arte entre artes liberales y vulgares –llamadas estas últimas entonces “mecánicas”–, si bien hubo nuevos intentos de clasificación: Boecio dividió las artes en ars y artificium, clasificación similar a la de artes liberales y vulgares, pero en una acepción que casi excluía las formas manuales del campo del arte, dependiendo este tan solo de la mente. En el siglo XII, Radulfo de Campo Lungo intentó hacer una clasificación de las artes mecánicas, reduciéndolas a siete, igual número que las liberales. En función de su utilidad cara a la sociedad, las dividió en: ars victuaria, para alimentar a la gente; lanificaria, para vestirles; architectura, para procurarles una casa; suffragatoria, para darles medios de transporte; medicinaria, que les curaba; negotiatoria, para el comercio; militaria, para defenderse.[41]​

En el siglo XVI empezó a considerarse que la arquitectura, la pintura y la escultura eran actividades que requerían no solo oficio y destreza, sino también un tipo de concepción intelectual que las hacían superiores a otros tipos de manualidades. Se gestaba así el concepto moderno de arte, que durante el Renacimiento adquirió el nombre de arti del disegno (artes del diseño), por cuanto comprendían que esta actividad –el diseñar– era la principal en la génesis de las obras de arte.[42]​

Sin embargo, faltaba aglutinar estas artes del diseño con el resto de actividades consideradas artísticas (música, poesía y teatro), tarea que se desarrolló durante los dos siglos siguientes con varios intentos de buscar un nexo común a todas estas actividades: así, el humanista florentino Giannozzo Manetti propuso el término “artes ingeniosas”, donde incluía las artes liberales, por lo que solo cambiaba el vocablo; el filósofo neoplatónico Marsilio Ficino elaboró el concepto de “artes musicales”, argumentando que la música era la inspiración para todas las artes; en 1555, Giovanni Pietro Capriano introdujo en su De vera poetica la acepción “artes nobles”, apelando a la elevada finalidad de estas actividades; Lodovico Castelvetro habló en su Correttione (1572) de “artes memoriales”, ya que según él estas artes buscaban fijar en objetos la memoria de cosas y acontecimientos; Claude-François Menestrier, historiador francés del siglo XVII, formuló la idea de “artes pictóricas”, remarcando el carácter visual del arte; Emanuele Tesauro ideó en 1658 la noción de “artes poéticas”, inspirado en la célebre cita de Horacio ut pictura poesis (la pintura como la poesía), describiendo el componente poético y metafórico de estas artes; ya en el siglo XVIII, coincidieron en un mismo año (1744) dos definiciones, la de “artes agradables” de Giambattista Vico, y la de “artes elegantes” de James Harris; por último, en 1746, Charles Batteux estableció en Las bellas artes reducidas a un único principio la concepción actual de bellas artes, remarcando su aspecto de imitación (imitatio).[43]​

Batteux incluyó en las bellas artes pintura, escultura, música, poesía y danza, mientras que mantuvo el término artes mecánicas para el resto de actividades artísticas, y señaló como actividades entre ambas categorías la arquitectura y la retórica, si bien al poco tiempo se eliminó el grupo intermedio y la arquitectura y la retórica se incorporaron plenamente a las bellas artes. Sin embargo, con el tiempo, esta lista sufrió diversas variaciones, y si bien se aceptaba comúnmente la presencia de arquitectura, pintura, escultura, música y poesía, los dos puestos restantes oscilaron entre la danza, la retórica, el teatro y la jardinería, o, más adelante, nuevas disciplinas como la fotografía y el cine. El término “bellas artes” hizo fortuna, y quedó fijado como definición de todas las actividades basadas en la elaboración de objetos con finalidad estética, producidos de forma intelectual y con voluntad expresiva y trascendente. Así, desde entonces las artes fueron “bellas artes”, separadas tanto de las ciencias como de los oficios manuales. Por eso mismo, durante el siglo XIX se fue produciendo un nuevo cambio terminológico: ya que las artes eran solo las bellas artes, y el resto de actividades no lo eran, poco a poco se fue perdiendo el término ‘bellas’ para quedar solo el de ‘artes’, quedando la acepción ‘arte’ tal como la entendemos hoy día. Incluso sucedió que entonces se restringió el término “bellas artes” para designar las artes visuales, las que en el Renacimiento se denominaban “artes del diseño” (arquitectura, pintura y escultura), siendo las demás las “artes en general”. También hubo una tendencia cada vez más creciente a separar las artes visuales de las literarias, que recibieron el nombre de “bellas letras”.[44]​ Se podría decir que las “bellas artes” son aquellas que cumplen con ciertas características estéticas dignas de ser admiradas: tienen como objetivo expresar la belleza aunque esta sea definida por el artista o por la particular perspectiva del observador, cayendo en la ambigüedad de lo que es bello. Gary Martin señaló que debido a que constituye una experiencia subjetiva, a menudo se dice que «la belleza está en el ojo del observador». Las “bellas artes” han tenido históricamente tal adjetivo debido a que representan la máxima expresión sentimental del ser humano desde épocas remotas.

Sin embargo, pese a la aceptación general de la clasificación propuesta por Batteux, en los siglos siguientes todavía se produjeron intentos de nuevas clasificaciones del arte: Immanuel Kant distinguió entre “artes mecánicas” y “artes estéticas”; Robert von Zimmermann habló de artes de la representación material (arquitectura y escultura), de la representación perceptiva (pintura y música) y de la representación del pensamiento (literatura); y Alois Riegl, en Arte industrial de la época romana tardía, dividió el arte en arquitectura, plástica y ornamento. Hegel, en su Estética (1835-1838), estableció tres formas de manifestación artística: arte simbólico, clásico y romántico, que se relacionan con tres formas diferentes de arte, tres estadios de evolución histórica y tres maneras distintas de tomar forma la idea:

En la idea, primero hay una relación de desajuste, donde la idea no encuentra forma; después es de ajuste, cuando la idea se ajusta a la forma; por último, en el desbordamiento, la idea sobrepasa la forma, tiende al infinito. En la evolución histórica, equipara infancia con el arte prehistórico, antiguo y oriental; madurez, con el arte griego y romano; y vejez, con el arte cristiano. En cuanto a la forma, la arquitectura (forma monumental) es un arte tectónico, depende de la materia, de pesos, medidas, etc.; la escultura (forma antropomórfica) depende más de la forma volumétrica, por lo que se acerca más al hombre; la pintura, música y poesía (formas suprasensibles) son la etapa más espiritual, más desmaterializada. La creación artística no ha de ser una mimesis, sino un proceso de libertad espiritual. En su evolución, cuando el artista llega a su límite, se van perdiendo las formas sensibles, el arte se vuelve más conceptual y reflexivo; al final de este proceso se produce la “muerte del arte”.[45]​

Pese a todo, estos intentos de clasificación resultaron un tanto baldíos y, cuando parecía que por fin se había llegado a una definición del arte universalmente aceptable, después de tantos siglos de evolución, los cambios sociales, culturales y tecnológicos producidos durante los siglos XIX y XX han comportado un nuevo intento de definir el arte con base en parámetros más abiertos y omnicomprensivos, intentando abarcar tanto una definición teórica del arte como una catalogación práctica que incluyese las nuevas formas artísticas que han ido surgiendo en los últimos tiempos (fotografía, cine, cómic, nuevas tecnologías, etc.). Como el de Juan Acha con su ensayo Arte y sociedad. Latinoamérica: el producto artístico y estructura (1979), cuya compleja organización de las artes es según su aplicación y origen; en grupos como "Cuerpo-Objeto", "Superficie-Objetos", "Superficies-Icónicas", "Superficies-Literarias", "Espectáculos" y "Audiciones". Y otra más simple en Lógica del Límite (1991) de Eugenio Trías, en la que el artista es como un habitante y a un determinado oficio artístico como un habitáculo, que constituyen tres grandes áreas del arte: artes estáticas o del espacio, artes mixtas y artes temporales o dinámicas.

Estos intentos, un tanto infructuosos, han producido en cierta forma el efecto contrario, acentuando aún más la indefinición del arte, que hoy día es un concepto abierto e interpretable, donde caben muchas fórmulas y concepciones, si bien se suele aceptar un mínimo denominador común basado en cualidades estéticas y expresivas, así como un componente de creatividad.[35]​

Cinco artes son comúnmente citadas en el siglo XIX, a las cuales en el siglo XX se le añadirán cuatro más para llegar a un total de nueve artes, sin ser capaces los expertos y críticos de ponerse de acuerdo sobre la clasificación un "décimo arte". 

Al final del siglo XX, la siguiente lista establece las nuevas clasificaciones, al igual que el número de musas antiguas: 

Ciertos críticos e historiadores consideran otras artes en la lista, como la gastronomía, la perfumería, la televisión, la moda, la publicidad, la animación y los videojuegos. En la actualidad existe aún cierta discrepancia sobre cuál sería el “décimo arte”.[46]​

Las artes creativas a menudo son divididas en categorías más específicas, como las artes decorativas, las artes plásticas, las artes escénicas o la literatura. Así, la pintura es una forma de arte visual, y la poesía es una forma de literatura. Algunos ejemplos son:

Cada periodo histórico ha tenido unas características concretas y definibles, comunes a otras regiones y culturas, o bien únicas y diferenciadas, que han ido evolucionando con el devenir de los tiempos. De ahí surgen los estilos artísticos, que pueden tener un origen geográfico o temporal, o incluso reducirse a la obra de un artista en concreto, siempre y cuando se produzcan unas formas artísticas claramente definitorias. ‘Estilo’ proviene del latín stilus (‘punzón’), escrito en época medieval como stylus por influencia del término griego στύλος (stylos, ‘columna’). Antiguamente, se denominaba así a un tipo de punzón para escribir sobre tablillas de cera; con el tiempo, pasó a designar tanto el instrumento, como el trabajo del escritor y su manera de escribir. El concepto de estilo surgió en literatura, pero pronto se extendió al resto de artes, especialmente música y danza. Actualmente se emplea este término en su sentido metonímico, es decir, como aquella cualidad que identifica la forma de trabajar, de expresarse o de concebir una obra de arte por parte del artista, o bien, en sentido más genérico, de un conjunto de artistas u obras que tienen diversos puntos en común, agrupados geográfica o cronológicamente. Así, el estilo puede ser tanto un conjunto de caracteres formales, bien individuales –la forma de escribir, de componer o de elaborar una obra de arte por parte de un artista–, o bien colectivos –de un grupo, una época o un lugar geográfico–, como un sistema orgánico de formas, en que sería la conjunción de determinados factores la que generaría la forma de trabajar del grupo, como en el arte románico, gótico, barroco, etc. Según Focillon, un estilo es «un conjunto coherente de formas unidas por una conveniencia recíproca, sumisas a una lógica interna que las organiza». 

Estos caracteres individuales o sociales son signos distintivos que permiten diferenciar, definir y catalogar de forma empírica la obra de un artista o un grupo de artistas adscritos a un mismo estilo o “escuela” –término que designa un grupo de autores con características comunes definitorias–. Así, la “estilística” es la ciencia que estudia los diversos signos distintivos, objetivos y unívocos, de la obra de un artista o escuela. Este estudio ha servido en la Historia del arte como punto de partida para el análisis del devenir histórico artístico basado en el estilo, como se puede apreciar en alguna escuela historiográfica como el formalismo.[60]​

El estilo estudia al artista y a la obra de arte como materialización de una idea, plasmada en la materia a través de la técnica, lo que constituye un lenguaje formal susceptible de análisis y de catalogación y periodificación. Por otro lado, así como la similitud de formas crean un lenguaje y, por tanto, un estilo, una misma forma puede tener distinta significación en diversos estilos. Así, los estilos están sujetos a una dinámica evolutiva que suele ser cíclica, recurrente, perceptible en mayor o menor grado en cada periodo histórico. Se suelen distinguir en cada estilo, escuela o periodo artístico diversas fases –con las naturales variaciones concretas en cada caso–: “fase preclásica”, donde se comienzan a configurar los signos distintivos de cada estilo concreto –se suelen denominar con los prefijos ‘proto’ o ‘pre’, como el prerromanticismo–; “fase clásica”, donde se concretan los principales signos característicos del estilo, que servirán de puntos de referencia y supondrán la materialización de sus principales realizaciones; “fase manierista”, donde se reinterpretan las formas clásicas, elaboradas desde un punto de vista más subjetivo por parte del autor; “fase barroca”, que es una reacción contra las formas clásicas, deformadas a gusto y capricho del artista; “fase arcaizante”, donde se vuelve a las formas clásicas, pero ya con la evidente falta de naturalidad que le es intrínseca –se suele denominar con el prefijo ‘post’, como el postimpresionismo–; y “fase recurrente”, donde la falta de referentes provoca una tendencia al eclecticismo –se suelen denominar con el prefijo ‘neo’, como el neoclasicismo–.[61]​

Un género artístico es una especialización temática en que se suelen dividir las diversas artes. Antiguamente se denominaba “pintores de género” a los que se ocupaban de un solo tema: retratos, paisajes, pinturas de flores, animales, etc. El término tenía un cierto sentido peyorativo, ya que parecía que el artista que trataba solo esos asuntos no valía para otros, y se contraponía al “pintor de historia”, que en una sola composición trataba diversos elementos (paisaje, arquitectura, figuras humanas). En el siglo XVIII, el término se aplicó al pintor que representaba escenas de la vida cotidiana, opuesto igualmente al pintor de historia, que trataba temas históricos, mitológicos, etc. En cambio, en el siglo XIX, al perder la pintura de historia su posición privilegiada, se otorgó igual categoría a la historia que al paisaje, retrato, etc. Entonces, la pintura de género pasó a ser la que no trataba las principales cuatro clases reconocidas: historia, retrato, paisaje y marina. Así, un pintor de género era el que no tenía ningún género definido. Por último, al eliminar cualquier jerarquía en la representación artística, actualmente se considera pintura de género cualquier obra que represente escenas de la vida cotidiana, temas anecdóticos, al tiempo que aún se habla de géneros artísticos para designar los diversos temas que han sido recurrentes en la Historia del arte (paisaje, retrato, desnudo, bodegón), haciendo así una síntesis entre los diversos conceptos anteriores.[62]​

La pintura, como elemento bidimensional, necesita un soporte (muro, madera, lienzo, cristal, metal, papel, etc.); sobre este soporte se pone el pigmento (colorante + aglutinante). Es el aglutinante el que clasifica los distintos procedimientos pictóricos:

Según el material, se puede trabajar en tres sistemas: “aditivo”, modelando y añadiendo materia, generalmente en materias blandas (cera, plastilina, barro); “sustractivo”, eliminando materia hasta descubrir la figura, generalmente en materiales duros (piedra, mármol, madera, bronce, hierro); y “mixto”, añadiendo y quitando. También se puede hacer por fundición, a través de un molde. Hecha la escultura, se puede dejar al natural o policromarla, con colorantes vegetales o minerales o en encausto, al temple o al óleo, en dorado o estofado (imitación de oro).

Existen diversos tipos de vidrio: “vidrio sódico” (el más básico, a partir de sílice), cristal (sílice y óxido de plomo o potasio), “vidrio calcedonio” (sílice y óxidos metálicos) y “vidrio lácteo” (sílice, bióxido de manganeso y óxido de estaño). La principal técnica para trabajarlo es el soplado, donde se le puede dar cualquier forma y espesor. En cuanto a la decoración, puede ser pintada, esgrafiada, tallada, con pinzas, a filigrana, etc.[68]​

Se realiza con arcilla, en cuatro clases: barro cocido poroso rojo-amarillento (alfarería, terracota, bizcocho); barro cocido poroso blanco (loza); barro cocido no poroso gris, pardo o marrón (gres); barro cocido compacto no poroso blanco medio transparente (porcelana). Se puede elaborar de forma manual o mecánica —con torno—, después se cuece en el horno –a temperaturas entre 400° y 1300°, según el tipo–, y se decora con esmalte o pintura.[69]​

Es el arte de confeccionar objetos decorativos con metales nobles o piedras preciosas, como el oro, plata, diamante, perla, ámbar, coral, etc. 

Se hace con hierro (limonita, pirita o magnetita), reduciéndolo con calor, saliendo una pasta al rojo con la que se hacen lingotes. Hay tres clases: “colado”, con mucho carbono, sílice, azufre y manganeso, no sirve para forjar, solo para fundir en molde; “hierro dulce o forjado”, con menos carbono, es más maleable y dúctil, se puede forjar, pero es blando y desafilable; “acero”, con manganeso, tungsteno, cobalto y wolframio, es más duro, para instrumentos cortantes. El modelado se realiza sin añadir ni quitar material, sino que existen diversas técnicas alternativas: estirar, ensanchar, hendir, curvar, recalcar, etc.

La restauración de obras de arte es una actividad que tiene por objeto la reparación o actuación preventiva de cualquier obra que, debido a su antigüedad o estado de conservación, sea susceptible de ser intervenida para preservar su integridad física, así como sus valores artísticos, respetando al máximo la esencia original de la obra.[71]​

En arquitectura, la restauración suele ser de tipo funcional, para preservar la estructura y unidad del edificio, o reparar grietas o pequeños defectos que puedan surgir en los materiales constructivos. Hasta el siglo XVIII, las restauraciones arquitectónicas solo preservaban las obras de culto religioso, dado su carácter litúrgico y simbólico, reconstruyendo otro tipo de edificios sin respetar siquiera el estilo original. Sin embargo, desde el auge de la arqueología a finales del siglo XVIII, especialmente con las excavaciones de Pompeya y Herculano, se tendió a preservar en la medida de lo posible cualquier estructura del pasado, siempre y cuando tuviese un valor artístico y cultural. Aun así, en el siglo XIX los ideales románticos llevaron a buscar la pureza estilística del edificio, y la moda del historicismo llevó a planteamientos como los de Viollet-le-Duc, defensor de la intervención en monumentos en función de cierto ideal estilístico. En la actualidad, se tiende a preservar al máximo la integridad de los edificios históricos.

En el terreno de la pintura, se ha evolucionado desde una primera perspectiva de intentar recuperar la legibilidad de la imagen, añadiendo si fuese necesario partes perdidas de la obra, a respetar la integridad tanto física como estética de la obra de arte, haciendo las intervenciones necesarias para su conservación sin que se produzca una transformación radical de la obra. La restauración pictórica adquirió un creciente impulso a partir del siglo XVII, debido al mal estado de conservación de pinturas al fresco, técnica bastante corriente en la Edad Media y el Renacimiento. Igualmente, el aumento del mercado de las antigüedades propició la restauración de obras antiguas cara a su posterior comercialización. Por último, en escultura ha habido una evolución paralela: desde la reconstrucción de obras antiguas, generalmente en cuanto a miembros mutilados (como en la reconstrucción del Laocoonte en 1523-1533 por parte de Giovanni Angelo Montorsoli), hasta la actuación sobre la obra preservando su estructura original, manteniendo en caso necesario un cierto grado de reversibilidad de la actuación practicada.[72]​

La estética es una rama de la filosofía que se encarga de estudiar la manera cómo el razonamiento del ser humano interpreta los estímulos sensoriales que recibe del mundo circundante. Se podría decir, así como la lógica estudia el conocimiento racional, que la estética es la ciencia que estudia el conocimiento sensible, el que adquirimos a través de los sentidos.[73]​ Entre los diversos objetos de estudio de la estética figuran la belleza o los juicios de gusto, así como las distintas maneras de interpretarlos por parte del ser humano. Por tanto, la estética está íntimamente ligada al arte, analizando los diversos estilos y periodos artísticos conforme a los diversos componentes estéticos que en ellos se encuentran. A menudo se suele denominar la estética como una “filosofía del arte”. La estética es una reflexión filosófica que se hace sobre objetos artísticos y naturales, y que produce un “juicio estético”. La percepción sensorial, una vez analizada por la inteligencia humana, produce ideas, que son abstracciones de la mente, y que pueden ser objetivas o subjetivas. Las ideas provocan juicios, al relacionar elementos sensoriales; a su vez, la relación de juicios es razonamiento. El objetivo de la estética es analizar los razonamientos producidos por dichas relaciones de juicios.[74]​

El término estética proviene del griego αἴσθησις (aísthêsis, ‘sensación’). Fue introducido por el filósofo alemán Alexander Gottlieb Baumgarten en su obra Reflexiones filosóficas acerca de la poesía (1735), y más tarde en su Aesthetica (1750).[75]​ Así pues, la Historia de la estética, rigurosamente hablando, comenzaría con Baumgarten en el siglo XVIII, sobre todo con la sistematización de esta disciplina realizada por Immanuel Kant. Sin embargo, el concepto es extrapolable a los estudios sobre el tema efectuados por los filósofos anteriores, especialmente desde la Grecia clásica. Cabe señalar, por ejemplo, que los antiguos griegos tenían un vocablo equiparable al actual concepto de estética, que era Φιλοκαλία (filocalía, ‘amor a la belleza’). Se podría decir que en Grecia nació la estética como concepto, mientras que con Baumgarten se convierte en una ciencia filosófica.

Según Arnold Hauser, las «obras de arte son provocaciones con las cuales polemizamos», pero que no nos explicamos. Las interpretamos de acuerdo con nuestras propias finalidades y aspiraciones, les trasladamos un sentido cuyo origen está en nuestras formas de vida y hábitos mentales. Nosotros, «de todo arte con el cual tenemos una relación auténtica hacemos un arte moderno». Hoy día, el arte ha establecido unos conjuntos de relaciones que permiten englobar dentro de una sola interacción la obra de arte, el artista o creador y el público receptor o destinatario. Hegel, en su Estética, intentó definir la trascendencia de esta relación diciendo que «la belleza artística es más elevada que la belleza de la naturaleza, ya que cambia las formas ilusorias de este mundo imperfecto, donde la verdad se esconde tras las falsas apariencias para alcanzar una verdad más elevada creada por el espíritu».

El arte es también un juego con las apariencias sensibles, los colores, las formas, los volúmenes, los sonidos, etc. Es un juego gratuito donde se crea de la nada o de poco más que la nada una apariencia que no pretende otra cosa que engañarnos. Es un juego placentero que satisface nuestras necesidades eternas de simetría, de ritmo o de sorpresa. La sorpresa que para Baudelaire es el origen de la poesía. Así, según Kant, el placer estético deriva menos de la intensidad y la diversidad de sensaciones, que de la manera, en apariencia espontánea, por la cual ellas manifiestan una profunda unidad, sensible en su reflejo, pero no conceptualizable.

Para Ernst Gombrich, «en realidad el arte no existe: solo hay artistas». Más adelante, en la introducción de su obra La historia del arte, dice que no tiene nada de malo que nos deleitemos en el cuadro de un paisaje porque nos recuerda nuestra casa, o en un retrato porque nos recuerda un amigo, ya que, como humanos que somos, cuando miramos una obra de arte estamos sometidos a un conjunto de recuerdos que para bien o para mal influyen sobre nuestros gustos. Siguiendo a Gombrich, se puede ver cómo a los artistas también les sucede algo parecido: en el Retrato de un niño (Nicholas Rubens), el pintor flamenco Rubens lo representó hermoso, ya que seguramente se sentía orgulloso del aspecto del niño, y nos quiso transmitir su pasión de padre a la vez que de artista; en el Retrato de la madre, el pintor alemán Alberto Durero la dibujó con la misma devoción y amor que Rubens sentía por su hijo, pero aquí vemos un estudio fiel de la cara de una mujer vieja, no hay belleza natural, pero Durero, con su enorme sinceridad, creó una gran obra de arte.

La sociología del arte es una disciplina de las ciencias sociales que estudia el arte desde un planteamiento metodológico basado en la sociología. Su objetivo es estudiar el arte como producto de la sociedad humana, analizando los diversos componentes sociales que concurren en la génesis y difusión de la obra artística. La sociología del arte es una ciencia multidisciplinar, recurriendo para sus análisis a diversas disciplinas como la cultura, la política, la economía, la antropología, la lingüística, la filosofía, y demás ciencias sociales que influyan en el devenir de la sociedad. Entre los diversos objetos de estudio de la sociología del arte se encuentran varios factores que intervienen desde un punto de vista social en la creación artística, desde aspectos más genéricos como la situación social del artista o la estructura sociocultural del público, hasta más específicos como el mecenazgo, el mercantilismo y comercialización del arte, las galerías de arte, la crítica de arte, el coleccionismo, la museografía, las instituciones y fundaciones artísticas, etc.[76]​ También cabe remarcar en el siglo XX la aparición de nuevos factores como el avance en la difusión de los medios de comunicación, la cultura de masas, la categorización de la moda, la incorporación de nuevas tecnologías o la apertura de conceptos en la creación material de la obra de arte (arte conceptual, arte de acción).

La sociología del arte debe sus primeros planteamientos al interés de diversos historiadores por el análisis del entorno social del arte desde mediados del siglo XIX, sobre todo tras la irrupción del positivismo como método de análisis científico de la cultura, y la creación de la sociología como ciencia autónoma por Auguste Comte. Sin embargo, la sociología del arte se desarrolló como disciplina particular durante el siglo XX, con su propia metodología y sus objetos de estudio determinados. Principalmente, el punto de partida de esta disciplina se suele situar inmediatamente después de la Segunda Guerra Mundial, con la aparición de diversas obras decisivas en el desarrollo de esta corriente disciplinar: Arte y revolución industrial, de Francis Klingender (1947); La pintura florentina y su ambiente social, de Friedrich Antal (1948); e Historia social de la literatura y el arte, de Arnold Hauser (1951). En sus inicios, la sociología del arte estuvo estrechamente vinculada al marxismo —como los propios Hauser y Antal, o Nikos Hadjinikolaou, autor de Historia del arte y lucha de clases (1973)—, si bien luego se desmarcó de esta tendencia para adquirir autonomía propia como ciencia. Otros autores destacados de esta disciplina son Pierre Francastel, Herbert Read, Francis Haskell, Michael Baxandall, Peter Burke, Giulio Carlo Argan, etc.[77]​

La psicología del arte es la ciencia que estudia los fenómenos de la creación y la apreciación artística desde una perspectiva psicológica. El arte es, como manifestación de la actividad humana, susceptible de ser analizado de forma psicológica, estudiando los diversos procesos mentales y culturales que en la génesis del arte se encuentran, tanto en su creación como en su recepción por parte del público. A su vez, como fenómeno de la conducta humana, puede servir como base de análisis de la conciencia humana, siendo la percepción estética un factor distintivo del ser humano como especie, que lo aleja de los animales. La psicología del arte es una ciencia interdisciplinar, que debe recurrir forzosamente a otras disciplinas científicas para poder efectuar sus análisis, desde –lógicamente– la Historia del arte, hasta la filosofía y la estética, pasando por la sociología, la antropología, la neurobiología, etc. También está estrechamente conectada con el resto de ramas de la psicología, desde el psicoanálisis hasta la psicología cognitiva, evolutiva o social, o bien la psicobiología y los estudios de personalidad. Asimismo, a nivel fisiológico, la psicología del arte estudia los procesos básicos de la actividad humana —como la percepción, la emoción y la memoria—, así como las funciones superiores del pensamiento y el lenguaje. Entre sus objetos de estudio se encuentran tanto la percepción del color (recepción retiniana y procesamiento cortical) y el análisis de la forma, como los estudios sobre creatividad, capacidades cognitivas (símbolos, iconos), el arte como terapia, etc. Para el desarrollo de esta disciplina han sido esenciales las contribuciones de Sigmund Freud, Gustav Fechner, la Escuela de la Gestalt (dentro de la que destacan los trabajos de Rudolf Arnheim), Lev Vygotski, Howard Gardner, etc.[78]​

Una de las principales corrientes de la psicología del arte ha sido la Escuela de la Gestalt, que afirma que estamos condicionados por nuestra cultura –en sentido antropológico–, tanto que la cultura condiciona nuestra percepción. Toma un punto de partida con la obra de Karl Popper, quien afirmó que en la apreciación estética hay un punto de inseguridad (gusto), que no tiene base científica y no se puede generalizar; llevamos una idea preconcebida (“hipótesis previa”), que hace que encontremos en el objeto lo que buscamos. Según la Gestalt, la mente configura, a través de ciertas leyes, los elementos que llegan a ella a través de los canales sensoriales (percepción) o de la memoria (pensamiento, inteligencia y resolución de problemas). En nuestra experiencia del medio ambiente, esta configuración tiene un carácter primario sobre los elementos que la conforman, y la suma de estos últimos por sí solos no podría llevarnos a la comprensión del funcionamiento mental. Se fundamentan en la noción de estructura, entendida como un todo significativo de relaciones entre estímulos y respuestas, e intentan entender los fenómenos en su totalidad, sin separar los elementos del conjunto, que forman una estructura integrada fuera de la cual dichos elementos no tendrían significación. Sus principales exponentes fueron Rudolf Arnheim, Max Wertheimer, Wolfgang Köhler, Kurt Koffka y Kurt Lewin.[79]​

La crítica de arte es un género, entre literario y académico, que hace una valoración sobre las obras de arte, artistas o exposiciones, en principio de forma personal y subjetiva, pero basándose en la Historia del arte y sus múltiples disciplinas, valorando el arte según su contexto o evolución. Es a la vez valorativa, informativa y comparativa, redactada de forma concisa y amena, sin pretender ser un estudio académico pero aportando datos empíricos y contrastables. Denis Diderot es considerado el primer crítico de arte moderno, por sus comentarios sobre las obras de arte expuestas en los salones parisinos, realizados en el Salón Carré del Louvre desde 1725. Estos salones, abiertos al público, actuaron como centro difusor de tendencias artísticas, propiciando modas y gustos en relación al arte, por lo que fueron objeto de debate y crítica. Diderot escribió sus impresiones sobre estos salones primero en una carta escrita en 1759, que fue publicada en la Correspondance littéraire de Grimm, y desde entonces hasta 1781, siendo el punto de arranque del género.[80]​

En la génesis de la crítica de arte hay que valorar, por un lado, el acceso del público a las exposiciones artísticas, que unido a la proliferación de los medios de comunicación de masas desde el siglo XVIII produjo una vía de comunicación directa entre el crítico y el público al que se dirige. Por otro lado, el auge de la burguesía como clase social que invirtió en el arte como objeto de ostentación, y el crecimiento del mercado artístico que llevó consigo, propiciaron el ambiente social necesario para la consolidación de la crítica artística. La crítica de arte ha estado generalmente vinculada al periodismo, ejerciendo una labor de portavoces del gusto artístico que, por una parte, les ha conferido un gran poder, al ser capaces de hundir o encumbrar la obra de un artista, pero por otra les ha hecho objeto de feroces ataques y controversias. Otra faceta a remarcar es el carácter de actualidad de la crítica de arte, ya que se centra en el contexto histórico y geográfico en el que el crítico desarrolla su labor, inmersa en un fenómeno cada vez más dinámico como es el de las corrientes de moda. Así, la falta de historicidad para emitir un juicio sobre bases consolidadas, lleva a la crítica de arte a estar frecuentemente sustentada en la intuición del crítico, con el factor de riesgo que ello conlleva. Sin embargo, como disciplina sujeta a su tiempo y a la evolución cultural de la sociedad, la crítica de arte siempre revela un componente de pensamiento social en el que se ve inmersa, existiendo así diversas corrientes de crítica de arte: romántica, positivista, fenomenológica, semiológica, etc.[81]​

Entre los críticos de arte ha habido desde famosos escritores hasta los propios historiadores del arte, que muchas veces han pasado del análisis metodológico a la crítica personal y subjetiva, conscientes de que era un arma de gran poder hoy día. Como nombres, se podría citar a Charles Baudelaire, John Ruskin, Oscar Wilde, Émile Zola, Joris-Karl Huysmans, Guillaume Apollinaire, Wilhelm Worringer, Clement Greenberg, Michel Tapié, etc.; en el mundo hispanohablante, destacan Eugeni d'Ors, Aureliano de Beruete, Jorge Romero Brest, Juan Antonio Gaya Nuño, Alexandre Cirici, Juan Eduardo Cirlot, Enrique Lafuente Ferrari, Rafael Santos Torroella, Francisco Calvo Serraller, José Corredor Matheos, Irma Arestizábal, Ticio Escobar, Raúl Zamudio, etc.[83]​

La historiografía del arte es la ciencia que analiza el estudio de la Historia del arte, desde un punto de vista metodológico, es decir, de la forma cómo el historiador afronta el estudio del arte, las herramientas y disciplinas que le pueden ser de utilidad para este estudio. El mundo del arte siempre ha llevado en paralelo un componente de autorreflexión, desde antiguo los artistas, u otras personas a su alrededor, han plasmado por escrito diversas reflexiones sobre su actividad. Vitruvio escribió el tratado sobre arquitectura más antiguo que se conserva, De Architectura. Su descripción de las formas arquitectónicas de la antigüedad grecorromana influyó poderosamente en el Renacimiento, siendo a la vez una importante fuente documental por las informaciones que aporta sobre la pintura y la escultura griegas y romanas.[84]​ Giorgio Vasari, en Vida de los más excelentes arquitectos, pintores y escultores italianos desde Cimabue hasta nuestros tiempos (1542–1550), fue uno de los predecesores de la historiografía del arte, haciendo una crónica de los principales artistas de su tiempo, poniendo especial énfasis en la progresión y el desarrollo del arte. Sin embargo, estos escritos, generalmente crónicas, inventarios, biografías u otros escritos más o menos literarios, carecían de perspectiva histórica y el rigor científico necesarios para ser considerados historiografía del arte.[85]​

Johann Joachim Winckelmann es considerado el padre de la Historia del arte, creando una metodología científica para la clasificación de las artes y basando la Historia del arte en una teoría estética de influencia neoplatónica: la belleza es el resultado de una materialización de la idea. Gran admirador de la cultura griega, postuló que en la Grecia antigua se dio la belleza perfecta, generando un mito sobre la perfección de la belleza clásica que aún condiciona la percepción del arte hoy día. En Reflexión sobre la imitación de las obras de arte griegas (1755) afirmó que los griegos llegaron a un estado de perfección total en la imitación de la naturaleza, por lo que nosotros solo podemos imitar a los griegos. Asimismo, relacionó el arte con las etapas de la vida humana (infancia, madurez, vejez), estableciendo una evolución del arte en tres estilos: arcaico, clásico y helenístico.[86]​

Durante el siglo XIX, la nueva disciplina buscó una formulación más práctica y rigurosa, sobre todo desde la aparición del positivismo. Sin embargo, esta tarea se abordó desde diversas metodologías que supusieron una gran multiplicidad de tendencias historiográficas: el romanticismo impuso una visión historicista y revivalista del pasado, rescatando y poniendo nuevamente de moda estilos artísticos que habían sido minusvalorados por el neoclasicismo winckelmanniano; así lo vemos en la obra de Ruskin, Viollet-le-Duc, Goethe, Schlegel, Wackenroder, etc. En cambio, la obra de autores como Karl Friedrich von Rumohr, Jacob Burckhardt o Hippolyte Taine, supuso un primer intento serio de formular una Historia del arte basada en criterios científicos, basándose en el análisis crítico de las fuentes historiográficas. Por otro lado, Giovanni Morelli introdujo el concepto del connoisseur, el experto en arte, que lo analiza en base tanto a sus conocimientos como a su intuición.[87]​

La primera escuela historiográfica de gran relevancia fue el formalismo, que defendía el estudio del arte a partir del estilo, aplicando una metodología evolucionista que otorgaba al arte una autonomía alejada de cualquier consideración filosófica, rechazando la estética romántica y el idealismo hegeliano, y acercándose al neokantismo. Su principal teórico fue Heinrich Wölfflin, considerado el padre de la moderna Historia del arte. Aplicó al arte criterios científicos, como el estudio psicológico o el método comparativo: definía los estilos por las diferencias estructurales inherentes a los mismos, como argumentó en su obra Conceptos fundamentales de la Historia del Arte (1915). Wölfflin no otorgaba importancia a las biografías de los artistas, defendiendo en cambio la idea de nacionalidad, de escuelas artísticas y estilos nacionales. Las teorías de Wölfflin fueron continuadas por la llamada Escuela de Viena, con autores como Alois Riegl, Max Dvořák, Hans Sedlmayr y Otto Pächt.[88]​

Ya en el siglo XX, la historiografía del arte ha continuado dividida en múltiples tendencias, desde autores aún enmarcados en el formalismo (Roger Fry, Henri Focillon), pasando por las escuelas sociológica (Friedrich Antal, Arnold Hauser, Pierre Francastel, Giulio Carlo Argan) o psicológica (Rudolf Arnheim, Max Wertheimer, Wolfgang Köhler), hasta perspectivas individuales y sintetizadoras como las de Adolf Goldschmidt o Adolfo Venturi. Una de las escuelas más reconocidas ha sido la de la iconología, que centra sus estudios en la simbología del arte, en el significado de la obra artística. A través del estudio de imágenes, emblemas, alegorías y demás elementos de significación visual, pretenden esclarecer el mensaje que el artista pretendió transmitir en su obra, estudiando la imagen desde postulados mitológicos, religiosos o históricos, o de cualquier índole semántica presente en cualquier estilo artístico. Los principales teóricos de este movimiento fueron Aby Warburg, Erwin Panofsky, Ernst Gombrich, Rudolf Wittkower y Fritz Saxl.[89]​

En Egipto y Mesopotamia surgieron las primeras civilizaciones, y sus artistas/artesanos elaboraron complejas obras de arte que suponen ya una especialización profesional. 

Entre finales del siglo XVIII y principios del XIX se sentaron las bases de la sociedad contemporánea, marcada en el terreno político por el fin del absolutismo y la instauración de gobiernos democráticos –impulso iniciado con la Revolución francesa–; y, en lo económico, por la Revolución industrial y el afianzamiento del capitalismo, que tendrá respuesta en el marxismo y la lucha de clases. En el terreno del arte, comienza una dinámica evolutiva de estilos que se suceden cronológicamente cada vez con mayor celeridad, que culminará en el siglo XX con una atomización de estilos y corrientes que conviven y se contraponen, se influyen y se enfrentan.

El arte del siglo XX padece una profunda transformación: en una sociedad más materialista, más consumista, el arte se dirige a los sentidos, no al intelecto. Igualmente, cobra especial relevancia el concepto de moda, una combinación entre la rapidez de las comunicaciones y el aspecto consumista de la civilización actual. Surgen así los movimientos de vanguardia, que pretenden integrar el arte en la sociedad, buscando una mayor interrelación artista-espectador, ya que es este último el que interpreta la obra, pudiendo descubrir significados que el artista ni conocía. Las últimas tendencias artísticas pierden incluso el interés por el objeto artístico: el arte tradicional era un arte de objeto, el actual de concepto. Hay una revalorización del arte activo, de la acción, de la manifestación espontánea, efímera, del arte no comercial (arte conceptual, happening, environment).

La Biblia (del latín biblĭa, y este del griego βιβλία biblía, ‘libros’)[1]​ es un conjunto de libros canónicos que en el cristianismo y en otras religiones, se consideran producto de inspiración divina y un reflejo o registro de la relación entre Dios y la humanidad. La Biblia está organizada por dos partes principales; Antiguo Testamento (Tanaj, libros sagrados canónicos en el judaísmo) y el Nuevo Testamento que se enfoca en Jesucristo y el cristianismo primitivo.

Fue en el Concilio de Roma del año 382, cuando la Iglesia católica junto al papa Dámaso I instituyeron el Canon Bíblico con la lista del Nuevo Testamento similar al de Atanasio de Alejandría y los libros del Antiguo Testamento de la Versión de los LXX. Esta versión fue traducida del griego al latín por Jerónimo (la Vulgata) por encargo de la Iglesia. Posteriormente los Concilios regionales III de Hipona del 393, III de Cartago del 397 y IV de Cartago del 419, en los cuales participó Agustín de Hipona, aprobaron definitivamente dicho canon. En el año 405 esta lista fue enviada por Inocencio al obispo Exuperio de Tolosa (en la Galia, hoy Francia), donde aparece el canon bíblico con los 73 libros ya existentes. El concilio de Trento fijó el canon de la Iglesia católica declarándolo dogma.[2]​

Se estima que a lo largo de los siglos se han producido alrededor de cinco mil millones de copias de la Biblia en todas sus variedades (aunque algunos las cifran en muchas más[3]​), la mayoría en las últimas décadas (tres mil novecientos millones entre los años 1960 y 2013[4]​), lo que la convierte en el libro más distribuido y vendido de la historia, siendo frecuentemente reconocido como el libro más influyente de todos los tiempos.[5]​[6]​[7]​

Se atribuye el gran éxito de su distribución en los últimos tiempos a la imprenta, habiendo sido el primer libro realizado por medio de la impresión con tipos móviles (la conocida como Biblia de Gutenberg).[8]​ En mayo de 2000, se afirmó que «la Biblia ha hecho más para dar forma a la literatura, la cultura y el entretenimiento, que ningún otro libro que se haya escrito. Su influencia en la historia mundial no tiene equiparable, y no tiene síntomas de estar menguando».[9]​ Cada año se venden unos cien millones de ejemplares de la Biblia,[10]​[11]​ habiendo sido traducida a 438 idiomas en su totalidad (Antiguo Testamento, Nuevo Testamento y textos adicionales), y de forma parcial al menos a 2454 idiomas.[12]​[13]​

La palabra Biblia procede, a través del latín biblĭa, de la expresión griega τὰ βιβλία τὰ ἅγια (ta biblía ta hágia; ‘los libros sagrados’), acuñada por primera vez en el deuterocanónico 1 Macabeos 12:9,[14]​ donde βιβλία es el plural de βιβλίον (biblíon, ‘papiro’ o ‘rollo’ y, por extensión, ‘libro’).[15]​ Se cree que este nombre nació como diminutivo del nombre de la ciudad de Biblos (Βύβλος, Býblos), importante mercado de papiros de la antigüedad.[16]​

No obstante, ya que Biblos solamente con dificultad podría ser un préstamo del nombre original de dicha ciudad en fenicio, Gubla, existe la posibilidad de que fuera la ciudad la que recibiera su nombre griego a partir del término que designaba a la planta de papiro, y no al revés.[17]​

Dicha expresión fue empleada por los hebreos helenizados (aquellos que habitaban en ciudades de habla griega) mucho tiempo antes del nacimiento de Jesús de Nazaret para referirse al Tanaj o Antiguo Testamento. Muchos años después empezó a ser utilizada por los cristianos para referirse al conjunto de libros que forman el Antiguo Testamento, así como los Evangelios y las cartas apostólicas (es decir, el Nuevo Testamento). Por entonces, ya era común utilizar únicamente el primer sintagma, τὰ βιβλία, a manera de título.

Ya como título, se empezó a utilizar en latín Biblia Sacra (‘los libros sagrados’), sin artículo, pues este no existía en latín. Sin embargo, al ser Biblia un cultismo en latín, acabó pasando de considerarse un neutro plural a un femenino singular («la Sagrada Biblia»), entendiendo ya Biblia como el nombre propio de todo el conjunto. A través del latín se derivó a la gran mayoría de las lenguas modernas.

Los libros bíblicos fueron escritos inicialmente en distintas lenguas, llamadas lenguas bíblicas (hebreo, arameo y griego helenístico). En distintas épocas históricas fueron traducidos de unas de ellas a otras, y posteriormente a las demás.

Biblia hebrea o Biblia hebraica es un término genérico para referirse a los libros de la Biblia escritos originalmente en hebreo y arameo antiguos. Se ajusta muy estrechamente al concepto judío Tanaj y al cristiano Antiguo Testamento (particularmente en la versión de algunos grupos cristianos (Evangélicos), que no incluyen las partes deuterocanónicas del Antiguo Testamento y el Anagignoskomena ortodoxo).

El término Biblia hebrea no implica ningún género de denominación, numeración u ordenación de libros, que es muy variable. (Véase Canon bíblico).

En el estudio erudito de hoy, es común referirse a las tres ediciones de la obra denominada Biblia hebrea editada por Rudolf Kittel. En este contexto es frecuente la abreviatura BH, o BHK (K por Kittel), o (donde se refieren a las distintas ediciones), BH1, BH2 y BH3.

La Torá o "ley" entendida como "instrucción" es la base de las reglas y regulaciones religiosas judías y consiste en:

El título hebreo proviene de la primera palabra en cada parte excepto Éxodo, donde es la quinta palabra. Los libros contienen 613 mitzvot o mandamientos de Dios, que forman la base de la ley religiosa judía (Halakha).

La Torá describe tres etapas en la relación entre Dios y el hombre. Primero Génesis 1-11 describe la historia general de la creación de la humanidad, la caída y decadencia del hombre a partir de entonces. Los últimos 39 capítulos del Génesis, donde Abraham es elegido como el antepasado de un pueblo numeroso a través del cual será bendecido.  Abraham fue llamado por Dios para ir a Canaán, donde la promesa se repitió a sus descendientes, Isaac, Jacob y José. En los últimos cuatro libros cuenta la historia de Moisés, que vivió cientos de años después de los patriarcas de Egipto y cuenta la historia del éxodo de los israelitas de Egipto, el éxodo del desierto y la renovación del pacto con Dios en el monte Sinaí. La Torá termina con la muerte de Moisés.

Los Nevi'im o "profetas" incluyen algunas escrituras que se refieren a las escrituras históricas de la Biblia. Los libros describen el reinado del Juicio , el establecimiento de la monarquía israelita, la división en dos reinos y profetas que, en nombre de Dios, advierten y juzgan a los reyes y al pueblo de Israel. Las escrituras terminan con la conquista babilónica del reino sureño de Judá. Según la tradición judía, Nevi'im se divide en ocho libros. Esa división no se sigue en las Biblias danesas normales:

Ketuvim, o "los escritos" en hebreo, son 11 libros escritos por varios autores y contienen la literatura de sabiduría israelí. Según la tradición rabínica, muchos de los himnos fueron escritos por David; Se presume que el rey Salomón de joven fue el autor del Cantar de los Cantares, el Libro de Proverbios en la mitad de la vida y el Libro de Eclesiastés en su vejez. El libro de Rut es el único libro bíblico sobre un no judío. Cinco de los libros se llaman "Los cinco rollos" (Megilot) y se leen en voz alta durante las fiestas judías: El Cantar de los Cantares en Pascua; Libro de Rut por shavuot; Libro de las Lamentaciones de tisha b'av; El Libro de Eclesiastés de Sucot; y el Libro de Ester de Purim. En general, las "escrituras" contienen poesía, reflexiones filosóficas sobre la vida, las vidas de los profetas y otros líderes israelitas durante el cautiverio babilónico. Termina con el decreto persa, que permite a los judíos regresar a Jerusalén y reconstruir el templo.

La Biblia griega, comúnmente llamada Biblia Septuaginta o Biblia de los Setenta (ἡ μετάφρασις τῶν ἑβδομήκοντα), y generalmente abreviada simplemente LXX, es una antigua recopilación en griego koiné de los libros hebreos y arameos del Tanaj o Biblia hebrea y otros libros, incluidos algunos escritos originalmente en griego.

Las biblias cristianas están constituidas por escritos hebreos, arameos y griegos, que han sido retomados de la Biblia griega, llamada Septuaginta, y del Tanaj hebreo-arameo, y luego reagrupados bajo el nombre de Antiguo Testamento. A estos se ha sumado una tercera serie de escritos griegos cristianos agrupados bajo el nombre de Nuevo Testamento. Distintos grupos cristianos han debatido largamente sobre la inclusión o exclusión de algunos de los libros de ambos testamentos, surgiendo los conceptos de apócrifos y deuterocanónicos para hacer referencia a algunos de estos textos.

La comunidad judía actual reserva la expresión «Biblia cristiana» para identificar solo a los libros que han sido añadidos al Tanaj hebreo-arameo por el judaísmo tardío helenizante alejandrino, y luego por el cristianismo, y evita referirse a su Tanaj con los términos «Biblia» o «Antiguo Testamento». Varias denominaciones cristianas incorporan otros libros en el canon de ambos Testamentos.

La biblia protestante versión Reina Valera cuenta con 66 libros, divididos entre el Antiguo Testamento que comprenden 39 libros y el Nuevo Testamento 27 libros.

El Antiguo Testamento es la serie de textos sagrados israelitas anteriores a Cristo, y que es aceptada por todos los cristianos como primera parte de las biblias cristianas. En términos generales, no existe un consenso general entre los diferentes grupos de cristianos sobre si el canon del Antiguo Testamento debe corresponder al de la Biblia griega, con deuterocanónicos, que es lo que plantean las iglesias cristianas ortodoxas y católica a través de su historia, o al del Tanaj hebreo, que es lo que plantean los judíos actuales, algunos protestantes, y otros grupos cristianos emanados de estos. En total se numeran en el Antiguo Testamento 39 libros en la versión protestante, 46 libros en la versión de la Iglesia católica, y 51 libros en la de la Iglesia ortodoxa. Sin embargo, el orden, nombres y particiones de los libros del Antiguo Testamento de las biblias cristianas, a través de la historia, siguen la usanza griega y no la hebrea. Y, de la misma forma, varía del judaísmo en la interpretación y énfasis (Véase, por ejemplo, el Libro de Isaías, capítulo 7, verso 14).[20]​ Aparte de los libros propios del texto griego de la Biblia, el canon de la Iglesia copta admite otros libros, como el Libro de Enoc y el Libro de los Jubileos.

El Nuevo Testamento es una colección de 27 libros, representativos de 4 diferentes géneros literarios judeocristianos:

La palabra canon significa ‘regla’ o ‘medida’, así que se le llama canon bíblico al conjunto de libros que integran la Biblia según una tradición religiosa concreta, que los considera así «divinamente inspirados» y los distingue de otros textos que no se consideran revelados. Estas diferencias entre las distintas ramas del cristianismo se dan únicamente para el Antiguo Testamento; por ejemplo, según la Iglesia católica son 46 libros, y según la mayoría de iglesias protestantes son 39. Con relación al Nuevo Testamento todas tienen el mismo número de libros.

El primer canon es el Pentateuco, el cual se compone de los libros del Génesis, Éxodo, Levítico, Números y Deuteronomio y contiene la «Ley de Dios», que es el conjunto de los 613 preceptos del judaísmo (Mitzvá).

Dentro del judaísmo surge disputa sobre el canon correcto. Un grupo religioso, los saduceos, sostiene que solamente conforma el canon de las Escrituras la Torá (‘la Ley’) o Pentateuco (‘cinco libros’), mientras que otros grupos también incluyen los Nevi'im (Profetas) y los Ketuvim (los Escritos). Después de la destrucción de Jerusalén en el año 70 d. C., el grupo judío predominante fue el de los fariseos, que sí considera al canon como conformado por la Ley, los Profetas y los Escritos. Así, a finales del siglo I el judaísmo estableció en Yamnia (Yavne) como canon de sus libros sagrados aquellos que cumplieran tres requisitos: que hubiera una copia del libro en cuestión que se supiera que fue escrito antes del año 300 a.C. (cuando la helenización llegó a Judea, con los problemas culturales y religiosos subsecuentes, y que pueden leerse en libros como el Libro de los macabeos o el Libro de Daniel), que dicha copia estuviera escrita en hebreo o cuando menos arameo (no griego, la lengua y cultura invasora) y que tuviera un mensaje considerado como inspirado o dirigido al pueblo de Dios (con lo que también algunos libros que cumplían las dos características anteriores tuvieron que salir del canon).

En tiempos de Jesús de Nazaret es dominante la segunda opinión, la cual es sostenida y transmitida por muchos cristianos hasta tiempos de la Reforma protestante con la controversia de los libros deuterocanónicos (ver «Estructura», ut supra). Esta controversia probablemente se originó precisamente por el hecho de que el judaísmo había establecido su canon a fines del siglo I, con lo que para ellos ya no estaban presentes aquellos textos que solo se encontrarían en griego (en la versión de la Biblia judía de los Setenta). Estos libros fueron precisamente los que se considerarían, posteriormente, como deuterocanónicos.

La versión judía de la Biblia, llamada el Tanaj, consta de 24 libros, con ciertas diferencias respecto a las Biblias cristianas. Algunas de ellas son:

Actualmente, los libros que no son considerados canónicos por católicos y ortodoxos, reciben el nombre de libros apócrifos; a su vez, esos mismos libros suelen ser denominados pseudoepígrafos por los protestantes, que, habitualmente, respetan también el nombre de deuterocanónicos (literalmente, ‘del segundo canon’) para aquellos que han recibido reconocimiento canónico de católicos y ortodoxos (en general, son libros escritos originalmente en griego, incluidos en la traducción al griego de la Biblia judía conocida como Septuaginta o de los LXX). No obstante, algunas corrientes protestantes fundamentalistas insisten en conservar el nombre de apócrifos para los libros deuterocanónicos. Con todo, hay que señalar, que los primeros cristianos no usaban la Biblia hebrea, sino que usaban la Septuaginta o de los LXX por cuanto varios de los nuevos cristianos fueron judíos de cultura griega, como por ejemplo, Pablo de Tarso, Esteban, y los evangelistas Lucas y Marcos.

Así pues, las versiones católicas de la Biblia constan de 73 escritos, en tanto que las más de las versiones protestantes solo contienen 66. Sin embargo, las Biblias de los anabaptistas, luteranos, anglicanos y episcopalianos, incluyen los deuterocanónicos, si bien bajo el rubro de «apócrifos»; ya que los consideran «lectura edificante», pero no canónica. Las versiones ortodoxas, por su parte, incluyen 76 libros en total. Además, la Iglesia copta incluye en su canon del Antiguo Testamento el Libro de Enoc y el Libro de los Jubileos, que no incluye ninguna de las otras corrientes actuales del judeocristianismo, pero que eran libros bastante populares en los tiempos de Cristo; de lo cual han quedado vestigios incluso en los escritos del Nuevo Testamento. La Iglesia siria disponía inicialmente de solo 22 en su Nuevo Testamento, aunque posteriormente acabó aceptando los demás.

La Biblia es una recopilación de textos que en un principio eran documentos separados (llamados «libros»), escritos primero en hebreo, arameo y griego durante un periodo muy dilatado y después reunidos para formar el Tanaj y la Septuaginta (Antiguo Testamento para los cristianos) y luego el Nuevo Testamento. Ambos testamentos forman la Biblia cristiana. En sí, los textos que componen la Biblia fueron escritos a lo largo de aproximadamente 1000 años (entre el 900a.C. y el 100 d.C.). Los textos más antiguos se encuentran en el Libro de los Jueces («Canto de Débora») y en las denominadas fuentes (tradición elohísta) y (tradición yahvista) de la Torá (llamada Pentateuco por los cristianos), que son datadas en la época de los dos reinos (siglos X a VIII a.C.). El libro completo más antiguo, el de Oseas, es también de la misma época. El pueblo judío identifica a la Biblia con el Tanaj, para el que carece de sentido y no es aceptada la denominación como Antiguo Testamento al no aceptar la validez del Nuevo Testamento.

El canon católico de la Biblia que se conoce hoy fue creado por la Iglesia primitiva que, en las Cartas de Ignacio de Antioquia a la Iglesia de Esmirna se menciona como Católica (Universal), bajo el pontificado del papa Dámaso I, en el Sínodo de Roma del año 382, y esta versión es la que Jerónimo de Estridón tradujo al latín. Dicho canon consta de 73 libros: 46 constitutivos del llamado Antiguo Testamento, incluyendo 7 libros llamados actualmente deuterocanónicos (Tobit, Judit, Primer libro de los Macabeos, Segundo libro de los Macabeos, Sabiduría, Eclesiástico (Sirácida), y Baruc) y 27 del Nuevo Testamento. Fue confirmado en el Concilio de Hipona en el año 393, y ratificado en el Concilio III de Cartago (en el año 397), y el IV Concilio de Cartago, en el año 419.

A raíz de la reforma protestante, el concilio de Trento (1546 d.C.) reafirmó el canon bíblico que ya había sido afirmado en concilios previos, por medio de una declaración dogmática en la cuarta sesión del Concilio de Trento, del 8 de abril de 1546. Las definiciones doctrinales del Concilio de Trento no fueron reconocidas ni asumidas por muchos protestantes, surgidos a partir del siglo XVI, ni por distintas denominaciones vinculadas al protestantismo surgidas a partir del siglo XIX. El canon de las biblias cristianas ortodoxas es aún más amplio que el canon bíblico católico, e incluye el Salmo 151, la Oración de Manasés, el Tercer libro de Esdras y el Tercer libro de los Macabeos. En adición a estos, el Cuarto libro de Esdras y el Cuarto libro de los Macabeos figuran, asimismo, como apéndices en muchas importantes versiones y ediciones de la Biblia cristiana ortodoxa.

El Antiguo Testamento narra principalmente la historia de los hebreos y el Nuevo Testamento la vida, muerte y resurrección de Jesús, su mensaje y la historia de los primeros cristianos. El Nuevo Testamento fue escrito en lengua griega koiné. En él se cita con frecuencia al Antiguo Testamento de la versión de los Setenta, traducción al griego del Antiguo Testamento realizada en Alejandría (Egipto) en el siglo III a.C.

Para los creyentes, la Biblia es la palabra de Dios, de inspiración divina, aunque su redacción se realizó a través de hombres elegidos que usaron de sus facultades como verdaderos autores. Se trata de una obra eminentemente espiritual que los creyentes interpretan como la forma que tuvo Dios de revelarse a sí mismo y manifestar su voluntad de salvación de la Humanidad, además de su carácter y atributos.

Para los creyentes cristianos, la Biblia es la principal fuente de fe y doctrina en Cristo. En el siglo XVI los diferentes movimientos de la Reforma protestante comenzaron a experimentar un alto desgaste en discusiones filosóficas y a separarse unos de otros; para menguar este problema se definió el principio llamado «sola escritura», que significa que solamente la Biblia puede ser considerada fuente de doctrina cristiana. Para la Iglesia católica, además de la Biblia, también son fuente doctrinal la Tradición, las enseñanzas de los Padres de la Iglesia (discípulos de los apóstoles), y las decisiones emanadas de los Concilios. Esta divergencia entre cristianos se intensificó después de 1870, cuando el papa Pío IX promulgó la constitución Pastor Aeternus, del Concilio Vaticano I, que reafirma el Primado papal y proclama la infalibilidad del sumo pontífice en asuntos de fe, moral y doctrina cristiana (dogma de la infalibilidad papal) cuando habla ex cathedra (18 de julio de 1870) en cuanto único «sucesor de Pedro» y, consecuentemente, «custodio y depositario de las llaves del Reino de los Cielos». Mientras que los cristianos protestantes rechazan esta aseveración y consideran como cabeza única de la iglesia a Jesucristo. Para ambas partes esta gran diferencia ya no es considerada tan solo en términos filosóficos o religiosos, sino como designios divinos plasmados y asentados en la Biblia misma.

Para los judíos ortodoxos, por supuesto, el Nuevo Testamento no tiene validez. El judaísmo rabínico considera como fuente de doctrina el Talmud, mientras los caraítas defienden desde el siglo VIII el Tanaj como única fuente de fe.

El canon del Antiguo Testamento cristiano entró en uso en la Septuaginta griega, traducciones y libros originales, y sus diferentes listas de los textos. Además de la Septuaginta, el cristianismo posteriormente añadió diversos escritos que se convertirían en el Nuevo Testamento. Poco diferentes listas de las obras aceptadas siguió desarrollando en la antigüedad. En el siglo IV, varios sínodos fueron elaborando listas de escritos sagrados que fijaban un canon del Antiguo Testamento de entre 46 y 54 distintos documentos y un canon del Nuevo Testamento de 20 a 27, siendo este último el utilizado hasta el día de hoy; el cual fue definido finalmente en el Concilio de Hipona en el año 393. Hacia el año 400, Jerónimo había escrito una edición definitiva de la Biblia en latín (véase la Vulgata), el Canon de la cual, debido en parte a la insistencia del papa Dámaso, fue hecho coincidir con decisiones de varios de los Sínodos reunidos con anterioridad. Con el beneficio de la retrospectiva se puede decir que estos procesos establecieron de manera eficaz el canon del Nuevo Testamento, aunque hay otros ejemplos de listas canónicas en uso después de este tiempo. Sin embargo, esta lista definitiva de 27 libros no fue cerrada por ningún Concilio ecuménico sino hasta el Concilio de Trento (1545-63).

Durante la Reforma protestante, algunos reformadores canónicos propusieron diferentes listas de las que se encuentran actualmente en uso en la Iglesia de San Pedro en Roma. Aunque no sin debate, la lista de los libros del Nuevo Testamento vendría a seguir siendo la misma, sin embargo, en el Antiguo Testamento algunos textos presentes en la Septuaginta fueron eliminados de la mayoría de los cánones protestantes. Por lo tanto, en un contexto católico, estos textos se denominan libros deuterocanónicos, mientras que en el contexto protestante, en el que se les llama libros apócrifos, la etiqueta se aplica a todos los textos excluidos del canon bíblico que estaban en la Septuaginta. Cabe señalar también, que tanto católicos como protestantes describen algunos otros libros, como el Libro de los hechos de Pedro, como apócrifos.

Por lo tanto, el Antiguo Testamento protestante de hoy tiene 39 libros —el número varía del número de los libros en el Tanaj (aunque no en contenido) a causa de un método diferente de la división—. También varía el orden y el nombre de los libros, mientras que la Iglesia católica reconoce a 46 libros como parte del Antiguo Testamento canónico. El libro de Enoc es aceptado en el canon del Antiguo Testamento solo por la Iglesia ortodoxa de Etiopía. El término «Escrituras hebreas» es solo sinónimo del Antiguo Testamento protestante (no católico) que contiene las Escrituras hebreas y textos adicionales. En cuanto al canon del Nuevo Testamento, son 27 libros en el canon de la Iglesia católica, aceptado por la mayoría de las Iglesias de la Reforma. La Iglesia siria acepta en la actualidad los 27 libros en su canon. Libros como el Primer libro de Clemente y el Segundo libro de Clemente, el Libro de la Alianza, el Octateuco y otros, han sido motivo de disputas, y se encuentran canonizados por algunas iglesias ortodoxas orientales.

La Biblia describe el desarrollo histórico de un pueblo, los israelitas, durante un largo período de tiempo. La historia comienza en Mesopotamia, donde Dios llama a Abraham para que se establezca en Canaán (hoy el Estado de Israel y Palestina). Debido al hambre, los descendientes de Abraham, Isaac y Jacob viajaron a Egipto. Después de una larga estadía en Egipto, la familia emigró a Canaán, después de 40 años en el desierto bajo el liderazgo de Moisés. La gente ahora tiene la ley. Se ha debatido la datación del evento. El sucesor, Joshua, los condujo a Canaán y dirigió la invasión de la tierra, que estaba habitada por otras tribus. Después de 400 años de cambiar jueces, la gente quería un rey, el rey Saúl, a quien sucedieron el rey David y el rey Salomón. Después del próspero reinado de Salomón, la tierra se dividió en dos partes. Las diez tribus del norte en el "Reino del Norte" (Israel), las dos tribus Judá y Benjamín en el "Reino del Sur" (Judá), donde los descendientes de David se sentaron en el trono hasta el 586 a. C., donde Judá fue tomada por los babilonios y el pueblo exiliado.

Aproximadamente 70 años después, a parte de la población se le permitió regresar y construir el templo y la muralla de la ciudad de Jerusalén. Fue dirigido por Esdras y Nehemías. Desde entonces, el país estuvo en manos de varios gobernantes y unos años bajo un gobierno independiente, hasta que los romanos la incorporaron como provincia de Iudea al Imperio Romano en el año 63 a. C.

Entre los años 7 y 4 a.C. fue Jesús nació bajo Herodes I el Grande, que había ampliado el templo y construido varios palacios y fortalezas grandes en el país y la metrópoli de Cesárea Marítima como un tributo al emperador en Roma. En el año 66 d.C. acaeció el levantamiento judío fallido y en 135 a los judíos se les prohibió el acceso a Jerusalén, que pasó a llamarse Aelia Capitolina.

Todos los escritos del Nuevo Testamento están marcados por Jesucristo como el punto de partida para la fe y la predicación y son la figura central del cristianismo. El evangelio de Marcos se presenta así: "Principio del evangelio de Jesucristo, Hijo de Dios" (Marcos 1:1). Los evangelios hablan de las enseñanzas y obras maravillosas de Jesús; sobre cómo reunió discípulos a su alrededor, entró en conflicto con la sociedad existente, fue acusado de blasfemia; cómo fue crucificado, muerto y resucitado después de tres días. Desde entonces, ha enviado a sus discípulos a predicar el evangelio a todas las naciones. Lo mismo ocurre con las cartas y otros escritos del Nuevo Testamento. Jesucristo juega un papel dominante en los escritos de Pablo. La epístola a los Romanos comienza con: "el evangelio de su Hijo, Jesucristo nuestro Señor" (Rom. 3:1). Los escritores del Nuevo Testamento y la Iglesia consideran que Jesús es el cumplimiento de las profecías mesiánicas del Antiguo Testamento. Según ellos, Jesús "ascendió al cielo, sentado a la diestra de Dios Padre Todopoderoso, de donde vendrá para juzgar a vivos y muertos". 

Las cartas de los apóstoles, según su testimonio de sí mismos, son el intento de realizar el mandato de la misión transmitiendo las enseñanzas de Jesús e interpretando sus enseñanzas. Según Jesús, los apóstoles hablan con su autoridad y sus palabras deben recibir tanta importancia como las suyas.

La arqueología bíblica es la rama de la arqueología que se ocupa de los testimonios bíblicos. Con el tiempo, los arqueólogos han buscado corroborar o socavar la credibilidad de la Biblia a través de excavaciones arqueológicas en el Medio Oriente: restos de edificios y ciudades, hallazgos de textos e inscripciones. Los manuscritos bíblicos se han comparado con otros textos de la comunidad circundante para obtener una mayor conciencia de la Biblia y los textos culturales escritos en ellos. Entre los sitios más importantes se incluyen: el túnel de Hizkias en Jerusalén, las murallas de Jericó, la rampa de asedio de Senaquerib, el estanque de Siloé, el templo en Jerusalén y los Rollos del Mar Muerto en Qumran . Además, hay innumerables excavaciones más pequeñas y hallazgos individuales relacionados con los relatos bíblicos.

La arqueología bíblica muestra que los eventos de la Biblia tienen sus raíces en la historia contemporánea, pero lejos de todo es demostrable arqueológicamente. El arqueólogo William Matthew Flinders Petrie fue el primero en llevar a cabo una excavación científica en Palestina en 1890, durante la cual excavó Tell el-Hesi durante seis semanas, que se identificó incorrectamente con Laquis. Se le llama el padre de la arqueología palestina, categorizando y dividiendo los hallazgos en períodos arqueológicos y tratando de datar a partir de fragmentos de cerámica y estratificación en varias excavaciones. 

Un libro de la Biblia es un grupo establecido de escrituras. Por ejemplo, el Libro de los Salmos (en hebreo Tehilim o ‘canciones de alabanza’) tiene 150 canciones (151 en la versión de los Setenta), mientras que la Epístola de Judas es una carta de media página.

La Biblia hebrea o Tanaj está dividida en tres secciones: los cinco libros de Moisés (la Torá), los libros escritos por los profetas hebreos (los Profetas o Nevi'im) y unos libros que no entran en las dos categorías anteriores (las Escrituras o Ketuvim); estos son conocidos como hagiógrafa o simplemente «las Escrituras».

La Biblia judía fue escrita predominantemente en hebreo, pero tiene algunas pequeñas partes que fueron escritas en arameo. En la Biblia cristiana, la Biblia hebrea es llamada Antiguo Testamento, para distinguirla del Nuevo Testamento, que es la parte que narra la vida de Jesús y su predicación, entre otras cosas. El Nuevo Testamento está dividido en los cuatro Evangelios, historia (Hechos de los Apóstoles), las cartas (epístolas) a iglesias cristianas por Pablo y otros apóstoles, y el Apocalipsis.

Las Biblias cristianas contienen la totalidad del Tanaj (o Antiguo Testamento), junto con un grupo de textos posteriores cristianos, conocidos como el Nuevo Testamento. Dentro del cristianismo no hay acuerdo completo sobre el número exacto de libros que debe tener (con igual reconocimiento) el Antiguo Testamento, es decir, sobre su canon. Hasta el siglo XVI se mantuvo en Occidente la traducción latina de Jerónimo conocida como «la Vulgata» (proveniente del latín vulgar) que incorporaba tanto el canon judío como aquellos escritos de la Septuaginta griega. Con la Reforma protestante, Martín Lutero cuestionó la necesidad de mantener los libros de la Septuaginta junto a los del canon judío y los agrupó como un apéndice considerándolos útiles para la instrucción pero no canónicos al final de su traducción al alemán de la Biblia. La Iglesia católica confirmó, sin embargo, el canon de la Biblia de los Setenta y de la Vulgata en el Concilio de Trento (1545-1563), reconociendo más claramente la canonicidad de algunas escrituras cuestionadas por Lutero, que desde ese mismo siglo comenzaron a ser llamados deuterocanónicos (concepto introducido por Sixto de Siena). Las iglesias orientales también reconocen plena canonicidad a los deuterocanónicos, agregando también otros libros que se encuentran en códices antiguos, como el Salmo 151, la Oración de Manasés, III y IV Esdras, y III y IV Macabeos. La Iglesia copta acepta asimismo en su canon el Libro de Enoc y el Libro de los Jubileos. El Nuevo Testamento hace referencia tanto a los libros deuterocanónicos como al Libro de Enoc, y narra los sucesos de la pasión de Cristo de acuerdo con el cómputo asentado en el Libro de los jubileos. En cuanto al resto de los libros, no hay disputa alguna y todos los grupos cristianos tienen los mismos libros en el Nuevo Testamento de la Biblia.

Dentro del texto bíblico se mencionan algunos libros y epístolas de los cuales no se cuenta con copias reportadas actualmente o solo se conservan fragmentos. Generalmente se les menciona como referencias primarias, escritos de elaboración anterior o como complemento de lo escrito dentro del contexto donde se los menciona. En el caso del Libro de Enoc, este ha venido siendo tenido por apócrifo por la mayoría de religiones (siendo caso contrario el canon de la Iglesia ortodoxa de Etiopía) a pesar de haber sido referenciado en la Biblia y encontrarse en un estado íntegro de contenido.

La siguiente lista parcial muestra algunos de los libros que no están disponibles hoy en día en la mayoría de ediciones bíblicas. Dichos libros son:

Existen opiniones divididas en cuanto a la afirmación de que gran parte de la Biblia se ha conservado sin cambios importantes hasta nuestros días. Actualmente, la creencia común en casi toda la cristiandad supone la infalibilidad y/o inerrancia del texto bíblico, dando por sentado que la Biblia está exenta de todo error, siendo perfecta como palabra de Dios al hombre. Este concepto es similar a la doctrina de la sola scriptura, donde se considera que la Biblia contiene todo lo necesario para la salvación del hombre. En el credo de Nicea se confiesa la creencia de que el Espíritu Santo «ha hablado por medio de los profetas». Este credo ha sido sostenido por los católicos, ortodoxos, anglicanos, luteranos y la mayoría de denominaciones protestantes. Sin embargo, como nota Alister E. McGrath, «los reformadores no vieron conectado el asunto de la inspiración con la absoluta fiabilidad o verdadera inerrancia de los textos bíblicos». Él dice:

Los defensores de la idea de que las escrituras bíblicas son fieles y están completas se basan en la cantidad de copias idénticas que, desde tiempos remotos, se ha realizado de las mismas. Los copistas hebreos de las Escrituras, denominados masoretas, que copiaron las Escrituras hebreas entre los siglos VI y X solían contar las letras para evitar errores.

Quienes no están de acuerdo con estas afirmaciones apelan a circunstancias tales como traducciones de un idioma a otro, copiado de manuscritos, opiniones divergentes en dogmas y/o destrucción deliberada y sostienen por tanto que la Biblia no ha llegado como un volumen completo. Hallazgos tales como los manuscritos del Mar Muerto han mostrado que, en gran parte, esto sucedió antes del siglo I de nuestra era, aunque los textos encontrados allí, y los conocidos hasta entonces, parecen presentar cambios menores. En otros casos, libros tales como los Evangelios apócrifos fueron descartados del canon aceptado durante los concilios ecuménicos, como parte de un esfuerzo por mantener la integridad doctrinal. Casos como el del hallazgo del texto completo del Evangelio de Tomás entre los Manuscritos de Nag Hammadi, entre otros textos tomados por heréticos en su momento, evidencian un proceso editorial paulatino en épocas pasadas. Hay otros textos relevantes relacionados con la Biblia «original» como los escritos apócrifos hallados en Egipto y Cisjordania (Qumrán, cerca del mar Muerto), e incluso en países muy lejanos hacia el Sur y el Oriente. Estos han supuesto una nueva interrogante acerca de si ya estaría completo el canon bíblico, o habría que revisarlo de forma detallada.

Las investigaciones arqueológicas en la zona donde se desarrollan los hechos narrados en la Biblia tienen como un resultado añadido la comprobación de los hechos, lugares y personajes que aparecen citados en los diferentes libros que componen la Biblia. Incluso se ha llegado a crear el término de arqueología bíblica para denominar a una parte de la arqueología que se encarga de estudiar los lugares indicados en la Biblia.

Hay varios casos en que los descubrimientos arqueológicos han señalado congruencias con los hechos o personajes bíblicos. Entre esos descubrimientos se encuentran los siguientes:

La arqueología también ha brindado descubrimientos interesantes con relación a la conformación de los propios textos bíblicos.[cita requerida] Los descubrimientos del Evangelio de Tomás y del Evangelio de Felipe, por ejemplo, reforzaron la Hipótesis de Q. Unos pocos eruditos se inclinan a opinar que el Evangelio de Tomás es más antiguo que los 4 canónicos y que al igual que Mateo y Lucas, tuvo a Q por fuente documental. Conforme a quienes apoyan la hipótesis de la Fuente Q, los evangelios más antiguos serían colecciones de dichos de Jesús que no narrarían la crucifixión ni la resurrección, sino que se preocuparían por mantener el registro de las enseñanzas del Maestro.

La crítica bíblica es el estudio e investigación de los escritos bíblicos que busca discernir juicios sobre estos escritos.[57]​ Viendo los textos bíblicos con un origen humano más que sobrenatural, se pregunta cuándo y dónde se origina un determinado texto. Cómo, por qué, por quién, para quién y en qué circunstancias fue producido, qué influencias existen en su producción, qué fuentes se habrían utilizado en su composición, y qué mensaje se pretende transmitir.

La crítica bíblica varía levemente según se focalice en el Antiguo Testamento, las cartas del Nuevo Testamento o los Evangelios canónicos y juega también un papel importante en la búsqueda del Jesús histórico.

También alude al texto físico, incluyendo el significado de cada palabra y el modo en el que se utiliza cada una de ellas, su preservación, historia e integridad. De hecho, la crítica bíblica es una disciplina que abarca un amplio rango de materias como la arqueología, la antropología, el folclore, la lingüística, las tradiciones orales evangélicas y los estudios religiosos e históricos.

Las traducciones de la Biblia han sido numerosas: a 450 lenguas de forma completa y a más de 2000 de forma parcial, lo que convierte la Biblia en el libro (o conjunto de libros) más traducido de la historia.[58]​ Algunas de ellas han sido trascendentales para el desarrollo de las lenguas y las culturas en que se dieron.

Las primeras traducciones bíblicas comenzaron en el mismo amplio periodo en que sus libros se fueron redactando: el texto masorético en hebreo, los tárgum en arameo y la traducción al griego denominada Biblia de los Setenta (siglo III a II a. C.), que se realizó de los textos originales que componen la Biblia hebrea del judaísmo (Antiguo Testamento para el cristianismo). Los textos del Nuevo Testamento fueron escritos directamente en griego. Esa versión sigue siendo utilizada directamente en buena parte de la cristiandad oriental (iglesia ortodoxa), además de las traducciones de la Biblia a las lenguas eslavas[59]​ (desde la traducción al antiguo eslavo eclesiástico de Cirilo y Metodio, 863) y otras en distintos ámbitos lingüísticos, algunas de las cuales se cuentan entre las más antiguas (Diatessaron[60]​ de Taciano en siríaco, ca. 170, traducciones coptas,[61]​ etíopes,[62]​ el Codex Argenteus del godo Ulfilas, siglo VI, etc.)

Aunque hay traducciones anteriores (las llamadas Vetus Latina), la traducción al latín de San Jerónimo (denominada Vulgata, 382) fue la dominante en la cristiandad occidental hasta la reforma protestante (siglo XVI), y continuó siéndolo en el catolicismo hasta la Edad Contemporánea.[63]​ La crítica filológica del humanismo buscaba la obtención de traducciones depuradas (Biblia políglota complutense impulsada por el cardenal Cisneros, 1514-1522, Textus Receptus iniciado por Erasmo, 1516-1522, continuado por Beza, 1565-1604, Biblia regia o Políglota de Amberes, de Arias Montano, 1568-1572, etc.)

Coincidiendo con la Edad de Oro del islam (siglos VIII al XIII) hubo traducciones de la Biblia al árabe tanto en el Próximo Oriente como en la España musulmana (realizadas por mozárabes).[64]​

A finales de la Edad Media ya habían aparecido traducciones totales o parciales de la Biblia a las lenguas vulgares romances y germánicas. Las primeras traducciones de la Biblia al castellano son las llamadas prealfonsinas, que preceden a la denominada Biblia alfonsina (1260-1280) incorporada en la General estoria de Alfonso X el Sabio. Por la misma época se produjeron la primera traducción de la Biblia al portugués (la Biblia de don Dinis, patrocinada por el rey Dionisio I de Portugal)[65]​ y la primera traducción de la Biblia al catalán, denominada Biblia de Montjuich (patrocinada por Alfonso III de Aragón en 1287, sobre una preexistente versión francesa[66]​). La Biblia Valenciana (de Bonifacio Ferrer, comienzos del siglo XV) fue uno de los primeros libros impresos en España (en 1478). Unos años antes, en 1471, había aparecido la primera Biblia impresa en italiano (traducción de Nicolò Malermi).[67]​ La más trascendente de las traducciones de la Biblia al alemán[68]​ fue resultado del trabajo de Martín Lutero entre los años 1521 y 1534 (Biblia de Lutero). Entre las más importantes traducciones de la Biblia al inglés[69]​ están la Biblia de los Obispos (1568), la Tyndale[70]​ (1525-1536) y la King James (1611). La Reina-Valera (1565-1602) fue la más usada por los protestantes españoles.

Joanes Leizarraga tradujo el Nuevo Testamento al euskera en 1571.[71]​ Algunos misioneros católicos españoles del siglo XVI tradujeron la Biblia a las lenguas americanas, como Bernardino de Sahagún, que lo hizo parcialmente en la lengua nahuatl; pero tal forma de evangelización fue prohibida por la Inquisición desde 1576.[72]​

Salmo 90 traducido al griego de la Septuaginta en uno de los papiros de Oxirrinco (ca. 450).

Codex Glazier, en copto, siglo IV o V.[73]​

Cánones eusebianos del Evangeliario Rabbula, en siríaco, siglo VI.[74]​

Codex Marchalianus, en griego, siglo VI.[75]​

Codex Argenteus, en lengua gótica, siglo VI.

Pentateuco de Tours, en griego, siglo VII.

Codex Amiatinus de la Vulgata, siglo VIII.

Traducción al árabe del Diatessaron siríaco, siglo XI.

Bible Historiale, en francés, ca. 1350.[76]​

Biblia eslava de Francysk Skaryna, 1517.

Biblia políglota complutense, 1522.

Biblia Tyndalle, 1526.

Biblia de Lutero, 1541.

Biblia del Oso, de Casiodoro de Reina, 1569.

Biblia del Cántaro, de Cipriano de Valera, 1602.

King James (Biblia del rey Jacobo), 1611.

Traducción parcial de la Biblia al chino del jesuita portugués Emmanuel Díaz, 1636.

Biblia en chino clásico, 1853.

Monje etíope con una Biblia ilustrada.





Italia (en italiano, Italia, pronunciado /i'talja/ ( escuchar)), oficialmente la República Italiana (en italiano, Repubblica Italiana pronunciado /re'pub:lika ita'ljana/), es un país soberano transcontinental, miembro y fundador de la Unión Europea, constituido en una república parlamentaria compuesta por veinte regiones, integradas estas, a su vez, por 110 provincias.[8]​ 

Italia se ubica en el centro del mar Mediterráneo, en Europa meridional. Ocupa la península itálica, así como la llanura Padana, las islas de Sicilia y Cerdeña y alrededor de ochocientas islas menores, entre las que se destacan las islas Tremiti en el mar Adriático, los archipiélagos Campano y Toscano en el mar Tirreno, o las islas Pelagias en África septentrional, entre otras. En el norte, está rodeada por los Alpes y tiene frontera con Francia, Suiza, Austria y Eslovenia. Los micro-Estados de San Marino y Ciudad del Vaticano son enclaves dentro del territorio italiano. A su vez, Campione d'Italia, es un municipio que forma un pequeño exclave italiano en territorio suizo.[9]​ 

Debido a su localización central en el Mar Mediterráneo, Italia recibió, durante la Antigüedad,[10]​[11]​ diversas influencias de civilizaciones mediterráneas exteriores, como la de los fenicios y cartagineses en sus islas mayores[12]​[13]​ y de los antiguos griegos en la llamada Magna Grecia,[14]​[15]​ así como también fue el hogar de muchas culturas propias distintas, como la civilización nurágica, los etruscos y los latinos, siendo estos últimos quienes dieron vida a la civilización romana,[16]​[17]​ y asistió al nacimiento de la República y del posterior Imperio romano.[18]​[19]​ Tras la caída del Imperio romano de Occidente, bizantinos, lombardos y musulmanes se disputaron el control sobre el territorio itálico,[20]​[21]​ quebrando así su anterior unidad política.[22]​[23]​ A partir de la Plena Edad Media, Italia fue la cuna de repúblicas marítimas como Venecia, Génova, Pisa y Amalfi, de los Estados Pontificios y también del humanismo, del Renacimiento y del movimiento barroco, entre otros Estados y movimientos culturales.[24]​[25]​ En el curso del siglo XIX, mediante el proceso histórico conocido como Risorgimento, los varios territorios italianos lograron unificarse bajo un mismo Estado: el Reino de Italia.[26]​[27]​ 

La capital de Italia, Roma, ha sido durante siglos el centro político y cultural de la civilización occidental. Además, es la ciudad santa para la Iglesia católica, siendo el papa el obispo de Roma y encontrándose dentro de la ciudad el micro-Estado del Vaticano. El significado cultural del país se refleja en todos sus Patrimonios de la Humanidad, ya que tiene 58,[28]​ el país con mayor número del mundo.[29]​ 

Es el tercer país de Europa que más turistas recibe por año,[30]​ siendo Roma la tercera ciudad más visitada del continente.[31]​ Otras ciudades importantes son: Milán, centro de finanzas y de industria, y, según el Global Language Monitor, la capital de la Moda;[32]​ Nápoles, importante puerto en el Mediterráneo, capital histórica y ciudad más poblada del Mezzogiorno;[33]​ Turín, centro de industria automovilística y de diseño industrial. Italia es una república democrática, forma parte del G7 o grupo de las siete más grandes naciones avanzadas del mundo y es un país desarrollado con una calidad de vida muy alta,[34]​ encontrándose en 2005 entre las siete primeras del mundo.[35]​

Es el país número 28 (informe 2017) en materia de alto índice de desarrollo humano.[36]​ Es una potencia regional y mundial,[37]​[38]​[39]​[40]​ miembro fundador de la Unión Europea, firmante del Tratado de Roma en 1957. También es miembro fundador de la Organización del Tratado del Atlántico Norte (OTAN) y miembro del OTAN Quint, de la Organización para la Cooperación y el Desarrollo Económico, de la Organización Mundial del Comercio, del Consejo de Europa, del G-4, del G-12 y del G20. El país, y especialmente su capital, tiene una fuerte repercusión en temas de política y cultura, en organizaciones mundiales como la Organización para la Agricultura y la Alimentación (FAO),[41]​ el Fondo Internacional de Desarrollo Agrícola (IFAD), el Glocal Forum[42]​ o el Programa Mundial de Alimentos (WFP).

El nombre de Italia ha sido usado desde antiguo, al menos desde el siglo VIII a. C., inicialmente para designar a las regiones del sur, y posteriormente también a las del centro, de la que se conoce como península itálica, haciendo referencia a los pueblos itálicos, hablantes de las lenguas llamadas igualmente.[43]​

Según el historiador griego Antíoco de Siracusa, el vocablo Italia designaba, antes del siglo V a. C.,[44]​ a la parte meridional de la actual región italiana de Calabria —el antiguo Brucio—, habitada por los itàlii, el grupo más meridional de los itálicos (actualmente esta zona comprende las provincias calabresas de Reggio, Vibo Valentia y partes de la provincia de Catanzaro).[45]​  Es posible que los itálicos tomaran su nombre de un animal-tótem, el ternero, que, en una lejana  primavera sagrada, los había guiado hasta los lugares en los que se asentaron definitivamente. También según el arqueólogo Pallottino el nombre de Italia derivaría del gentilicio de uno de los pueblos itálicos nativos de la región de Calabria, los (v)itàlii, el cual mutua su nombre de su animal sagrado: el ternero (víteliú en idioma osco, vitulus en latín y vitello en italiano); y que fue usado por los antiguos griegos como término general para designar a los habitantes de toda la península.[46]​

En el siglo II a. C., el historiógrafo griego Polibio llamaba Italia al territorio comprendido entre el estrecho de Mesina y los Apeninos septentrionales, aunque su contemporáneo Catón el Viejo extendió el concepto territorial de Italia hasta el arco alpino. El término se consolidó de manera definitiva sobre todo desde que, la ciudad itálica de Roma, a partir del siglo V a. C., unificó gradualmente toda la península conquistando y federando al resto de pueblos itálicos peninsulares, empezando por los latinos, de los cuales la misma constituía una aldea, y terminando con los etruscos hacia el norte y los brucios hacia el sur, unificando así todo el territorio peninsular bajo un único régimen y dándole nombre de Italia, la cual, desde entonces, constituirá el territorio metropolitano de la misma Roma.[47]​[48]​  

El nombre de Italia fue usado también en monedas acuñadas durante la guerra Social por la coalición de los socii (aliados) itálicos, en lucha contra Roma y las demás ciudades itálicas ya provistas de ciudadanía romana, para obtener, a su vez, la plena ciudadanía romana,[49]​ la cual fue otorgada tras la guerra Social a todos los  habitantes libres de Italia a través de la  Lex Plautia Papiria.[50]​Posteriormente, el norte de Italia (ex Galia Cisalpina), fue añadido oficialmente al territorio de la Italia romana en el curso del siglo I a. C., llevando así, de iure , el nombre de Italia hasta los pies de los Alpes;[51]​ mientras, las islas de Sicilia, Cerdeña y Córcega, no pasarán a formar parte de Italia hasta el siglo III d. C., como consecuencia de las reformas administrativas de Diocleciano, aunque sus estrechos lazos culturales con la península permiten considerarlas como parte integrante. 

Entre siglo XVIII a. C. y el siglo II existió en Cerdeña la cultura nurágica. Durante la Edad del Hierro se sucedieron varias culturas que pueden ser diferenciadas en cuatro grandes núcleos geográficos principales: los oscos, asentados a lo largo de toda la cordillera de los Apeninos (y pertenecientes al macro-grupo de los pueblos itálicos conocido como osco-umbros), los latinos del Latium Vetus (pueblo itálico del grupo latino-falisco), los italiotas de la Magna Grecia y el pueblo preindoeuropeo de los etruscos, en Etruria.[52]​ Otra cultura, diferenciada de las ya mencionadas, la de los ligures, constituía un enigmático pueblo preindoeuropeo que habitaba en el noroeste de Italia.[53]​

Los etruscos fueron un pueblo de lengua preindoeuropea cuyo núcleo histórico fue la Toscana, a la cual dieron su nombre (eran llamados Τυρσηνοί (tyrsenoi) o Τυρρηνοί (tyrrhenoi) por los griegos y tuscii o luego etruscii por los romanos; ellos se denominaban a sí mismos rasena o rašna). Por mucho tiempo los orígenes de los etruscos se creían desconocidos, sin embargo, las modernas investigaciones sobre el origen de los etruscos, llevadas a cabo por un grupo de genetistas y coordinadas por Guido Barbujani, miembro del departamento de Biología y Evolución de la Universidad de Ferrara, llegaron a la conclusión que, genéticamente, el origen de los etruscos es autóctono de la península itálica, [54]​
y que consistiría en una evolución de la anterior civilización villanoviana.[55]​

Desde la Toscana se extendieron por el sur, hacia el Lacio y la parte septentrional de Campania, en donde chocaron con las polis italiotas de la Magna Grecia ( sur de Italia ); mientras hacia el norte de la península itálica ocuparon la zona alrededor del valle del río Po, hasta el sur de la actual región de Lombardía. Llegaron a ser una gran potencia naval en el Mediterráneo Occidental, lo cual les permitió establecer factorías en Cerdeña y Córcega. Sin embargo, hacia el siglo V a. C. comenzó a deteriorarse fuertemente su poderío, en gran medida al tener que afrontar, casi al mismo tiempo, las invasiones de los celtas, desde el norte, y la competencia de los cartagineses para los comercios marítimos, desde el sur. A partir del siglo IV a. C., Etruria (nombre del territorio de los etruscos), fue gradualmente conquistada y absorbida por la República romana y, los etruscos, al igual de los demás itálicos, federados por los romanos, volviéndose así parte integrante de la Italia romana.[56]​[57]​

Como Antigua Roma se designa a una sociedad agrícola surgida a mediados del siglo VIII a. C. en el Latium Vetus (actual Lacio), que se expandió desde la ciudad de Roma a toda la península itálica, unificándola bajo el nombre de Italia, y que creció durante siglos hasta convertirse en un imperio que, en su época de apogeo, llegó a abarcar desde la península ibérica a Anatolia y desde las islas británicas hasta Egipto, provocando un importante florecimiento cultural en cada lugar en el que gobernó. En un principio, tras su fundación (según la tradición en 753 a. C.), Roma fue una monarquía etrusca, más tarde (509 a. C.) se convirtió en República Romana latina y, en 27 a. C., se convirtió en un imperio.

Al período de mayor esplendor se le conoce como Paz romana, debido al relativo estado de armonía que prevaleció en las provincias[58]​ (los territorios conquistados por los romanos fuera de Italia, la cual no era una provincia, sino el territorio metropolitano de la misma Roma y centro absoluto del Imperio Romano),[59]​ que estaban bajo el dominio romano de Julio César y luego del emperador Augusto, que cerró las puertas del templo de Jano (que permanecían abiertas en periodos de guerra), cuando creyó haber vencido a cántabros y astures, entre otros pueblos, en el año 24 a. C.. Se suele aceptar como fecha de inicio de la Paz romana (o Pax Augustea) el 29 a. C., cuando Augusto declara el fin de las guerras civiles, y su duración hasta la muerte de Marco Aurelio (año 180).

Con el emperador Diocleciano se reorganizó el Imperio, pero tras Constantino I el Grande no volvió a estar unificado puesto que Teodosio I el Grande lo dividió entre sus dos hijos, Arcadio y Flavio Honorio, adjudicándoles a uno el Imperio romano de Oriente —con sede en Constantinopla— y al otro el Imperio romano de Occidente. Las invasiones bárbaras pondrán fin al Imperio Occidental en 476, dando paso a la Edad Media.[60]​ Italia en este periodo quedó como Regnum Italiae (Reino ostrogodo de Italia), bajo los ostrogodos.

Los ostrogodos eran un grupo de godos que habían sido sojuzgados por los hunos, pero tras su liberación de estos, Teodorico el Grande, con la bendición del emperador romano de Oriente, condujo a su pueblo a Roma en 488.[61]​En la  península itálica  gobernaba el hérulo Odoacro, tras deponer al último emperador romano de Occidente, Rómulo Augústulo, en 476, pero tras una campaña en el norte de la península, Teodorico tomó la capital, Rávena, matando a Odoacro en 493. En 526 la muerte de Teodorico acabó con la paz, heredando Italia su nieto, Atalarico, que murió sin hijos, lo que produjo una crisis que llevó al reino a la desaparición.[61]​

Bajo Justiniano I, el Imperio romano de Oriente, inició una serie de campañas con el objetivo de reconstruir la unidad mediterránea. La debilidad del reino ostrogodo, y los deseos del Imperio de recobrar Roma, convirtieron a Italia en un objetivo. En 535 el general Belisario invadió Sicilia y marchó a través de la península, tomando Nápoles  y llegando a Roma en 536. Prosiguió hacia el norte y tomó Mediolanum (Milán) y Rávena en 540, y para el 561 había pacificado la zona.[62]​

Entre los diferentes pueblos germánicos que habían abandonado su antigua morada para vivir en mejores tierras, se contaban los lombardos, a los que Justiniano I había dejado asentarse en Panonia, a condición de que defendieran la frontera,[63]​ y que posteriormente  se dirigieron hacia Italia. La presión de los lombardos sobre el papa hizo que el rey de los francos, Pipino el Breve, realizara, entre 756 y 758, repetidas campañas en el norte de Italia. La situación se recrudeció a la muerte de Pipino, pero la reunificación de los francos bajo Carlomagno llevó a una nueva intervención en Italia en 774. Tras una breve batalla, Carlomagno se hizo con el Reino lombardo de Italia, que, manteniendo su autonomía, se integró en el Imperio carolingio.[64]​ 

Entre los siglos X y XIII, las repúblicas marítimas italianas gozaron de una gran prosperidad económica, gracias a su actividad comercial, en un marco de amplia autonomía política. Generalmente, la definición se refiere en especial a cuatro ciudades y sus respectivos territorios republicanos: Amalfi, Génova, Pisa y Venecia. También otras ciudades del área gozaban de independencia (gobierno autónomo con forma de república oligárquica, moneda propia, ejército, etc.), habían participado en las cruzadas, contaban con una flota naval, tenían fundagos,  cónsules que vigilaban los intereses comerciales de sus respectivos Estados en los puertos mediterráneos, y pueden ser incluidas de pleno derecho entre las repúblicas marítimas italianas. Entre estas, cabría destacar Ancona, Gaeta y Noli.

Durante los siglos XIV y XV, la Italia septentrional estaba compuesta por distintas ciudades-Estado, siendo el resto de la península ocupado en su mayoría por los Estados Pontificios y el Reino de Nápoles (partición peninsular del anterior Reino de Sicilia). La mayoría de las ciudades-Estado estaban subordinadas a soberanías extranjeras, como el Ducado de Milán, Estado constituyente del Sacro Imperio Romano Germánico, sin embargo la mayoría mantenían la independencia de facto de estas soberanías extranjeras, que habían gobernado buena parte de la península desde la Caída del Imperio romano de Occidente. 

Las más fuertes entre estas ciudades-Estado gradualmente absorbieron los territorios que las rodeaban, dando lugar a las Signorie, Estados regionales dirigidos por familias mercantes que fundaban dinastías locales. La guerra entre estas ciudades-Estado era habitual y principalmente llevada a cabo por bandas de mercenarios dirigidos por capitanes italianos conocidos como condottieri. Décadas de enfrentamientos dejaron como potencias regionales a las repúblicas de Florencia y Venecia y al Ducado de Milán, quienes firmaron el Tratado de Lodi en 1454, que llevó a la paz en la región por primera vez en siglos. La paz duraría por los siguientes cuarenta años.

El Renacimiento europeo fue un periodo de reavivamiento de las artes y ciencias originado en Italia gracias a varios factores, como la gran riqueza acumulada por las ciudades italianas, el mecenazgo de las familias dominantes como los Medici en Florencia, y la migración de los estudiosos bizantinos debido a la conquista de Constantinopla por parte del Imperio otomano. El Renacimiento terminó a mediados del siglo XVI debido a las acontecimientos resultantes de las Guerras italianas. Las ideas e ideales del  Renacimiento italiano se esparcieron por la Europa Nórdica, Francia, Inglaterra, y el resto de Europa.[65]​

Durante las Guerras italianas (1494-1559), provocadas por la rivalidad entre el Reino de Francia y la Monarquía Hispánica, varios Estados italianos perdieron gradualmente su independencia política, siendo gobernados primero por los Habsburgo de España (1559-1713) y después por los Habsburgo de Austria (1713-1796). Entre 1629-1631 una fuerte plaga, consistente en una serie de brotes de peste bubónica, aniquiló el 14 % de la población. La decadencia del Imperio español en el siglo XVII se llevó consigo a los reinos de  Nápoles, Sicilia, Cerdeña y al  Ducado de Milán.[66]​ En el siglo XVIII, debido a la Guerra de sucesión española, Austria reemplazo a España como principal potencia extranjera. Durante las Guerras Napoleónicas, el norte de Italia fue reorganizado como Reino de Italia (1805-1814), un Estado títere del Primer Imperio francés, mientras que el sur fue gobernado por Joaquín Murat, cuñado de Napoleón, coronado como rey de Nápoles. En 1814 el Congreso de Viena restauró la situación del siglo XVIII, aunque los ideales de la Revolución francesa no fueron erradicados. [67]​

El nacimiento del Reino de Italia fue gracias a los esfuerzos unidos de los nacionalistas y monárquicos leales a la casa de Saboya, para establecer un Estado unificado en la península itálica. En el contexto de las revoluciones liberales de 1848 que atravesaron Europa, se produjo una infructuosa primera guerra de independencia contra el Imperio austríaco. El Reino de Piamonte-Cerdeña atacó nuevamente a Austria en la segunda guerra de independencia italiana en 1859, con la ayuda de Francia, resultando en la liberación de Lombardía.

En 1860-61, el general Giuseppe Garibaldi llevó a cabo la  Unificación en el Reino de las Dos Sicilias, haciendo que el conde de Cavour declarara un Reino de Italia unificado el 17 de marzo de 1861. En 1866, Víctor Manuel II se alió con Prusia durante la guerra austro-prusiana, en la tercera guerra de independencia italiana que permitió la anexión del Véneto. Finalmente, después de la guerra franco-prusiana de 1870, Francia abandonó sus intereses en Roma, lo cual permitió la captura de Roma y el fin de los Estados Pontificios.[68]​

El Estatuto Albertino de 1848 se extendió a todo el Reino de Italia en 1871, proveyéndole de libertades básicas, aunque las leyes electorales excluían a las personas sin propiedades y los no educados. El nuevo gobierno del reino era una monarquía parlamentaria constitucional, dominada por las fuerzas liberales. El sufragio universal masculino fue adoptado en 1913. Mientras el norte se industrializaba rápidamente, el sur y las zonas rurales del norte permanecieron subdesarrolladas y sobrepobladas, forzando a millones de personas a emigrar. El Partido Socialista Italiano se fortalecía y desafiaba a los tradicionales partidos liberales y conservadores. Desde finales del siglo XIX,  Italia se convirtió en una fuerza colonial, con colonias en Somalia, Eritrea, Libia y el Dodecaneso.

Italia, aliada de los Imperios alemán y austrohúngaro en la Triple Alianza, en 1915 se unió a las fuerzas Aliadas en la Primera Guerra Mundial, con la promesa de extender su territorio, con los terrenos de Carniola Interior, el Litoral austríaco y Dalmacia. El ejército italiano quedó inicialmente estancado en una guerra de trincheras en los Alpes. En octubre de 1918, los italianos lanzaron una feroz ofensiva que culminó en victoria en la batalla de Vittorio Veneto. La victoria aseguró el final de la guerra en el frente italiano. Dos semanas después acababa el conflicto.[69]​

Durante la guerra, murieron 650 000 soldados y muchos civiles,[70]​ llevando a la quiebra al reino. Los tratados de Saint Germain, Rapallo y Roma, concedieron la mayoría de los territorios reclamados, mas no la costa dálmata, lo que hizo que varios grupos nacionalistas definieran la victoria como mutilada. Más adelante, tras la creación del Estado Libre de Fiume por el poeta Gabriele D'Annunzio, también Fiume fue anexionada.

Las agitaciones socialistas que siguieron a la Primera Guerra Mundial, inspiradas por la Revolución rusa, llevaron a una contrarrevolución y represión. Debido al temor de una revolución, el pequeño Partido Nacional Fascista, liderado por Benito Mussolini, se convirtió en una importante fuerza política. En octubre de 1922, las camisas negras del PNF, llevaron a cabo un intento de golpe de Estado (la Marcha sobre Roma), que fracasó en último instante, a pesar de esto el rey Víctor Manuel III rehusó declarar el estado de sitio y convirtió a Mussolini en primer ministro. En los siguientes años, Mussolini eliminó todos los partidos políticos y libertades personales, estableciéndose una dictadura fascista. Estas acciones inspiraron el surgimiento de otras dictaduras parecidas en Europa, como la Alemania nazi o la España franquista.

En 1935, Italia invadió Etiopía en la segunda guerra ítalo-etíope, llevando a la salida del país de la Sociedad de las Naciones. Italia se alió con la Alemania nazi, el Imperio del Japón, y apoyó a Francisco Franco en la guerra civil española. En 1939 la Italia fascista se anexionó Albania, ya protectorado italiano de facto durante décadas. Italia entró en la Segunda Guerra Mundial el 10 de junio de 1940. Después de haber avanzado inicialmente en la Somalia Británica y en Egipto, fue derrotada en el Norte de África, en Grecia y en el Frente Oriental.

Después del ataque sobre Yugoslavia de la Alemania nazi y la Italia fascista, la fuerte presión sobre la resistencia partisana y los intentos de italianización de los eslavos resultaron en los crímenes de guerra italianos y en la deportación de 25 000 personas a los campos de concentración.[71]​ Cerca de 250 000 italianos y eslavos anticomunistas abandonaron el país en el éxodo istriano.

La invasión aliada de Sicilia comenzó en julio de 1943, lo cual llevó al colapso del régimen el 25 de julio. El 8 de septiembre hubo el Armisticio entre Italia y las fuerzas armadas aliadas. Rápidamente los alemanes tomaron el poder sobre el centro y sur del territorio. El país se mantuvo como un campo de batalla el resto de la guerra, mientras los Aliados avanzaban lentamente fuera del sur.

Para contrarrestar el avance Aliado se creó la República Social Italiana, un Estado títere nazi, con Mussolini a su cabeza. Los paisanos organizaron un movimiento de resistencia contra el nazismo y el fascismo. Las hostilidades acabaron el 29 de abril de 1945, cuando la resistencia derrotó a los nazis, obligándolos a abandonar el país. Mussolini fue fusilado. Casi un millón de italianos (incluyendo civiles) murieron en la guerra y la economía nacional estaba totalmente destruida.[72]​[73]​ 

Italia se convirtió en república después de un plebiscito realizado el 2 de junio de 1946. En esta oportunidad por primera vez las mujeres pudieron votar. Humberto II fue forzado a la abdicación y el exilio. La Constitución Republicana fue aprobada el 1.º de enero de 1948. Se perdió la mayoría de la Venecia Julia con Yugoslavia, y el Territorio libre de Trieste se dividió entre los dos Estados. Se perdieron todas las posesiones coloniales, acabando con el Imperio Italiano.[74]​

El miedo al triunfo del comunismo fue crucial en la primera elección del país, en abril de 1948, la cual dio la victoria a la Democracia Cristiana, bajo el liderazgo de Alcide De Gásperi. Consecuentemente, en 1949, Italia se unió a la OTAN. El Plan Marshall ayudó a revivir a la economía nacional, la cual hasta finales de la década de 1960, vivió una época de auge, conocida como el milagro económico. En 1957 fue un miembro fundador de la Comunidad Económica Europea (CEE), que en 1993, se convirtió en la Unión Europea (UE).

Desde finales de los años 1960 hasta los finales de los años 1980, se vivieron los años de plomo (anni di piombo), caracterizados por la crisis económica (especialmente en la crisis del petróleo de 1973), conflictos sociales y ataques terroristas por grupos de extrema oposición, debidos a la guerra fría y la intromisión de las inteligencias norteamericanas y soviéticas.[75]​ La época culminó con el asesinato del líder democratacristiano Aldo Moro en 1978 y la masacre de la estación de tren de Bologna en 1980, dejando 85 muertos.[76]​

En los años 1980, se rompió la hegemonía de la Democracia Cristiana, con un gobierno liberal (Giovanni Spadolini en 1981) y otro socialista (Bettino Craxi en 1983), pero la Democracia Cristiana siguió siendo el principal partido. Durante el gobierno de Craxi la economía se recuperó, llegando a ser la quinta nación más industrializada del mundo, siendo parte del G7. Pero, debido a los gastos del gobierno, la deuda se disparó, llegando al 100 % del PIB.

Las elecciones de 1992 se caracterizaron por el fracaso de los grandes partidos, producto de la parálisis política, la excesiva deuda y la corrupción del sistema electoral, desvelada por la investigación Manos Limpias, requiriéndose cambios radicales. Los escándalos envolvían a la mayoría de los partidos, pero especialmente en el partido gobernante: la Democracia Cristiana, que gobernaban desde hace más de cincuenta años, sufrieron una fuerte crisis y se desintegraron entre varias facciones. Los comunistas se reorganizaron como una fuerza socialdemócrata. En 1993 se sucedieron distintas dimisiones, entre ellas las del entonces primer ministro y de Bettino Craxi. Durante los años 1990 y 2000, la coalición de centro-derecha (liderada por el magnate de los medios Silvio Berlusconi) y las coaliciones de centro-izquierda (lideradas por el profesor universitario Romano Prodi) se alternaron el gobierno del país.

En el 2008 el país fue víctima de la recesión. Hasta el 2015, sufrió 42 meses de recesión económica. La crisis económica fue uno de los principales factores que hicieron que Berlusconi renunciara en 2011. En 2012 se produce el accidente del Costa Concordia, crucero semihundido con 32 muertes. En la elección general de 2013, el secretario general del Partido Democrático, Enrico Letta, formó un nuevo gobierno a la cabeza de la Gran Coalición. En 2014, desafiado por el nuevo secretario del PD, Matteo Renzi, renunció y fue reemplazado por el mismo. Este emprendió importantes reformas constitucionales como la abolición del senado y una nueva ley electoral. El 4 de diciembre el referéndum constitucional fue rechazado, provocando la renuncia de Renzi 12 días después. El ministro de relaciones exteriores, Paolo Gentiloni, fue nombrado nuevo primer ministro.

Italia fue afectada por la crisis migratoria europea en 2015 debido a que se convirtió en el punto de entrada y principal destino para la mayoría de los buscadores de asilo en la Unión Europea. El país recibió sobre medio millón de refugiados, causando gran repudio en la población y un surgimiento hacia el apoyo de los partidos de extrema derecha y euro-escépticos, basados en el Brexit, lo que condujo al primer gobierno antisistema de la Unión Europea, en 2018.

La política se basa en un sistema republicano parlamentarista con democracia representativa desde el 2 de junio de 1946, cuando la monarquía fue abolida por referéndum popular. El poder ejecutivo está a cargo del Consejo de ministros que están liderados por el jefe de gobierno (Presidente del Consiglio dei Ministri), informalmente llamado primer ministro, uno de los cinco cargos más importantes del país junto a los de presidente de la República, presidente del Senado de la República, presidente de la Cámara de diputados y presidente de la Corte constitucional.[77]​

El poder legislativo está a cargo del Parlamento y del Consejo de ministros. El poder judicial es independiente del ejecutivo y el legislativo. Además, es un sistema multipartidista. En el sur de la península y en la isla de Sicilia, la mafia tiene tanto o más poder que el Estado, llegando a controlar periódicos, jueces y policías.[78]​
En 1992, el asesinato de Giovanni Falcone, un magistrado que investigaba el crimen organizado, y la subsecuente campaña de «mani pulite» que se desató, conmocionaron a las instituciones italianas, pero tras años de intensas investigaciones, ha habido pocos resultados.[79]​ Silvio Berlusconi, ex primer ministro, siempre ha sido sospechoso de corrupción, y, sin embargo fue elegido en tres ocasiones para su cargo.[80]​ Dimitió el 12 de noviembre de 2011 debido a la grave situación económica.[81]​ 

En 2018, luego de una elección reñida, asumió un gobierno de coalición entre europeístas como Luigi Di Maio del Movimiento 5 Estrellas y euroescépticos como la Liga Norte. La cabeza del gobierno fue Giuseppe Conte. En 2019 por diferencias internas el gobierno se disolvió quedando fuera del mismo la Liga Norte. El nuevo gobierno se formó a partir de los votos favorables de los parlamentarios y del presidente Mattarella, quienes dieron su confianza al primer ministro para formar el Gobierno Conte bis. En febrero de 2021, el gobierno dimitió luego de realizada una votación en sede parlamentaria, donde se ponía en entredicho la confianza en el gobierno. Si bien la votación fue favorable en ambas cámaras, en el Senado no fue un resultado absoluto. Luego de una votación favorable en el parlamento y con la confianza del presidente Mattarella, se formó un nuevo gobierno técnico encabezado por el ex presidente del BCE, Mario Draghi.

Camera dei deputati, la cámara baja del Parlamento de la República Italiana

Palacio del Quirinal, residencia oficial del Presidente de la República Italiana

Palacio Chigi, residencia oficial del Presidente del Consejo de Ministros de Italia

Italia es parte de la ONU, la UE, la OTAN, la OCDE, la OSCE, el CAD, la OMC, el G3, el G10, el G12, el G20, la Unión Latina, el Consejo de Europa y la Unión por el Mediterráneo, entre otros. Asimismo, Italia es miembro de grupos de toma de decisiones importantes como el G4,[82]​ el G6, el G7,[83]​ el "OTAN Quint",[84]​[85]​[86]​ el Tratado de Compartición nuclear y el Grupo de contacto.[87]​

Fue miembro fundador de la Comunidad Europea, ahora Unión Europea. Fue admitida en la Organización de las Naciones Unidas en 1955, y es asimismo miembro fundador de la OTAN, del GATT, de la Organización para la Cooperación y el Desarrollo Económico, de la Organización para la Seguridad y la Cooperación en Europa y del Consejo de Europa. Italia actualmente cumple un rol importante como Potencia Occidental en la lucha contra el terrorismo,[88]​ al liderar varias fuerzas multinacionales y al tener tropas desplegadas en el Medio Oriente, en países como Libia, Irak y Afganistán. 

También, es importante destacar que desplegó tropas de apoyo en misiones de pacificación de las Naciones Unidas en Somalia, Mozambique y Timor Oriental, y apoyó a la OTAN y a las Naciones Unidas en Bosnia, Kosovo y Albania.[89]​[90]​ Asimismo, el país también juega un papel importante en las antiguas colonias y territorios del Imperio italiano, y es considerado un actor clave en la región mediterránea.

Las Fuerzas Armadas de Italia están formadas por el Ejército, la Marina, la Aeronáutica y el Arma de Carabineros, todos bajo el Consejo Supremo de Defensa presidido por el Presidente de la República Italiana. Desde el año 2005, en el país el servicio militar es enteramente voluntario.[91]​ En el año 2010, las fuerzas armadas italianas tenían un personal de 293 202 militares,[92]​ de los cuales 114 778 eran carabineros.[93]​ Ese mismo año, el presupuesto militar de Italia fue el décimo más alto del mundo, equivalente al 1,7 % del PIB de la nación. Como miembro de la estrategia de reparto nuclear de la OTAN, el país transalpino custodia noventa armas nucleares estadounidenses, que están almacenadas en las bases aéreas de Ghedi y Aviano.[94]​

Italia es la tercera potencia militar de Europa, tras Francia y Reino Unido,[95]​ y cuarta potencia europea en cuanto a gastos en presupuesto militar, tras Francia, Reino Unido y Alemania.[96]​

El Ejército italiano es la fuerza militar terrestre, compuesta en el año 2012 por 105 062 efectivos. Sus materiales de combate principales son el vehículo de combate de infantería Dardo, el cazacarros Centauro, el tanque Ariete o el helicóptero de ataque Mangusta, desplegado en misiones de la ONU. Además, el ejército italiano dispone de otros vehículos acorazados como el Leopard 1 y el M113.[97]​

La Marina Militare tenía 32 000 militares en el año 2013[98]​ y cuenta como naves destacadas con dos portaaviones, el Giuseppe Garibaldi (el cual será reemplazado próximamente con la entrada en servicio del portaeronaves Trieste) y  el Cavour, once fragatas, entre ellas tres nuevas de la clase FREMM, y ocho submarinos. En los últimos tiempos la marina italiana, como miembro de la OTAN, ha participado en varias operaciones de la coalición en diversas partes del mundo, tales como la Intervención militar en Libia y la Guerra de Afganistán.

La Aeronautica Militare cuenta con más de 40 000 militares y en el año 2013 operaba 470 aeronaves y seis aviones no tripulados. Entre estos aparatos había 218 cazas de combate y 108 helicópteros.[99]​ El equipamiento más destacado de la fuerza aérea transalpina son sus 87 cazas Eurofighter Typhoon, a los que sumarán en próximos años otros nueve que están encargados y que reemplazan a los más antiguos F-16.[100]​La aviación tiene 67 Panavia Tornado tipo IDS y ECR. Italia también está adquiriendo 90
Lockheed Martin F-35 Lightning II tipo A (60) y B (30) para la aviación y marina capaz de transportar todos los modelos B61, incluido el nuevo tipo 12.[101]​ Las capacidades de transporte aéreo están cubiertas por doce aviones de carga Alenia C-27J Spartan, cuatro Boeing KC-767 y veintiuna aeronaves de transporte militar C-130J Super Hercules.

Además, Italia cuenta con un cuerpo autónomo de las fuerzas armadas, el Arma de Carabineros, que cumple funciones tanto civiles como militares, pues son la gendarmería y la policía militar italianas.[102]​

En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Italia ha firmado o ratificado:

La Constitución de la República Italiana organiza el territorio desde 1948 en tres niveles de gobierno local, y declara[109]​ a Roma como la capital de la República. Tradicionalmente se divide en cinco grandes áreas geopolíticas y en veinte regiones administrativas:

De las veinte regiones, cinco (Valle de Aosta, Friul-Venecia Julia, Sicilia, Cerdeña y Trentino-Alto Adigio) gozan,[110]​ por motivos históricos y geográficos, de autonomía y de un estatuto especial. De ellas, Sicilia adquirió su derecho a un estatuto especial autonómico en 1946 debido a su condición geográfica y política; las otras adquirieron estatuto propio en los siguientes años: Cerdeña, Valle de Aosta y Trentino-Alto Adigio en 1948, por motivos lingüísticos, y en 1963 Friul-Venecia Julia. La provincia (en italiano: provincia) es una división administrativa de nivel intermedio entre el municipio o comuna (en italiano: comune) y la región (en italiano: regione).

El relieve presenta cuatro grandes unidades regionales: al norte, un sector continental dominado por los Alpes; al sur un sector peninsular articulado por los Apeninos; entre ambas está el valle del Po o llanura Padana; y finalmente las islas volcánicas.[111]​ El sistema alpino extiende por territorio italiano la casi totalidad de su vertiente meridional. En este gran conjunto montañoso destacan las formaciones calcáreas de los Dolomitas (Marmolada, 3342 m de altura) y en el sector cristalino, las principales cumbres de todo el sistema alpino como el Monte Bianco (4810 m), el Monte Rosa (4634 m) o el Cervino (4478 m).[112]​ Algunos pasos de montaña (Mont Cenis, Simplon, Brennero) facilitan la comunicación con las regiones vecinas. 

La región prealpina presenta largos y profundos valles, con numerosos lagos: Garda (370 km²), Mayor, Como, Iseo. Al sur de los Alpes, entre estos y los Apeninos, se extiende el valle del Po (el río más largo del país, con 652 km de longitud), fosa tectónica rellenada por los depósitos sedimentarios aportados por los ríos que descienden de los Apeninos y, sobre todo, de los Alpes (Adigio, 410 km; Piave), y que se abre al mar Adriático por el litoral noreste de Italia.[113]​

El resto de Italia, aunque presenta numerosos valles, estos son de escasa extensión, y se localizan principalmente en el litoral tirrénico, donde algunos están formados por importantes ríos como el Arno o el Tíber. La cadena de los Apeninos constituye la espina dorsal de la península italiana, y en ella se distinguen tres sectores: los Apeninos septentrionales, los de menor altura y de formas más suaves (monte Cimone, 2163 m); los Apeninos centrales, también denominados Abruzos, que constituyen el techo de la cadena (Gran Sasso d'Italia, 2914 m), y presentan modelados de tipo cárstico; y por último, los Apeninos meridionales, que tienen su punto culminante en el monte Pollino (2271 m).[114]​ En ambas vertientes de la cadena se extienden otras formaciones montañosas menores, denominadas Subapeninos y Antiapeninos, destacando las del reborde oeste, donde se elevan algunos volcanes (Vesubio, monte Amiata, Campos Flégreos).

En el extremo sur de la península itálica, la isla de Sicilia es considerada una prolongación de los Apeninos (montes Nebrodi, Peloritanos y Madonia), destacando el monte Etna, que con sus 3345 m es el volcán activo más alto de Europa.[115]​ La isla de Cerdeña es asimismo montañosa (Gennargentu), aunque cabe destacar la llanura de origen fluvial de Campidano, entre Oristán y Cagliari.[116]​

La climatología italiana presenta cuatro climas principales: alpino, continental (en la llanura Padana), mediterráneo y subtropical, pero con notables variaciones regionales. En primer lugar, por efecto de su considerable extensión en latitud: medias anuales en Milán de 25,0 °C en julio y 1,4 °C en enero, mientras que en Palermo, dichas medias son de 29.3 y 13 °C, respectivamente.[117]​ El lugar con más precipitaciones del país es la provincia de Udine, en el noreste, con 1530 mm y, por el contrario, los lugares con menores precipitaciones están en el sureste, concretamente en la región de Apulia (en la provincia de Foggia) y en la parte sur de Sicilia, las regiones áridas con aproximadamente 460 mm.[118]​ 

Se puede diferenciar el país en cuatro regiones climáticas: el clima mediterráneo a lo largo de las costas peninsulares y en las islas, con veranos calurosos superando los 30 °C e inviernos suaves, los llanos del río Po, donde el clima es marcadamente continentalizado, con inviernos muy fríos y veranos muy húmedos y calurosos, los Alpes, con un clima de montaña marcadamente frío pero con veranos frescos, y los Apeninos, con un clima de montaña más templado y cercano al mediterráneo, aunque con frecuentes precipitaciones.[118]​

La mayor parte de Italia corresponde al bioma del bosque mediterráneo,[119]​ aunque también están presentes el bosque templado de frondosas, entre el valle del Po y los Apeninos, y el bosque templado de coníferas en los Alpes.[120]​

Según el Fondo Mundial para la Naturaleza, el territorio de Italia se divide en ocho ecorregiones diferentes:[121]​

La actividad industrial ha sido el motor del desarrollo italiano, y el actual eje de su economía. Frente a ello, las actividades agrícolas han experimentado un considerable retroceso, tanto en ocupación de la población activa (7,3 %), como en su participación en el PIB (3,7 %). La producción agrícola no abastece la demanda alimenticia de la población, y es especialmente escasa en la rama ganadera: bovino (Cerdeña) y porcino (Emilia-Romania).

La agricultura está más extendida con cultivos de cereales (trigo, arroz ―primera productora europea―, maíz), leguminosas, plantas industriales (remolacha azucarera), hortalizas (pimientos, berenjenas, cebollas) y flores. Mención especial merece la fruticultura (peras, melocotones y manzanas en Emilia, Véneto y Campania; agrios en Sicilia), el olivo (en Liguria y el Mezzogiorno), que genera la segunda producción mundial de aceite (435 300 t), y finalmente, la vid, cuyo cultivo sitúa a Italia a la cabeza de la producción mundial de vinos (68,6 millones de hl), reconocidos internacionalmente por su calidad.[122]​

El turismo es uno de los sectores con más crecimiento de la economía nacional con 60,5 millones de turistas por año y un total de 52 700 millones de dólares generados, siendo así el cuarto país con más turismo del mundo.[123]​ Roma, la capital, es uno de los destinos más visitados del mundo, con una media de 12 a 15 millones de turistas al año.[124]​ 

El Coliseo de Roma, con cuatro millones de turistas, es uno de los lugares más visitados de Italia.[125]​ También se beneficia del turismo religioso y cultural que genera la Ciudad del Vaticano, con lugares tan visitados como los Museos Vaticanos, la basílica de San Pedro o la Capilla Sixtina, así como de una infinidad de otros lugares de interés espiritual, artístico, arquitectónico, paisajístico, naturalístico, arqueológico, histórico y cultural esparcidos por toda la geografía italiana.

Algunos otros lugares de gran interés de la capital italiana incluyen: el Panteón de Agripa, la Fontana de Trevi, la Plaza Navona, la Plaza de España, la Plaza del Popolo, el Foro Romano, los Foros imperiales, el Monte Palatino, los Museos Capitolinos, el Museo Nacional Romano, la Galería Borghese, el Campidoglio, el Palacio del Quirinal, las Termas de Caracalla, las Termas de Diocleciano,[126]​ el Castillo Sant'Angelo, el Vittoriano, el Ara Pacis, el Arco de Tito, el Arco de Constantino, la Domus Aurea, la Pirámide Cestia, el Mausoleo de Augusto, la Basílica de San Juan de Letrán, la Basílica de San Pablo Extramuros o la Basílica de Santa María la Mayor, entre otros.

El interés cultural del país también se refleja en todos los Patrimonios de la Humanidad de la Unesco que posee, ya que es el país que contiene el mayor número de lugares en el mundo con 58,[127]​ además de sus casi 8000 km de costas con innumerables y diversificados paisajes marinos tan famosos como la Costa Amalfitana, la Costa Smeralda, la Riviera Italiana, la isla de Capri, las islas Eolias, etc., grandes instalaciones de esquí y amplias posibilidades de montañismo en los Alpes, los Dolomitas y los Apeninos, los famosos grandes lagos italianos como el lago de Garda, el lago de Como, el lago Mayor, etc., importantes sitios arqueológicos como Pompeya, Herculano, el Valle de los Templos de Agrigento, Ostia Antica, o ciudades tan conocidas y visitadas como Florencia, Venecia, Nápoles, Milán, Verona, Turín, Bolonia, Génova, Pisa, Siena, Palermo, Matera, Siracusa, entre otras. Muy importantes son también el turismo gastronómico, el enoturismo y el turismo termal. 

Según el Fondo Monetario Internacional, en 2018, la italiana fue la octava economía mundial y la cuarta de Europa por PIB nominal. Como país avanzado tiene la sexta riqueza nacional en el mundo.[128]​ Pertenece al G7, a la Unión Europea y a la Organización para la Cooperación y el Desarrollo Económico.[129]​ El comercio exterior, en su mayor parte desarrollado dentro de la órbita de la UE, presenta habitualmente una balanza comercial en positivo, siendo el sexto país del mundo en volumen de exportación en 2008 con 546 900 millones de dólares.[130]​ 

Su PIB per cápita (PPP) es de 30 200 dólares (estimaciones de 2009), 101 % de la UE-27 en 2007.[131]​[132]​Milán y Roma son la 11.ª y la 18.ª ciudades más caras del mundo,[133]​ además de ser Milán la 26.ª con mayor producto interno bruto, con 115 000 millones[cita requerida] de dólares.[134]​

Las mayores exportaciones del país son los vehículos automotores: Ferrari, Maserati, Fiat, Alfa Romeo, Lancia, Aprilia, Piaggio, Gilera, todas las empresas mencionadas  pertenecen al Grupo Fiat, que es desde 2009 el accionista mayoritario de la Chrysler, y es considerada ahora como la 3ª empresa automotriz más grande del mundo. Muy importantes son también Lamborghini y las motocicletas (Ducati, Moto Guzzi, Cagiva, Bimota). Otros sectores importantes del Made in Italy son la alimentación (Ferrero, De Cecco, Barilla, Campari, Lavazza, Parmalat, Bertolli), los astilleros para la fabricación de barcos de crucero y militares (Fincantieri), los yates (Gruppo Ferretti), la petroquímica (ENI, ERG), la energía (Enel), los electrodomésticos (Indesit, Candy, Ariston, De'Longhi), la ingeniería aeroespacial (Alenia, Piaggio Aero, Leonardo-Finmeccanica, Agusta), las armas de fuego (Beretta). 

Otro rubro muy usado fuera de sus fronteras por incontables empresas de manufactura son las consultorías de diseño industrial o gráfico: Ital Design, Pininfarina, Bertone, Ghia, Domus, Cassina o Alessi, por nombrar las más destacadas. Una parte importante del PIB del país es producido por la moda, con marcas como Gucci, Armani, Versace, Dolce & Gabbana, Benetton Group, Prada, Miu Miu, Gianfranco Ferre, Fendi, Lotto, Salvatore Ferragamo, Bvlgari, Tod's, Sergio Rossi, Intimissimi, Etro, Luxottica, Moschino, Diesel, Cavalli, Valentino, Bottega Veneta, Diadora o Kappa.

En el contexto socioeconómico, según datos del Banco Mundial, Italia destaca por tener una tasa de natalidad muy reducida: apenas un niño por mujer, con un descenso continuado en las últimas dos décadas, que la sitúan en el puesto 162 de los países con mayor índice de nacimientos. También es relevante la baja penetración de Internet (41 %), entre la población global italiana, un ratio relativamente bajo para un país europeo, especialmente en ciertas áreas del país. Eurostat indica que Italia es el quinto país de Europa en número de muertes por VIH, algo que sin embargo no afecta (al menos de manera significativa) a la esperanza de vida, en la que Italia se mantiene en los primeros puestos. La siguiente tabla muestra el contexto socio-económico de Italia partiendo de datos del Banco Mundial, Eurostat y el Foro Económico Mundial:

Ferrovie dello Stato nació en 1905, y es la más importante compañía ferroviaria pública de Italia. A partir del año 2000, siguiendo la normativa europea que obliga a la separación del sector transporte de pasajeros del sector de infraestructura, la sociedad fue reorganizada. Por ejemplo, Ferrovie dello Stato Italiane es la sociedad principal, Trenitalia es la sociedad que se encarga del transporte de cargas y de pasajeros, la Rete Ferroviaria Italiana es la sociedad encargada de la infraestructura ferroviaria, y las Frecciarossa y Frecciargento son las sociedades que proporcionan los servicios de alta velocidad, aunque existen otras.[171]​ Actualmente, los trenes de alta velocidad italianos son los ETR 500 y ETR 1000, y las líneas que existen en este momento son: Roma-Florencia, Roma-Nápoles, Turín-Novara, Padua-Venecia, Milán-Treviglio y Milán-Bolonia.[172]​

En total, en 2003, había 16 287 kilómetros de vías de tren, 668 721 kilómetros de carreteras, de los cuales 6487 kilómetros eran de autopista, y 4379 kilómetros de transporte por tubería.[173]​ Los aeropuertos con más tráfico aéreo en 2003 fueron Roma-Fiumicino, Milán-Malpensa, Milán-Linate, Venecia y Catania-Fontanarossa.[173]​ Por su parte, los puertos con más carga fueron Génova, Trieste, Nápoles, Augusta y Gioia Tauro.[173]​ En 2005, 590 de cada 1000 italianos poseían un coche y en la mayoría de las ciudades el 60 % de los ciudadanos no estaban satisfechos con el transporte público, razones por las cuales el número de pasajeros en dichos transportes ha disminuido.[173]​

A finales de 2008 la población del país superó los 60 millones, siendo el cuarto país más poblado de Europa y con la quinta mayor densidad poblacional, con un promedio de 198 personas por kilómetro cuadrado.[174]​ A partir de los años sesenta del siglo XX, la población italiana experimentó un cambio en su ritmo de crecimiento, que decreció hasta el 0 % de media anual entre 1985 y 1990. El descenso de la tasa de mortalidad fue acompañado por un descenso considerable de la tasa de natalidad, siendo en 2008 uno de cada cinco italianos mayor de 65 años.[175]​ 

El cambio en las tendencias demográficas afectó asimismo los tradicionales movimientos migratorios que hasta entonces habían hecho de Italia una de las mayores reservas de mano de obra de Europa (Francia, Reino Unido y Alemania, principalmente) y América (Estados Unidos, Brasil, Argentina, Venezuela , Paraguay y Uruguay cuentan con numerosas comunidades de origen italiano). Italia pasó a convertirse en punto de llegada de inmigrantes del tercer mundo, pero, sobre todo, se establecieron importantes corrientes migratorias internas. Con un movimiento masivo de población del sur hacia Roma y el norte industrializado (Turín, Milán, Génova y Bolonia), pero no hacia el noreste, aún muy pobre, lo cual no ha hecho sino radicalizar las diferencias entre el norte y el sur, pero que a su vez ayudó a que la natalidad creciera.[176]​ La tasa de fertilidad creció en pocos años desde 1,32 niños por mujer en 2005 hasta 1,41 en el año 2008.[177]​ La concentración de la población italiana en los núcleos urbanos (69 % de población urbana) ha generado una red homogénea de grandes ciudades, que desempeñan el papel de centros regionales (Nápoles, 973 132 habitantes; Turín, 963 128; Palermo, 663 173; Génova, 610 887; Bolonia, 372 256, y Florencia, 364 710), con dos destacados núcleos a nivel nacional; Roma (2 718 768 hab.), la capital política, y Milán (1 299 633), la capital económica.

Ciudades metropolitanas y sus delimitaciones previstas por el ordenamiento jurídico nacional[178]​ y regiones con estatuto especial:[179]​[180]​

Los grupos minoritarios son pequeños, siendo el mayor de estos el de habla alemana en la provincia autónoma de Bolzano (según el censo de 1991, la población de la provincia de Bolzano se encuentra compuesta por 287 503 personas de habla alemana y 116 914 de habla italiana), seguido por los francoprovenzales en la región del Valle de Aosta y los eslovenos alrededor de Trieste. El idioma ladino es el más hablado de la región de los Dolomitas.[182]​

Otros grupos minoritarios con lenguajes parcialmente tutelados incluyen los friulanos y los sardos, incluyendo los hablantes de catalán en Alguer. Italia tiene 66 676 779 habitantes (Istat 04.2008), y está compuesta étnicamente (datos 2006) por 97,6 % de europeos (italianos 95,9 % + otros europeos 1,5 %), 0,5 % de africanos (mayoría de marroquíes), 1,3 % de asiáticos (mayoría de chinos), 0,8 % de americanos (mayoría de ecuatorianos).[cita requerida]

La lengua oficial de Italia es el italiano, una lengua romance procedente del latín hablado, especialmente de la variante toscana arcaica, y perteneciente a la familia italorromance de las lenguas itálicas, integrantes a su vez las lenguas indoeuropeas.[183]​ Hasta el siglo XVI el italiano se identificó plenamente con el toscano y con los grandes escritores prerrenacentistas de aquella región (Dante Alighieri, Francesco Petrarca y Giovanni Boccaccio) cuyas obras gozaron de gran prestigio y tuvieron una notable difusión en toda Italia y Europa. A partir de aquel siglo, con la internacionalización del Renacimiento, la literatura y el idioma italiano se propagaron aun más rápidamente que en el período anterior en todo el mundo occidental. En aquella época la lengua italiana (denominación que había terminado por prevalecer, durante el siglo XVI, sobre cualquier otra) había dejado de identificarse con el vulgar florentino[184]​ y, gracias al alto nivel de su literatura, se había ido imponiendo como uno de los grandes idiomas de cultura en la Europa del tiempo.[185]​

Hacia 1550 se empezaron a escribir gramáticas y vocabularios italianos destinados a extranjeros y a menudo escritos por extranjeros.[186]​ A finales del siglo XVI las publicaciones en lengua italiana superaron en número, por primera vez en Italia, a las escritas en latín, que, sin embargo, siguió manteniendo una notable importancia en el campo de la filosofía, del derecho y de las ciencias. No sin razón, un célebre lingüista italiano puso de relieve que entre 1500 y 1600 se produjo la primera unificación lingüística de Italia gracias al italiano escrito, cuando todavía no existía una unidad política del país.[187]​ Antes de que Italia se constituyera en Estado unitario (1861), el italiano ya era el único idioma administrativo y de cultura con difusión nacional y monopolizaba la comunicación pública y literaria,[188]​ pero, a pesar de eso, tenía un carácter fuertemente elitista y solo una pequeña minoría de italianos lo hablaba, o sea todos los que habían cursado estudios superiores. La gran mayoría prefería expresarse en los varios dialectos, hablas e idiomas locales que caracterizaban la comunicación oral en la Italia de entonces. Por lo que se refiere al italiano escrito, su difusión estaba condicionada negativamente en la edad preunitaria por el bajo nivel de alfabetización (algo menos de la cuarta parte de la población italiana sabía leer y escribir en el año 1861).

Con la proclamación del Reino de Italia el italiano fue proclamado lengua oficial del nuevo Estado y tuvo inicio un largo proceso de escolarización de las masas que culminó en el siglo siguiente con la desaparición del analfabetismo, el desarrollo de un tipo de italiano neoestándar contemporáneo universalmente aceptado,[189]​ y la italianización irreversible de los dialectos.[190]​ Durante aquel mismo siglo, gracias también a la difusión de los medios de comunicación de masas (radio, televisión, y, en nuestros días, la informática), se produjo una difusión generalizada del idioma italiano como medio de comunicación coloquial y familiar, convirtiéndose con el tiempo en la lengua materna o primera lengua de todos, o casi todos, los sesenta millones de italianos. Según un informe de la Comisión Europea del año 2006, el 95 % de los italianos y de los extranjeros empadronados en Italia, habla como lengua materna o primera lengua el italiano (el porcentaje de los hablantes de las respectivas lenguas nacionales en los otros cuatro más importantes países europeos es el siguiente: Francia 95 %, Reino Unido 92 %, Alemania 90 % y España 89 %).[191]​

Existen dentro de Italia algunos grupos minoritarios hablantes de lenguas no romances, encontrándose dentro de estos el grupo de habla alemana en la zona del Alto Adigio y unos pocos hablantes de esloveno alrededor de Trieste, además de los arbëreshë (albanoparlantes o albaneses de Italia) y los hablantes de griko y grecánico (unas formas de griego antiguo aún vivas y existentes solo en algunos pueblos del sur de Italia). Otros grupos minoritarios de idiomas parcialmente oficiales incluyen hablantes de francés en las regiones de Valle de Aosta (co-oficial) y Piamonte, la minoría de habla francoprovenzal en la región del Valle de Aosta, el occitano en el Piamonte, el sardo de Cerdeña, el friulano del Friul, el ladino en los picos dolomitas y, exclusivamente en el pueblo sardo de Alguer (Alghero en italiano) el catalán, siendo todos ellos idiomas romances. Entre los idiomas y dialectos no reconocidos por el Estado italiano se encuentra el véneto, pero recibió un reconocimiento por la Asamblea Legislativa Regional véneta como lengua desde 2007 tras la aprobación de la ley 8/2007 para su «protección» y «fomento».[192]​ A pesar de esto, la única lengua oficial del Véneto sigue siendo el italiano.

La educación en Italia es gratuita y obligatoria entre los 6 y los 16 años.[193]​ Consta de cinco niveles: scuola dell'infanzia, scuola primaria, scuola secondaria di primo grado, scuola secondaria di secondo grado y università. Las Scuole superiori universitarie son instituciones independientes similares a las Grandes Escuelas francesas que ofrecen formación e investigación avanzadas a través de cursos de tipo universitario o se dedican a la enseñanza a nivel de graduado o doctorado.

En Italia existen una amplia variedad de universidades y academias. La universidad más antigua del país y de todo Occidente es la Universidad de Bolonia, fundada en 1088,[194]​ una institución que además está considerada por el periódico The Times como la mejor de Italia y una de las 200 mejores del mundo. La Universidad Bocconi de Milán es una de las mejores escuelas de negocio del mundo gracias a su máster en Administración y Dirección de Empresas, cuyos estudiantes acaban trabajando en grandes compañías multinacionales.[195]​ Entre las instituciones politécnicas italianas sobresalen el Politécnico de Turín y el de Milán,[196]​ la Universidad de Roma "La Sapienza" y la Universidad de Milán, todas ellas con presencia habitual en los listados de los mejores centros de estudio en el campo científico.[197]​

Según los Indicadores Científicos Nacionales (1981-2002), una base de datos creada por el Grupo de Servicios de Investigación que contiene listados de estadísticas de citación de publicaciones de más de noventa países, Italia está por encima de la media mundial en la citación en revistas científicas sobre ciencia espacial, matemáticas, informática, neurociencia y física. También están por encima de la media, aunque menos destacadas, la citación de publicaciones italianas sobre ciencias sociales, psicología, psiquiatría, economía y negocios.[198]​

Italia es conocida por tener en general un buen sistema de salud, siendo uno de los mejores de toda Europa y también a nivel mundial según la Organización Mundial de la Salud, con una muy alta esperanza de vida entre su población y unas tasas muy bajas de mortalidad infantil, mortalidad neonatal y mortalidad materna. Al igual que con cualquier otro país desarrollado, Italia también ha desarrollado una distribución adecuada y suficiente del agua y de los alimentos, y los niveles de nutrición y saneamiento son también altos, así como la cocina y dieta relativamente saludables.[199]​[200]​[201]​

Según un estudio publicado en el periódico Corriere della Sera en el año 2006, el 87,8 % de los italianos se declaran católicos, uno de los porcentajes más altos de Europa. Los practicantes alcanzan el 36,8 %, mientras que se reúne en misa todos los domingos el 30,8 % de los entrevistados entre 18 y 24 años, frente al 22,4 % y el 28,5 % de los sujetos entrevistados pertenecientes, respectivamente, a la franja de edad entre 24 y 34 años y entre 34 y 44 años. La discrepancia que hay tras el que se declara católico y el de estricta observancia, aunque es menor respecto a los otros países de Europa occidental, es importante, como indican las opiniones relativas a la fecundación asistida y uniones civiles.[202]​

Los cristianos (católicos, protestantes, ortodoxos, etc.) junto con Testigos de Jehová y mormones representan la religión mayoritaria. Como en muchos países occidentales, el proceso de secularización es creciente, sobre todo entre los jóvenes, aunque no falta la presencia de movimientos católicos como Acción Católica, la Juventud Franciscana, la AGESCI, Comunión y Liberación y Camino Neocatecumenal que intentan revertir o paliar este proceso. La religión más antigua presente en el país es el judaísmo, el cual tiene una presencia ininterrumpida en Roma.[203]​ Actualmente, la comunidad judía se compone de unas 45 000 personas.

Importante para la gastronomía italiana, así como para otras gastronomías europeas, fue el descubrimiento de América, debido a la adquisición de nuevos vegetales como la patata, el tomate, el morrón o el maíz, aunque no fueron utilizados a gran escala hasta el siglo XVIII.[204]​ La gastronomía de Italia es muy variada: el país fue unificado en el año 1861, y sus cocinas reflejan la variedad cultural de sus regiones así como la diversidad de su historia. La cocina italiana está incluida dentro de la denominada gastronomía mediterránea y, como dieta mediterránea, ha sido declarada Patrimonio Cultural Inmaterial de la Humanidad por la Unesco y es imitada y practicada en todo el mundo. Es muy común que se conozca a la gastronomía de Italia por sus platos más famosos, como son la pizza, la pasta, el risotto y el gelato, pero lo cierto es que es una cocina donde existen los abundantes aromas y los sabores del mar Mediterráneo. Se trata de una cocina con fuerte carácter histórico y tradicional, que ha sabido perpetuar recetas antiguas como la polenta (alimento de la legión romana) o la porchetta, entre muchos otros platos italianos que hoy en día se pueden degustar en cualquier lugar del mundo, así como en una típica trattoria italiana. Italia es también el mayor productor de vinos a nivel mundial[205]​ y posee la más grande variedad de quesos en el mundo.[206]​

Los orígenes de la pintura renacentista se hallan en el arte de la Antigua Roma y en el arte helenístico, que fueron retomados por mano de los artistas italianos del Quattrocento y del Cinquecento.[207]​ Los procedimientos usados en esta pintura debieron ser el encausto, el temple y el fresco.[207]​ Sus géneros son el decorativo de vajillas y muros, y el histórico y mitológico en los cuadros murales. Se cultivaron con dicho carácter decorativo mural, el paisaje, la caricatura, el retrato, los cuadros de costumbres, las imitaciones arquitectónicas y las combinaciones fantásticas de objetos naturales, constituyendo el género que los artistas del Renacimiento llamaron grutesco, hallado en las antiguas Termas de Tito y que sirvió al célebre Rafael Sanzio como fuente de inspiración para decorar las Logias del Vaticano. Destacó también el arte pictórico de la civilización romana en el procedimiento del mosaico o la miniatura sobre pergamino.

La pintura renacentista llegó a su fase perfecta poco después que su precursora la escultura, es decir, durante el siglo XV en Florencia y ya entrado el siglo siguiente en los otros países. En general, el siglo XV es de iniciación y los siglos XVI y XVII lo son de apogeo para la pintura del renacimiento clásico.[208]​ Algunos de sus pintores más conocidos son: Sandro Botticelli, Leonardo da Vinci, Miguel Ángel, Donatello, Marco Palmezzano, Andrea Mantegna, Cariani o Rafael Sanzio.[208]​ En Italia, una fase de decadencia a finales del siglo XVI, lleva a los grandes maestros italianos de Renacimiento en otra fase y al desarrollo del arte Barroco. La decadencia total en los diferentes países europeos corresponde al siglo XVIII, siguiéndole la restauración a finales de dicho siglo.

La escultura de Roma, lo mismo que la arquitectura, es original, pero en ella pesan mucho los aportes formales etruscas y griegas (helenísticas), siendo de hecho buena parte de la producción escultórica romana copia de originales griegos.[209]​ Se conservan muchas esculturas romanas, hechas preferentemente en mármol y en menor medida en bronce u otros materiales como el marfil, si bien parte de ella está dañada.[209]​ Son frecuentes el retrato y el relieve histórico narrativo, en los que los romanos fueron grandes creadores. Hay también muchas esculturas de emperadores romanos.

La escultura del Renacimiento clásico se reconoce por dos principios fundamentales: el estudio e imitación de la naturaleza y la adopción de las formas y maneras clásicas de Grecia y Roma para la interpretación de la misma naturaleza en el terreno plástico. Así logró interpretar la naturaleza y traducirla con libertad y soltura por medio del pincel y el escoplo en gran multitud de obras maestras.[210]​ Lorenzo Ghiberti, Donatello y Luca della Robbia, con los discípulos del segundo Verrocchio y Antonio Pollaiuolo, constituyeron la llamada escuela florentina, al mismo tiempo que Jacopo della Quercia formaba en Siena la escuela sienense. También destaca Miguel Ángel, que resume en su persona casi todo el arte escultórico de su época en Italia (1475-1564). A esta misma época de apogeo en el estilo renacentista pertenecen: Benvenuto Cellini, Jacopo Tatti, Pietro Torrigiano, Leone Leoni y Pompeo Leoni. Gian Lorenzo Bernini es el más importante escultor del Barroco. El periodo Neoclásico o de restauración greco-romana comienza con el último cuarto del siglo XVIII, iniciándose por el escultor Antonio Canova (1757-1822).

Un personaje italiano entre los más importantes de la historia de la música es Guido d'Arezzo. Conocido también con el nombre de Guido Aretinus, fue un monje benedictino que reformó el sistema de notación musical. Durante el siglo XI, Guido d'Arezzo perfeccionó la escritura musical con la implementación definitiva de líneas horizontales que fijaron alturas de sonido e inventó, además, las actuales notas musicales, así como el famoso tetragrama, que luego evolucionó al pentagrama.[211]​ Actualmente se le considera el "padre de la música". En la Edad Media, las notas musicales se denominaban por medio de las primeras letras del alfabeto: A, B, C, D, E, F, G (comenzando por la actual nota la). En aquella época solía cantarse un himno a San Juan el Bautista, conocido como Ut queant laxis, atribuido a Pablo el Diácono, que tenía la particularidad de que cada frase musical empezaba con una nota superior a la que antecedía.

La primera obra considerada una ópera, data aproximadamente del año 1597. Esta fue Dafne (obra actualmente desaparecida) escrita por Jacopo Peri para un círculo de humanistas letrados florentinos conocidos como los Camerata Florentina y que fue un intento por revivir la tragedia griega propia del Renacimiento.[212]​ Un siguiente trabajo de Peri, Eurídice, que data del año 1600, es la primera ópera que haya sobrevivido hasta la actualidad.[212]​ No obstante, el uso del término ópera se inicia cincuenta años después, a mediados del siglo XVII para definir las piezas de teatro musical, a las cuales se les refería como dramma per musica ('drama musical') o favola in musica ('fábula musical'). En el año 1637 en Venecia emergió la idea de una "temporada" de óperas de asistencia abierta a todo público, financiada por la venta de entradas.

Influyentes compositores del Renacimiento incluyen a Giovanni Pierluigi da Palestrina, Francesco Cavalli, Carlo Gesualdo y Claudio Monteverdi, cuyo Orfeo (1607) es la ópera más antigua que todavía se representa hoy en día.[213]​ Los libretti italianos fueron la norma, incluso para compositores alemanes como Georg Friedrich Händel que escribía para audiencias londinenses, o Wolfgang Amadeus Mozart en Viena, cerca de finales del siglo XVIII. Los compositores más importantes del Barroco incluyen a Alessandro Scarlatti, Arcangelo Corelli, Antonio Vivaldi y Domenico Scarlatti, los clásicos a Giovanni Paisiello, Domenico Cimarosa, Niccolò Paganini y Gioachino Rossini, y los románticos Vincenzo Bellini, Giuseppe Verdi y Giacomo Puccini. Además en el país son habituales los centros musicales, como los importantes Teatro de La Scala o Teatro de San Carlos.

Numerosos instrumentos musicales fueron inventados en Italia y por italianos, entre los más famosos el piano, el violín, la viola, el violonchelo, el contrabajo o la mandolina, entre otros.

En los años 70, el movimiento del rock progresivo creó bandas como Premiata Forneria Marconi, Goblin, Area. Otros cantantes famosos son Luciano Pavarotti, Domenico Modugno, Raffaella Carrà, Ricchi e Poveri, Al Bano & Romina Power, Mina, Fabrizio De André, Francesco Guccini, Paolo Conte, Lucio Dalla, Lucio Battisti, Nicola di Bari, Sandro Giacobbe, Umberto Tozzi, Laura Pausini, Eros Ramazzotti, Marco Mengoni, Mango, Andrea Bocelli, Tiziano Ferro y Il Volo, estos últimos, pertenecen a la nueva generación de artistas italianos de gran renombre. Además la música popular napolitana del siglo XIX y comienzos del XX ha hecho canciones como ''O sole mio, Funiculì, funiculà y 'O surdato 'nnammurato.

La arquitectura de la Antigua Roma se caracteriza por lo grandioso de las edificaciones, y su solidez que ha permitido que muchas de ellas perduren hasta nuestros días.[214]​ La organización del Imperio romano normalizó las técnicas constructivas de forma que se pueden ver construcciones muy semejantes a miles de kilómetros unas de otras. Tiene su origen en la arquitectura etrusca, sumada a influjos de la arquitectura griega, sobre todo después de las guerras púnicas (146 a. C.).[214]​ Hoy se hace datar la arquitectura romana en la fecha en que se construyeron la primera vía (Vía Apia) y el primer acueducto (Aqua Appia), año 312 a. C. Los elementos más significativos de la arquitectura romana son la bóveda, el arco y por tanto la cúpula.[214]​ Un ejemplo soberbio es la cúpula del panteón de Agripa. Los romanos, no solo construyeron bóvedas de cañón y cúpulas, sino rudimentarias bóvedas de arista y de crucería, como las termas de Caracalla y las de la basílica de Majencio.

La arquitectura gótica llegó de forma tardía y arraigó poco,[215]​ fueron los cistercienses los introductores y fundaron en la región del Lazio la abadía de Fossanuova, primer monumento gótico italiano.[216]​ En el siglo XIII las órdenes mendicantes de dominicos y franciscanos se adhieren al estilo cisterciense, y en este siglo se crean la catedral de Siena, los palacios comunales de Siena y el Palazzo Vecchio de Florencia. Durante el siglo XIV, destacan la catedral de Orvieto, la iglesia de la Santa Cruz y el interior de la iglesia de Santa María Novella. En el siglo XV, el final del gótico empieza a confundirse con los inicios del Renacimiento. En Venecia se termina el palacio Ducal, destacando también el palacio Contarini del Bovolo y Ca' d'Oro. La obra magna del gótico italiano es la catedral de Milán, que destaca por el recargamiento de su decoración y su magnitud.

La arquitectura del Renacimiento es aquella producida durante el período artístico del Renacimiento europeo, que abarcó los siglos XIV, XV y XVI. Se caracteriza por ser un momento de ruptura en la historia de la arquitectura, en especial con respecto al estilo arquitectónico previo, el gótico.[217]​ Produce innovaciones en los medios de producción, como en el lenguaje arquitectónico, que se plasmó en una adecuada y completa teorización, en la nueva actitud de los arquitectos, pasando de ser artesanos a verdaderos profesionales, marcando en cada obra su estilo personal. Las grandes catedrales góticas son en su mayoría anónimas, sin embargo las grandes obras renacentistas están todas firmadas. Inspiraron su labor en su interpretación propia de la Antigüedad clásica, en particular en su vertiente arquitectónica, que consideraban modelo perfecto de las bellas artes. La arquitectura del Renacimiento estuvo bastante relacionada con una visión del mundo durante ese período sostenida en dos pilares esenciales, el clasicismo y el humanismo.

La palabra «barroco» significa "irregular", y es un arte muy cercano al catolicismo en una época de división entre católicos y protestantes.[218]​ La arquitectura del Barroco se inicia con figuras tan determinantes como Gian Lorenzo Bernini y Francesco Borromini. En este período se crean monumentos como la plaza de San Pedro, la iglesia de Sant'Andrea al Quirinale, la fontana de Trevi y la iglesia iglesia de San Carlo alle Quattro Fontane. Destaca a su vez el Barroco siciliano que creció durante la gran reconstrucción edilicia que siguió al terremoto de 1693. El estilo decorativo del barroco siciliano duró apenas cincuenta años, y reflejó perfectamente el orden social de la isla en una época en que -dominada nominalmente por España- fue gobernada de hecho por una aristocracia hedonista y extravagante. La arquitectura barroca ha dado a la isla un carácter arquitectónico que permanece en el siglo XXI.

La historia del cine italiano comenzó apenas algunos meses después de que los hermanos Lumière hubieran descubierto el medio, cuando el papa León XIII fue filmado durante algunos segundos en los Jardines Vaticanos, terminando con la bendición de la cámara.[219]​ La industria cinematográfica italiana nació entre 1903 y 1908 con la Società Italiana Cines, la Ambrosio Film y la Itala Film.[220]​ Más tarde, el cine fue utilizado por Benito Mussolini como propaganda para la Segunda Guerra Mundial.[221]​

Algunos de los más famosos directores italianos, sobre todo de la época del Neorrealismo italiano, han sido Luchino Visconti, Vittorio De Sica, Federico Fellini, Sergio Leone, Pier Paolo Pasolini, Roberto Rossellini o Michelangelo Antonioni u otros contemporáneos como Dario Argento, Sergio Rubini, Giuseppe Tornatore, Matteo Garrone o Paolo Sorrentino. Algunas de las películas más conocidas han sido El gatopardo, La dolce vita, Il buono, il brutto, il cattivo, Ladri di biciclette, La vita é bella o Il Postino, estas últimas con los famosos actores Roberto Benigni y Massimo Troisi u otras como Malèna, La gran belleza o Tale of Tales. Italia es el país que más premios Óscar a la mejor película internacional ha recibido en la historia del cine.

A lo largo de los siglos en la península itálica han nacido grandes científicos. Famosos polímatas italianos como Leonardo da Vinci, Miguel Ángel o Leon Battista Alberti han hecho importantes contribuciones a diversos campos del saber como la biología, la arquitectura o la ingeniería. El físico, astrónomo y matemático Galileo Galilei jugó un papel esencial en la denominada revolución científica gracias a logros como la decisiva mejora del telescopio, que permitió aumentar las observaciones astronómicas y confirmar de manera irrefutable el triunfo de las teorías copernicanas sobre el sistema Ptolemaico.[224]​ Los astrónomos Giovanni Domenico Cassini y Giovanni Schiaparelli realizaron importantes descubrimientos sobre el sistema solar. Joseph-Louis de Lagrange (nacido como Giuseppe Lodovico Lagrangia), Fibonacci y Gerolamo Cardano consiguieron decisivos avances en las matemáticas. El físico Enrico Fermi, galardonado con el Premio Nobel, lideró el equipo que construyó el primer reactor nuclear y codesarrolló la teoría cuántica.[225]​ 

Algunos otros 
científicos italianos de renombre son Amedeo Avogadro (célebre por sus aportaciones a la teoría molecular, por la Ley y la Constante de Avogadro), Evangelista Torricelli (inventor del barómetro), Alessandro Volta (inventor de la batería eléctrica), Guillermo Marconi (inventor de la radio), Antonio Meucci (inventor del teléfono), Ettore Majorana (que descubrió el fermión de Majorana), Emilio G. Segrè (que descubrió dos elementos químicos, el tecnecio y el astato, así como el antiprotón), Federico Faggin (creador del primer microprocesador comercial), Leonardo Chiariglione (creador de los formatos MPEG y MP3) o Carlo Rubbia (premio Nobel de física en 1984 por su trabajo liderando el descubrimiento de los bosones W y Z en el CERN), entre otros.

En biología sobresalen Marcello Malpighi, que fundó en el siglo XVII la histología, Lazzaro Spallanzani, que desarrolló importantes investigaciones sobre las funciones corporales, la reproducción animal y la teoría celular, Camillo Golgi, que entre sus muchos logros tiene el descubrimiento del aparato de Golgi y allanó el camino para la aceptación de la doctrina de la neurona, o Rita Levi-Montalcini, cuyo descubrimiento del factor de crecimiento nervioso le valió el Nobel de fisiología de 1986. En química, Giulio Natta fue premio Nobel de química en 1963 por su trabajo sobre los polímeros. Giuseppe Occhialini recibió el Premio Wolf en Física por el descubrimiento del pion en 1947 y Ennio de Giorgi, que fue Premio Wolf en Matemática en 1990, resolvió el problema de Bernstein sobre superficies mínimas y el decimonoveno problema de Hilbert.

Conforme el Imperio romano de Occidente desaparecía, el latín tradicional se mantuvo vivo gracias a escritores como Casiodoro, Boecio y Símaco. Las artes liberales florecieron en Rávena bajo Teodorico el Grande y los reyes godos se rodearon con maestros de retórica y gramáticos. El año 1230 marca el comienzo de la Escuela poética siciliana y el inicio de una literatura que muestra ya rasgos más uniformes.[226]​

El italiano moderno es un dialecto que ha conseguido imponerse como lengua propia de una región mucho más vasta que su región dialectal, en este caso se trata del dialecto toscano de Florencia, Pisa y Siena y que ha evolucionado a partir del latín.[227]​ El toscano es en efecto la lengua en la que escribieron durante la Edad Media y el Renacimiento Dante Alighieri, Petrarca, Giacomo Leopardi, Alessandro Manzoni, Torquato Tasso, Ludovico Ariosto y Giovanni Boccaccio, considerados como los grandes escritores italianos que ejercieron una gran influencia sobre la literatura europea en general y española en particular; siendo adoptadas algunas formas estróficas como el soneto, la lira o la octava real, al popularizarse los versos endecasílabos y octosílabos.[228]​

Algunos filósofos importantes han sido Tomás de Aquino, Bernardino Telesio, Giordano Bruno, Marsilio Ficino, Giovanni Pico della Mirandola, Nicolás Maquiavelo y Giambattista Vico. Otras figuras importantes del país han sido los poetas Giosuè Carducci, Gabriele D'Annunzio, Salvatore Quasimodo, Eugenio Montale, Giuseppe Ungaretti, la escritora Grazia Deledda, y los autores teatrales Luigi Pirandello y Dario Fo, todos ellos ganadores del Premio Nobel de Literatura.[229]​

El deporte más popular en el país es el fútbol, denominado en italiano calcio. Desde el siglo XVI se practica el llamado calcio florentino, que consiste en dos equipos de 27 jugadores y 5 porteros, donde el objetivo es sumar más puntos que el equipo rival. El calcio se ha intensificado a nivel local, llegando a fundarse en 1898 la Federación Italiana de Fútbol, que se encarga de los campeonatos de fútbol de clubes y de la selección nacional, que ha ganado, entre otros trofeos, cuatro Copas del Mundo FIFA en 1934, 1938, 1982 y 2006, y dos Eurocopas, en 1968 y 2020. Además, ha logrado alcanzar el número 1 de la clasificación de la FIFA en 1993 y 2007. Los principales clubes del país son la Juventus FC, el AC Milan y el Inter de Milán.

La Juventus FC es el club que más títulos ha logrado en el fútbol italiano,[230]​[231]​ es el octavo club con el mayor número de trofeos internacionales conquistados en el mundo (cuarto en Europa)[232]​[233]​ y además, el único club en el planeta que ha conquistado todas las competiciones organizadas por alguna de las seis confederaciones continentales de fútbol y el título mundial interclubes.[234]​[235]​ El AC Milan es el tercer club que más títulos internacionales ostenta (18),[232]​ y el Inter de Milán además de poseer varios títulos nacionales e internacionales es el único que ha participado en todas las ediciones de la Serie A, desde su institución en 1929, además es el único equipo italiano que ha ganado cinco títulos en un mismo año solar. Otros clubes que han ganado torneos internacionales son: la ACF Fiorentina, la SS Lazio, el SSC Napoli, el Parma FC, la AS Roma, el UC Sampdoria y el Torino FC.

En voleibol, tanto a nivel de selección como de equipos de clubes, Italia forma parte, sobre todo en masculino, de la élite mundial del deporte. La selección masculina es una de las más exitosas de la historia tras ganar, entre otros, tres Mundiales (en 1990, 1994 y 1998), siete Eurocopas (en 1989, 1993, 1995, 1999, 2003, 2005 y 2021), ocho World Leagues (en 1990, 1991, 1992, 1994, 1995, 1997, 1999 y 2000) y seis medallas olímpicas (tres platas y tres bronces). También la selección femenina ha conseguido levantar un Mundial (en 2002) y tres Eurocopas (en 2007, 2009 y 2021).

Los equipos de clubes a nivel masculino son los más laureados de Europa, con 70 títulos conseguidos en todas la competiciones, y del Mundo, gracias a sus ocho Campeonatos Mundial de Clubes en nueve ediciones.[236]​ Ganaron trofeos sobre todo conjuntos como el Pallavolo Modena, el Pallavolo Parma, el Pallavolo Torino y el Porto Ravenna Volley en los años 80 y en los primeros años 1990 y el Sisley Treviso, el PV Cuneo, el Lube Macerata y el Trentino Volley en la segunda mitad de los años 1990 y en el siglo XXI. Los equipos femeninos han logrado 39 títulos europeos y 1 mundial de clubes.

En automovilismo, es sede de la famosa escudería Ferrari, la más reconocida en disciplinas como la Fórmula 1. Además aquí se celebra el Gran Premio de Italia, una de las competiciones más importantes a escala internacional. En motociclismo cabe destacar la celebración del Gran Premio de Italia y a los pilotos Giacomo Agostini (15 títulos mundiales), Carlo Ubbiali (9) y Valentino Rossi (9).[237]​

En ciclismo posee una de las tres grandes Vueltas a nivel mundial, el Giro de Italia. Además han destacado ciclistas como Alfredo Binda ganador en tres ocasiones del Campeonato Mundial y en cinco del Giro de Italia, Fausto Coppi también ganador en cinco ocasiones del Giro de Italia y en dos del Tour de Francia o Marco Pantani ganador de un Giro de Italia y un Tour de Francia. También destacan en el ciclismo en pista, modalidad en la que han conseguido varios campeonatos del mundo.

En rugby, los equipos disputan el Pro14 que da acceso a disputar la competición europea Heineken Cup. La selección de rugby participa en el Torneo de las Seis Naciones y habitualmente en la Copa del Mundo, siendo su único título el obtenido en 1997 en la European Nations Cup.

En béisbol han logrado grandes resultados a nivel europeo y participado en todas las competiciones internacionales gracias a la presencia en su equipo nacional de jugadores estadounidenses con ascendencia italiana, los cuales componen casi en su totalidad la selección.

Francesco Molinari ganó el Open Británico del año 2018, siendo el primer jugador italiano en conseguir ganar uno de los torneos majors de golf.

Otro deportista destacado ha sido Reinhold Messner, primera persona del mundo en escalar las 14 cumbres de más de 8000 metros, además de ser la primera persona en ascender el monte Everest en solitario y sin ayuda de oxígeno en 1980.[238]​

En los Juegos Olímpicos es el tercer país con más medallas de oro acumuladas, tras los Estados Unidos, la Unión Soviética y Alemania, siendo el cuarto con más participaciones (45) tras Francia, Reino Unido y Suiza con 46 ediciones.[239]​
Los deportistas que más medallas han obtenido han sido Edoardo Mangiarotti (6 medallas de oro, 5 de plata y 2 de bronce), Nedo Nadi (6 medallas de oro) y Valentina Vezzali (5 medallas de oro, 1 de plata y 1 de bronce) todos en esgrima. Además el país ha organizado cuatro ediciones de los Juegos Olímpicos, una de verano en Roma 1960 y tres de invierno, en Cortina d'Ampezzo en 1944 y 1956 y en Turín en 2006.[240]​

 Partido Conservador

Jorge Ricardo Isaacs Ferrer (Santiago de Cali, 1 de abril de 1837-Ibagué, 17 de abril de 1895) fue un novelista, escritor y poeta colombiano del género romántico. Jorge Isaacs vivió durante la época de consolidación de la República.[1]​ Su única novela, María, se convirtió en una de las obras más notables del movimiento romántico en la literatura en español.

Se sabe muy poco sobre su infancia. Su lugar de nacimiento fue Santiago de Cali.[2]​ Estudió en Cali, luego en Popayán, y por último en Bogotá, entre 1848 y 1852, durante el gobierno de José Hilario López. Su padre era George Henry Isaacs, un judío inglés de origen sefardí oriundo de Jamaica.

En su poesía, Isaacs evoca a Valle del Cauca como el espacio idílico en que transcurrió su infancia, y la marcha a Bogotá debió suponer para él un cambio difícil. Regresó a Cali en 1852. 

En 1854, luchó en las campañas de Cauca contra la dictadura del general José María Melo, por 7 meses. Su familia atravesó por una difícil situación económica a causa de la guerra civil. 

En 1856 se casó con Felisa González Umaña, quien contaba por entonces diecinueve años, y que le daría abundante descendencia y perseverancia para que se escudara de ellos.

Intentó dedicarse al comercio, sin mucho éxito, y probó suerte con la literatura. Sus primeros poemas, los cuales datan de los años 1859 a 1860; en la misma época, emprende la escritura de varios dramas históricos. 

En 1860 tomó de nuevo las armas para combatir al general Tomás Cipriano de Mosquera, que se había levantado contra el gobierno central, y combatió en la batalla de Manizales. 

En 1861 murió su padre; terminada la guerra, Isaacs regresó a Cali para encargarse de los negocios paternos, llenos de deudas. Tuvo que desprenderse de las haciendas "La Rita" y "La Manuelita". Sus desventuras económicas le llevaron en busca de abogados a Bogotá, donde encontró eco su actividad literaria. Leyó sus poemas a los miembros de la tertulia "El Mosaico", quienes decidieron costear su publicación (Poesías, 1864). 

En 1864 supervisó los trabajos del camino de herradura entre Buenaventura y Cali. Durante el año en que desempeñó este trabajo, comenzó a escribir su novela María. En esta época, así mismo, debido a lo insalubre del clima, contrajo  paludismo, enfermedad que lo conduciría a su deceso a los 58 años de edad.

En 1868 llega al Congreso de la República de Colombia como representante del Estado Soberano del Tolima [3]​, en el año 1870 fue el redactor e impulsor de la ley 78 del mismo año, mediante la cual el Estado colombiano ofrecía su ciudadanía a todos los paraguayos, para evitar que tuvieran la condición de apátridas si el Paraguay llegaba a desaparecer en el marco de Guerra de la Triple Alianza[4]​[5]​[6]​

Militó al principio en el Partido Conservador, pero después adhirió al liberalismo radical y, en 1870, fue nombrado cónsul general en Chile. A su regreso, intervino activamente en la política de Cauca, tanto como editor de periódicos como representando a su departamento en la Cámara de Representantes. Intervino de nuevo en las luchas políticas de 1876, en las que tomó de nuevo las armas. Fue expulsado de la Cámara de Representantes en 1879, a raíz de un incidente en que Isaacs, ante una sublevación conservadora, se proclamó jefe político y militar de Antioquia.

Tras este incidente, se retiró de la política, y publicó, en 1881, el primer canto de un extenso poema que no llegó a concluir, titulado Saulo. Nombrado secretario de la Comisión Científica, exploró el departamento de Magdalena, en el norte de Colombia, hallando importantes yacimientos de carbón, petróleo.

Los últimos años de su vida los pasó retirado en Ibagué (donde había dejado alojada su familia años antes), departamento de Tolima, proyectando una novela histórica que habría de ser su obra maestra y que jamás llegó a escribir. Murió en Ibagué el 17 de abril de 1895, siendo su última voluntad que su cadáver fuera sepultado en el Cementerio San Pedro de Medellín; no obstante, siempre expresó su amor por Cauca (su querido "país vallecaucano"): «¡Sí, mucho amo al Cauca, aunque es tan ingrato con sus propios hijos.

La obra literaria de Isaacs se reduce al libro de poemas que publicó en 1864 y a su única novela, María (1867), considerada una de las obras más destacadas de la literatura hispanoamericana del siglo xix. 

La novela, basada en experiencias románticas, tiene un tono elegíaco, y narra la historia de los amores trágicos de María y su primo Efraín, en el departamento del Valle del Cauca. Como el propio autor, Efraín debe abandonar el Valle para seguir estudios en Bogotá. Este viaje lo obliga a separarse de su prima María, de la que está enamorado.

Efraín y María vivieron juntos durante meses, al cabo de los cuales el joven debe viajar a Londres para completar su educación. Cuando regresa, seis años después, María está muerta. Algunos autores afirman que el personaje de María fue inspirada por María Mercedes Cabal quien vivió en la hacienda "El Paraíso" y luego sería esposa del Presidente Manuel María Mallarino.[7]​

La obra se ha relacionado con Chateaubriand, pero puede encontrarse también en ella un sentimiento ominoso de la existencia que recuerda a Edgar Allan Poe. La novela destaca por el sentimiento del paisaje, así como por la calidad artística de su prosa. Puede considerarse precursora de la novela criollista de las décadas de 1920 y 1930.

María se publicó en 1867 y tuvo un éxito inmediato. Fue traducida a 31 idiomas. Tanto en Colombia como en otros países latinoamericanos Isaacs se convirtió en una figura muy conocida, lo que dio inicio a una dilatada carrera periodística y política. Como periodista, Isaacs dirigió en 1867 el diario La República, de orientación conservadora moderada, donde publicaba artículos de índole política. En 1881 publicó la poesía "Primer Canto del Poema Saulo" y en 1893 "La Tierra de Córdoba".

Hay dos obras de Isaacs que dan cuenta de su interés por la literatura campesina y las culturas indígenas de Colombia. Canciones y coplas populares y Las tribus indígenas del Magdalena. Ambas obras han sido reeditadas de forma digital por el Banco de la República, a través de su Biblioteca Virtual.

Casa Jorge Isaacs en Santiago de Cali, Valle del Cauca,  Colombia

Monumento a Jorge Isaacs, Santiago de Cali, Valle del Cauca.

Jorge Isaacs en 1856.

Portada de la novela María de Jorge Isaacs publicada en 1897 por Éditions Mateu. Prólogo de José María de Pereda

Manuela Ferrer Scarpetta y George Henry Isaacs, los padres del poeta, con otro de sus hijos.



La Guerra Fría fue un enfrentamiento político, económico, social, ideológico, militar e informativo iniciado tras finalizar la Segunda Guerra Mundial entre el bloque Occidental (occidental-capitalista), liderado por los Estados Unidos, y el bloque del Este (oriental-comunista), liderado por la Unión Soviética.

La primera fase de la Guerra Fría comenzó tras el final de la Segunda Guerra Mundial, en 1945. Estados Unidos creó la alianza militar de la OTAN en 1949, con el objetivo de frenar la influencia soviética en Europa. La Unión Soviética respondió a la creación de esta alianza con el establecimiento del Pacto de Varsovia en 1955. Las principales crisis de esta fase incluyeron el bloqueo de Berlín de 1948-1949, la segunda fase de la guerra civil china (1946-1949), la guerra de Corea (1950-1953), la crisis de Suez de 1956, la crisis de Berlín de 1961 y la crisis de los misiles cubanos de 1962.

La Unión Soviética y los Estados Unidos comenzaron a competir por la influencia en América Latina, Oriente Próximo y los estados recién descolonizados de África y Asia, donde el comunismo tenía una gran fuerza y donde se vivieron conflictos  tales como la Emergencia Malaya o la guerra de Indochina.

Después de la crisis de los misiles cubanos, comenzó una nueva fase que vio cómo la ruptura sino-soviética —entre la República Popular China y la URSS— complicaba las relaciones dentro de la esfera comunista, mientras que Francia, aliado de los Estados Unidos, comenzó a exigir una mayor autonomía de acción llegando incluso a abandonar la estructura militar de la OTAN.[1]​[2]​ La URSS invadió Checoslovaquia para reprimir la Primavera de Praga de 1968, mientras que Estados Unidos experimentó una agitación interna del movimiento de derechos civiles y oposición a la guerra de Vietnam. En las décadas de 1960 y 1970, un movimiento internacional por la paz se arraigó entre los ciudadanos de todo el mundo. Se produjeron movimientos contra las pruebas de armas nucleares y por el desarme nuclear, con grandes protestas contra la guerra. En la década de 1970 ambos comenzaron a hacer concesiones para la paz y la seguridad, marcando el comienzo de un período de distensión (o détente) que vio las conversaciones estratégicas de limitación de armas y las relaciones de apertura de los Estados Unidos con la República Popular China como un contrapeso estratégico para la URSS. Simultáneamente los Estados Unidos desarrolló la Doctrina de la Seguridad Nacional, para promover en América Latina, a través del Plan Cóndor, la instalación de dictaduras militares que reprimieran mediante el terrorismo de Estado, los movimientos políticos, sociales, sindicales y estudiantiles de sus poblaciones.  

La fase de estabilidad se derrumbó a finales de la década con el comienzo de la guerra de Afganistán de 1979. La década de 1980 fue otro período de tensión elevada. Estados Unidos aumentó las presiones diplomáticas, militares y económicas contra la Unión Soviética, en un momento en que esta ya sufría un estancamiento económico. A mediados de la década de 1980, el nuevo líder soviético Mikhail Gorbachev introdujo las reformas conocidas como Glásnost (1985) y Perestroika (1987) y puso fin a la participación soviética en Afganistán. Las presiones por la soberanía nacional se fortalecieron en Europa del Este, y Gorbachov se negó a apoyar militarmente a sus gobiernos por más tiempo en la llamada Doctrina Sinatra. El resultado en el año 1989 fue una ola de revoluciones que (con la excepción de Rumanía) derrocó pacíficamente a todos los gobiernos comunistas de Europa Central y Oriental. El propio Partido Comunista de la Unión Soviética perdió el control del territorio y fue prohibido luego de un intento fallido de golpe de Estado en agosto de 1991 contra el gobierno anticomunista de Borís Yeltsin en la RSFS de Rusia. Esto a su vez condujo a la disolución formal de la URSS en diciembre de 1991, la declaración de independencia de sus repúblicas constituyentes y el colapso de los gobiernos comunistas en gran parte de África y Asia.

A fines de la Segunda Guerra Mundial, el escritor inglés George Orwell usó «guerra fría» como un término general en su ensayo You and the Atomic Bomb (en español, «La bomba atómica y tú»), publicado el 19 de octubre de 1945 en el periódico británico Tribune. En un mundo amenazado por la guerra nuclear, Orwell se refirió a las predicciones de James Burnham de un mundo polarizado y escribió:

El mismo Orwell escribió en el The Observer del 10 de marzo de 1946 que «después de la conferencia de Moscú en diciembre pasado, Rusia comenzó a hacer una guerra fría contra Reino Unido y el Imperio británico».[4]​

El primer uso del término para describir específicamente la confrontación geopolítica entre la Unión Soviética y los Estados Unidos de posguerra fue en un discurso de Bernard Baruch, un financiero e influyente asesor presidencial estadounidense, el 16 de abril de 1947.[5]​ En el discurso Baruch dijo: «no nos engañemos: estamos inmersos en una guerra fría». El término fue popularizado por el columnista Walter Lippmann con su libro The Cold War.[6]​ Cuando se le preguntó en 1947 sobre la fuente de la expresión, Lippmann lo remontó a la guerre froide, un término francés de los años treinta.[7]​

Existe un cierto desacuerdo sobre cuándo comenzó exactamente la Guerra Fría. Mientras que la mayoría de historiadores sostienen que empezó nada más acabar la Segunda Guerra Mundial, otros afirman que los inicios de la Guerra Fría se remontan al final de la Primera Guerra Mundial, en las tensiones que se produjeron entre el Imperio ruso, por un lado, y el Imperio británico y Estados Unidos, por el otro.[8]​ El choque ideológico entre el comunismo y el capitalismo empezó en 1917, tras el triunfo de la Revolución rusa, de la que Rusia emergió como el primer país socialista. Este fue uno de los primeros eventos que provocó erosiones considerables en las relaciones ruso-estadounidenses.[8]​

Algunos eventos previos al final de la I Guerra Mundial fomentaron las sospechas y recelos entre soviéticos y estadounidenses: la idea bolchevique en el cual el capitalismo debía ser derribado por la fuerza para ser reemplazado por un sistema comunista,[9]​ la retirada rusa de la I Guerra Mundial tras la firma del Tratado de Brest-Litovsk con el Segundo Reich, la intervención estadounidense en apoyo del Movimiento Blanco durante la guerra civil rusa y el rechazo estadounidense a reconocer diplomáticamente a la Unión Soviética hasta 1933.[10]​ Junto a estos diferentes acontecimientos durante el periodo de entreguerras agudizaron las sospechas: Acuerdos de Múnich, y la firma del pacto antikomintern esos dos son antecedentes de alianzas anticomunistas previas a la OTAN, la firma del Tratado de Rapallo y del Pacto germano-soviético de no agresión son otros ejemplos.[11]​

Durante las etapas finales de la Segunda Guerra Mundial, los soviéticos comienzan a sospechar que británicos y estadounidenses, quienes habían optado por dejar a los rusos el grueso del esfuerzo bélico, forjarían una unión contra los soviéticos (Operación Impensable) una vez que la guerra estuviera decidida a favor de los Aliados, para forzar a la Unión Soviética a firmar un tratado de paz ventajoso para los intereses occidentales. Estas sospechas minaron las relaciones entre los aliados durante la fase final de la contienda.[12]​

Los Aliados no estaban de acuerdo en cómo deberían dibujarse las fronteras europeas tras la guerra.[13]​ El modelo estadounidense de estabilidad se basaba en la instauración de gobiernos y mercados económicos parecidos al estadounidense (capitalista), y la creencia de que los países así gobernados acudirían a organizaciones internacionales, como la recién creada ONU, para arreglar sus diferencias.[14]​

Sin embargo, los soviéticos creían que la estabilidad habría de basarse en la integridad de las propias fronteras de la Unión Soviética.[15]​ Este razonamiento nace de la experiencia histórica de los rusos, que habían sido invadidos desde el Oeste durante los últimos ciento cincuenta años.[16]​ El daño sin precedentes infligido a la Unión Soviética durante la invasión nazi (alrededor de veintisiete millones de muertos y una destrucción generalizada y casi total del territorio invadido)[17]​ conminó a los líderes soviéticos a asegurarse de que el nuevo orden europeo posibilitara la existencia a largo plazo del régimen soviético, y que este objetivo solo podría conseguirse mediante la eliminación de cualquier gobierno hostil a lo largo de la frontera occidental de la Unión, y el control directo o indirecto de los países limítrofes a esta frontera, para evitar la aparición de fuerzas hostiles en estos países.[13]​

Durante la Conferencia de Yalta, en febrero de 1945, los aliados trataron de crear un marco sobre el que trabajar en la reconstrucción de la Europa de la posguerra, pero no se llegó a ningún consenso.[18]​ Tras el fin de la Segunda Guerra Mundial en Europa, los soviéticos ocuparon de facto las zonas de la Europa del Este que habían defendido, mientras que las fuerzas estadounidenses y sus aliados se mantenían en la Europa Occidental. En el caso de la Alemania ocupada, se crearon las zonas de ocupación aliada en Alemania y una difusa organización cuatripartita compartida con franceses y británicos. Para el mantenimiento de la paz mundial, los aliados crearon las Naciones Unidas, pero su capacidad de actuación estaba limitada por el Consejo de Seguridad, en el que las potencias victoriosas de la Segunda Guerra Mundial se aseguraron el poder de vetar aquellas acciones contrarias a sus intereses.[19]​ La ONU se convirtió así durante sus primeros años en un foro donde las potencias se enzarzaban en luchas retóricas, y que los soviéticos utilizaban con fines propagandísticos.[20]​

En la Conferencia de Potsdam, iniciada a finales de julio de 1945, emergieron las primeras diferencias relevantes acerca de Alemania y la Europa del Este;[21]​ Los participantes de la conferencia no ocultaron sus antipatías, y el uso de un lenguaje belicoso confirmó las intenciones mutuamente hostiles que defendían cada vez con más ahínco.[22]​ Durante esta conferencia, Truman informó a Stalin que los Estados Unidos habían creado una nueva arma. Stalin, que ya estaba al tanto de los avances estadounidenses en el desarrollo de la bomba atómica, expresó su deseo de que aquella nueva arma fuera usada contra Japón.[23]​ Una semana después de finalizar la conferencia, los Estados Unidos lanzaron la bomba atómica sobre Hiroshima y Nagasaki.

En febrero de 1946, George Kennan escribió desde Moscú el conocido como Telegrama Largo, en el que se apoyaba una política de inflexibilidad con los soviéticos, y que se convertiría en una de las teorías básicas de los estadounidenses durante el resto de la Guerra Fría.[24]​ En septiembre de ese mismo año, los soviéticos respondieron con otro telegrama firmado por Nikolái Vasílievich Novikov, aunque escrito junto con Viacheslav Mólotov; en este telegrama se sostenía que Estados Unidos usaba su monopolio en el mundo capitalista para desarrollar una capacidad militar que creara las condiciones para la consecución de la supremacía mundial a través de una nueva guerra.[25]​

Semanas después de la recepción del «Telegrama Largo», el primer ministro británico Winston Churchill pronunció su famoso discurso sobre la Cortina de Hierro o Telón de Acero en una Universidad de Misuri.[26]​ El discurso trataba de promover una alianza anglo-estadounidense contra los soviéticos, a los que acusó de haber creado una «cortina de hierro» (iron curtain) desde Stettin, en el Báltico, a Trieste, en el Adriático.[27]​

Hacia 1947, los consejeros del presidente estadounidense Harry S. Truman le urgieron a tomar acciones para contrarrestar la creciente influencia de la Unión Soviética, citando los esfuerzos de Stalin para desestabilizar los Estados Unidos y azuzar las rivalidades entre los países capitalistas con el fin de provocar una nueva guerra.[28]​

En Asia, el ejército comunista chino había ocupado Manchuria durante el último mes de la Segunda Guerra Mundial y se preparaba para invadir la península coreana más allá del paralelo 38.[29]​ Finalmente, el ejército comunista de Mao Zedong, aunque fue poco receptivo a la escasa ayuda soviética, consiguió derrotar al prooccidental ejército nacionalista chino (Kuomintang), apoyado por Estados Unidos.[30]​

Desde finales de la década de 1940, la Unión Soviética consiguió instaurar gobiernos marioneta en Bulgaria, Checoslovaquia, Hungría, Polonia, Rumanía y Alemania Oriental, lo que le permitió mantener una fuerte presencia militar en estos países.[31]​ En febrero de 1947, el gobierno británico anunció que no podía seguir financiando al régimen militar griego contra los insurgentes comunistas en el contexto de la Guerra civil griega. El gobierno estadounidense puso en práctica por primera vez la Teoría de la Contención,[32]​ que tenía como objetivo frenar la expansión comunista, especialmente en Europa. Truman enmarcó esta teoría dentro de la Doctrina Truman, dada a conocer a través de un discurso del presidente en el que se definía el conflicto entre capitalistas y comunistas como una lucha entre «pueblos libres» y «regímenes totalitarios».[32]​

En Estados Unidos, se extendió la idea de que el equilibrio de poder en Europa no se alcanzaría solo por la defensa militar del territorio, sino que también se necesitaba atajar los problemas políticos y económicos para evitar la caída de la Europa Occidental en manos comunistas.[31]​ Sobre la base de estas ideas, la Doctrina Truman sería complementada en junio de 1947 con la creación del Plan Marshall, un plan de ayudas económicas destinado a la reconstrucción de los sistemas político-económicos de los países europeos y, mediante el afianzamiento de las estructuras económicas capitalistas y el desarrollo de las democracias parlamentarias, frenar el posible acceso al poder de partidos comunistas en las democracias occidentales europeas (como en Francia o Italia). Asimismo, el Plan Marshall constituyó la remodelación de numerosas ciudades europeas que habían quedado destruidas por la Segunda Guerra Mundial.[33]​

Stalin vio en el Plan Marshall una táctica estadounidense para mermar el control soviético sobre la Europa Oriental. Creyó que la integración económica de ambos bloques permitiría a los países bajo órbita soviética escapar del control de Moscú, y que el Plan no era más que una manera que tenían los EE. UU. para «comprar» a los países europeos.[34]​ Por lo tanto, Stalin prohibió a los países de la Europa Oriental participar en el Plan Marshall. A modo de remiendo, Moscú creó una serie de subsidios y canales de comercio conocidos primero como el Plan Molotov, que poco después se desarrollaría dentro del COMECON.[10]​ Stalin también se mostró muy crítico con el Plan Marshall porque temía que dichas ayudas provocaran un rearme de Alemania, que fue una de sus mayores preocupaciones respecto al futuro de Alemania tras la guerra.

En 1948 como represalia por los esfuerzos de Estados Unidos por reconstruir la economía alemana, Stalin, quien temía que la población del Sector Soviético de Alemania se posicionase a favor del Bloque capitalista, cerró las vías terrestres de acceso a Berlín Oeste, imposibilitando la llegada de materiales y otros suministros a la ciudad.[35]​ Este hecho, conocido como el bloqueo de Berlín, precipitó una de las mayores crisis de principios de la Guerra Fría. 

El puente aéreo organizado por Estados Unidos y el Reino Unido, destinado a proveer de suministros al bloqueado sector occidental de la ciudad, superó todas las previsiones, desbaratando la suposición soviética de que el sector occidental se rendiría ante el oriental por falta de suministros. Finalmente el bloqueo se levantó pacíficamente. Ambos bandos usaron este bloqueo con fines propagandísticos: los soviéticos para denunciar el supuesto rearme de Alemania favorecido por Estados Unidos, y los estadounidenses para explotar su imagen de benefactores. El mejor ejemplo de esto fue la llamada Operación Little Vittles, donde los aviones que contrarrestaban el bloqueo de Berlín lanzaron dulces entre los niños berlineses.

En julio, el presidente Truman anula el Plan Morgenthau, una serie de proposiciones acordadas con los soviéticos tras el fin de la guerra, que imponía severas condiciones a la reconstrucción alemana (entre ellas, la prohibición explícita de que los EE. UU. facilitaran ayudas a la reconstrucción del sistema económico alemán). Este plan fue sustituido por una nueva directiva (llamada JSC 1779) mucho más benévola con la reconstrucción alemana, y que enfatizaba la necesidad de crear una Alemania económicamente fuerte y estable para conseguir la prosperidad en toda Europa.[36]​

En septiembre los soviéticos crean el Kominform, una organización cuyo propósito era mantener la ortodoxia ideológica comunista dentro del movimiento comunista internacional. En la práctica, se convirtió en un mecanismo de control sobre las políticas de los estados satélite soviéticos, coordinando el ideario y las acciones de los partidos comunistas del bloque del Este.[34]​ El Kominform tuvo que hacer frente a una inesperada oposición cuando, en junio del siguiente año, la ruptura Tito-Stalin obligó a expulsar a Yugoslavia de la organización, que mantuvo un gobierno comunista, pero se identificó como un país neutral dentro de la Guerra Fría.[37]​ Junto con el Kominform, la policía secreta soviética, el NKVD, se ocupaba de mantener una red de espionaje en los países satélite bajo el pretexto de acabar con elementos anticomunistas.[38]​ El NKVD (y sus sucesores) acabaron por convertirse en organizaciones parapoliciales encargadas de sesgar cualquier intento de alejarse de la órbita de Moscú y la ortodoxia soviético-comunista (ortodoxia: Conformidad con los principios de una doctrina o con las normas o prácticas tradicionales, aceptadas por la mayoría como las más adecuadas en un determinado ámbito).[39]​

En abril de 1949 se constituye la OTAN, con lo que los Estados Unidos tomaron formalmente la responsabilidad de defender la Europa Occidental, junto a los países europeos que se adhirieron a la OTAN.[38]​ En agosto de ese año, la Unión Soviética detona su primera bomba atómica.[10]​

En mayo de 1949, se establece la República Federal de Alemania como producto de la fusión de las zonas de ocupación aliada.[21]​ Como réplica, en octubre de ese año, los soviéticos proclaman su zona de ocupación como la República Democrática Alemana.[21]​ Desde el inicio de la existencia de la RFA, Estados Unidos ayuda a su desarrollo militar. Para evitar que la RFA acabe por convertirse en miembro de la OTAN, el primer ministro soviético, Lavrenti Beria, propone fusionar ambos países en una sola Alemania que se mantendría neutral.[40]​ La proposición no salió adelante y en 1955 se admite a la RFA como miembro de la OTAN.[21]​

Dentro de esta estrategia de generalización de la «contención», el teatro de operaciones se amplió de Europa a Asia, África y América Latina, con la intención de detener los movimientos revolucionarios, muchas veces financiados desde la Unión Soviética, como ocurría en el caso de las excolonias europeas del Sudeste Asiático.[41]​ A principios de la década de 1950, los EE. UU. formalizaron alianzas militares con Japón, Australia, Nueva Zelanda, Tailandia y Filipinas (alianzas englobadas en el ANZUS y el SEATO), garantizando a Estados Unidos una serie de bases militares a lo largo de la costa asiática del Pacífico.[21]​

En otoño de 1945 la Unión Soviética se negó a evacuar sus tropas del Azerbaiyán iraní, ocupado desde la Invasión anglosoviética de Irán en 1941 y donde el partido comunista Tudeh mantenía una república con gobierno propio. Debido a la influencia del Tudeh se declaró una huelga general en la refinería de Abadán, de la Anglo-Iranian Oil Company (anteriormente Anglo-Persian Oil Company). El gobierno se mantuvo firme y en octubre de 1946 caía Azerbaiyán. En enero de 1948 se suspendió la ley marcial tras siete años de aplicación. El 4 de febrero de 1949 se prohibió el Tudeh tras sufrir un atentado el sah Mohammad Reza Pahleví.

En 1949, el Ejército Rojo de Mao Zedong se proclama vencedor de la guerra civil china tras derrotar a los nacionalistas del Kuomintang, que contaban con el respaldo de Estados Unidos. Inmediatamente, la Unión Soviética establece una alianza con los vencedores, que habían creado un nuevo Estado comunista con la denominación de República Popular China.[42]​ Al coincidir en el tiempo la Revolución China con la pérdida del monopolio atómico de Estados Unidos (tras el inesperado éxito del RDS-1), la administración del presidente Truman trató de generalizar la Teoría de la Contención.[10]​ En un documento secreto fechado en 1950 (conocido como el NSC-68)[43]​ la administración de Truman proponía reforzar los sistemas de alianzas prooccidentales y cuadruplicar los gastos en Defensa.[10]​

Uno de los ejemplos más significativos de la implementación de la contención fue la intervención estadounidense en la guerra de Corea. En junio de 1950, después de años de hostilidades mutuas, Corea del Norte, gobernada por Kim Il-sung invadió Corea del Sur a través del Paralelo 38. Stalin había sido reacio a apoyar la invasión, pero finalmente envió asesores y pilotos.[44]​ Para sorpresa de Stalin, las Resoluciones 82 y 83 del Consejo de Seguridad de las Naciones Unidas respaldaron la defensa de Corea del Sur, aunque los soviéticos estaban boicoteando reuniones en protesta por el hecho de que la República de China (Taiwán), no la República Popular de China, tenía un asiento permanente en el consejo.[45]​ Una fuerza de la ONU de dieciséis países[46]​ se enfrentó a Corea del Norte, aunque el 40 % de las tropas eran surcoreanas, y alrededor del 50 % eran de los Estados Unidos.[47]​

Estados Unidos inicialmente parecía seguir la contención cuando entró por primera vez en la guerra. Esto dirigió la acción de los EE. UU. Para hacer retroceder a Corea del Norte a través del paralelo 38 y restaurar la soberanía de Corea del Sur, permitiendo la supervivencia de Corea del Norte como estado. Sin embargo, el éxito del desembarco de Inchon inspiró a los Estados Unidos y las Naciones Unidas a adoptar una estrategia de reversión y derrocar a Corea del Norte comunista, lo que permitió elecciones a nivel nacional bajo los auspicios de la ONU.[48]​ El general Douglas MacArthur avanzó a través del paralelo 38 hasta Corea del Norte. Los chinos, temerosos de una posible presencia estadounidense en su frontera o incluso de una invasión de ellos, enviaron al Ejército Popular de Liberación y derrotaron a las fuerzas de la ONU, empujándolos nuevamente por debajo del paralelo 38. Truman insinuó públicamente que podría usar la bomba atómica, pero Mao no se conmovió. El episodio se usó para apoyar la sabiduría de la doctrina de la contención en oposición al retroceso. Los comunistas fueron empujados más tarde alrededor de la frontera original, con cambios mínimos. Entre otros efectos, la guerra de Corea impulsó a la OTAN a desarrollar una estructura militar unificada.[49]​

En 1953 se produjeron cambios en el liderazgo político de ambos bandos, que dieron comienzo a una nueva fase en la Guerra Fría.[50]​ En enero de 1953, Dwight D. Eisenhower fue investido presidente de EE. UU. Durante los últimos meses de la administración Truman, el presupuesto para Defensa se había cuadruplicado; Eisenhower pretendió reducir el gasto militar apoyándose en la superioridad nuclear estadounidense y en una gestión más efectiva de las situaciones provocadas por la Guerra Fría.[10]​

En marzo, muere Stalin, y Nikita Jrushchov se convierte en el nuevo líder de la Unión Soviética, tras haber depuesto y ejecutado al jefe de la NKVD, Lavrenti Beria, y finalmente al apartar del poder a Georgy Malenkov y Vyacheslav Molotov. El 25 de febrero de 1956, Jrushchov impresionó a los delegados del XX Congreso del PCUS al denunciar los crímenes cometidos por Stalin durante su discurso Acerca del culto a la personalidad y sus consecuencias. En el discurso se sostenía que la única manera de conseguir una reforma exitosa era siendo conscientes de los errores cometidos en el pasado apartándose de las políticas llevadas a cabo por Stalin.[50]​

Tras el cambio de líder en la Unión Soviética se produjeron numerosas fricciones con algunos de los aliados soviéticos más proclives al estalinismo o a la figura de Stalin. La más notable de estas discrepancias entre países comunistas se plasmó en la ruptura de la alianza chino-soviética. Mao Tse Tung defendió la figura de Stalin tras la muerte de este en 1953, y describió a Jrushchov como un arribista superficial, acusándolo de haber perdido el perfil revolucionario del Estado.[51]​

Jrushchov se obcecó en reconstruir la alianza chino-soviética, pero Mao consideró que sus propuestas eran inútiles y descartó cualquier tipo de proposición.[51]​ Chinos y soviéticos comenzaron un despliegue propagandístico dentro de la propia esfera comunista[52]​ que acabaría convirtiéndose en una lucha por el liderazgo del movimiento comunista internacional,[53]​ hasta llegar tres años más tarde al enfrentamiento militar directo en la frontera que ambas potencias compartían.[54]​

El 18 de noviembre de 1956, durante un discurso frente a embajadores del bloque occidental en la embajada de Polonia, Jrushchov pronunció unas polémicas palabras que impresionaron a los presentes: «Os guste o no, la Historia está de nuestro lado. ¡Os enterraremos!»[55]​ Sin embargo, posteriormente aclaró que no se refería a la posibilidad de una guerra nuclear, sino a la inevitabilidad histórica de la victoria del comunismo sobre el capitalismo.[56]​

El Secretario de Estado de Eisenhower, John Foster Dulles, inició un nuevo giro en la Teoría de la Contención al enfatizar en el posible uso de armas nucleares contra los enemigos de EE. UU.[50]​ Agregó al discurso clásico de la «contención» un nuevo punto de apoyo al anunciar la posibilidad de una «represalia masiva», haciendo entender que cualquier agresión soviética sería respondida con todos los medios necesarios. Esta nueva teoría se puso en práctica durante la crisis de Suez, donde la superioridad nuclear de Estados Unidos, junto con la amenaza de usarla, retrajo a los soviéticos de comenzar una batalla abierta contra intereses estadounidenses.[10]​

Desde 1957 hasta 1961, Jrushchov mostró abiertamente su confianza en la superioridad nuclear de la Unión Soviética. Afirmaba que la capacidad destructiva de los misiles de la Unión Soviética era muy superior a la de Estados Unidos y que podrían alcanzar cualquier ciudad estadounidense o europea. Sin embargo, Jrushchov rechazaba la visión de Stalin de una guerra inevitable y declaró que su intención era abrir una nueva época de coexistencia pacífica.[57]​ Jrushchov trató de reformular la idea soviético-estaliniana, según la cual la lucha de clases a nivel mundial provocaría inevitablemente una gran guerra entre proletarios y capitalistas cuyo resultado final sería el triunfo del Comunismo. Jrushchov arguyó que la guerra era evitable, pues durante el tiempo de paz el capitalismo se colapsaría por sí mismo,[58]​ mientras que la paz dejaba tiempo y recursos disponibles para mejorar la capacidad económico-militar de la Unión Soviética.[59]​ Los EE. UU. se defendían mostrando su capacidad militar fuera de sus fronteras y el éxito del capitalismo liberal en todo el mundo.[60]​ A pesar del discurso de Kennedy que caracterizó a la Guerra Fría como una «lucha por las mentes de los hombres» entre dos sistemas de organización social, a mediados de la década de 1960 la lucha ideológica había quedado apartada frente a los objetivos geopolíticos de carácter militar y económico.[61]​

Aunque ciertamente hubo una transitoria relajación de las tensiones tras la muerte de Stalin en 1953, la situación en Europa seguía siendo incómoda, con ambos bandos fuertemente armados, pero sin movimientos aparentes.[62]​ Las tropas estadounidenses seguían apostadas indefinidamente en la Alemania del Oeste y las tropas soviéticas continuaban estacionadas indefinidamente por toda la Europa del Este.

Para contrarrestar el rearme de la Alemania Occidental tras su entrada en la OTAN, los países de la órbita soviética sellaron una alianza militar conocida como el Pacto de Varsovia en 1955. Sin embargo, este movimiento fue más político que estratégico, pues la Unión Soviética ya había construido una red de defensa mutua con todos sus satélites antes incluso de que se formara la OTAN en 1949.[63]​

Así, el status quo de Europa se mantuvo inalterado. Los soviéticos reprimieron la Revolución Húngara de 1956[64]​ sin que ninguna de las potencias occidentales tratara de movilizar su ejército contra la invasión del Pacto de Varsovia en suelo húngaro. Igualmente, la ciudad de Berlín continuó dividida y disputada.[65]​

Durante noviembre de 1958, Jrushchov trató de desmilitarizar la ciudad de Berlín. Planteó a estadounidenses, británicos y franceses abandonar sus respectivas zonas de ocupación bajo la amenaza de transferir el control de los accesos de las potencias occidentales a la Alemania Oriental (lo que significaría el aislamiento del sector occidental de Berlín). La OTAN rechazó el ultimátum y a mediados de diciembre, Jrushchov abandonó la idea a cambio de una conferencia en Ginebra para dilucidar la cuestión berlinesa.[66]​

La última gran crisis de la ciudad se vivió en 1961. Desde principios de la década de 1950, la Unión Soviética y después sus estados satélite comenzaron a restringir fuertemente los movimientos migratorios.[67]​ A pesar de ello, cientos de miles de alemanes orientales conseguían emigrar a Alemania Occidental a través del agujero en la frontera que existía en la ciudad de Berlín, donde la circulación entre sectores orientales y occidentales era libre, creando así un trampolín para la emigración a Europa Occidental.[68]​

Esta facilidad provocó una masiva fuga de cerebros de Alemania Oriental hacia Alemania Occidental de jóvenes cualificados: en 1961, el 20 % de la población activa en territorio oriental había emigrado a occidente.[69]​ En julio de ese año, la Unión Soviética volvió a plantear como ultimátum el abandono de la ciudad de todas las potencias ocupantes y la devolución de las zonas ocupadas de Berlín Occidental a Alemania Oriental, con lo que el agujero fronterizo sería eliminado.[70]​ Las potencias occidentales hicieron caso omiso del ultimátum.

Dos meses después del ultimátum soviético, Alemania Oriental comenzó la construcción de una barrera de cemento y alambre que separaba físicamente ambas zonas de la ciudad berlinesa, impidiendo la libre circulación entre las zonas oriental y occidental. La barrera fue creciendo hasta convertirse en el Muro de Berlín.[71]​

Aprovechando la aceleración de la descolonización durante la década de 1950 y primeros años de 1960, tanto EE. UU. como la Unión Soviética compitieron por aumentar su influencia en los países descolonizados.[72]​ Además, desde el punto de vista soviético, la desaparición de los grandes imperios coloniales era una señal inequívoca de la victoria de la ideología comunista.[73]​ Los movimientos nacionalistas en algunos países (especialmente en Guatemala, Irán, Filipinas e Indochina) fueron iniciados o apoyados en muchos casos por grupos comunistas autóctonos —o, equívocamente, fue la idea más extendida entre los aliados Occidentales—.[50]​

En este contexto, los EE. UU. usaron a la CIA para derrocar a ciertos gobiernos y favorecer a otros.[50]​ La CIA tuvo un papel clave en el derrocamiento de países sospechosos de ser procomunistas, como en el caso del primer gobierno electo democráticamente en Irán (Operación Ajax) en 1953 y la caída de Jacobo Arbenz Guzmán tras el Golpe de 1954 en Guatemala.[43]​ A su vez, EE. UU. trató de ayudar a gobiernos amigos con ayuda económica y militar, como en el caso de Vietnam del Sur.

La mayoría de naciones y gobiernos surgidos tras la descolonización en Asia, África y América Latina trataron de zafarse de la presión de elegir el bando procapitalista o procomunista. En 1955, durante la Conferencia de Bandung, decenas de países del Tercer Mundo acordaron mantenerse al margen de la dinámica de la Guerra Fría.[74]​ Este consenso se plasmó en la creación del Movimiento de Países No Alineados en 1961.[50]​ Como resultado de la aparición de un nuevo factor en la Guerra Fría, estadounidenses y soviéticos moderaron sus políticas y trataron de acercarse a estos nuevos países neutrales (sobre todo en caso de países clave como India o Egipto) de una manera menos agresiva que la sostenida hasta entonces. Los movimientos nacionalistas e independentistas consiguieron así crear un nuevo escenario más plural, superando la confrontación bipolar de la posguerra, y crearon las bases para las reivindicaciones nacionalistas en Asia y América Latina.[10]​

Al terminar la Segunda Guerra Mundial, las dos potencias vencedoras disponían de una enorme variedad de armas, muchas de ellas desarrolladas y mejoradas durante el conflicto. Tanques, aviones, submarinos y otros avanzados diseños de navíos de guerra, constituían las llamadas armas convencionales. No obstante, la desigualdad resultaba patente, o por lo menos eso les parecía a los estadistas. Antes de la Segunda Guerra Mundial, la Unión Soviética contaba con el mismo número de carros de combate que el resto de las naciones juntas, y superaba en aviones de combate, al conjunto de todas las demás fuerzas aéreas.[75]​

Después del conflicto, la diferencia numérica no era tan abrumadora, pero aún resultaba ostentosa. Sin embargo, su flota no podía competir en condiciones de igualdad con la de Estados Unidos. Tras la batalla de Midway quedó demostrada la importancia del avión naval de ataque y el portaaviones en los conflictos marítimos. La armada soviética disponía de muchos menos barcos de este tipo que la estadounidense, y además, sus naves eran de menor tamaño, y no disponían de cubierta corrida para operar dos aeronaves simultáneamente, por lo que su inferioridad resultaba manifiesta.[76]​ Para la Unión Soviética, más problemático aún que la falta de portaaviones, era la falta de una red mundial de bases de aprovisionamiento abiertas durante todo el año. Mientras que Estados Unidos podía atracar sus buques en Nápoles, Rota, Hawái, Filipinas y muchos otros puertos más, la Unión Soviética no podía sacar sus barcos de puertos propios durante varios meses al año, pues sus puertos o estaban helados, o podían ser fácilmente bloqueados por los aliados. Era el caso de la flota del mar Negro, que debía atravesar los 35 kilómetros del estrecho del Bósforo, que Turquía podía bloquear fácilmente.

En la aviación convencional, tanto en número como en calidad, los nuevos cazas y bombarderos soviéticos, no solo estaban a la altura, sino por encima de los occidentales, los aviones bombarderos Tu-4 lanzaron la primera bomba atómica soviética. Pese a que el Pentágono siempre afirmaba poseer aparatos superiores a los de cualquier otro país, los enfrentamientos vividos durante la guerra de Corea, guerra de Vietnam y posteriormente, en la guerra de la Frontera de Sudáfrica demostraron la igualdad, cuando no la superioridad, de los aviones soviéticos.

Pero eran las denominadas armas no convencionales las que llamaban poderosamente la atención: más poderosas, eficientes, difíciles de fabricar y extremadamente caras. La principal de estas armas era la bomba atómica. Al principio de la Guerra Fría solo Estados Unidos disponía de estas armas, lo que aumentaba significativamente su poder bélico. La Unión Soviética inició su propio programa de investigaciones, para producir también tales bombas, algo que consiguió en cuatro años; relativo poco tiempo, ayudándose de espionaje. En un principio, Estados Unidos centró sus investigaciones en perfeccionar el vector que transportara las bombas (misil o bombardero estratégico); pero fue cuando se supo que Moscú había detonado su primera bomba nuclear de fisión, cuando se dio luz verde al proyecto para fabricar la bomba de hidrógeno, arma que no tiene límite de potencia conocido. Esto se logró en 1952, y la Unión Soviética la obtuvo al año siguiente.[77]​ Pese a que la carrera iba muy pareja en el plano cualitativo no era lo mismo en el cuantitativo: contradiciendo a la preocupación occidental de aquella época, el ciudadano estadounidense y miembro del Instituto Thomas Watson, Serguéi Jrushchov afirma que en tiempo de la crisis de los misiles de Cuba el poder nuclear estadounidense superaba al oriental en 10 veces o más.[78]​

Esta carrera armamentística fue promovida por el llamado Equilibrio de Terror, según el cual, la potencia que se colocase al frente en la producción de armas, provocaría un desequilibrio en el escenario internacional: si una de ellas tuviera mayor número de armas, sería capaz de destruir a la otra. No obstante, ya en el siglo XXI fuentes como The Times consideran que el esfuerzo soviético no se encaminó a superar al otro adversario, sino a alcanzarlo para, seguidamente, obligarlo a poner en práctica una estrategia defensiva no ofensiva (arrebatarle cuantos aliados pudiese conseguir). De esta misma opinión es Serguéi Jrushchov, quien afirma que la carrera estaba solo en la mente de los occidentales, porque para los soviéticos se trataba de ir incrementando su arsenal y perfeccionando sus vectores (misiles, bombarderos y submarinos) según sus posibilidades, porque no podía igualar o superar a occidente.[78]​ Esta desproporción parecen confirmarla hechos como que los misiles intercontinentales (ICBM) solo comenzaron a estar a la altura de los estadounidenses, en lo que a operatividad y fiabilidad se refiere, hacia finales de los setenta. Tampoco los submarinos nucleares parecían poder medirse con los occidentales, como prueba la gran cantidad de accidentes que padecieron.[79]​

La carrera espacial se puede definir como una subdivisión del conflicto no declarado entre Estados Unidos y la Unión Soviética en el ámbito espacial. Entre 1957 y 1975, y como consecuencia de la rivalidad surgida dentro del esquema de la Guerra Fría, ambos países iniciaron una carrera en la búsqueda de hitos históricos que se justificaron por razones tanto de seguridad nacional como por razones ideológicas asociadas a la superioridad tecnológica.

La carrera se da por iniciada en 1957, cuando los soviéticos lanzaron el Sputnik, primer artefacto humano capaz de alcanzar el espacio y orbitar el planeta. Así mismo, los primeros hitos en la carrera espacial los alcanzaron los soviéticos: en noviembre de ese mismo año, lanzan el Sputnik II y, dentro de la nave, el primer ser vivo sale al espacio: una perra Kudriavka, de nombre Laika, que murió a las siete horas de salir de la atmósfera. El siguiente hito también sería obra de los soviéticos, al conseguir lanzar en 1961 la nave Vostok 1, tripulada por Yuri Gagarin, el primer ser humano en ir al espacio y regresar sano y salvo.

La llegada del hombre al espacio fue celebrado como un gran triunfo para la humanidad. En Estados Unidos, la ciudadanía recibió la noticia como un duro golpe a la creencia de la superior capacidad tecnológica estadounidense.[80]​ Como respuesta, el presidente Kennedy anunció, mes y medio después del viaje de Gagarin, que Estados Unidos sería capaz de poner un hombre en la Luna y traerlo sano y salvo a la Tierra antes de acabar la década.[81]​

A principios de 1969, Estados Unidos consiguió fabricar el primer artefacto humano que orbitó sobre la Luna (el Apolo 8) mientras que los soviéticos tenían graves problemas en su programa lunar. El 20 de julio de 1969 se alcanzaba el cénit en la exploración espacial cuando la misión Apolo 11 consiguió realizar con éxito su tarea y Armstrong y Edwin Aldrin se convirtieron en los primeros humanos en caminar sobre otro cuerpo celeste. Poco después, los soviéticos cancelaban su programa lunar.

Estados Unidos siguió mandando astronautas a la Luna, hasta que la falta de interés y presupuesto hicieron cancelar el programa. En 1975, la Misión Conjunta soviético-norteamericana Apolo-Soyuz dio por finalizada la carrera espacial.

Al triunfar la Revolución cubana en 1959, se da un verdadero giro en la historia de América Latina, pues el naciente proceso de nacionalizaciones y reforma agraria afecta gravemente los intereses estadounidenses en la isla que se habían asegurado con la Enmienda Platt en 1902, esto conduce a fuertes roces entre Cuba y los Estados Unidos que desencadenan en la ruptura de relaciones diplomáticas y a la expulsión de Cuba de la OEA, debido al aislamiento del resto del hemisferio y el bloqueo económico, el país se convierte en un fuerte aliado de la Unión Soviética y el resto del bloque comunista, convirtiéndose posteriormente en miembro del COMECON. Esta crisis llevó al mundo al borde de la guerra nuclear. Después del fracasado intento de invasión de la Bahía de Cochinos en abril de 1961. En 1962, la Unión Soviética fue descubierta construyendo 40 silos nucleares en Cuba. Según Jrushchov, la medida era puramente defensiva, para evitar que los Estados Unidos intentaran una nueva embestida contra los cubanos. Por otro lado, era sabido que los soviéticos querían realmente responder ante la instalación estadounidense de misiles Júpiter II en la ciudad de Esmirna, Turquía, que podrían ser usados para bombardear el sudoeste soviético.

La Unión Soviética envió navíos de carga y submarinos transportando armas atómicas hacía Cuba. Un avión espía descubrió las rampas de lanzamiento, y Estados Unidos tomó inmediatamente decisiones dando inicio a la crisis de los misiles.

El 22 de octubre de 1962, Estados Unidos ordenó una cuarentena total sobre la isla, posicionando navíos militares en el mar Caribe y cerrando los contactos marítimos entre la Unión Soviética y Cuba. Kennedy dirigió un ultimátum a la Unión Soviética: demandó a la Unión Soviética que detuviera esos navíos bajo amenaza de emprender represalias masivas. Los soviéticos argumentaron que no entendían por qué Kennedy tomaba esta medida cuando varios misiles estadounidenses estaban instalados en territorios de países miembros de la OTAN contra los soviéticos, en distancias idénticas. Fidel Castro adujo que no había nada de ilegal en instalar misiles soviéticos en su territorio, y el primer ministro británico Harold Macmillan dijo no haber entendido por qué no fue propuesta siquiera la hipótesis de un acuerdo diplomático.

El 23 y 24 de octubre Jrushchov habría enviado mensaje a Kennedy señalando: «La URSS ve el bloqueo como una agresión y no instruirá a los barcos que se desvíen»; pero en las primeras horas de la mañana, los buques soviéticos disminuyeron la velocidad en sus desplazamientos hacía Cuba, con el fin de evitar algún conflicto mayor, mientras se abrían las posibilidades de una negociación entre las partes. El 26 de octubre informó que retiraría sus misiles de Cuba si Washington se comprometía a no invadir Cuba. Al día siguiente, pidió además la retirada de los misiles balísticos Júpiter de Turquía. Dos aviones espía estadounidenses U-2 fueron derribados en Cuba y Siberia el 27 de octubre, justo en el ápice de la crisis. El 28 de octubre, Kennedy aceptó retirar los misiles de Turquía y no agredir a Cuba. Así Nikita Jrushchov retiró sus misiles nucleares de la isla cubana.

Esta crisis dio nacimiento a un nuevo periodo: la distensión, señalada por la puesta en marcha del teléfono rojo —en realidad blanco—, línea directa entre Moscú y Washington, que aligeraría las comunicaciones en caso de otra crisis.

En el transcurso de las décadas de 1960 y 1970, las superpotencias tuvieron que gestionar un nuevo modelo de geopolítica, en el que el mundo dejó de estar claramente dividido en dos bloques antagónicos.[50]​ Europa y Japón se recuperaron rápidamente de la destrucción de la Segunda Guerra Mundial y su renta per cápita se acercaba a la de Estados Unidos. Mientras tanto, la economía del Bloque del Este entraba en un ciclo de estancamiento económico.[50]​[82]​ A su vez, el Tercer Mundo conseguía establecerse como bloque independiente a través de organizaciones como el Movimiento de Países No Alineados y demostraron su fuerza de negociación con el papel fundamental que tuvo la OPEP durante la crisis del petróleo de 1973.[41]​

En la Unión Soviética, la gestión de los problemas económicos internos apartó la necesidad de extender la influencia soviética en el orden mundial.[50]​ Como consecuencia, líderes soviéticos como Alekséi Kosygin y el propio Leonid Brézhnev apostaron por una relajación en las relaciones internacionales abriendo un nuevo período conocido como la distensión o détente.[50]​

En 1968, tuvo lugar un período de liberalización política en Checoslovaquia llamado Primavera de Praga. Las reformas incluyeron una mayor libertad de prensa y libertad de expresión junto con un énfasis económico en bienes de consumo, la posibilidad de un gobierno multipartidista, limitaciones en el poder de la policía secreta, y posible retirada del Pacto de Varsovia.[83]​[84]​ 

En respuesta a la Primavera de Praga, el 20 de agosto de 1968, el Ejército soviético, junto con la mayoría de sus aliados del Pacto de Varsovia, a excepción de la República Socialista de Rumania de Nicolae Ceaușescu invadieron Checoslovaquia.[85]​ La invasión fue seguida por un éxodo masivo de 70 000 checos y eslovacos[86]​

El «Mayo del 68» es el nombre dado a una serie de protestas estudiantiles y huelgas generales que provocaron la caída del gobierno de De Gaulle en Francia. La gran mayoría de los protestantes seguía ideologías de izquierdas, aunque las organizaciones políticas y sindicalistas de la izquierda tradicional trataron de distanciarse del movimiento. Las protestas se dirigieron especialmente al sistema educativo y laboral imperante.

Aunque el Mayo del 68 acabó por ser un relativo fracaso político, el impacto social fue muy importante. Especialmente en Francia (y de manera menos evidente, en el resto del mundo occidental) la revuelta marcó el paso de una sociedad moralmente conservadora proveniente de aquellos que vivieron la II Guerra Mundial (basada en la religión, el patriotismo y el respeto por la autoridad) a una moral más liberal de la generación que nació tras la guerra (basada en la igualdad, la liberación sexual y el respeto por los derechos humanos).

La Doctrina de la Seguridad Nacional es un concepto utilizado por varios historiadores lationamericanos para definir ciertas acciones de política exterior, tendientes a que las fuerzas armadas de los países latinoamericanos modificaran su misión para dedicarse con exclusividad a garantizar el orden interno, con el fin de combatir aquellas ideologías, organizaciones o movimientos que, dentro de cada país, pudieran favorecer o apoyar al comunismo en el contexto de la Guerra Fría, legitimando la toma del poder por parte de las fuerzas armadas y la violación sistemática de los derechos humanos, mediante políticas de terrorismo de Estado.[87]​[88]​[89]​[90]​[91]​[92]​[93]​[94]​[95]​[96]​

En 1946 Estados Unidos instaló en la Zona del Canal de Panamá, por entonces bajo su poder, la Escuela de las Américas. La Escuela educó y entrenó a decenas de miles de militares latinoamericanos, con vistas a su actuación en la Guerra Fría. Surgió como iniciativa en el marco de la Doctrina de Seguridad Nacional,[97]​ para lograr que las naciones latinoamericanas cooperaran con los Estados Unidos, contrarrestando la influencia de las corrientes políticas de ideología marxista, socialista, antiimperialista, izquierdista y en general de asimiladas al llamado "populismo".[98]​

A finales de abril de 1965, el presidente Lyndon B. Johnson ordenó el despliegue de 42 000 soldados en la República Dominicana para la ocupación del territorio dominicano durante un año, en una operación conocida como Operación Power Pack, escudándose en la posible aparición de una «nueva Revolución Cubana» en América Latina.[10]​ Durante las elecciones dominicanas de 1966, bajo ocupación estadounidense, se proclamó ganador al conservador Joaquín Balaguer. Aunque es cierto que Balaguer tenía el apoyo real de las élites del país, así como de los campesinos, las elecciones se vieron desprestigiadas por la negativa del anterior presidente Juan Bosch de disputarlas. Tras la victoria de Balaguer, los activistas del Partido Revolucionario Dominicano (PRD) del expresidente Bosch, comenzaron una campaña de ataques contra la policía y el ejército.[99]​

En Chile, el candidato del Partido Socialista de Chile Salvador Allende ganó las elecciones presidenciales de 1970, convirtiéndose en el primer marxista elegido democráticamente en convertirse en presidente de un país de América La CIA apuntó a Allende para que lo expulsara y actuó para socavar su apoyo a nivel nacional, lo que contribuyó a un período de disturbios que culminó con el golpe de Estado del general Augusto Pinochet el 11 de septiembre de 1973. Pinochet consolidó el poder como un dictador militar, las reformas de la economía de Allende se pusieron en marcha atrás, y opositores izquierdistas fueron asesinados o detenidos en campos de internamiento bajo la Dirección de Inteligencia Nacional (DINA). Los estados socialistas —con la excepción de China y Rumanía— rompieron relaciones con Chile.[100]​ El régimen de Pinochet se convertiría en uno de los principales participantes en la Operación Cóndor, una campaña internacional de asesinatos políticos y terrorismo de Estado organizada por dictaduras militares de derecha en el Cono Sur de América del Sur que fue encubiertamente respaldada por el gobierno de los Estados Unidos.[101]​[102]​[103]​

En Argentina hubo varios intentos fracasados de implantación de grupos de guerrilleros con mayor o menor apoyo de la Cuba de Fidel Castro, aun antes de la creación de los luego muy conocidas bandas Montoneros y Ejército Revolucionario del Pueblo. Estos y otros grupos comenzaron a actuar en épocas del gobierno de facto llamado Revolución Argentina y luego de un breve período de inacción bélica durante el retorno al país de Perón, reanudaron su accionar durante el gobierno mayoritariamente elegido de este último, durante el gobierno de su sucesora legal y durante el gobierno de facto iniciado en 1976 que terminó derrotándolos en el plano militar.

El terrorismo de Estado en América Latina fue parte de una operación continental. La Operación o Plan Cóndor fue el nombre con el que se designó el plan de inteligencia y coordinación entre los servicios de seguridad de los regímenes militares del Cono Sur (Argentina, Chile, Uruguay, Brasil, Paraguay y Bolivia), con conexiones con las fuerzas militares de Perú, Ecuador, Colombia y Venezuela, y cooperación y apoyo operativo de los Estados Unidos. La Operación Cóndor constituyó una organización clandestina internacional para la práctica del terrorismo de Estado a escala continental.

La Operación Cóndor ha podido ser descubierta básicamente a partir de los documentos secretos del gobierno estadounidense desclasificados en época del presidente Bill Clinton.

Fue concebida y diseñada por el entonces coronel chileno Manuel Contreras quien en 1975, redactó un extenso documento con las proposiciones para su funcionamiento. El primer paso hacia la organización se produjo a mediados de 1975 cuando el coronel chileno Mario Jahn, viajó a Paraguay y entregó al coronel paraguayo Benito Guanes, el documento de organización del mecanismo y lo invitó a participar en la Primera Reunión de Trabajo de Inteligencia Nacional, realizada en Santiago de Chile entre el 25 de noviembre y el 1 de diciembre de 1975. En esa reunión se decidió organizar la Operación Cóndor entre los seis países del Cono Sur (Argentina, Bolivia, Brasil, Chile, Paraguay y Uruguay). Luego se sumarían, con distintos grados de compromiso, Perú, Ecuador, Colombia y Venezuela. Tuvo su centro de operaciones en Santiago de Chile y su principal coordinador fue Manuel Contreras, quien era conocido como "Cóndor Uno".

Entre decenas de secuestros y atentados contra opositores, la Operación Cóndor concretó acciones de gran resonancia pública como:

El 26 de abril de 2000 el exgobernador de Río de Janeiro Leonel Brizola sostuvo que los expresidentes del Brasil, João Goulart y Juscelino Kubitschek, fueron asesinados en el marco de la Operación Cóndor, simulándose un ataque cardíaco y un accidente, respectivamente y que ello debía ser investigado.[104]​[105]​

La Fuerza Aérea Uruguaya ha reconocido oficialmente la realización de vuelos de la muerte conjuntos con el régimen militar argentino.[106]​ Alrededor de 110 uruguayos fueron detenidos-desaparecidos en Argentina entre 1976 y 1983.[107]​

El gobierno de Estados Unidos participó activamente de la Operación Cóndor. El 22 de agosto de 1978 el servicio de inteligencia estadounidense envió a sus principales embajadas en Sudamérica la siguiente advertencia:

Orgánicamente, la Operación Cóndor comenzó a ser desmontada cuando cayó la dictadura argentina en 1983. Sin embargo, los contactos y los asesinatos coordinados continuaron. En abril de 1991, se puso en marcha la Operación Silencio para impedir el enjuiciamiento de los responsables.

El 31 de mayo de 2001, mientras Henry Kissinger se encontraba en París, fue notificado por el juez Roger Le Loire que debía presentarse a declarar sobre su participación en la Operación Cóndor, lo que provocó la inmediata salida del exsecretario norteamericano, de Francia. Pocos meses después, Kissinger debió cancelar una visita a Brasil, porque el gobierno no podía garantizarle inmunidad judicial.[108]​

El 22 de diciembre de 1992, se descubrió en una estación de policía de Lambaré, Asunción (Paraguay), los llamados Archivos del Terror, expedientes en los que existen constancias documentales sobre el terrorismo de Estado en el Cono Sur. Según los archivos descubiertos en Lambaré (Asunción) en 1992, la Operación Cóndor causó 50 000 muertos, 30 000 desaparecidos y 400 000 presos.[109]​[110]​

En febrero de 2004, el periodista estadounidense John Dinges, publicó Operación Cóndor: una década de terrorismo internacional en el Cono Sur,[111]​ donde entre otras cosas revela que los militares uruguayos intentaron asesinar al diputado estadounidense Edward Koch en 1976.

Bajo la presidencia de John F. Kennedy, los niveles de tropas estadounidenses en Vietnam crecieron bajo el programa del Grupo Asesor de Asistencia Militar de poco menos de mil en 1959 a 16 000 en 1963. La fuerte represión del presidente vietnamita Ngo Dinh Diem contra los budistas los monjes en 1963 llevaron a los Estados Unidos a respaldar un golpe militar mortal contra Diem. La guerra se intensificó aún más en 1964 después del controvertido Incidente del Golfo de Tonkin, en el que se alega que un destructor estadounidense se enfrentó con una nave de ataque rápido norvietnamita. La Resolución del Golfo de Tonkin otorgó al presidente Lyndon B. Johnson una amplia autorización para aumentar la presencia militar de los EE. UU., Desplegando unidades de combate terrestre por primera vez y aumentando los niveles de tropas a 184 000.[112]​ El líder soviético Leonid Brezhnev respondió invirtiendo la política de retirada de Kruschev y aumentando la ayuda a los norvietnamitas, con la esperanza de atraer al norte de su posición prochina. Sin embargo, la URSS desalentó una nueva escalada de la guerra, proporcionando la asistencia militar suficiente para unir a las fuerzas estadounidenses. Desde este punto, el Ejército Popular de Vietnam (PAVN), también conocido como el Ejército de Vietnam del Norte (EVN), participó en una guerra más convencional con las fuerzas estadounidenses y de Vietnam del Sur.[113]​

La Ofensiva del Tet de 1968 resultó ser el punto de inflexión de la guerra. A pesar de los años de tutela y ayuda estadounidenses, las fuerzas de Vietnam del Sur no pudieron resistir la ofensiva comunista y la tarea recayó en las fuerzas estadounidenses. La Ofensiva del Tet demostró que el final de la participación de Estados Unidos no estaba a la vista, aumentando el escepticismo interno de la guerra y dando lugar a lo que se conoce como el Síndrome de Vietnam, una aversión pública a las implicaciones militares estadounidenses en el extranjero. No obstante, las operaciones continuaron cruzando las fronteras internacionales: Vietnam del Norte utilizó áreas limítrofes de Laos y Camboya como rutas de suministro, y fueron fuertemente bombardeadas por las fuerzas estadounidenses.[114]​

Al mismo tiempo, 1963-65, la política doméstica estadounidense vio el triunfo del liberalismo. Según el historiador Joseph Crespino:

Como resultado de la ruptura sino-soviética, las tensiones a lo largo de la frontera chino-soviética alcanzaron su punto máximo en 1969, con el estallido del llamado conflicto fronterizo sino-soviético. El presidente de los Estados Unidos, Richard Nixon, decidió utilizar el conflicto para cambiar el equilibrio de poder hacia Occidente en la Guerra Fría.[116]​ Los chinos habían buscado mejorar las relaciones con los estadounidenses para y ganar ventaja sobre los soviéticos.

En febrero de 1972, Nixon logró un sorprendente acercamiento con China, viajando a Pekín, en plena Revolución Cultural, y reuniéndose con Mao Zedong y Zhou Enlai. En este momento, la URSS logró una paridad nuclear aproximada con los Estados Unidos; mientras tanto, la guerra de Vietnam debilitó la influencia de Estados Unidos en el Tercer Mundo.[117]​

En Indonesia, el anticomunista general Suharto arrebató la presidencia a su predecesor, Sukarno, para imponer lo que se conoció como el Nuevo Orden (Orde Baru). Entre 1965 y 1966, los militares asesinaron a más de medio millón de personas simpatizantes del Partido Comunista de Indonesia y otras organizaciones de izquierda.[118]​

Durante la guerra de Vietnam, Vietnam del Norte utilizó las zonas fronterizas con Camboya para establecer bases militares, que el jefe de Estado camboyano Norodom Sihanouk toleró en un intento por preservar la neutralidad de Camboya. Tras la deposición de Sihanouk en marzo de 1970 por el general pro estadounidense Lon Nol, que ordenó a los norvietnamitas que abandonaran Camboya, Vietnam del Norte intentó invadir toda Camboya tras las negociaciones con Nuon Chea, el segundo al mando de los comunistas camboyanos (apodados Jemeres Rojos) luchando para derrocar al gobierno camboyano.[119]​ Sihanouk huyó a China con el establecimiento del GRUNK en Pekín. Las fuerzas estadounidenses y de Vietnam del Sur respondieron a estas acciones con una campaña de bombardeos y una breve incursión terrestre, lo que contribuyó a la violencia de la guerra civil que pronto envolvió a toda Camboya. El bombardeo con alfombra estadounidense duró hasta 1973, y aunque evitó que los jemeres rojos tomaran la capital, también aceleró el colapso de la sociedad rural, aumentó la polarización social y mató a decenas de miles de civiles[120]​.

Después de tomar el poder y distanciarse de los vietnamitas, el líder comunista Pol Pot mató entre 1,5 y 2 millones de camboyanos en los campos de trabajo, aproximadamente una cuarta parte de la población camboyana (un evento comúnmente etiquetado como genocidio camboyano).[121]​[122]​[123]​ Martin Shaw describió estas atrocidades como el genocidio más puro de la era de la Guerra Fría. [266] Respaldado por el Frente Unido Kampucheo para la Salvación Nacional, una organización de comunistas jemer prosoviéticos y desertores jemeres rojos liderados por Heng Samrin, Vietnam invadió Camboya el 22 de diciembre de 1978. La invasión logró depositar Pol Pot, pero el nuevo estado lucharía por obtener el reconocimiento internacional más allá de la esfera del Bloque Soviético, a pesar de la protesta internacional previa por las graves violaciones de los derechos humanos del régimen de Pol Pot, se permitió a los representantes de los Jemeres Rojos se sentará en la Asamblea General de la ONU, con un fuerte apoyo de China y las potencias occidentales, los países miembros de la ASEAN, y se estancará en una guerra de guerrillas liderada por campamentos de refugiados ubicados en la frontera con Tailandia. Tras la destrucción de Khmer Rouge, la reconstrucción nacional de Camboya se vería gravemente obstaculizada, y Vietnam sufriría un ataque punitivo chino.[124]​

Egipto fue el centro de las disputas. 

Si bien  Egipto se declaraba neutral, la mayoría del armamento y la asistencia económica provenían de la Unión Soviética. Esta alianza, aunque de manera reacia, se comprobó con el apoyo técnico y militar de la Unión Soviética durante la guerra de los Seis Días y la guerra de Desgaste contra Israel, que se consideraba aliado de Estados Unidos.[125]​ Aunque con la llegada al poder de Anwar el Sadat en 1972 Egipto comenzara virar de prosoviético a prooccidental,[126]​ la amenaza de una posible intervención directa de la Unión Soviética en defensa de Egipto durante la guerra del Yom Kippur provocó la movilización de las fuerzas estadounidenses, en una serie de actos que pudieron desbaratar la noción de la «coexistencia pacífica».[127]​ Estratégicamente, los conflictos en Oriente Medio abrieron una nueva fase en la Guerra Fría, en la que la Unión Soviética podía amenazar los intereses de EE. UU. basándose en la paridad nuclear que habían conseguido los soviéticos.

Aunque Egipto fue el mayor foco de atención, las potencias también actuaron en otros países de la zona. Los soviéticos reforzaron sus relaciones con el gobierno comunista de Yemen del Sur y con el gobierno nacionalista de Irak.[126]​ Los soviéticos también apoyaron a la OLP de Yasir Arafat.[128]​ Por otro lado, entre 1973-1975, la CIA apoyó y conspiró con el gobierno de Irán para financiar y armar a los rebeldes kurdos durante la segunda guerra kurdo-iraquí, para debilitar el gobierno de Ahmed Hassan al-Bakr. El apoyo de la CIA finalizó cuando Irán e Irak firmaron el Acuerdo de Argel en 1975.[129]​

En 1974, estalló en Portugal la Revolución de los Claveles en contra de la dictadura del Estado Novo. Los cambios políticos en Portugal facilitaron la independencia de las colonias portuguesas de Angola y Timor Oriental. En Angola, donde las facciones rebeldes habían sostenido una guerra por la independencia contra Portugal desde 1961, tras la independencia en 1974 estas mismas facciones que habían luchado juntas contras las fuerzas colonialistas comenzaron una guerra civil al enfrentarse entre ellas. En una muestra de los equilibrios político-estratégicos de la Guerra Fría, la guerra civil angoleña enfrentó a tres facciones distintas: el MPLA, apoyado por cubanos y soviéticos, el FNLA, apoyado por EE. UU., China y Zaire y la UNITA apoyado también por Estados Unidos, el régimen del apartheid sudafricano y otra serie de países africanos. Finalmente, el MPLA, con tropas cubanas y apoyo soviéticos, vencerían a la UNITA a pesar del apoyo militar de Sudáfrica.[130]​ Igualmente, los soviéticos reforzaron sus relaciones con el gobierno nacionalista de Argelia.

Por otro lado oficiales del ejército somalí, encabezados por Mohamed Siad Barre, llevaron a cabo un golpe de Estado incruento, formando la República Democrática Somalí, de ideario socialista. La Unión Soviética prometió apoyo a Somalia. Cuatro años después, en el país vecino de Etiopía, el emperador Haile Selassie, prooccidental, fue derrocado por el Derg un grupo de oficiales radicales del ejército etíope, liderados por el prosoviético Mengistu Haile Mariam, que se apresuró a reforzar las relaciones con Cuba y la Unión Soviética.[130]​ Cuando estallaron las hostilidades entre Somalia y Etiopía (guerra de Ogaden) el somalí Siad Barre perdió el apoyo de los soviéticos, y a cambio buscó el la asistencia del conocido como Safari Club —una alianza de los servicios de inteligencia de Irán, Egipto y Arabia Saudí—. A través del Safari Club, Somalia consiguió armas soviéticas y tanques estadounidenses.[131]​[132]​ El ejército etíope estaba apoyado por soldados cubanos y asesores y armamento soviético.[130]​ La postura oficial de Estados Unidos era la de neutralidad en el conflicto, aunque defendiendo que fue Somalia la que violó la soberanía territorial de Etiopía. Aun así, la administración Carter comenzó a apoyar a Somalia desde 1980.[133]​

Tras su vista a China, Nixon se reunió con los líderes soviéticos en Moscú[134]​ y como resultado de estas reuniones, se firmó el primero de los Acuerdos SALT (SALT I), el primer acuerdo completo de limitación de armas firmado entre las dos superpotencias,[135]​ y el Tratado sobre Misiles Antibalísticos (Tratado ABM), que prohibía el desarrollo de sistemas diseñados para la interceptación de misiles en el aire. Con ello se intentaba limitar el desarrollo de los costosos misiles antibalísticos y misiles con carga nuclear.[50]​

Tras los acuerdos alcanzados, Nixon y Brézhnev proclamaron una nueva era de «coexistencia pacífica» basada en una nueva política de «détente» (o cooperación) entre las dos superpotencias. Durante esta «coexistencia», Brézhnev trataría de revitalizar la economía soviética, que estaba en declive en parte por los grandes gastos militares que provocaban una situación de tensión continua.[10]​ Entre 1972 y 1974, ambas potencias también fortalecieron sus lazos económicos, con la firma de varios acuerdos para aumentar el comercio entre ellas. Como resultado de todos estos pactos y acuerdos, la hostilidad mutua se reemplazaba por un nuevo marco histórico donde ambas potencias podían convivir y desarrollarse.[134]​

A su vez, en Europa, la situación de división interna se relajaba con el desarrollo de la «Ostpolitik», llevada a cabo por el canciller de la RFA Willy Brandt y la firma de los Acuerdos de Helsinki en el marco de la Conferencia sobre la Seguridad y la Cooperación en Europa en 1975.[136]​

A pesar de los llamamientos al acuerdo de ambas potencias, durante la década de 1970, el KGB, dirigido por Yuri Andrópov, continuó persiguiendo a personalidades disidentes como Aleksandr Solzhenitsyn y Andréi Sájarov, que criticaban duramente el régimen soviético.[137]​ Continuaron también los conflictos indirectos entre ambas superpotencias (guerras "proxy").[138]​ Aunque el presidente Jimmy Carter intentó frenar la carrera armamentística con la firma de un nuevo tratado de limitación de armas (SALT II) en 1979,[139]​ sus esfuerzos fueron socavados por los eventos que se produjeron ese año, como el triunfo de la Revolución Iraní y la Revolución Sandinista, apoyada por el KGB,[140]​ que derrocaron a los gobierno prooccidentales de ambos países. Como represalia, EE. UU. se opuso a la invasión soviética de Afganistán que se produjo en diciembre, dando por finalizada la era de la «coexistencia pacífica».[10]​

En abril de 1978, el comunista Partido Democrático Popular de Afganistán (PDPA) se hizo con el poder en Afganistán tras la Revolución de Saur. A los pocos meses, los opositores al gobierno comunista lanzaron una revuelta en el este del país, que se creció rápidamente hasta convertirse en una guerra civil que se extendía por todo el país, con los rebeldes muyahidín atacando a las fuerzas gubernamentales a lo largo y ancho del país. El gobierno de Pakistán proveía estos rebeldes de lugares donde esconderse y entrenamiento militar. En el otro bando, el PDPA era apoyado por los asesores militares mandados desde la Unión Soviética.[141]​ Mientras tanto, en el PDPA se luchaban guerras internas entre la mayoría Jalq y los moderados Parcham. Como resultado, los parchamíes renunciaron a sus cargos en el gobierno y los oficiales militares parchamíes fueron arrestados con la excusa de un supuesto golpe de Estado parchamí. Hacia 1979, Estados Unidos había comenzado un programa secreto para dar asistencia militar y armas a los muyahidines.[142]​

En septiembre de 1979 el presidente jalq, Nur Mohammad Taraki, fue asesinado en un golpe interno del PDPA orquestado por su primer ministro Hafizullah Amín, que asumió la presidencia. Los soviéticos, que desconfiaban de Amín, lo asesinaron en diciembre de 1979. Se formó un nuevo gobierno bajo las órdenes de los soviéticos, liderado por el parchamí Babrak Karmal y con la participación de ambas facciones. Se desplegaron más fuerzas soviéticas para estabilizar el país bajo el poder de Karmal, aunque los soviéticos no esperaban llevar el peso de las operaciones militares. Sin embargo, con su presencia y apoyo a uno de los bandos, los soviéticos se vieron envueltos en lo que debía haber sido una guerra doméstica.[143]​

El presidente Carter describió la intervención soviética como «la más seria amenaza para la paz desde la Segunda Guerra Mundial».[144]​ Como consecuencia, retiró el tratado SALT II de su aprobación en el Senado, impuso un embargo sobre los cereales y la transferencia de tecnología a la Unión Soviética, pidió un incremento significativo del gasto militar estadounidense y finalmente lideró el boicot de los Juegos Olímpicos de Moscú de 1980.

En enero de 1977, cuatro años antes de convertirse en presidente, Ronald Reagan reveló claramente en una entrevista su postura en relación a la Guerra Fría: «Mi idea de lo que debe ser la política estadounidense en lo que respecta a la Unión Soviética, es simple, y algunos dirán que simplista», dijo. «Es esta: nosotros ganamos y ellos pierden, ¿qué te parece?».[145]​ En 1980, Reagan ganó las elecciones, con la promesa de incrementar el gasto militar y enfrentarse a los soviéticos en cualquier lugar que fuera necesario.[146]​ Tanto Reagan, como la recién elegida primera ministra británica Margaret Thatcher, denunciaron tanto a la Unión Soviética como a la ideología comunista. Reagan calificó a la Unión Soviética como el «Imperio del mal» y predijo que el comunismo acabaría en «el montón de cenizas de la Historia».[147]​

A principios de 1985, el anticomunismo visceral de Reagan se desarrolló en una postura conocida como la Doctrina Reagan en la que, además de la Contención, abogaba por el derecho de los EE. UU. de subvertir y derrocar los gobiernos comunistas existentes.[148]​ Además de continuar con la política de la administración Carter de apoyar a los opositores islamistas de la Unión Soviética y del gobierno prosoviético del PDPA. La CIA también buscaba debilitar a la Unión Soviética promoviendo la aparición de un Islam político en aquellas Repúblicas Soviéticas de Asia Central de mayoría musulmana.[149]​ Además, la CIA alentó a la ISI pakistaní, de ideología anticomunista, a entrenar a musulmanes de todo el mundo para que participaran en la yihad contra la Unión Soviética.[149]​

A principios de la segunda mitad de la década de 1980, los gastos militares representaban el 25 % del PIB soviético, a costa del gasto en bienes de consumo para los ciudadanos y la inversión en sectores civiles.[150]​ Los gastos acumulados en la carrera armamentística y otros compromisos derivados de su implicación en la Guerra Fría, causaron y magnificaron los profundos problemas estructurales del sistema económico soviético,[151]​ que acabaron provocando una crisis económica permanente durante el mandato de Brezhnev.

La inversión soviética en el sector de la Defensa no estaba dirigida tanto por una necesidad militar real, sino por los intereses privados de los miembros de la Nomenklatura que dependían de las inversiones públicas en el sector para mantener su poder e influencia.[152]​ Las fuerzas armadas soviéticas se convirtieron en las más grandes en función de la cantidad y tipos de armas que poseían, en número de tropas y el tamaño de su complejo militar-industrial. Sin embargo, todas estas ventajas cuantitativas de bloque oriental se veían muchas veces superadas por las ventajas cualitativas de los ejércitos más modernos y tecnológicamente más avanzados del bloque occidental.[153]​

La escalada militar que comenzó Reagan no fue seguida de una escalada igual en la Unión Soviética, por falta de recursos económicos.[154]​Los gastos militares soviéticos ya se consideraban excesivos, y junto con una economía planificada ineficiente y una agricultura colectivizada poco productiva, eran un lastre muy pesado para el desarrollo de la economía soviética[155]​ Al mismo tiempo, tanto Arabia Saudí como otros países no-OPEP comenzaron a incrementar su producción,[156]​ saturando el mercado del petróleo y empujando los precios hacia abajo. Esta bajada de precios afectó gravemente a la Unión Soviética, ya que la exportación de petróleo era su fuente principal de divisas.[150]​[155]​ Los problemas derivados de una economía centralizada,[157]​ la bajada del precio del crudo y el gasto militar descontrolado condujeron a la economía soviética a una crisis sistémica.[155]​

Desde 1980, EE. UU. comenzó una escalada militar con el desarrollo de armas como el bombardero Rockwell B-1 Lancer, el misil LGM-118A Peacekeeper,[158]​ y sobre todo, el desarrollo experimental de la Iniciativa de Defensa Estratégica, conocida como «La Guerra de las Galaxias» que pretendía, mediante unos satélites colocados en la órbita terrestre, tener la capacidad de interceptar los misiles enemigos en pleno vuelo.[159]​

La ciudadanía estadounidense todavía guardaba muchos recelos a la intervención militar directa desde el desastre de la guerra de Vietnam.[160]​ La administración Reagan optó por el uso de tácticas rápidas y de bajo coste para la intervención en los conflictos en el extranjero, como el uso de la contrainsurgencia.[160]​ Durante 1983, la administración Reagan intervino en la guerra civil libanesa, invadió Granada, bombardeó Libia y apoyó a los Contras, un grupo de paramilitares anticomunistas que buscaban derrocar al gobierno sandinista prosoviético de Nicaragua. Mientras que sus actuaciones en Granada y Libia fueron populares, su apoyo a los contrainsurgentes fue más controvertido, como en el caso del Irán-Contra.[161]​

Mientras tanto, los soviéticos seguían aumentando el gasto de sus intervenciones en el extranjero. Aunque Brezhnev afirmaba que la intervención soviética en Afganistán sería breve, las guerrillas musulmanas, con el apoyo de EE. UU., ofrecían una resistencia fiera al invasor.[162]​ La Unión Soviética llegó a movilizar 100 000 soldados en suelo afgano para sostener su gobierno-marioneta, lo que llevó a muchos observadores a calificar la guerra en Afganistán como «el Vietnam de los soviéticos».[162]​ La guerra de Afganistán tuvo unas repercusiones peores aún que la de Vietnam para los estadounidenses, pues el conflicto afgano coincidió con un periodo de desintegración interna y crisis económica en el sistema soviético.

En el momento en el que el (comparativamente) joven Mijaíl Gorbachov se convirtió en secretario general en 1985,[147]​ la economía soviética estaba totalmente estancada y sin fondos de divisas extranjeras a causa de la caída de los precios del petróleo de la década de 1980.[163]​ Esta situación motivó a Gorbachov para buscar nuevas medidas que revivieran la economía y mejoraran la calidad de un Estado enfermo y podrido por la corrupción.[163]​

Tras unas reformas cosméticas, Gorbachov llegó a la conclusión de que eran necesarios cambios estructurales profundos, y en junio de 1987 anunció una serie de reformas económicas que se conocieron como la Perestroika[164]​ (reestructuración). La Perestroika relajó el sistema de producción soviético, permitió la actividad económica privada, y puso las primeras medidas para impulsar la inversión extranjera. Estas medidas pretendía redirigir los recursos del país de los costosos compromisos militares de la Guerra Fría a otras áreas más productivas de los sectores civiles.[164]​

A pesar del escepticismo inicial de Occidente, el nuevo líder soviético demostró estar más comprometido con el desarrollo económico de la Unión Soviética que de continuar con una costosa carrera armamentística con EE. UU.[62]​[165]​ Como medida para calmar a la oposición interna, Gorbachov introdujo la glásnost (apertura), que incrementaba la libertad de prensa y la transparencia de las instituciones del Estado.[166]​ La glásnost intentaba reducir la corrupción que se había instalado en las altas esferas del Partido Comunista y moderar los abusos del Comité Central.[167]​ La Glásnost también permitía un contacto más intenso de los ciudadanos soviéticos con el mundo occidental-capitalista, particularmente con los Estados Unidos, acelerando el proceso de «détente» entre ambas potencias.[168]​

Como respuesta a las concesiones militares y políticas del Kremlin, Reagan aceptó retomar las conversaciones sobre los asuntos económicos y el replanteamiento de la carrera armamentística.[169]​ La primera de estas reuniones tuvo lugar en Ginebra, en noviembre de 1985.[169]​ En la sala de deliberaciones sólo estuvieron presentes ambos mandatarios acompañados de un intérprete. En principio, acordaron reducir el arsenal nuclear de cada país en un 50 %.[170]​ La segunda reunión tuvo lugar en la Cumbre de Reikiavik. Las conversaciones marchaban por buen camino hasta que se discutió el asunto de la «Guerra de las Galaxias», que Gorbachov quería que se desmantelara, a lo que Reagan se negaba.[171]​ Las negociaciones fracasaron, pero en una tercera reunión en 1987 se produjo un gran avance con la firma del Tratado INF, que eliminó los misiles balísticos y de crucero nucleares o convencionales, cuyo rango estuviera entre 500 y 5500 kilómetros.

Las tensiones entre Occidente-Oriente iban desapareciendo rápidamente durante la segunda mitad de la década de 1980, hasta llegar a su punto máxima expresión en la cumbre final de Moscú, en 1989, para firmar los acuerdos START I[172]​ A lo largo de ese año, se hacía más aparente que los soviéticos no podrían mantener los subsidios con los que vendía gas y petróleo a precios bajos a sus aliados, ni soportar el coste de movilizar un gran número de tropas fuera de su frontera.[173]​ Además, con la proliferación de los misiles intercontinentales, la ventaja estratégica de una defensa basada en «países satélite» era irrelevante; por lo tanto, los soviéticos declararon oficialmente (Doctrina Sinatra) que no volverían a intervenir en los asuntos domésticos de sus aliados en la Europa del Este.[174]​ Las tropas soviéticas se retiraron de Afganistán[175]​ y ya en 1990, con el Muro de Berlín ya destruido, Gorbachov firmó el Tratado Dos más Cuatro que consagraba de iure la Reunificación alemana.[173]​

El 3 de diciembre de 1989, durante la Cumbre de Malta Gorbachov y el sucesor de Reagan, George H. W. Bush, declararon terminada la Guerra Fría.[176]​

A lo largo del verano de 1989, una serie de subterfugios legales permitieron a los ciudadanos de Alemania Oriental pasar a la Europa Occidental: la desaparición de controles en la frontera de Hungría con Austria permitía a los ciudadanos de Berlín Este salir como turistas a Hungría, y de allí a Austria.[177]​ El Gobierno de Alemania Oriental respondió prohibiendo los viajes a Hungría, solamente para encontrarse con que el mismo problema se reproducía en Checoslovaquia, desde donde los ciudadanos pasaba a Hungría y desde allí a Austria.

El 18 de octubre el presidente de Alemania Oriental Erich Honecker dimitía y asumía su cargo Egon Krenz. Mientras tanto, las protestas se sucedían a lo largo de toda Alemania Oriental, hasta llegar a su cénit el 4 de noviembre, cuando medio millón de personas se manifestaron en Alexanderplatz.[178]​

Los ciudadanos de Alemania Oriental seguían llegando en oleadas a Checoslovaquia para escapar a través de Hungría y Austria. La administración de Krenz acabó tolerando este subterfugio y finalmente, para facilitar las complicaciones aduaneras que se presentaban, el gobierno de Krenz decidió permitir a los ciudadanos de Berlín Este a salir directamente por los puestos fronterizos hacia Berlín Oeste. La nueva regulación que permitía los viajes privados entre ambas zonas se iba a presentar el 9 de noviembre, y entrarían en efecto al día siguiente.

Günter Schabowski, el portavoz del SED, tenía la tarea de anunciar estos cambios; sin embargo, Schabowski no participó en las conversaciones que dieron forma a la nueva regulación y no estaba enterado de todos los detalles.[179]​ Poco antes de la rueda de prensa que se daría para anunciar los cambios, se le pasó una nota con los cambios en la regulación, pero sin ofrecerle más información de cómo gestionar la noticia. En realidad, estas nuevas regulaciones se había completado solamente unas horas antes del anuncio, y deberían haber entrado en efecto al día siguiente para poder avisar a los guardas de los puestos fronterizos; pero nadie avisó a Schabowski de este detalle.[180]​

Schabowski, por lo tanto, no pudo hacer otra cosa que leer la nota en voz alta. Cuando comenzó el turno de preguntas, uno de los periodistas preguntó cuándo tendrían efecto las mencionadas regulaciones. Tras dudar unos segundos, respondió que la nueva regulación entraba en efecto de manera inmediata,[180]​ y siguiendo el turno de preguntas, afirmó que las regulaciones afectaban igualmente a los puestos fronterizos de Berlín Oeste, aunque en la nota que se había leído no se hacía referencia ninguna a la ciudad de Berlín.[181]​

Los extractos de esta rueda de prensa abrieron los informativos de Alemania Occidental (cuya señal llegaba también a la práctica totalidad de Alemania Oriental) el presentador de uno de los programas de la ARD, Hans Joachim Friedrichs, proclamó: «Este es un día histórico. Alemania Oriental ha anunciado que, con efecto inmediato, las fronteras han sido abiertas. La RDA está abriendo las fronteras... los puestos fronterizos de Berlín están abiertos».[180]​[179]​

Tras oír la retransmisión, los ossis (ciudadanos de Berlín Este) comenzaron a reunirse en los seis puestos fronterizos a lo largo del Muro de Berlín, exigiendo a los guardias fronterizos que abrieran inmediatamente los puestos de control.[179]​ Los guardias, sorprendidos y sobrepasados por la situación, comenzaron a llamar frenéticamente a sus superiores. En un principio, se ordenó controlar a las personas «más agresivas» y sellarles el pasaporte de manera que no pudieran volver a entrar a Alemania Oriental (lo que significaba revocarles la ciudadanía). Aun así, miles de personas seguían en los controles fronterizos, exigiendo pasar al otro lado «tal y como Schabowski ha dicho».[180]​

Al poco tiempo, estaba claro que ninguna autoridad del Berlín Oriental tomaría la responsabilidad de ordenar el uso de la fuerza letal, de manera que los guardias, superados claramente en número, se vieron impotentes ante las oleadas de ciudadanos. Finalmente, a las 22:45, los guardias cedieron y abrieron los puestos fronterizos dejando pasar a la gente sin apenas control, o directamente, sin pedir siquiera el pasaporte. Mientras transcurría la noche, cientos de personas empezaron a destruir varias zonas del muro fronterizo, lo que dio comienzo a la Caída del Muro de Berlín.[182]​

La división de la ciudad acabaría formalmente el 3 de octubre de 1990, culminando con la reunificación alemana.[183]​

En 1989, el sistema soviético de alianzas estaba al borde del colapso, y sin apoyo militar de la Unión Soviética, los líderes comunistas del Pacto de Varsovia perdieron gran parte de su poder.[175]​ Organizaciones de base, como el sindicato polaco Solidarność, aumentaron rápidamente su popularidad. En 1989, los gobiernos comunistas de Polonia y Hungría fueron los primeros en comenzar a negociar la organización de unas elecciones libres. En Checoslovaquia y Alemania Oriental las masivas protestas depusieron a los inmóviles líderes comunistas. También cayeron los regímenes de Bulgaria y Rumanía, siendo esta última la única en la que hubo derramamiento de sangre durante el cambio de régimen.

Dentro de la Unión Soviética, la nueva política de glásnost acabó por romper los lazos que mantenían a las distintas Repúblicas de la Unión Soviética.[174]​ La libertad de prensa y la disidencia amparada bajo la glásnost provocó un resurgimiento de la «cuestión nacional» y provocó que varias repúblicas proclamaran su autonomía de los designios de Moscú. En febrero de 1990, meses antes de la disolución total de la Unión Soviética, el Partido Comunista de la Unión Soviética tuvo que ceder el monopolio centralista del poder estatal tras 73 años.[184]​ Las repúblicas bálticas fueron más allá y proclamaron su independencia total de la Unión Soviética.[185]​

En un principio, la actitud tolerante que Gorbachov tenía hacia los cambios en Europa del Este, no significaba la misma tolerancia hacia los cambios radicales dentro del territorio de la Unión Soviética. La represión soviética que se ejerció en los países bálticos tras la declaración de su independencia, chocaban con la intención del presidente Bush de mantener unas relaciones normalizadas con la Unión Soviética, avisando a Gorbachov de que los lazos comerciales entre ambos países se verían gravemente afectados si la violencia continuaba.[186]​ Sin embargo, la realidad era que el Estado soviético se desmoronaba inexorablemente, hasta el golpe de gracia que supuso el fallido golpe de agosto de 1991. Un número cada vez mayor de Repúblicas soviéticas manifestaba su intención de independizarse de la Unión Soviética, especialmente la Rusia, lo que hubiese significado el hundimiento total y caótico de la Unión Soviética. El 21 de diciembre de 1991 se producía la disolución de la Unión Soviética firmándose el tratado que creaba la Comunidad de Estados Independientes, que debería ser la heredera legal de la Unión Soviética, en la que cada república sería independiente y libre de unirse, y se mantendría una unión muy laxa en una especie de confederación. La CEI acabó siendo el marco donde, según los líderes rusos, se llevaría a cabo «un divorcio civilizado» de las distintas repúblicas soviéticas.[187]​

La Unión de Repúblicas Socialistas Soviéticas, la Unión Soviética, se declaró oficialmente disuelta el 25 de diciembre de 1991.[188]​

La intervención estadounidense en la Guerra Fría se fraguó a través del apoyo político y económico a gobiernos militares de muchos países norte, centro y sudamericanos, y que eran en su seno económico y político contrarios a los procesos revolucionarios que apuntaban hacia el socialismo. Un ejemplo de esto lo encontramos en Guatemala, cuando por medio de una intervención de la CIA fue derrocado el presidente Jacobo Arbenz en 1954, interrumpiéndose así el proceso democratizador en Guatemala, iniciándose un período de dictaduras militares que duraría hasta 1985 y que sumiría al país en una guerra civil hasta 1996. Otro ejemplo es el de Chile; con el Gobierno de Salvador Allende, la Unidad Popular fue depuesta por el general Augusto Pinochet. En la Argentina la intervención armada de grupos inspirados ya en el comunismo chino, ya —mayoritariamente— en el castrismo o el llamado guevarismo fue abierta y sin ningún ocultamiento, con la acción de Montoneros y el Ejército Revolucionario del Pueblo, entre otros. Fieles a la táctica comunista de ampliar y explotar las diferencias (que por supuesto existían) en la sociedad, captaron inicialmente a una porción del peronismo y comenzaron su acción guerrillera ya desde los años 60. La prosiguieron durante gobiernos militares y durante los gobiernos democráticos de Perón y de su sucesora legal y también durante el gobierno militar comenzado en 1976. Este último, dio inicio al llamado terrorismo de Estado con el que aniquiló militarmente a estos grupos.

Del mismo modo, el intervencionismo del bloque oriental en asuntos más que todo sudamericanos se instauró a través del apoyo a diversos grupos guerrilleros en Bolivia, Colombia, Perú, Brasil y otras naciones centro y sudamericanas. Este proceso se inició con el apoyo soviético al régimen socialista implantado por Castro en Cuba, quien a su vez suministró un muy diligente apoyo a las guerrillas que por esa entonces se proclamaban «revolucionarias».

(Cronología indicativa):

A partir de 1975, las guerrillas comunistas toman el poder en los países recientemente independizados del antiguo imperio colonial portugués en África (Angola y Mozambique). Iniciaron acciones militares contra Sudáfrica con el apoyo del ejército cubano, que devinieron en auténticas batallas, especialmente en Namibia, ocupada por el régimen racista de Sudáfrica (Apartheid). A partir de 1976 en Etiopía, el ejército soviético y las fuerzas cubanas intervinieron contra movimientos opositores a la dictadura de Mengistu Haile Mariam. El ejército francés entabló acciones de desestabilización, como el salvamento de Kolwezi.

Hay tres períodos definidos en el estudio de la Guerra Fría en Occidente:[cita requerida] tradicionalista, revisionista y postrevisionista. Durante más de una década tras del final de la Segunda Guerra Mundial, pocos historiadores estadounidenses discutieron la interpretación «tradicionalista» acerca del comienzo de la Guerra Fría; la que sostenía que la ruptura de las relaciones fue resultado directo de la violación de Stalin de los acuerdos de Yalta, la imposición de gobiernos adictos a Moscú en la devastada Europa Oriental, la intransigencia soviética y el agresivo expansionismo soviético.

Sin embargo, posteriormente los historiadores revisionistas, especialmente William Appleman Williams en su obra de 1959 The Tragedy of American Diplomacy y Walter LaFeber en su obra America, Russia, and the Cold War, 1945-1966 (1968), señalaron una preocupación pasada por alto: el interés estadounidense en mantener una «puerta abierta» para el comercio estadounidense en los mercados mundiales. Se ha señalado por los revisionistas que la política de contención estadounidense expresada en la Doctrina Truman era equivalente a un intento de culpar al otro. Se indicaba como fecha de inicio de la Guerra Fría a las explosiones nucleares de Hiroshima y Nagasaki, interpretando el uso de armas nucleares por parte de los Estados Unidos como una advertencia (o velada amenaza) dirigida a una Unión Soviética que estaba a punto de entrar en guerra contra el ya derrotado Imperio japonés. Pronto los historiadores perdieron interés en la pregunta sobre el responsable de la ruptura de las relaciones soviético-estadounidenses, para señalar que el conflicto entre las superpotencias era en cierto modo inevitable. Esta aproximación revisionista al fenómeno de la Guerra Fría alcanzó especial auge durante la guerra de Vietnam, en la que muchos observaron a los Estados Unidos y la Unión Soviética como dos imperios moralmente comparables.

En últimos años de la Guerra Fría se hicieron esfuerzos para llegar a una síntesis posrevisionista, y desde el final de la Guerra Fría, la escuela postrevisionista ha llegado a ser predominante. Entre los historiadores postrevisionistas más destacados encontramos a John Lewis Gaddis y Robert Grogin. Más que atribuir la responsabilidad del inicio de la Guerra Fría a alguna de las superpotencias de entonces, los historiadores postrevisionistas se centran en temas como la mutua desconfianza, las mutuas falsas percepciones y reactividades, y las responsabilidades compartidas entre las dos superpotencias. Tomando elementos de la escuela realista de las relaciones internacionales, los historiadores posrevisionistas aceptan la política estadounidense en Europa, como la ayuda a Grecia en 1947 y el Plan Marshall.

De acuerdo con esta síntesis, la actividad comunista no fue el origen de las dificultades en Europa, sino que fue una consecuencia de los destructivos efectos de la guerra en la estructura económica, política y social de Europa. En este contexto, el Plan Marshall reconstruyó un sistema económico capitalista, frustrando el llamamiento político al radicalismo izquierdista.

En Europa Occidental, la ayuda económica terminó con la escasez de divisas y estímulo la inversión privada para la reconstrucción de postguerra. En los Estados Unidos, el plan sacó a la economía de una crisis de superproducción, y mantuvo la demanda por las exportaciones estadounidenses. La OTAN sirvió para integrar a Europa Occidental en una red de pactos de mutua defensa. De este modo, proporcionó salvaguardas contra la subversión, o al menos la neutralidad en bloque. Rechazando la percepción del comunismo como un monolito internacional caracterizado por agresivas alusiones al «mundo libre», la escuela postrevisionista sostiene que la intervención de los Estados Unidos en Europa fue una reacción contra la inestabilidad que amenazaba con alterar el equilibrio de poder en favor de la Unión Soviética, modificando el sistema político y económico occidental.

Este periodo vislumbró una guerra estratégica, política y científica. Se dio una disconformidad entre ambas naciones tanto en la creación de nuevas tecnologías y armamento, como en la conquista del espacio exterior. Si bien las condiciones en los tiempos de la Guerra Fría eran otras, la división geopolítica imperante en el mundo dependía del dominio de la extinta Unión Soviética (modelo de referencia para futuros estados socialistas) y Estados Unidos. La actualidad muestra que dicha atribución no está tan marcada como en aquella época, pero los hechos recientes muestran que el fin de la beligerancia dista mucho de ser un caso cerrado. Por otra parte, el fin de la Guerra Fría no resultó en la eliminación del conflicto en el sistema político internacional. En la era contemporánea, existe una tensión bastante pronunciada entre Estados Unidos y potencias revisionistas como China y Rusia.[189]​

Está claro que las diplomacias entre Rusia y Estados Unidos se encuentran en una posición delicada, los diferentes movimientos estratégicos dan a pensar que la guerra sigue latente, la tensión entre una nación y otra ha escalado más de la cuenta. El caso de Edward Snowden, la situación en Siria y la crisis de Ucrania han inducido a que las relaciones entre EE. UU. y Rusia comiencen a recordar a los años duros de la Guerra Fría; esperando ese momento que acabe con los años de pactos y negociaciones que han sostenido, pero sus intereses contrastados suelen impedirlo, y eso crea tanto riesgos como oportunidades para terceros.[cita requerida]

Henry Ford (Dearborn, Míchigan; 30 de julio de 1863-Ib., 7 de abril de 1947) fue un empresario y emprendedor estadounidense, fundador de la compañía Ford Motor Company y padre de las cadenas de producción modernas utilizadas para la producción en masa.

La introducción del Ford T en el mercado automovilístico revolucionó el transporte y la industria en Estados Unidos. Fue un inventor prolífico que obtuvo 161 patentes registradas en ese país. Como único propietario de la compañía Ford, se convirtió en una de las personas más conocidas y más ricas del mundo.

A él se le atribuye el fordismo, sistema que se difundió entre finales de los años treinta y principios de los setenta y que creó mediante la fabricación de un gran número de automóviles de bajo costo mediante la producción en cadena. Este sistema llevaba aparejada la utilización de maquinaria especializada y un número elevado de trabajadores en plantilla con salarios elevados.

Su visión global, con el consumismo como llave de la paz, es la clave de su éxito. Su intenso compromiso de reducción de costes llevó a una gran cantidad de inventos técnicos y de negocio, incluyendo un sistema de franquicias que estableció un concesionario en cada ciudad de Estados Unidos y Canadá y en las principales ciudades de los cinco continentes.

Ford dejó gran parte de su inmensa fortuna a la Fundación Ford, pero también se aseguró de que su familia controlase la compañía permanentemente.

Henry Ford nació en una granja, en el seno de una familia muy pobre, en un pueblo rural al oeste de Detroit (el área en cuestión es ahora parte de Dearborn, Míchigan). Sus padres fueron William Ford (1826-1905) y Mary Litogot (c. 1839-1876). Eran de ascendencia inglesa, pero habían vivido en Irlanda, en el Condado de Cork. Tuvo varios hermanos: Margaret (1867-1868), Jane (c. 1868-1945), William (1871-1917) y Robert (1873-1934).

Durante el verano de 1873, Henry vio por primera vez una máquina autopropulsada; una máquina de vapor estacionaria que podía ser usada para actividades agrícolas. El operador, Fred Reden, la había montado encima de ruedas a las que había conectado mediante una cadena. Henry quedó fascinado con la máquina y Reden durante el año siguiente enseñó al joven cómo encender y manejar el motor.
Ford dijo más adelante que esta experiencia fue la que le «enseñó que era por instinto un ingeniero».[1]​

Henry llevó esta pasión por los motores a su propia casa. Su padre le dio un reloj de pulsera al comienzo de su adolescencia. A los 15 tenía una buena reputación como reparador de relojes, habiendo desmantelado y vuelto a ensamblar los relojes de amigos y vecinos docenas de veces[2]​

Su madre murió en 1876. Fue un duro golpe que dejó al joven destrozado. Su padre esperaba que Henry finalmente se hiciera cargo de la granja familiar, pero Henry odiaba ese trabajo. Por otra parte, con su madre muerta ya había poco que le atase a la granja. Más tarde dijo, «nunca tuve un amor particular por la granja. Era la madre en la granja a la que amaba».[3]​

En 1879 dejó su casa y se dirigió a Detroit para trabajar como aprendiz de maquinista, primero en James F. Flower & Bros., y más tarde en Detroit Dry Dock Co. En 1882 volvió a Dearborn para trabajar en la granja y se encargó del manejo de la máquina de vapor portátil Westinghouse hasta hacerse un experto.
Esto le llevó a ser contratado por la compañía Westinghouse para dar servicio a sus máquinas de vapor.

Durante su matrimonio con Clara Bryant en 1888, Ford se mantuvo mediante la granja y operando un aserradero. Tuvieron un hijo, Edsel Bryant Ford (1893-1943).

En 1891, Ford consiguió el puesto de ingeniero en la compañía Edison, y tras su ascenso a ingeniero jefe en 1893 comenzó a tener suficiente tiempo y dinero como para dedicarlo a sus propios experimentos con motores de gasolina. Estos experimentos culminaron en 1896 con la invención de su propio vehículo autopropulsado denominado cuadriciclo, que hizo su primera prueba con éxito el 4 de junio de ese año. Tras varias pruebas, Ford comenzó a desarrollar ideas para mejorarlo.[4]​

Tras este exitoso comienzo, Ford llegó a Edison Illuminating en 1899 junto con otros inventores, y formaron la Detroit Automobile Company. La compañía pronto acabó en bancarrota por culpa de que Ford continuaba mejorando los prototipos en lugar de vender coches. Hacía carreras entre su coche y los de otros fabricantes para demostrar la superioridad de su diseño. Con este interés en los coches de carreras creó la Henry Ford Company. Durante este periodo condujo personalmente uno de sus coches en la victoria frente a Alexander Winton el 10 de octubre de 1901.

En 1902, Ford siguió trabajando en su coche de carreras, con el consecuente perjuicio a sus inversores. Querían un modelo preparado para la venta y trajeron a Henry M. Leland para que lo llevase a cabo. Ford renunció ante este menoscabo de su autoridad, y posteriormente dijo: Dimití determinado a nunca jamás volver a ponerme bajo las órdenes de nadie.[5]​
La compañía fue reorganizada bajo el nuevo nombre de Cadillac.

Henry Ford recién tuvo éxito en su tercer proyecto empresarial, lanzado en 1903: la Ford Motor Company, fundada el 16 de junio junto con otros 11 inversores y con una inversión inicial de 28 000 dólares estadounidenses.
En un automóvil de reciente diseño, Ford hizo una exhibición en la cual el coche corrió la distancia de una milla en el lago helado de St. Clair en 39,4 segundos, batiendo el récord de velocidad en tierra. Convencido por este éxito, el famoso piloto de coches Barney Oldfield, que llamó a este modelo de Ford 999 en honor a uno de los vehículos de carreras de la época, condujo el coche a lo largo y ancho del país, haciendo que la nueva marca de Ford fuese conocida en todo EE. UU. Ford también fue uno de los primeros impulsores de las 500 millas de Indianápolis.

Ford asombró al mundo en 1914 ofreciendo un salario a sus trabajadores de 5 dólares al día, que en esa época era más del doble de lo que se pagaba a la mayoría de estos empleados. Esta táctica le resultó inmensamente provechosa cuando los mejores mecánicos de Detroit comenzaron a cambiarse a la empresa Ford, trayendo con ellos su capital humano y experiencia, incrementando la productividad y reduciendo los costos de formación. Ford lo denominó «motivación salarial». El uso de la integración vertical en la compañía también resultó muy útil, cuando Ford construyó una fábrica gigantesca en la que entraban materias primas y salían automóviles terminados.

El Ford T apareció en el mercado el 1 de octubre de 1908 y presentaba una gran cantidad de innovaciones. Por ejemplo, tenía el volante a la izquierda, siendo esto algo que la gran mayoría de las otras compañías pronto copiaron. Todo el motor y la transmisión iban cerrados, los cuatro cilindros estaban encajados en un sólido bloque y la suspensión funcionaba mediante dos muelles semielípticos.
El automóvil era muy sencillo de conducir y, más importante, muy barato y fácil de reparar.
Era tan barato que, con un coste de 825 dólares estadounidenses en 1908 (el precio caía cada año), para 1920 la gran mayoría de conductores habían aprendido a conducir en el Ford T.

Ford también se preocupó de instaurar una publicidad masiva en Detroit, asegurándose de que en cada periódico apareciesen historias y anuncios sobre su nuevo producto. Su sistema de concesionarios locales permitió que el automóvil estuviese disponible en cada ciudad de EE. UU.
Por su parte, los concesionarios (empresarios independientes) fueron enriqueciéndose y ayudaron a publicitar la idea misma del automovilismo, comenzando a desarrollarse los clubes automovilísticos para ayudar a los conductores y para salir más allá de la ciudad. Ford estaba encantado de vender a los granjeros, que miraban el vehículo como un invento más para ayudarles en su trabajo.

Las ventas se dispararon.
Durante varios años se iban batiendo los propios récords del año anterior. Las ventas sobrepasaron los 250 000 vehículos en 1914. Por su parte, siempre a la caza de la reducción de costes y mayor eficiencia, Ford introdujo en sus plantas en 1913 las cintas de ensamblaje móviles, que permitían un incremento enorme de la producción. 

Si bien se le suele dar el mérito a Ford por esta idea, las fuentes contemporáneas indican que el concepto y su desarrollo partió de los empleados Clarence Avery, Peter E. Martin, Charles E. Sorensen y C. H. Wills. Para 1916 el precio había caído a 360 dólares por el automóvil básico, llegando las ventas a la cifra de 472 000.[6]​ Al mismo tiempo, se le atribuye erróneamente a Ford el haber sido el primero en aplicar el concepto de la producción por cadena de montaje, cuando en realidad fue Ransom Eli Olds el primero en aplicarla en el año 1900, para producir su modelo Oldsmobile Curved Dash. A pesar de ello, Ford pasó a la historia por haber perfeccionado ese sistema, logrando un rango de producción de mayor escala que con el sistema aplicado por Olds.[7]​

Para 1918 la mitad de los coches en EE. UU. eran el modelo T de Ford.[cita requerida] Ford escribió en su autobiografía que «cualquier cliente puede tener el coche del color que quiera siempre y cuando sea negro».[8]​
Hasta la aplicación de la cadena de ensamblaje, en la que el color que se utilizaba era el negro porque tenía un tiempo de secado más corto, sí que hubo Ford T en otros colores, incluyendo el rojo.
El diseño fue fervientemente impulsado y defendido por Henry Ford, y su producción continuó hasta finales de 1927. La producción total final fue de 15 007 034 unidades, récord que se mantuvo durante los siguientes 45 años.

En 1918 el presidente de EE. UU. Woodrow Wilson pidió personalmente a Henry Ford que se presentase a las elecciones al Senado por el estado de Míchigan como representante del Partido Demócrata.

Aunque la nación se encontraba en guerra, Ford se mostró como un político pacifista y defensor de la Sociedad de Naciones.[9]​
En diciembre de 1918 Henry Ford pasó la presidencia de su compañía a su hijo, Edsel Ford.

Henry, sin embargo, mantuvo su autoridad sobre las decisiones finales y en ocasiones modificó alguna de las decisiones de su hijo. Henry y Edsel compraron todas las acciones que quedaban del resto de inversores con lo que la propiedad absoluta de la compañía quedó en la familia.

En 1926, la caída de ventas del Ford T terminó por convencer a Henry de que convenía crear un nuevo modelo de automóvil. Henry se embarcó en el proyecto centrándose en el diseño del motor, el chasis y otras necesidades mecánicas, mientras que dejaba el diseño del cuerpo del automóvil a su hijo.
Edsel también logró vencer alguna de las objeciones iniciales de su padre e incluir algunos diseños técnicos como el de la caja de cambios.
El resultado fue el Ford A, que apareció en diciembre de 1927 y fue construido hasta 1931 con una producción total de unos cuatro millones de automóviles.
La compañía adoptó un modelo de modificaciones anuales del producto similar al que se realiza hoy en día.

En mayo de 1943 Edsel Ford muere debido a un cáncer de estómago, dejando vacante la presidencia de la compañía. Henry Ford defendió a Harry Bennett, su socio desde hace muchos años, para que tomase ese puesto. Por su parte, la viuda de Edsel, Eleanor, que había heredado los derechos de voto de Edsel, quería que fuese su hijo Henry Ford II quien se hiciese cargo de la compañía. El tema se zanjó durante un tiempo cuando Henry, a la edad de 79, se hizo cargo de la presidencia personalmente.
Henry Ford II fue liberado de sus deberes en la marina y se convirtió en vicepresidente ejecutivo, mientras que Harry Bennett tomó un puesto en el Consejo como responsable de personal, relaciones laborales y relaciones públicas.

La compañía pasó por una época complicada en los siguientes dos años, perdiendo 10 millones de dólares al mes. En 1945 la senilidad de Henry Ford era ya evidente, y su mujer y su nuera forzaron su dimisión en favor de su nieto, Henry Ford II.

Henry Ford fue un pionero del estado de bienestar a través de la sociedad de consumo.
Buscó mejorar el nivel de vida de sus trabajadores y reducir su rotación. La eficiencia suponía contratar y mantener a los mejores trabajadores. El 5 de enero de 1914, Ford anunció su programa retributivo de 5 dólares al día. Este programa revolucionario también incluía la reducción de la jornada laboral de 9 a 8 horas al día, 5 días a la semana, así como el ya mencionado incremento desde 2,34 dólares al día hasta 5 para los trabajadores cualificados.[10]​

Ford fue criticado por Wall Street por haber comenzado la implantación de la semana de 40 horas y por establecer un salario mínimo. Sin embargo, demostró que un pago así permitía a sus trabajadores el comprar los mismos coches que producían, y que por lo tanto era bueno para la economía. Ford denominó a este incremento en los salarios como una forma de compartir el beneficio. El salario de 5 dólares se ofrecía a los hombres mayores de 22 años que hubiesen trabajado en la compañía durante seis o más meses y, más importante si cabe, llevasen una vida que fuese aprobada por el «Departamento de Sociología».
No aprobaban ni la bebida en abundancia ni el juego. El departamento utilizaba a 150 investigadores y apoyaban que los jefes mantuviesen los estándares de los empleados. Un gran porcentaje de los empleados consiguieron calificar para recibir esta parte de los beneficios.

Ford estaba completamente en contra de los sindicatos en sus fábricas. Para parar este tipo de actividad promocionó a Harry Bennett, un antiguo boxeador de la marina, para que fuese la cabeza del Departamento de Servicio. Bennet utilizó varias tácticas de intimidación para acabar con la organización de sindicatos. El incidente más famoso, en 1937, fue una sangrienta pelea entre el cuerpo de seguridad y los sindicalistas enfrente de los medios de comunicación.

Ford, igual que otras compañías automovilísticas, entró en el negocio de la aviación durante la Primera Guerra Mundial, construyendo motores Liberty. Después de la guerra volvió a la fabricación propia hasta 1925, cuando Henry Ford adquirió la Stout Metal Airplane Company.

El avión con mayor éxito de Ford fue el Trimotor, comúnmente llamado Tin Goose. Utilizaba una nueva aleación llamada Alclad que combinaba la resistencia a la corrosión del aluminio con la dureza del duraluminio. El avión era parecido al Fokker V.VII-3m, y algunos dicen[¿quién?] que los ingenieros de Ford midieron el avión para luego copiarlo. El Trimotor voló por primera vez el 11 de junio de 1926 y fue el primer avión de pasajeros con éxito, que acomodaba a unos 12 pasajeros de una manera medianamente confortable. Hubo diversas variantes que fueron utilizadas por el ejército. En total se construyeron unos 200 de estos aparatos hasta que la compañía cerró por la caída de ventas producida por la Depresión.

Se considera que el Trimotor Tin Goose de Ford es el precursor de los actuales Fokker 50, Dash 7+, ATR 72, Antonov 24, Avro-HS748, etc. Estos aviones fueron éxitos de la aviación internacional y todavía existen trimotores en vuelo en algunos lugares.

En 1915 financió un viaje a Europa, en donde estaba transcurriendo la Primera Guerra Mundial, para sí mismo y otros 170 líderes defensores de la paz. Habló con el presidente Wilson sobre el viaje, pero no recibió apoyo gubernamental. El grupo se dirigió a la neutral Suiza y a los Países Bajos para encontrarse con activistas de la paz. Ford, objeto de muchas burlas, dejó la misión tan pronto como llegó a Suiza.

Un artículo que escribió el escritor británico G. K. Chesterton el 11 de diciembre de 1915 para Illustrated London News muestra el porqué de las burlas a sus esfuerzos. Refiriéndose a Ford como «el comediante estadounidense», Chesterton apunta que Ford había llegado a decir que «pienso que el hundimiento del Lusitania fue deliberadamente planeado para meter a este país, EE. UU. en la guerra. Fue planeado por quienes financian la guerra». Chesterton expresó su «dificultad para creer que los banqueros naden bajo el mar para cortar agujeros en los bajos de los barcos», y preguntó por qué, si lo que Ford decía era cierto, Alemania había aceptado su responsabilidad en el hundimiento y «defendía lo que no había hecho». Según él, los esfuerzos de Ford suponían un problema para pacifistas «más plausibles y presentables».

En el otro lado H. G. Wells, en su novela de ciencia ficción The Shape of Things to Come (La forma de las cosas que vendrán), dedicó un capítulo entero al barco de la paz de Ford, estableciendo que «pese a que no tuvo éxito, su esfuerzo de parar la guerra será recordado mientras que los generales y sus batallas y las masacres sin sentido serán olvidadas».
Wells acusaba a la industria armamentista estadounidense y a los bancos —que hicieron grandes beneficios con la venta de municiones a las naciones europeas en guerra— de haber esparcido mentiras, con la finalidad de provocar el fracaso de Ford en sus esfuerzos de paz. Sin embargo, apuntaba que cuando EE. UU. entró en la guerra en 1917, el mismo Ford obtuvo considerables beneficios de la venta de municiones.

En 1918, uno de los más cercanos colaboradores de Ford y su secretario personal, Ernest G. Liebold, compró un periódico semanal, The Dearborn Independent, para que Ford pudiese publicar sus opiniones. Para 1920 Ford se convirtió en un antisemita y en marzo de ese año comenzó una cruzada antijudía en las páginas de su periódico.[11]​
El periódico siguió funcionando durante ocho años, desde 1920 hasta 1927, y durante ese periodo Liebold fue el editor. El periódico publicó Los protocolos de los sabios de Sion, que fue posteriormente desacreditado por una investigación del periódico The Times de Londres.
La Sociedad Histórica Judía de Estados Unidos describe las ideas que aparecían en el periódico como contrarias a la inmigración, al sindicalismo, a las bebidas alcohólicas y a los judíos.
En febrero de 1921 el periódico New York World publicó una entrevista con Ford en la que dijo que «El único comentario que diré sobre los Protocolos es que encajan en lo que está pasando».
Durante este periodo Ford emergió como «un portavoz respetado para la extrema derecha y los prejuicios religiosos» llegando a tener alrededor de 700 000 lectores de su periódico.[12]​

Junto con los Protocolos se publicaron otros artículos antisemitas en The Dearborn Independent.
En los años veinte se publicó una recopilación de cuatro volúmenes denominada The International Jew, the World's Foremost Problem (El judío internacional, el mayor problema mundial).
Vincent Curcio escribe acerca de estas publicaciones que «eran distribuidas muy ampliamente y tuvieron gran influencia, en particular en la Alemania nazi, en donde nada menos que Adolf Hitler las leyó y las admiraba. Hitler colgó la foto de Ford en la pared, y basó varias secciones de Mein Kampf en sus escritos: es más, Ford es el único estadounidense mencionado en su libro.
Según Lacey «ningún estadounidense contribuyó tanto al nazismo como Henry Ford».[13]​
Steven Watts escribió que Hitler «reverenciaba» a Ford, proclamando que «haré lo que pueda para poner sus teorías en práctica en Alemania», y modelando el Volkswagen, el coche del pueblo, a imagen del Ford T».[14]​
En Mein Kampf (escrita a mediados de los años veinte) Hitler expresó su opinión de que, «Son los judíos quienes gobiernan las fuerzas de la Bolsa de Valores en la Unión Estadounidense.
Cada año les convierte más y más en los maestros que controlan a los productores de una nación de 120 millones.
Pero para la furia de ellos, solo un hombre, Ford, todavía mantiene la total independencia».[15]​

Fue denunciado por la Liga Antidifamación (organización judía fundada y dirigida por la B'nai B'rith), si bien los artículos condenaban explícitamente la violencia contra los judíos (volumen 4, capítulo 80), aunque preferían culpar a los judíos de provocar los episodios de violencia.[16]​
Sin embargo, estos artículos no fueron realmente escritos por Ford, que no escribió prácticamente nada según su testimonio en el juicio. Sus amigos y socios de negocio dijeron que le advirtieron sobre los contenidos del Independent y que Ford probablemente nunca lo leyese (él afirmaba que solo leía los titulares).[17]​
En cualquier caso, uno de los testigos del juicio traído por uno de los objetivos del periódico afirmaba que Ford sí que conocía los contenidos del Independent antes de su publicación.[18]​

Una demanda judicial de Aaron Sapiro (abogado judío de San Francisco que organizaba una granja cooperativa), en respuesta a los comentarios antisemitas de la revista, llevó a que Ford cerrase el Independent en diciembre de 1927. Las noticias dijeron entonces que se había visto sorprendido por el contenido y que no conocía su naturaleza. Durante el juicio, el editor de la «propia página» de Ford, William Cameron, testificó que Ford no tenía nada que ver con los editoriales a pesar de que incluso estuviesen bajo su firma. También testificó que nunca había discutido su contenido ni pedido la aprobación de Ford.[19]​
Por otro lado, el periodista de investigación Max Wallace dijo que cualquier credibilidad que esta absurda afirmación hubiera podido tener pronto fue echada abajo cuando James M. Miller, un antiguo empleado del periódico, dijese bajo juramento que Ford le había dicho que pretendía ir a por Sapiro.[18]​

Michael Barkun observó que Cameron hubiese continuado publicando ese material tan controvertido sin las instrucciones explícitas de Ford parecía impensable para aquellos que conociesen a ambos.
Mrs. Stanley Ruddiman, una íntima de la familia de Ford, remarcó que ella «no creía que Cameron escribiese nada para la publicación sin la aprobación de Ford».[20]​
Según Spencer Blakesle

Ford se asoció después con el notorio antisemita Gerald L. K. Smith, que comentó tras su encuentro en los años treinta que él era «menos antisemita que Ford». Smith también recalcó que en 1940 Ford no mostraba ningún arrepentimiento por la visión antisemita del Independent y «esperaba publicar The International Jew otra vez más adelante».[22]​ Ese mismo año Ford comentó en The Guardian que «los banqueros internacionales judíos» eran responsables de la Segunda Guerra Mundial.[23]​

En 1938 el cónsul alemán en Cleveland otorgó a Ford la condecoración de la Gran Cruz de la Orden del Águila Alemana, la condecoración más alta que la Alemania nazi podía otorgar a un extranjero.[24]​

La distribución de International Jew fue interrumpida en 1942, aunque los grupos extremistas a menudo reciclan el material, que todavía aparece en información y páginas web antisemitas y neonazis.

La filosofía de Ford en cuanto a economía internacional era la de la independencia económica de EE. UU. Su planta del río Rouge, en Míchigan, se convertiría en el mayor complejo industrial del mundo capaz incluso de producir su propio acero. El objetivo de Ford era producir un vehículo desde el comienzo sin depender del comercio exterior. Creía que el comercio y la cooperación internacional llevaban a la paz internacional y usaba línea de producción del Ford T para demostrarlo.[25]​

Abrió plantas de producción en el Reino Unido y Canadá en 1911 y pronto se convirtió en el mayor productor de automóviles de esos países. En 1912 Ford cooperó con Agnelli de Fiat para lanzar las primeras líneas de producción italianas. Las primeras plantas en Alemania se construyeron en los años veinte con el apoyo de Herbert Hoover y el departamento de comercio, que estaban de acuerdo con la teoría de Ford de que el comercio internacional era esencial para la paz mundial.[26]​

Durante los años veinte Ford también abrió plantas en Australia, India y Francia y para 1929 tenía distribuidores en cinco continentes. Ford experimentó con una plantación de hule comercial en la jungla del Amazonas llamada Fordlândia que fue uno de sus pocos fracasos.
En 1929 aceptó la invitación de Stalin para construir una planta modelo (NNAZ, hoy llamada GAZ) en Gorki, ciudad que más tarde fue renombrada Nizhni Nóvgorod, y envió a ingenieros y técnicos estadounidenses a ayudar a ponerla en marcha, incluyendo al futuro líder sindicalista Walter Reuther.

El acuerdo de asistencia técnica entre la Ford Motor Company, VSNH y Amtorg[27]​ (como comprador) concluyó a los 9 años y se prorrogó el 31 de mayo de 1929 firmado por Ford, el vicepresidente Peter E. Martin, V. I. Mezhlauk, y el presidente de Amtorg, Saul G. Bron. Con cualquier nación con la que EE. UU. tuviera relaciones diplomáticas pacíficas, la Ford Motor Company tendría negocios. Para 1932, Ford producía un tercio de toda la producción mundial de automóviles.

La imagen de Ford transfiguró a los europeos, especialmente a los alemanes, provocando «el miedo de algunos, la visión romántica de otros y la fascinación en todos».[28]​
Los alemanes que discutían el fordismo a menudo creían que representaba algo de la quinta esencia de EE. UU. Veían que el tamaño, el tiempo, la estandarización y la filosofía de producción demostraba que la Ford trabajaba como un servicio nacional: una «cosa estadounidense» que representaba la cultura de EE. UU.
Tanto los que lo apoyaban como los críticos insistían en que el Fordismo era el epítome del desarrollo capitalista, y que la industria de automóviles era la llave para entender las relaciones económicas y sociales en EE. UU. Un alemán explicó que «los automóviles han cambiado tanto el modo de vida estadounidense que hoy en día uno apenas puede imaginar estar sin coche. Es difícil recordar cómo era la vida antes de que el Sr. Ford comenzase a predicar su doctrina de salvación».[28]​
Para muchos alemanes, Henry Ford encarnaba la esencia del sueño estadounidense.

Ford comenzó su carrera de piloto de carreras y mantuvo su interés en estas. Desde 1909 hasta 1913, Ford llevó al Modelo T a las carreras, acabando primero (aunque luego fue descalificado) en una carrera a través de EE. UU. en 1909, y estableciendo el récord de velocidad de una milla en Detroit en 1911 con el conductor Frank Kulick.
En 1913, Ford intentó meter a un nuevo modelo T en las 500 millas de Indianápolis, pero le dijeron que las normas obligaban a que se añadiesen unos 450 kg de peso al coche para participar en la carrera.
Ford se retiró de la carrera y pronto dejó permanentemente las carreras alegando no estar satisfecho con las normas del deporte y las demandas de ese momento.

Henry Ford creó junto a su hijo, Edsel, la Fundación Ford en 1936 con el amplio objetivo de promocionar el bienestar de la gente.
Ford dividió su capital en un pequeño número de acciones con voto, que repartió entre su familia, y un gran número de acciones sin voto que dio a la Fundación.
La Fundación creció inmensamente y, para 1950, ya tenía un ámbito internacional.
Gradualmente fue vendiendo todas sus acciones en el mercado desde 1955 hasta 1974,[29]​
y perdió su conexión con la Ford Motor Company y la familia Ford.[30]​

Cuando Edsel Ford, presidente de Ford Motor Company, murió de cáncer en mayo de 1943, el anciano y enfermo Henry Ford decidió asumir la presidencia. En este punto, Ford, que tenía cerca de 80 años, había tenido varios eventos cardiovasculares (citados como ataques cardíacos o derrames cerebrales) y era mentalmente inconsistente, sospechoso y, en general, ya no era apto para tales inmensas responsabilidades.

La mayoría de los directores no querían verlo como presidente. Pero durante los últimos 20 años, aunque había estado sin ningún título ejecutivo oficial, siempre había tenido un control de facto sobre la compañía; la junta y la gerencia nunca lo habían desafiado seriamente, y este momento no fue diferente. Los directores lo eligieron y sirvió hasta el final de la guerra. Durante este período, la compañía comenzó a declinar, perdiendo más de $ 10 millones de dólares al mes (147 750 000 de dólares hoy). La administración del presidente Franklin Roosevelt había estado considerando una toma de control de la compañía por parte del gobierno para asegurar la producción de guerra continua, pero la idea nunca progresó.

Al fallar su salud, Ford cedió la presidencia de la compañía a su nieto, Henry Ford II, en septiembre de 1945 y se retiró. Murió el 7 de abril de 1947, de una hemorragia cerebral en Fair Lane, su propiedad en Dearborn, a la edad de 83 años. Se realizó una visita pública en Greenfield Village, donde hasta 5000 personas por hora pasaron el ataúd. Los servicios funerarios se llevaron a cabo en la Iglesia Catedral de San Pablo de Detroit y fue enterrado en el cementerio de Ford en Detroit.

La Real Academia Española (RAE)[3]​ es una institución cultural española privada financiada con fondos públicos con sede en Madrid, España. Esta y otras veintitrés academias de la Lengua correspondientes a cada uno de los países donde se habla el español conforman la Asociación de Academias de la Lengua Española (ASALE).

Se dedica a la regularización lingüística mediante la promulgación de normativas dirigidas a fomentar la unidad idiomática dentro de los diversos territorios que componen el mundo hispanohablante y garantizar una norma común, en concordancia con sus estatutos fundacionales: «velar por que los cambios que experimente [...] no quiebren la esencial unidad que mantiene en todo el ámbito hispánico».[4]​

Fue fundada en 1713 por iniciativa del ilustrado Juan Manuel Fernández Pacheco, VIII marqués de Villena y duque de Escalona, a imitación de la Academia Francesa. Al año siguiente, el rey Felipe V aprobó su constitución y la colocó bajo su protección.[5]​ En 1715, la Academia aprobó sus primeros estatutos.[5]​

Las directrices lingüísticas que propone se recogen en diversas obras. Las prioritarias son el diccionario, abreviado DLE (art. 2.º de sus estatutos), editado periódicamente veintitrés veces desde 1780 hasta hoy; y la gramática (4.º), editada entre 2009 y 2011.[6]​Desempeña sus funciones en la sede principal, inaugurada en 1894, en la calle Felipe IV, 4, en el barrio de Los Jerónimos, y en el Centro de Estudios de la Real Academia Española y de la ASALE, en la calle Serrano 187-189, en 2007.[7]​

En 1711, España, a diferencia de Francia, Italia y Portugal, no tenía un gran diccionario que contase con un repertorio lexicográfico comprehensivo y elaborado de forma colegiada.[8]​ El núcleo inicial de la futura Academia lo formaron ese mismo año los ocho novatores que se reunían en la biblioteca del palacio madrileño de Juan Manuel Fernández Pacheco, situado en la plaza de las Descalzas Reales en Madrid.[9]​

La Real Academia Española fue fundada en 1713 por iniciativa de Juan Manuel Fernández Pacheco, VIII marqués de Villena y duque de Escalona, con el propósito de «fijar las voces y vocablos de la lengua castellana en su mayor propiedad, elegancia y pureza».[5]​ El objetivo era fijar el idioma en el estado de plenitud que había alcanzado durante el siglo XVI y que se había consolidado en el XVII. Se tomaron como modelo para su creación la Accademia della Crusca italiana (1582) y la Academia Francesa (1635). La primera sesión oficial de la nueva corporación se celebró en la propia casa del marqués de Villena el 6 de julio de 1713,[10]​ acontecimiento que se registra en el libro de actas, iniciado el 3 de agosto de 1713. Su creación, con veinticuatro sillas,[nota 1]​ fue aprobada el 3 de octubre de 1714 por Real Cédula de Felipe V, quien la acogió bajo su «amparo y Real Protección». Esto significaba que los académicos gozaban de las preeminencias y exenciones concedidas a la servidumbre de la Casa Real.[5]​ Tuvo su primera sede en el número 26 de la calle de Valverde, de donde se trasladó a la de Alarcón esquina a Felipe IV, su sede definitiva.[11]​

En la conciencia, según la visión de la época de que la lengua española había llegado a un momento de suma perfección, el propósito de la Real Academia fue «fijar las voces y vocablos de la lengua castellana en su mayor propiedad, elegancia y pureza». Se representó tal finalidad con un emblema formado por un crisol puesto al fuego, con la leyenda: "Limpia, fija y da esplendor". Nació, por tanto, la institución como un centro de trabajo eficaz, según decían los fundadores, «al servicio del honor de la nación».

Esta vocación de utilidad colectiva se convirtió en la principal seña de identidad de la Academia Española, diferenciándose de otras academias que habían proliferado en los siglos de oro y que estaban concebidas como meras tertulias literarias de carácter ocasional.

En 1723, se le concedieron al marqués 60 000 reales anuales para sus publicaciones. Fernando VI le permitió publicar sus obras y las de sus miembros sin censura previa.

En 1726, se publica el primer volumen del gran diccionario de la época (el Diccionario de autoridades) y, en 1741, el de ortografía (la Orthographía española). Y después, en 1771, una gramática.[12]​

En 1784, María Isidra de Guzmán y de la Cerda, primera mujer doctora por la Universidad de Alcalá, fue admitida como académica honoraria y, aunque pronunció su discurso de agradecimiento, no volvió a comparecer más. Se cuenta entre las primeras mujeres académicas del mundo.[13]​ No volvió a haber otra académica mujer hasta la elección de Carmen Conde como académica de número en 1978.

En 1842, solicitaron un crédito de ochenta mil reales por dos años para financiar el nuevo Diccionario a José Nicasio Gallego, que era el secretario de la propia Real Corporación. Mediante dicho préstamo la Academia hipotecó todos sus bienes. En 1847, se pudo saldar la hipoteca.[14]​

En 1848, la Academia reformó su organización por medio de unos nuevos estatutos, aprobados por Real Decreto. Sucesivos reales decretos (1859, 1977, 1993) aprobaron nuevas reformas.

Tras la independencia de los países americanos, la Real Academia Española promovió el nacimiento de academias correspondientes en cada una de las jóvenes repúblicas hispanoamericanas. Esta decisión estuvo motivada por la idea central del movimiento llamado panhispanismo o hispanoamericanismo, según la cual los ciudadanos de todas las naciones de matriz española tienen por patria común una misma lengua (el español) y comparten el patrimonio de una misma literatura.[15]​ A pesar de que hubo precedentes de academias nacionales creadas con independencia de la Española, como la Academia de la Lengua de México (1835),[16]​ que se disolvió para dar paso a la correspondiente Academia Mexicana de la Lengua (1875), y de que alguna de las academias americanas, como la Academia Argentina de Letras (1931), no tuvo vinculación estatutaria con la RAE hasta fundarse la ASALE, desde 1870 se establecieron en América diversas academias hispanoamericanas subordinadas estatutariamente a la RAE, a las que se llamó correspondientes por mantener con la academia matriz una relación por correspondencia postal. A ellas se añadieron la Academia Argentina de Letras, la Academia Filipina de la Lengua Española, la Academia Norteamericana de la Lengua Española y la Academia Ecuatoguineana de la Lengua Española. Estas veintidós academias, que tienen igual rango y condiciones que la RAE, constituyen con ella la Asociación de Academias de la Lengua Española (ASALE), fundada en 1951 en el marco del I Congreso de Academias celebrado en México.

La ASALE es el órgano de colaboración de todas ellas en la promoción de una política lingüística panhispánica[17]​.[18]​ Esta política, plasmada en numerosos proyectos de trabajo conjunto, fue galardonada en el año 2000 con el Premio Príncipe de Asturias de la Concordia, concedido a la Real Academia Española, junto con la Asociación de Academias de la Lengua Española.

El 20 de octubre de 1993 se constituyó la Fundación pro Real Academia Española, entidad que tiene como finalidad atraer recursos económicos para la financiación de las actividades e iniciativas de la Academia.[19]​ Está regida por un patronato, cuya presidencia de honor corresponde al rey de España, la presidencia al gobernador del Banco de España y la vicepresidencia al director de la Real Academia Española. Las vocalías corresponden a otros académicos, presidentes de las comunidades autónomas y de empresas privadas, como socios fundadores.

En los últimos estatutos aprobados en 1993, se consideró necesario supeditar el antiguo lema fundacional -Limpia, fija y da esplendor- al objetivo superior de trabajar al servicio de la unidad idiomática.[20]​ El artículo primero establece, en tal sentido, que la Academia “tiene como misión principal velar por que los cambios que experimente la lengua española en su constante adaptación a las necesidades de sus hablantes no quiebren la esencial unidad que mantiene en todo el ámbito hispánico”. De esa forma quedaba sancionado un compromiso que la Academia había asumido ya desde el siglo XIX.

La Fundación está abierta a la participación de particulares mediante la correspondiente cuota económica, miembros benefactores, y entre las actividades subvencionadas se encuentran la realización del banco de datos, el Diccionario del estudiante, el Diccionario panhispánico de dudas, la Gramática normativa y otras obras en proyecto o desarrollo como el CORPES (Corpus del Español del Siglo XXI) o el Diccionario histórico.

El artículo primero de los estatutos de la RAE dice:

Los 46 miembros de número de la Real Academia Española son elegidos por cooptación por el resto de los académicos. Las plazas de académico de número se denominan «sillas», que tradicionalmente se han distribuido de acuerdo a letras del alfabeto latino de uso para el castellano, tanto mayúsculas como minúsculas (excepción hecha de las plazas de las secciones especiales o regionales). De acuerdo a una norma de respeto, la provisión de la plaza para un nuevo académico se inicia a partir del sexto mes desde el fallecimiento del anterior ocupante de la silla correspondiente.

Los académicos de número actualmente son:

Como dato de interés, el único Premio Nobel de Literatura español que no ingresó como académico en la RAE fue Juan Ramón Jiménez (galardonado en 1956 y fallecido dos años después).

En 1784, tal vez por presiones de la Corte, María Isidra de Guzmán y de la Cerda fue admitida como Académica honoraria.[23]​

En 1853, Gertrudis Gómez de Avellaneda solicitó su ingreso, lo que planteó un largo debate tras el cual se tomó el acuerdo de no aceptar mujeres como académicas de número, resolución que la Academia utilizó hasta principios del siglo XX y que le valdría la consideración de antifeminista.[24]​[25]​ En 1912 la petición de Emilia Pardo Bazán fue rechazada, a pesar de los apoyos de diferentes instituciones, en virtud del acuerdo de 1853.[26]​

La candidatura de Concha Espina fue igualmente rechazada en dos ocasiones (1928 y 1930),[27]​si bien en 1928 la Academia admite la de Blanca de los Ríos, candidatura que llegó a someterse a votación aunque no resultó elegida.[28]​También fue aceptada y sometida a votación la candidatura de María Moliner en 1972, aunque en esta ocasión la votación fue ganada por amplia mayoría por el lingüista Emilio Alarcos Llorach.[29]​

En 1978, casi 300 años después de su fundación, fue aceptada la presencia femenina en la Real Academia, siendo Carmen Conde la primera mujer que ejerció como Académica de número, ocupando la silla K.[30]​[27]​ Al ingreso de esta escritora se han sucedido los de otras mujeres de reconocido prestigio en el mundo de las letras, como Elena Quiroga de Abarca (1983), Ana María Matute (1998), [30]​[27]​María del Carmen Iglesias Cano (2002), Margarita Salas (2003),[30]​[31]​ Soledad Puértolas (2010), Inés Fernández-Ordóñez (2011), Carme Riera (2013), Aurora Egido (2014), Clara Janés (2016)[32]​, Paz Battaner (2017), Paloma Díaz-Mas (electa en 2021) y Dolores Corbella (electa en 2022).

Según sus estatutos, la RAE está compuesta por los siguientes miembros:

Una junta de gobierno rige la Academia y supervisa todos los asuntos relativos a su buena operación, tanto en lo relacionado con su funcionamiento interno como con sus relaciones con los organismos del estado, y las demás Academias. Esta junta la preside el director de la Academia y está constituida por el vicedirector, el secretario, el censor, el bibliotecario, el tesorero, el vicesecretario y dos vocales adjuntos. Todos estos cargos son electivos y, a excepción de los vocales, que se eligen cada dos años, pueden ejercerse durante cuatro años, prorrogables solo una vez.

La Academia funciona en Pleno y en Comisiones que se reúnen semanalmente. Las Comisiones tienen la misión de elaborar las propuestas que posteriormente examinará el Pleno para decidir sobre su aprobación. En la actualidad existen las siguientes comisiones: Delegada del Pleno y para el Diccionario, Instituto de Lexicografía, Diccionario Histórico de la Lengua Española, Publicaciones y Boletín, Armonización de las Obras Académicas, Armonización de Terminología Lingüística, Comisión Conservadora de la Casa Museo Lope de Vega, Comisión para el III Centenario de la RAE, Ciencias Sociales, Vocabulario Científico y Técnico, Ciencias Humanas, Cultura I y Cultura II.

El Pleno, formado por todos los académicos, se reúne durante el curso académico los jueves por la tarde. Una vez aprobada las actas de la sesión anterior y de debatir cualquier tema general, los asistentes presentan enmiendas y adiciones al Diccionario. Acto seguido se examinan las propuestas formuladas por las diversas Comisiones. Las resoluciones, en el caso de que se produzca disparidad de criterio, se adoptan mediante votación.

Al servicio de los trabajos que la Academia desarrolla en Pleno o en Comisiones, funciona el Instituto de Lexicografía, integrado por filólogos y lexicógrafos que realizan las tareas de apoyo para la elaboración de los diccionarios académicos.

El 16 de abril de 2020, celebró el primer pleno virtual de su historia ante la prolongación de las medidas de confinamiento con motivo del estado de alarma provocado por el coronavirus.[33]​

Desde su creación, la RAE ha tenido treinta directores. También hubo algunos casos de directores temporales, como Vicente García de Diego, director accidental (1965-1968), y Rafael Lapesa, director interino (1988).[34]​

El cargo de director de la Real Academia Española conlleva el cargo de presidente de la Asociación de Academias de la Lengua Española (ASALE).

Publicaciones conjuntas de la RAE y la Asociación de Academias de la Lengua Española (integrada por las 24 academias de la lengua española existentes en el mundo).

Todas las obras son publicadas por la RAE y la ASALE.[42]​

Son ediciones especiales de obras de autores españoles de los siglos XIX y XX que conmemoran los 300 años de la institución. La colección se inició con La busca, de Pío Baroja, y Misericordia, de Benito Pérez-Galdós.[43]​

La RAE edita dos revistas en formato digital y acceso abierto:[44]​

El «Premio Real Academia Española» es un galardón literario entregado por la Real Academia Española desde el 2004, con periodicidad anual, alternando entre las categorías de creación literaria e investigación filológica.

La Revolución Industrial o Primera Revolución Industrial es el proceso de transformación económica, social y tecnológica que se inició en la segunda mitad del siglo XVIII en el Reino de Gran Bretaña, que se extendió unas décadas después a gran parte de Europa occidental y América Anglosajona, y que concluyó entre 1820 y 1840. Durante este periodo se vivió el mayor conjunto de transformaciones económicas, tecnológicas y sociales de la historia de la humanidad desde el Neolítico,[1]​ que vio el paso desde una economía rural basada fundamentalmente en la agricultura y el comercio a una economía de carácter urbano, industrializada y mecanizada.[2]​

La Revolución Industrial marca un punto de inflexión en la historia, modificando e influenciando todos los aspectos de la vida cotidiana de una u otra manera. La producción tanto agrícola como de la naciente industria se multiplicó a la vez que disminuía el tiempo de producción. A partir de 1800 la riqueza y la renta per cápita se multiplicó como no lo había hecho nunca en la historia,[3]​ pues hasta entonces el PIB per cápita se había mantenido prácticamente estancado durante siglos.[4]​ En palabras del premio Nobel Robert Lucas:


A partir de este momento se inició una transición que acabaría con siglos de una mano de obra basada en el trabajo manual y el uso de la tracción animal, siendo estos sustituidos por maquinaria para la fabricación industrial y para el transporte de mercancías y pasajeros. Esta transición se inició hacia finales del siglo XVIII en la industria textil, así como en lo relacionado con la extracción y utilización de carbón. La expansión del comercio fue posible gracias al desarrollo de las comunicaciones, con la construcción de vías férreas, canales, y carreteras. El paso de una economía fundamentalmente agrícola a una economía industrial influyó sobremanera en la población, que experimentó un rápido crecimiento sobre todo en el ámbito urbano. La introducción de la máquina de vapor de James Watt (patentada en 1769) en las distintas industrias, fue el paso definitivo en el éxito de esta revolución, pues su uso significó un aumento espectacular de la capacidad de producción. Más tarde, el desarrollo de los barcos y de los ferrocarriles a vapor, así como el desarrollo en la segunda mitad del XIX del motor de combustión interna y la energía eléctrica, supusieron un progreso tecnológico sin precedentes.[6]​[7]​ 

Como consecuencia del desarrollo industrial nacieron nuevos grupos o clases sociales encabezadas por el proletariado —los trabajadores industriales y campesinos pobres— y la burguesía, dueña de los medios de producción y poseedora de la mayor parte de la renta y el capital. Esta nueva división social dio pie al desarrollo de problemas sociales y laborales, protestas populares y nuevas ideologías que propugnaban y demandaban una mejora de las condiciones de vida de las clases más desfavorecidas, por la vía del sindicalismo, el socialismo, el anarquismo, o el comunismo.[8]​

Aún sigue habiendo discusión entre historiadores y economistas sobre las fechas de los grandes cambios provocados por la Revolución Industrial. El comienzo más aceptado de lo que podríamos llamar Primera Revolución Industrial, se podría situar a finales del siglo XVIII, mientras su conclusión se podría situar a mediados del siglo XIX, con un período de transición ubicado entre 1840 y 1870. Por su parte, lo que podríamos llamar Segunda Revolución Industrial, partiría desde mediados del siglo XIX a principios del siglo XX, destacando como fecha más aceptada de finalización a 1914, año del comienzo de la Primera Guerra Mundial. El historiador marxista Eric Hobsbawm, considerado pensador clave de la historia del siglo XX[9]​ sostenía que el comienzo de la revolución industrial debía situarse en la década de 1780, pero que sus efectos no se sentirían claramente hasta 1830 o 1840.[10]​ En cambio, el historiador económico inglés T.S. Ashton declaraba por su parte, que la revolución industrial tuvo sus inicios entre 1760 y 1830.[11]​

El término «revolución industrial» es también materia de discusión. Algunos historiadores del siglo XX, como John Clapham y Nicholas Crafts, argumentan que el proceso de cambio económico y social fue muy gradual, por lo que el término «revolución» resultaría inapropiado. Asimismo, es cuestionado el mote de «industrial», ya que el proceso englobó también cambios agrarios, sociales, energéticos, y demográficos.[12]​ Estas cuestiones siguen siendo tema de debate entre historiadores y economistas.[13]​[14]​

Los inicios de la industrialización europea hay que buscarlos en la Edad Moderna. A partir del siglo XVI se vislumbra un avance en el comercio, métodos financieros, banca y un cierto progreso técnico en la navegación, impresión o relojería. Sin embargo, estos avances siempre se veían lastrados por epidemias, constantes y largas guerras y hambrunas que no permitían la dispersión de los nuevos conocimientos ni un gran crecimiento demográfico. Según el historiador Angus Maddison, Europa Occidental experimentó un crecimiento demográfico prácticamente nulo entre 1500 y 1800. 

El Renacimiento marcó otro punto de inflexión con la aparición de las primeras sociedades capitalistas en Holanda y el norte de Italia. Es a partir de mediados del siglo XVIII cuando Europa comenzó a distanciarse del resto del mundo y a asentar las bases de la futura sociedad industrial debido al desarrollo, aún primitivo, de la industria pesada y la minería.[15]​[16]​ La alianza de los comerciantes con los agricultores hizo aumentar la productividad, lo que a su vez provocó una explosión demográfica, acentuada a partir del XIX. La Revolución Industrial se caracterizó por la transición de una economía agrícola y manual a una comercial e industrial[17]​ cuya ideología se basaba en el racionalismo la razón y la innovación científica.[18]​

Otro de los principales desencadenantes de la Revolución nace de la necesidad.[19]​ Aunque en algunos lugares de Europa como Gran Bretaña ya existía una base industrial, las Guerras Napoleónicas consolidaron la industria europea. Debido a la guerra, que se extendía por la mayor parte de Europa, las importaciones de muchos productos y materias primas se suspendieron. Esto obligó a los gobiernos a presionar a sus industrias y a la nación en general para producir más y mejor que antes, desarrollándose industrias antes inexistentes.
La industrialización tuvo lugar en diferentes oleadas en los distintos países. Las primeras áreas industriales aparecieron en Gran Bretaña a finales del siglo XVIII, extendiéndose a Bélgica y Francia a principios del siglo XIX y a Alemania y a Estados Unidos a mediados de siglo, a Japón a partir de 1868 y a Rusia, Italia y España a finales de siglo. Entre las razones se encontraron algunas tan dispares como la notable ausencia de grandes guerras entre 1815 y 1914, la aceptación de la economía de mercado y el consecuente nacimiento del capitalismo, la ruptura con el pasado, un cierto equilibrio monetario y la ausencia de inflación.

Otras interpretaciones sugieren que este nuevo cambio de mentalidad y la posterior evolución del sistema económico fue por causas morales y religiosas. La Reforma protestante de Martín Lutero y Juan Calvino trajo consigo un cambio de mentalidad en el trato y visión respecto del trabajo. Según Max Weber el protestantismo considera al trabajo y al esfuerzo como un bien y un valor fundamental, al contrario que la ética católica que lo considera un castigo a raíz del pecado original.[20]​ Esto explicaría en parte las diferencias a la hora de desarrollarse de las distintas naciones europeas, teniendo como pioneros a países protestantes como Gran Bretaña, Alemania u Países Bajos y como países atrasados a España, Portugal e Italia, todos ellos católicos.[21]​ Esta interpretación sigue siendo muy discutida.

La Revolución Industrial se originó en Inglaterra a causa de diversos factores, cuya elucidación es uno de los temas historiográficos más trascendentes.

Como factores técnicos, era uno de los países con mayor disponibilidad de las materias primas esenciales, sobre todo el carbón mineral, indispensable para alimentar la máquina de vapor que fue el gran motor de la Revolución Industrial temprana, así como para utilizar coque en los altos hornos de la siderurgia,[22]​ sector principal desde mediados del siglo XIX. Su ventaja frente a la madera, el combustible tradicional, no es tanto su poder calorífico como la mera posibilidad en la continuidad de suministro (la madera, a pesar de ser fuente renovable, está limitada por la deforestación; mientras que el carbón, combustible fósil y por tanto no renovable, solo lo está por el agotamiento de las reservas, cuya extensión se amplía con el precio y las posibilidades técnicas de extracción). La fundición con coque permiite una mayor oferta de hierro, mejoras en la calidad y reducción de costos de este material.[22]​[23]​[24]​[nota 1]​

Como factores ideológicos, políticos y sociales, la sociedad inglesa poseía una tendencia hacia un menor absolutismo e instituciones más inclusivas. Comenzando por la Carta Magna de 1215, que limitaba el poder del rey,[25]​[26]​[27]​ había atravesado la llamada crisis del siglo XVII de una manera particular: mientras la Europa meridional y oriental se refeudalizaba y establecía monarquías absolutas, la guerra civil inglesa (1642-1651) y la posterior revolución gloriosa (1688) determinaron el establecimiento de una monarquía parlamentaria basada en la división de poderes, la libertad individual y un nivel de seguridad jurídica (con el Common Law)[28]​[29]​ que proporcionaba suficientes garantías para el empresario privado; muchos de ellos surgidos de entre activas minorías de disidentes religiosos que en otras naciones no se hubieran consentido (autores como Max Weber vinculan explícitamente La ética protestante y el espíritu del capitalismo). Síntoma importante fue el espectacular desarrollo del sistema de patentes industriales.[30]​

Algunos autores también mencionan los enclosures (cercamientos) como un factor que contribuyó a la industrialización, como una incipiente «privatización» de los recursos.[25]​[31]​ También, la creciente liberalización, y la reducción de las restricciones impuestas por los gremios a la instalación de industrias y el cambio tecnológico.[32]​[33]​[34]​

Como factor geoestratégico, durante el siglo XVIII Inglaterra (que tras las firmas del Acta de Unión con Escocia en 1707 y del Acta de Unión con Irlanda en 1800, después de la derrota de la rebelión irlandesa de 1798, consiguieron la unión con Escocia e Irlanda, formando el Reino Unido de Gran Bretaña e Irlanda) construyó una flota naval que la convirtió (desde el tratado de Utrecht, 1714, y de forma indiscutible desde la batalla de Trafalgar, 1805) en una verdadera talasocracia dueña de los mares y de un extensísimo imperio colonial. A pesar de la pérdida de las Trece Colonias, emancipadas en la guerra de Independencia de Estados Unidos (1776-1781), controlaba, entre otros, los territorios del subcontinente Indio, fuente importante de materias primas para su industria, destacadamente el algodón que alimentaba la industria textil, así como mercado cautivo para los productos de la metrópolis. La canción patriótica Rule Britannia (1740) explícitamente indicaba: rule the waves (gobierna las olas).

Durante la revolución industrial se vivió un incremento espectacular de la población, debido fundamentalmente a la caída de la tasa de mortalidad provocada por la mejora de las condiciones higiénicas, sanitarias y alimenticias que se plasmó en gran medida en la reducción de la mortandad infantil. En este periodo nacen las primeras vacunaciones y se mejoran los sistemas de alcantarillado y de depuración de aguas residuales. Una alimentación más abundante y regular, no sometida a las fluctuaciones de las cosechas, bajó la incidencia de las epidemias e hizo posible la casi desaparición de la mortalidad catastrófica, sobre todo la infantil.

La población de Inglaterra y Gales, que había permanecido constante alrededor de 6 millones desde 1700 a 1740, se incrementó bruscamente a partir de esta fecha y alcanzó 8,3 millones en 1801, para doblarse en cincuenta años y llegar a los 16,8 millones en 1850 y en 1901 casi se había doblado de nuevo con 30,5 millones.[35]​ En Europa, la población pasó de 100 millones en 1700 hasta alcanzar 400 millones en 1900.[36]​ La revolución industrial fue así el primer periodo histórico durante el que hubo simultáneamente un incremento de la población y un incremento de la renta per cápita.[37]​ El aumento de la población fue un estímulo para el crecimiento industrial, ya que proporcionó a la vez mano de obra abundante para las nuevas industrias y de otro lado supuso un incremento de la demanda interna para los nuevos productos.

El aumento de la población urbana en ciudades con trazado medieval supuso el hacinamiento, la insalubridad y la aparición de las primeras patologías sociales (alcoholismo, prostitución y delincuencia).[38]​

Entre finales del siglo XVII y principios del XVIII el gobierno británico aprobó una serie de leyes con el fin de proteger a la industria de la lana británica de la creciente cantidad de tela de algodón que se importaba desde India Oriental.

También empezó a darse una mayor demanda de tejidos gruesos, los cuales eran fabricados por la industria británica en la localidad de Lancashire, donde destacaba la producción de pana, fabricada a partir de fibras entrecruzadas de lino y algodón. El lino era utilizado para dotar de más resistencia al tejido, cuyo material principal, el algodón, no tenía una resistencia suficiente, aunque esta mezcla resultante no era tan suave como los tejidos 100% algodón y era más difícil de coser.[39]​

Hasta el nacimiento de la industria textil, los tejidos y el hilado en general se realizaba en los hogares, en la mayor parte de los casos para consumo propio. Este método productivo, basado en que la producción estaba dispersa y se desarrollaba en los domicilios de los trabajadores, es a menudo denominado en inglés como sistema Putting-out (Putting-out system) en contraposición al posterior sistema industrial o factory system.[40]​ Solo en ocasiones puntuales los trabajos se realizaban en el taller de un maestro tejedor. Bajo el sistema putting-out los trabajadores, antes de fabricar su producto, pactaban contratos con comerciantes y vendedores, quienes les suministraban a menudo las materias primas necesarias. Fuera de temporada, por la general, las esposas de los agricultores hacían los hilados mientras que los hombres producían los tejidos. Utilizando la máquina de hilar o rueca, en cualquier momento entre cuatro y ocho hilanderas podían echar una mano al tejedor.[39]​[41]​[42]​

Uno de los grandes inventos de la industria textil fue la lanzadera volante, patentada en 1733 por John Kay,[43]​ que permitió una cierta automatización del proceso de tejido. Posteriores mejoras, destacando las de 1747, permitieron duplicar la capacidad de producción de los tejedores, lo que también agravó el desequilibrio que existía entre el hilado y el tejido. Este invento empezó a ser ampliamente utilizado en todo Lancashire en la década de 1760, cuando Robert Kay, hijo de John Kay, inventó la caja ascendente (drop box).[44]​

Lewis Paul patentó en Birmingham, con la ayuda de John Wyatt, la máquina de hilar mediante rodillos y el sistema flyer-and-bobbin, que conseguían un espesor más uniforme en el proceso de elaboración de la lana. Paul y Wyatt abrieron una fábrica en Birmingham que utilizaba una nueva máquina de laminado impulsada por un burro. En 1743 se abrió una fábrica en Northampton que empleaba cinco máquinas como la de Paul con cincuenta husos cada una. Estuvo en funcionamiento hasta 1764. Una fábrica similar fue construida por Daniel Bourn en Leominster, pero un incendio la destruyó. Tanto Paul como Bourn habían patentado el cardador de lana en 1748. El uso de dos conjuntos de rodillos que giraban a diferentes velocidades fue utilizado posteriormente en la primera fábrica de hilados de algodón. La invención de Lewis fue posteriormente mejorada por Richard Arkwright con su Water frame (1769) y por Samuel Crompton con su Spinning mule (1779).[43]​

En 1764 en el pueblo de Stanhill, Lancashire, James Hargreaves inventó la hiladora Jenny, que patentó en 1770. Fue la primera máquina que empleaba varios husos de una manera eficaz. La hiladora Jenny trabajaba de una manera similar a la rueca. Era una máquina simple, construida con madera y que solo costaba alrededor de 6 libras (un modelo de 40 husos) en 1792. Era utilizada principalmente en los hogares o por pequeños artesanos. La hiladora Jenny producía un hilo ligeramente torcido solo adecuado para la trama, que se torcía.[46]​

La máquina de hilar (Water frame) inventada por Richard Arkwright, fue patentada por este junto con dos socios en 1769. El diseño se basaba en parte en una máquina de hilado construida por Thomas High, quien fue contratado por Arkwright.[47]​

En 1786, Edmund Cartwright inventó el primer telar mecánico.[48]​ En 1793, Eli Whitney inventó la desmotadora de algodón, lo que le permitió a Gran Bretaña importar grandes cantidades de algodón para su industria textil, a bajo costo, desde el sur de Estados Unidos.[49]​

Sin embargo, y a pesar de todos los factores anteriores, la Revolución industrial no hubiese podido prosperar sin el concurso y el desarrollo de los transportes, que llevarán las mercancías producidas en la fábrica hasta los mercados donde se consumían.

Estos nuevos transportes se hacen necesarios no solo en el comercio interior, sino también en el comercio internacional, ya que en esta época se crean los grandes mercados nacionales e internacionales. El comercio internacional se liberaliza, sobre todo tras el Tratado de Utrecht (1713) que liberaliza las relaciones comerciales de Inglaterra, y otros países europeos, con la América española. Se termina con las compañías privilegiadas y con el proteccionismo económico; y se aboga por una política imperialista y la eliminación de los privilegios gremiales. Además, se desamortizan las tierras eclesiásticas, señoriales y comunales, para poner en el mercado nuevas tierras y crear un nuevo concepto de propiedad. La Revolución industrial generó también un ensanchamiento de los mercados extranjeros y una nueva división internacional del trabajo (DIT). Los nuevos mercados se conquistaron mediante el abaratamiento de los productos hechos con la máquina, por los nuevos sistemas de transporte y la apertura de vías de comunicación, así como también, mediante una política expansionista.

El Reino Unido fue el primero que llevó a cabo toda una serie de transformaciones que la colocaron a la cabeza de todos los países del mundo. Los cambios en la agricultura, en la población, en los transportes, en la tecnología y en las industrias, favorecieron un desarrollo industrial. La industria textil algodonera fue el sector líder de la industrialización y la base de la acumulación de capital que abrirá paso, en una segunda fase, a la siderurgia y al ferrocarril.

A mediados del siglo XVIII, la industria británica tenía sólidas bases y con una doble expansión: las industrias de bienes de producción y de bienes de consumo. Incluso se estimuló el crecimiento de la minería del carbón y de la siderurgia con la construcción del ferrocarril. Así, en Gran Bretaña se desarrolló de pleno el capitalismo industrial, lo que explica su supremacía industrial hasta 1870 aproximadamente, como también financiera y comercial desde mediados de siglo XVIII hasta la Primera Guerra Mundial (1914). En el resto de Europa y en otras regiones como América del Norte o Japón, la industrialización fue muy posterior y siguió pautas diferentes a la británica.

Unos países tuvieron la industrialización entre 1850 y 1914: Francia, Alemania y Bélgica. En 1850 apenas existe la fábrica moderna en Europa continental, solo en Bélgica hay un proceso de revolución seguido al del Reino Unido. En la segunda mitad del siglo XIX se fortalece en Turingia y Sajonia la industrialización de Alemania.

Otros países siguieron un modelo de industrialización diferente y muy tardía: Italia, Imperio austrohúngaro, España o Rusia. La industrialización de éstos se inició tímidamente en las últimas décadas del siglo XIX, para terminar mucho después de 1914.

El ferrocarril, nacido en el siglo XVIII, es uno de los grandes protagonistas de la Revolución Industrial.
En sus comienzos se empleaba la fuerza animal como medio de locomoción, los raíles eran de madera y su empleo se limitaba a las minas para el transporte de carbón.[nota 2]​ En un libro publicado en 1797, Carz aseguraba haber sido el primero que pensó en sustituir la madera por hierro.[50]​ La primera concesión del Parlamento de Inglaterra para la construcción de un ferrocarril —movido por caballos— se remonta a 1801; se trataba de una línea entre Wandsworth y Croydon con unos 13 kilómetros de longitud y con un coste de 60 000 libras. La gran revolución del ferrocarril comenzó en 1814, cuando George Stephenson utilizó la máquina de vapor como medio de locomoción. Su invento fue un éxito y comenzó a usarse de inmediato en las minas, pudiendo transportar ocho vagones de 30 toneladas a una velocidad de 7 km/h. Estos resultados eran suficientes para expandir el uso de la máquina a otros servicios. Fue un 1821 cuando el Parlamento autorizó la construcción de la primera línea de ferrocarril con tracción de vapor entre Stockton y Darlington. La línea fue inaugurada en 1825 con una máquina maniobrada por el propio Stephenson tirando de 34 vagones a una velocidad de entre 10 y 12 millas por hora —16-19 km/h— ;[51]​ El periódico The Times describió esta hazaña de la siguiente manera:


En los 5 años posteriores el Parlamento autorizó la construcción de 23 nuevas líneas de ferrocarril entre las que se encontraba la célebre línea entre Mánchester y Liverpool, siendo sus constructores los primeros en ofrecer en el ferrocarril el servicio de transporte de pasajeros. En aquel momento se desconfiaba de la seguridad que podían ofrecer las locomotoras, pero la acogida fue muy buena, mejorando en un 10% los beneficios derivados de este servicio, aunque los ingresos por el transporte de algodón, tejidos, carbón y ganado aún seguían siendo mayoritarios. Este éxito también fue tratado por George Porter, quien en su libro El progreso de la nación dice :


Fue en esta ocasión el propio Stephenson el que ganó la puja en esta línea convirtiéndose su Cohete en el encargado de remolcar un tren de 12 toneladas a 22 km/h.[53]​ El primer correo por ferrocarril se envió el 11 de noviembre de 1830.[52]​ Los tiempos de llegada se redujeron considerablemente, llegando el correo entre Londres y Mánchester en aproximadamente 18 horas. En Inglaterra, siguiendo la consigna laissez faire, el Estado no intervenía en la construcción o subvención del ferrocarril sino que se limitaba a otorgar las licencias y permisos de construcción y explotación;[53]​ de esta manera se gastaron enormes fortunas con el objetivo de obtener los distintos permisos; por ejemplo el Great Western costó en gastos preliminares 89 000 libras y otros como el London and Birmingham 62 000.[54]​

Los ferrocarriles eran al principio de vía estrecha y solo admitían velocidades comprendidas entre los 15 y los 20 kilómetros por hora, pero en 1840 se habían ensanchado las vías y se podían conseguir unas velocidades de casi 40 km/h.

El primer país continental en seguir el ejemplo inglés fue Bélgica con dos líneas Bruselas-Malinas y Malinas-Amberes en 1835. El primer año transportaron 70 000 pasajeros. El coste fue bajísimo y el billete Bruselas-Amberes costaba solo un franco.[55]​ El invento entró en Francia con algo de retraso, pues mientras jóvenes, ingenieros y adeptos al saintsimonismo reclamaban su construcción, tropezaban con el rechazo y la desconfianza de muchos, además de la carencia de hierro. El gobierno francés, que veía el potencial del aparato, ordenó un estudio para un plan nacional de los ferrocarriles. El estudio quedó finalizado en 1837 y los capitalistas, impacientes, presionaban al gobierno para la ejecución del proyecto con el fin de especular con las obras y los terrenos. El plan consistía en siete líneas con centro en París, que unirían el Atlántico, el Mediterráneo y el Rin. Al contrario que en Inglaterra y Bélgica, el estado se hizo cargo, al menos en parte, de su construcción y explotación, aportando 150 000 francos por kilómetro de vía y construyendo las infraestructuras necesarias.[55]​ Mientras, las compañías privadas aportaron 100 000 francos para edificios y material.[56]​ Tras 40 años de administración y explotación privada, el sistema pasaría al Estado. Socialistas románticos y conservadores se oponían al proyecto, los primeros reclamaban que el sistema fuera del estado desde el primer día y los segundos lo consideraban demasiado caro.[56]​ Finalmente el plan fue aprobado, pero algunos acuerdos se revisaron y en la práctica la construcción y explotación corrió a cuenta casi exclusiva del sector privado.[56]​ En 1857 la red estaba consolidada siendo propiedad de 6 grandes compañías. Debido a la obligación de ceder la propiedad al Estado a los 40 años de explotación se descuidó sobremanera su cuidado y mantenimiento por lo que el gobierno francés se vio en la obligación de ampliar el plazo en 99 años más, comprometiéndose incluso a pagar las obligaciones a su vencimiento.[56]​

En Alemania la primera línea se construyó en 1835 con una extensión de siete kilómetros entre Núremberg y Fürth, pero fue en 1839 cuando se construyó la primera línea de importancia entre Dresde y Leipzig, promovida por el profesor de economía política List, uno de los principales promotores de la línea Núremberg-Fürth. Pronto se vio al ferrocarril como una poderosa arma política; en el momento de la aparición del ferrocarril, Alemania se encontraba dividida en más de 300 pequeños estados y ciudades autónomas. Desde la construcción de la línea Dresde-Leipzig todas las ciudades alemanas quisieron unirse con su vecina lo que además de un gran impulso económico hizo un gran servicio para el triunfo del Zollverein.[57]​ Al contrario que en el resto de países, en Alemania fue la administración la encargada de vigilar o administrar todos los ferrocarriles.[58]​ En 1850 el Zollverein ya poseía 5800 kilómetros casi el doble que toda Francia. Hannover, Bremen, Hamburgo, Berlín, Fráncfort formaban una gran línea que transcurría sobre los principales focos industriales y unía Alemania con Suiza a través de Basilea y a Austria a través de Moravia y Silesia.

A partir de la década de 1820 el ferrocarril y el vapor saltaron a los Estados Unidos y pronto conquistaron a la opinión pública. Stevens realizó en Hoboken una primera prueba que causó un gran interés entre los hombre de negocios de Pensilvania, quienes compraron una locomotora a Inglaterra.[59]​ Al igual que en Gran Bretaña, la acumulación de capital hizo posible solo un año después el comienzo de la construcción de una primera línea entre Washington y Winchester. En 1830 una locomotora llamada Best Friend explotó cuando marchaba por la línea Charleston-Hambourg debido a que el maquinista se había sentado sobre la válvula de escape por las molestias que sentía debido al silbido del vapor al salir. Pero lejos de echarse atrás, el país progresó a un ritmo frenético y a mediados de 1830 ya producía sus propias locomotoras en la fundición de West Point[60]​ asegurando una industria nacional sólida. Desde entonces Estados Unidos colocó raíles a través de su vasto territorio a una velocidad mucho mayor que Europa. Si en 1830 poseía tan solo 65 kilómetros de trazado —contra 316 europeos, 276 de ellos en Gran Bretaña—, 10 años después ya superaba a Europa con 4509 kilómetros contra 3543 europeos.[59]​ En 1850 las vías férreas ya sumaban 14 400 kilómetros. Uno de los problemas que planteaban los ferrocarriles era el ancho de vía,[nota 3]​ que variaba en anchura en los distintos países, lo que obligaba a numerosos transbordos para deleite de los hosteleros. Pero problemas aparte el tiempo de viaje no hizo sino disminuir; así, en apenas unos años no se tardaban más de 20 horas en viajar de Boston a Nueva York en ferrocarril cuando antes se tardaban unas 80.[59]​

En Italia los augurios de d´Azeglio de que los ferrocarriles coserían la bota no pasaron de simples promesas, pues hasta 1845 solo se encontraban pequeñas líneas aisladas como la línea Milán-Monza, Padua-Venecia, Liorna-Pisa o la línea de Campania que Fernando de Nápoles construyó para su recreo y uso privado.[61]​ En Hungría solo existía una pequeña vía alrededor de Budapest y en Rusia el zarismo tuvo que imponer la construcción de la línea Moscú-San Petersburgo debido a los numerosos detractores.[61]​ En España, el gran tirón y entusiasmo que de manera muy temprana había producido el invento se apaga en la guerra civil de 1833, que paraliza todas las obras de construcción ante la desconfianza de los capitalistas.[61]​ Hubo que esperar hasta 1843 cuando se concedió a Juan Manuel Roca y Miguel Biada la construcción y explotación del ferrocarril Barcelona-Mataró, que estuvo construido en solo cinco años bajo la dirección del ingeniero inglés Locke, su inauguración fue el 28 de octubre de 1848, un trayecto de 28 km y 600 m que se completaba en 35 minutos.[nota 4]​ En 1851 realizó su primer viaje el segundo ferrocarril español que cubría la línea Madrid-Aranjuez, cuya concesión había sido otorgada en 1844 con prolongación hasta Cádiz. En 1850 se inició la construcción de la primera locomotora española, finalizada en 1852.[61]​

Excepciones aparte, en el periodo entre 1820 y 1840, Gran Bretaña conservaba un adelanto manifiesto sobre el resto del mundo.[61]​ Era la única que poseía una buena red de transporte entre sus principales ciudades. Trabajó con verdadero frenesí entre 1840 y 1847 a pesar de la rivalidad latente entre la oposición, los grupos financieros, los Turnpike trusts y la población, cuyo medio de subsistencia continuaban siendo las carreteras. Similar situación se dio en Bélgica, que en 1843 tenía incluso más kilómetros que Francia y una opinión pública muy favorable al ferrocarril.[61]​
No fueron pocos los que vieron en el ferrocarril un gran peligro, incluso mortal. Desde el siglo XVIII, cuando se pusieron en marcha en Inglaterra hubo voces, incluso procedentes de la Real Academia de Ciencias británica, que sugerían que a unas velocidades superiores a los 40 km/h los pasajeros se asfixiarían, se volverían ciegos y el ganado enloquecería. Se temía también la destrucción de las tierras de cultivo o que la gente y mercancías salieran despedidas del aparato por sus "endiabladas" velocidades.[62]​

Pasada la primera mitad de siglo, el medio siglo siguiente entre 1851 y 1901, conocido con el nombre de Railway Age vive el apogeo y reinado definitivo del ferrocarril. Pero la tracción mecánica sobre raíles es sobre todo, obra de Occidente. En 1860 Europa y EE. UU. se reparten más o menos 198 000 en igualdad mientras que el resto del mundo no cuenta con más de 15 000 kilómetros, la mayoría ubicados en colonias europeas.[63]​ En 1910 ya se han construido más de un millón de kilómetros de los que 380 000 están en EE. UU. y 330 000 en Europa.[63]​ Su construcción necesitó de un esfuerzo enorme, movilizando grandes cantidades de capital, trabajadores y estimulando la industria metalúrgica y la construcción de gigantescos talleres de trabajo, además de dar su máximo esplendor a la máquina de vapor.[64]​
Además de los vagones y locomotoras, también evolucionaron los raíles sobre los que circulaban. El raíl de acero sustituye al de hierro y a la madera de las traviesas se le empezó a inyectar cloruro de cinc para evitar que se pudriera. El ferrocarril también necesitó de una gran infraestructura que fue necesario desarrollar, como túneles, que se excavaban a costa del sufrimiento obrero a altísimas temperaturas con el uso de perforadoras de aire comprimido y el revestimiento de las galerías con fundición, en sustitución de la madera; La ventilación se lograba con sopladoras. Hay que destacar algunos éxitos entre los que se encuentran el túnel que atraviesa el Mont Cenis, construido a lo largo de 15 años y con una extensión de 13 600 m a 1300 metros de altura.[65]​ Otros como el San Gotardo de más de 15 000 metros se terminaron en menos de 10 años usando la perforadora automática siendo las condiciones de trabajo nefastas: los obreros llegaron a trabajar a una temperatura de 86 grados.[65]​ Fuera de Europa los estadounidenses construyeron un túnel bajo el río Hudson. Escandinavia queda unida a Alemania a través del ferry-boats entre Rügen y Malmoe.
Mientras que en la primera mitad de siglo la locomotora apenas había ganado en velocidad sin sobrepasar nunca los 40 km/h, hace progresos decisivos a partir de la idea del ingeniero inglés Crampton de colocar las ruedas motrices detrás de la caldera (y no debajo), ruedas que están acopladas, transfiriéndose el movimiento de rotación. En 1850 la velocidad media que se situaba en 27 km/h se eleva en 1880 a 74 km/h en Inglaterra y a 59 km/h en Estados Unidos.[66]​ En 1890 el Empire-State-Express rebasó por primera vez en la historia los 100 km/h entre Nueva York y Búfalo.[66]​ Para cruzar Francia de un extremo en ferrocarril solo se precisaban 14 horas. En esta segunda parte del siglo el coste del billete disminuyó entre un 50 y un 70 %.[67]​

Las prestaciones de la locomotora aumentaron sin cesar. El freno de mano se sustituyó por un nuevo freno hidráulico de aire comprimido.[66]​ Los vagones de pasajeros fueron dotados de alumbrado de gas a base de aceite de esquisto o iluminación eléctrica a finales de siglo, siendo la línea Londres-Brighton la primera en incorporarla.[66]​ La máquina de vapor, el corazón de la máquina, también procura calefacción en los vagones. El llamado Boggie o bastidor de varios ejes permitió al convoy dar curvas mucho más acentuadas disminuyendo los riesgos, pues se adaptaba a la curvatura de la vía.[66]​ También se crearon los llamados palace-cars en las líneas más largas para las familias ricas en las que disfrutaban de todo tipo de comodidades y sin tener que mezclarse con el resto de pasajeros.[66]​ En 1880 se instaló en la línea del Pacífico un vagón imprenta en el que se editaba un periódico diario con las noticias recibidas telegráficamente en las estaciones.[66]​

Exceptuando Gran Bretaña, Bélgica y algunas partes de España y Alemania, las vías férreas no dibujaban redes en ninguna parte antes de 1860.[68]​ En Francia por fin se realizó un esfuerzo serio a partir del Segundo Imperio y en los albores de la Tercera República. En esta segunda mitad de siglo se empezaba a vislumbrar la columna vertebral de ferrocarriles europeos.[68]​ Sus límites se extendían desde el norte de Francia hasta la Alta Silesia de este a oeste y de Alemania al norte de Italia de norte a sur; en el centro, Suiza reparte el tráfico por el continente. En cambio la mayor parte de Italia, la península ibérica y los países del este quedaban fuera.[68]​ En Estados Unidos se siguen consiguiendo grandes logros. En 1869 se finalizó el primer transcontinental que conectó el país de este a oeste. La construcción fue dirigida por el implacable general Grenville M. Dodge como si se tratará de una campaña militar. Usó como mano de obra a los soldados desmovilizados, inmigrantes irlandeses y hasta chinos en California.[68]​ Pero este triunfo no se logró con facilidad; indios, el relieve irregular y sobre todo la competencia entre Union Pacific y Central Pacific dificultaron sobremanera la situación. Pero el entusiasmo predomina y en 1893 ya había en funcionamiento otras 5 líneas transcontinentales, usándose como medio de colonización en el oeste americano o en la Columbia británica como medio de presión para conseguir su adhesión a la Unión.[68]​

Aunque tardío, se presenta el esfuerzo ruso, logrado gracias a los préstamos de Occidente.[68]​ En primer lugar se construyó el transcaspiano al que a partir de 1905 complementó el transaraliano. En Siberia las dificultades eran mayúsculas: hielo, infiltraciones de agua, ríos inmensos, débil densidad humana, distancias enormes, sin olvidar el irregular relieve. Pero las viejas rutas y caminos ya no eran suficientes y el ferrocarril más largo del mundo se empezó en 1891 y alcanzó su destino, Vladivostok, gracias a un acuerdo con China, en 1902.[68]​

Así pues, el ferrocarril no solo sirvió para revolucionar el mundo del transporte tanto material como humano sino que fue empleado como un excelente instrumento de unión.[69]​ Sirvió bien en la reconciliación y la anexión de nuevos territorios a Estados Unidos y el Imperio alemán sabía lo mucho que le debía al ferrocarril como para dejarlo en manos privadas. En Italia facilitó la hegemonía de la Casa de Saboya. No ocurrió igual en Francia o en Gran Bretaña, donde se encontraban mayoritariamente en manos privadas, aunque en Inglaterra prestaron un servicio inigualable, encumbrando al naciente Imperio británico a la hegemonía mundial. Hacia 1850 el ferrocarril había conducido a entre 400 y 500 millones de viajeros y entre 200 y 300 millones de toneladas de mercancías desde su nacimiento. Cinco décadas después, solo en 1905 transportó a entre 4000 y 5000 millones de viajeros.[69]​

Antes del siglo XIX la larga tradición naval europea se había sustentado sobre el control de los vientos como medio de propulsión y la seguridad más que por la velocidad en el mar. A principios de siglo no se empleaban menos de dos o tres semanas en cruzar el Atlántico de este a oeste, necesitándose entre 30 y 40 días de oeste a este. Con la formación de los imperios coloniales europeos se hizo necesario desarrollar una tecnología que asegurase el viaje sobre las aguas; en el siglo XVIII se generalizó el uso del sextante, mapas con las notaciones de los vientos y el cronómetro. La invención de la nueva embarcación partió de los trabajos de Jouffroy d´Abbens sobre el Sena y los de Fulton con su máquina Clermont.[70]​ Fue en Estados Unidos donde tuvieron lugar las primeras pruebas del navío de ruedas sobre el río Hudson. En 1815 ya circulaban un centenar de estos navíos de ruedas que obtenían su energía de la leña, material barato y abundante. El Savannah consiguió cruzar en 29 días el Atlántico Norte en 1819 y la Sphink, que llevó a Francia las noticias de la toma de Argel, desarrollaba una velocidad de 6 nudos. Pero los problemas eran numerosos: las paletas utilizadas provocaban un gran desperdicio de energía, existía el riesgo de incendio o explosión a bordo, su velocidad era aún menor a la desarrollado por los veleros y el poder militar aún se oponía a su utilización como navío de guerra.[71]​

Pero a pesar de las dificultades los avances prosiguieron y en 1838, con una combinación de vapor y velas, los navíos Sirius y Great Western cruzaron el Atlántico entre Liverpool y Nueva York en 16 y 13 días respectivamente. Los grandes avances llegaron entre 1840 y 1860 con la invención de la hélice, basándose los primeros modelos en el tornillo de Arquímedes, el condensador de superficie y la máquina Compound, que logró ahorrar grandes cantidades de combustible y la introducción de calderas cilíndricas que posibilitaron la producción de vapor a alta presión.[70]​

Lo que sí es indudable es la supremacía del velero sobre el vapor durante la mayor parte del siglo; la seguridad y prestigio de la que aún gozaba, sobre todo en Estados Unidos, donde también tenía lugar la mayoría de los avances del barco de vapor era indiscutible. En 1850 el barco de vapor había transportado ya 750 000 toneladas, aunque el vapor aún estaba muy lejos de ganar la partida.[71]​

El esfuerzo en la construcción y mejora de carreteras (o caminos) comenzó en muchas partes de Europa antes de la Revolución Industrial. Desde el fin de las guerras napoleónicas a principios del siglo XVIII y en ausencia de otros medios de comunicación más eficaces, las carreteras fueron extensamente mejoradas. A principios del siglo XIX el país más adelantado en esta materia era Francia con una red de 33 000 kilómetros de gran calidad que se extendían hasta Alemania, Suiza e Italia. Los Países Bajos, el Reino de Prusia o Suiza también habían vivido una gran mejora en las comunicaciones. En el otro extremo se encontraban lugares como Sicilia, que no empezó su construcción hasta bien entrado el XIX, la Rusia zarista, que no tendría su primera calzada entre Moscú y San Petersburgo —sus principales ciudades— hasta 1834 o España, que cuenta antes de la mitad del siglo XIX con solo 6000 kilómetros de vías, siendo además estrechas y llenas de irregularidades y deficiencias. En Gran Bretaña el rápido desarrollo de ferrocarriles y canales quita importancia a su construcción, pero aun así se suceden las ampliaciones y modernizaciones de la maltrecha red británica contando en 1850 con más de 50 000 kilómetros de trazado, 18 000 más que veinte años atrás.[72]​

La técnica en la construcción de estas vías de comunicación también mejora. En cada país se construyen de manera distinta, pero los problemas clásicos derivados de estas construcciones como filtraciones de agua, mantenimiento o infraestructura se solucionan en las décadas de 1820 y 1830 a partir de las mejoras introducidas por Mac Adam o Telford.[73]​ El uso de la diligencia y los servicios públicos de transporte se desarrollan y generalizan con unas velocidades que oscilan entre los 10 y 15 km/h, usándose en el transporte de pasajeros, mercancías y correo.[74]​ No es hasta principios del siglo XX cuando gracias al motor de explosión y el desarrollo del automóvil se de un uso masivo a estos trazados.

Los primeros canales empezaron a ser construidos en Gran Bretaña en el siglo XVIII con el objeto de comunicar los centros industriales del norte británico con los puertos marítimos del sur y Londres. Los canales fueron la primera tecnología que permitió un fácil y relativamente rápido transporte de mercancías por todo el país, pudiéndose transportar varias docenas de veces más de tonelaje por viaje que con un transporte terrestre. A esto se unía el relieve del país, completamente llano, lo que permitía que los canales fueran construidos rápidamente y a un bajo precio. A principios de la década de 1820, ya existía una red nacional consolidada. El ejemplo inglés fue copiado en Francia que con un relieve similar al británico pudo desarrollar su propio sistema, que a mediados del siglo XIX contaba con 8500 kilómetros de vías. En Alemania gracias a sus grandes ríos como el Rín y el Elba, la navegación se vio muy favorecida, así como el comercio que vivió un gran desarrollo. En otros países como España la construcción de canales no pasó de un proyecto por el difícil relieve y la falta de capitales. Fuera del continente, los estadounidenses con su ímpetu emprendedor y sus numerosos lagos y grandes ríos consiguieron desarrollar con velocidad su propio sistema, que al igual que el ferrocarril, ayudó en la colonización y explotación de las vastas tierras del país. A principios de 1835 EE. UU. ya contaba con 7000 kilómetros de canales que allanaron el camino a la introducción del barco de vapor en el país con una rapidez incluso mayor a la siempre innovadora Gran Bretaña.[75]​

El uso de los canales en Gran Bretaña empezó a decaer a partir de 1840, cuando el ferrocarril se impuso en el transporte de mercancías y pasajeros.[76]​ El irregular y más tardío desarrollo a gran escala del ferrocarril en el resto de países, con la siempre notable excepción de los Estados Unidos, alargó en ocasiones el uso pleno de los canales hasta los albores del siglo XX. Hoy en día la red de canales británicos y la infraestructura ligada a esta es una de las características más perdurables y destacables de la Revolución Industrial en el país.

La existencia de controles fronterizos más intensos evitaron la propagación de enfermedades y disminuyó la propagación de epidemias como las ocurridas en tiempos anteriores. La revolución agrícola británica hizo además más eficiente la producción de alimentos con una menor aportación del factor trabajo, alentando a la población que no podía encontrar trabajos agrícolas a buscar empleos relacionados con la industria y, por ende, originando un movimiento migratorio desde el campo a las ciudades así como un nuevo desarrollo en las fábricas. La expansión colonial del siglo XVII acompañada del desarrollo del comercio internacional, la creación de mercados financieros y la acumulación de capital son considerados factores influyentes, como también lo fue la revolución científica del siglo XVII. Se puede decir que se produjo en Inglaterra por su desarrollo económico.

La presencia de un mayor mercado doméstico debería también ser considerada como un catalizador de la Revolución Industrial, explicando particularmente por qué ocurrió en el Reino Unido.

La invención de la máquina de vapor fue una de las más importantes innovaciones de la Revolución industrial. Hizo posible mejoramientos en el trabajo del metal basado en el uso de coque en vez de carbón vegetal. En el siglo XVIII la industria textil aprovechó el poder del agua para el funcionamiento de algunas máquinas. Estas industrias se convirtieron en el modelo de organización del trabajo humano en las fábricas.

Además de la innovación de la maquinaria, la cadena de montaje (fordismo) contribuyó mucho en la eficiencia de las fábricas.

La Revolución Industrial estuvo dividida en dos etapas: la primera del año 1750 hasta 1840, y la segunda de 1880 hasta 1914. Todos estos cambios trajeron consigo consecuencias tales como:

A mediados del siglo XIX, en Inglaterra se realizaron una serie de transformaciones que hoy conocemos como Revolución Industrial dentro de las cuales las más relevantes fueron:

La industrialización que se originó en Inglaterra y luego se extendió por toda Europa no solo tuvo un gran impacto económico, sino que además generó enormes transformaciones sociales.

Proletariado urbano. Como consecuencia de la revolución agrícola y demográfica, se produjo un éxodo masivo de campesinos hacia las ciudades; el antiguo agricultor se convirtió en obrero industrial. La ciudad industrial aumentó su población como consecuencia del crecimiento natural de sus habitantes y por el arribo de este nuevo contingente humano. La carencia de habitaciones fue el primer problema que sufrió esta población socialmente marginada; debía vivir en espacios reducidos sin comodidades mínimas y carentes de higiene. A ello se sumaban jornadas de trabajo, que llegaban a más de catorce horas diarias, en las que participaban hombres, mujeres y niños con salarios miserables, y carentes de protección legal frente a la arbitrariedad de los dueños de las fábricas o centros de producción. Este conjunto de males que afectaba al proletariado urbano se llamó la Cuestión social, haciendo alusión a las insuficiencias materiales y espirituales que les afectaban.

Burguesía industrial. Como contraste al proletariado industrial, se fortaleció el poder económico y social de los grandes empresarios, afianzando de este modo el sistema económico capitalista, caracterizado por la propiedad privada de los medios de producción y la regulación de los precios por el mercado, de acuerdo con la oferta y la demanda.

En este escenario, la burguesía desplaza definitivamente a la aristocracia terrateniente y su situación de privilegio social se basó fundamentalmente en la fortuna y no en el origen o la sangre. Avalados por una doctrina que defendía la libertad económica, los empresarios obtenían grandes riquezas, no solo vendiendo y compitiendo, sino que además pagando bajos salarios por la fuerza de trabajo aportada por los obreros.

Las propuestas para solucionar el problema social. Frente a la situación de pobreza y precariedad de los obreros, surgieron críticas y fórmulas para tratar de darles solución; por ejemplo, los socialistas utópicos, que aspiraban a crear una sociedad ideal, justa y libre de todo tipo de problemas sociales (para algunos, el comunismo). Otra propuesta fue el socialismo científico de Karl Marx, que proponía la revolución proletaria y la abolición de la propiedad privada (marxismo); también la Iglesia católica, a través del papa León XIII, dio a conocer la Encíclica Rerum Novarum (1891), primera encíclica social de la historia, la cual condenaba los abusos y exigía a los estados la obligación de proteger a lo más débiles. A continuación, un fragmento de dicha encíclica: 

 Estos elementos fueron decisivos para el surgimiento de los movimientos reivindicativos de los derechos de los trabajadores. Durante el siglo XX en medio de los procesos de democratización, el movimiento obrero lograba que se reconocieran los derechos de los trabajadores y su integración a la participación social. Otros ejemplos de tendencias que buscaron soluciones fueron los nacionalismos, así como también los fascismos en los cuales se consideraban a los obreros y trabajadores como una parte fundamental en el desarrollo productivo de la nación, por lo que debían ser protegidos por el Estado.

Uno de los principios fundamentales de la industria moderna es que nunca considera a los procesos de producción como definitivos o acabados. Su base técnico-científica es revolucionaria, generando así el problema de la obsolescencia tecnológica en períodos cada vez más breves. Desde esta perspectiva puede afirmarse que todas las formas de producción anteriores a la industria moderna (artesanía y manufactura) fueron esencialmente conservadoras, al trasmitirse los conocimientos de generación en generación sin apenas cambios. Sin embargo, esta característica de obsolescencia e innovación no se circunscribe a la ciencia y la tecnología, sino debe ampliarse a toda la estructura económica de las sociedades modernas. En este contexto la innovación es, por definición, negación, destrucción, cambio, la transformación es la esencia permanente de la modernidad.

El desarrollo de nuevas tecnologías, como ciencias aplicadas, en un receptivo clima social, es el momento y el sitio para una revolución industrial de innovaciones en cadena, como un proceso acumulativo de tecnología, que crea bienes y servicios, mejorando el nivel y la calidad de vida. Son básicos un capitalismo incipiente, un sistema educativo y espíritu emprendedor. La no adecuación o correspondencia entre unos y otros crea desequilibrios o injusticias. Parece ser que este desequilibrio en los procesos de industrialización, siempre socialmente muy inestables, es en la práctica inevitable, pero mensurable para poder construir modelos mejorados.[cita requerida]

Países transcontinentales:

Europa es un continente ubicado enteramente en el hemisferio norte y mayoritariamente en el hemisferio oriental. Las fronteras de Europa están situadas en la mitad occidental del hemisferio norte, limitada por el océano Ártico en el norte, hasta el mar Mediterráneo por el sur. Por el oeste, llega hasta el océano Atlántico. Por el este, limita con Asia, de la que la separan los montes Urales, el río Ural, el mar Caspio, la cordillera del Cáucaso, el mar Negro y los estrechos del Bósforo y de los Dardanelos.[1]​ Europa es uno de los continentes que conforman el supercontinente euroasiático, situado entre los paralelos 35º 30’ y 70º 30’ de latitud norte.[2]​ 

Europa es el segundo continente más pequeño en términos de superficie. Abarca 10 530 751 kilómetros cuadrados o el 2 % de la superficie del mundo y alrededor de 6,8 % del total de las tierras emergidas. Alberga un gran número de estados soberanos, cuyo número exacto depende de la definición de la frontera de Europa, así como de la exclusión o inclusión de estados parcialmente reconocidos. De todos los países europeos, Rusia es el mayor en superficie (al mismo tiempo que es el estado soberano reconocido internacionalmente más extenso del mundo), mientras que la Ciudad del Vaticano es el más pequeño (al mismo tiempo que es el estado soberano e internacionalmente reconocido más pequeño del mundo). Europa es el cuarto continente más poblado después de Asia, África y América, con una población de 740 813 959 (año 2015) o alrededor del 10,03 % de la población mundial.[3]​

Europa, en particular la Antigua Grecia, es la cuna de la cultura occidental. La caída del Imperio romano de Occidente, durante el período de la migración, marcó el fin de la Edad Antigua y el comienzo de una era conocida como la Edad Media. 

El Renacimiento con sus consiguientes humanismo, arte y ciencia, además de la exploración llevaron al "viejo continente", y finalmente al resto del mundo, a la Edad Moderna. A partir de este período las naciones europeas desempeñan un papel preponderante en los asuntos mundiales, desde el siglo XVI en adelante especialmente, después del comienzo de la colonización. En los siglos XVII y XVIII, las naciones europeas controlaron la mayor parte de África, América, y gran parte de Asia, y posteriormente también Oceanía.

La Revolución Industrial, que comenzó en el Reino Unido en el siglo XVIII, dio lugar a un cambio radical en los ámbitos económico, cultural y social en Europa Occidental, y posteriormente en el resto del mundo. 

La Primera Guerra Mundial y la Segunda Guerra Mundial condujeron a una disminución en el dominio de Europa en los asuntos mundiales cuando los Estados Unidos y la Unión Soviética tomaron la preeminencia. 

La Guerra Fría entre las dos superpotencias dividió Europa a lo largo del Telón de Acero. La integración europea dio lugar a la formación del Consejo de Europa y la Unión Europea en Europa occidental, las cuales se han expandido hacia el este desde la caída de la Unión Soviética en 1991.

Actualmente, naciones que ejercen poder hegemónico mundial como Estados Unidos son el resultado de la colonización europea.

El término Europa tiene diversos usos, y los principales son de carácter geográfico y político. Geográficamente, Europa es la península más occidental de Eurasia; limitada al norte, oeste y sur por el océano Atlántico, el mar Báltico, el mar del Norte, el mar Mediterráneo, el mar de Mármara, el mar Egeo, el mar Adriático, el mar Tirreno y el mar Negro y al este por los montes Urales, el río Ural, el mar Caspio y la cordillera del Cáucaso. La UE considera como tales a todos aquellos situados dentro de los límites geográficos tradicionales, incluyendo Chipre, Georgia, Armenia y Azerbaiyán.[4]​

En las obras de Homero, Εὐρώπη es solo la reina mitológica de Creta y no una denominación geográfica, sin embargo, el (así llamado) Himno Homérico a Apolo (250-251) usa el término para referirse a la parte continental de Grecia, por oposición al Peloponeso y a las islas. Hesíodo, en la conclusión de su Teogonía (verso 964) alude a "los continentes" pero sin precisar sus nombres, con todo se duda que dicho pasaje pertenezca a este autor. Más tarde, desde el año 500 a. C. su significado se refiere a toda la tierra al oeste del mar Egeo. En Heródoto Europa es el mayor de los continentes, extendiéndose al norte del Mediterráneo desde las Columnas de Hércules del estrecho de Gibraltar hasta más allá del río Indo. 

Desde la época helenística, se consideró a Europa como el territorio ubicado al norte del Mediterráneo, el Helesponto (Dardanelos), la Propóntide (mar de Mármara) y el Bósforo, siendo el río Tanais (Don) su frontera nororiental. Así, Varrón[5]​ considera que el mundo puede ser dividido a partir del Bósforo, las tierras situadas al norte de ese río corresponderían a Europa. A pesar de ello, el término no tuvo entonces ninguna connotación política. De hecho desde el siglo IV existió una provincia romana llamada Europa como una subdivisión menor de la provincia romana de Tracia,[6]​ en la cual estuvo incluida Constantinopla. Europa, en sentido literario o meramente geográfico, correspondía a una de las tres partes del mundo, separada de África por Gibraltar, y de Asia por el Don. Flavio Josefo, cuya obra influyó en las concepciones medievales, agregaba que había sido otorgada por Noé a su hijo Jafet.

Hasta finales del Medioevo se mantuvieron estas fronteras de Europa, pero a medida que se ampliaba el mundo conocido por los europeos, resultaba insuficiente. Fue el geógrafo sueco Philip Johan von Strahlenberg, quien hacia 1730 propuso fijar el límite oriental del continente en los montes Urales, propuesta acogida con agrado por la monarquía rusa.
Hasta el siglo XVI el término Europa no era de uso general y se prefería referirse a ella como "la Cristiandad". Esta expresión, desde el Renacimiento y la Reforma, comenzó a ser sustituida por el nombre propio del continente, con menos connotaciones confesionales-culturales y teniendo en cuenta la invasión y ocupación turca de gran parte de los Balcanes.

Actualmente, Europa puede ser usada tanto de manera amplia, para designar a la península más occidental de Eurasia, como de un modo más restringido; los estados miembros de la Unión Europea. También se utiliza para aludir a una serie de naciones que comparten una identidad cultural común a la cual se califica de "europea". En este sentido es como lo utiliza el Consejo de Europa de cuyos 49 países miembros solo 27 pertenecen a la Unión Europea y la pertenencia de Turquía, país que mayoritariamente se sitúa geográficamente en Asia.[7]​ Por otro lado, los habitantes de algunas islas europeas; especialmente Irlanda o el Reino Unido, pero también Escandinavia, se refieren a la Europa continental como "el Continente".[8]​

Tradicionalmente se asocia el origen del topónimo Europa con un personaje de la mitología griega. En efecto, Europa (en griego antiguo, Εὐρώπη, Eurṓpē) era hija de Agénor y de Telefasa, hermana de Cadmo, una princesa fenicia. Cuando estaba divirtiéndose con sus compañeras en la playa, Zeus la observó y acabó enamorándose de ella. Zeus se transformó en un toro blanco, tan manso, que Europa se acercó a él, puso flores sobre su cuello y finalmente se atrevió a montarlo; entonces, Zeus se levantó y cruzó el mar, llevándola a la isla de Creta, donde Europa dio a luz a Minos y a Sarpedón, con el cual regresó a Asia. Del nombre de esta mujer provendría el del continente.

El análisis más extendido de esta palabra la considera como una composición de las palabras griegas εὖρος («ancho») y ὤψ («vista, ojo»), pero se trata sin duda de una etimología popular. Muchos lingüistas piensan que Europa proviene de la raíz semítica ʔrb (“hrb”), que significa «ponerse el sol» (Occidente); irib en asirio, ereb en arameo, habiéndose propuesto la forma *ʔurūbā como la denominación original de las «tierras occidentales».[9]​ Desde una perspectiva asiática o medio-oriental, el sol se pone efectivamente en Europa, la tierra al oeste. Aun cuando esta sea la etimología más aceptada en la actualidad, algunos investigadores como M. L. West han sostenido que «fonológicamente, la coincidencia entre el nombre de Europa y cualquiera de las formas semíticas del vocablo es muy pobre».[10]​

La mayor parte de las lenguas utilizan palabras derivadas de Europa para referirse al continente. En chino, por ejemplo, se emplea Ōuzhōu (歐洲), que es una abreviatura del nombre transliterado; Ōuluóbā zhōu (歐羅巴洲). Sin embargo, en algunas lenguas túrquicas se utiliza el término Frangistán ('Tierra de los Francos') de manera coloquial, aunque el nombre "oficial" del continente sea Avrupa o Evropa.[11]​

El primer uso del término «europeos» (europenses) parece haberse dado en la Crónica mozárabe de 754, para referirse al enfrentamiento entre los reinos cristianos y la expansión musulmana.[12]​

La neandertal está considerada como la única especie humana autóctona de Europa. En efecto, según los hallazgos paleontológicos, se encontraba presente en Europa cuando llegó el Homo sapiens (identificado por los fósiles de Cromagnon), especie a la que pertenece toda la humanidad actual. Las dos especies humanas convivieron durante bastante tiempo hasta que el neandertal se extinguió, probablemente debido a la competencia con el sapiens, si bien aún quedan numerosos interrogantes sobre dicho evento y acerca de la posibilidad de hibridación entre ambas especies.[14]​

La Antigüedad estuvo marcada por el influjo de la civilización grecolatina, y del Imperio romano sobre el resto de Europa. La caída del Imperio romano de Occidente y la llegada de nuevos grupos étnicos con nuevos reinos, llevó a la fragmentación política de Europa. Durante ese período debido a la mayor productividad de la agricultura mediterránea y la mayor eficiencia del transporte marítimo sobre el terrestre, la zona mediterránea soportaba mayores densidades de población y tenía las principales ciudades. Por esa razón, los factores demográficos y geográficos condicionaron en esa época el mayor desarrollo económico relativo del sur de Europa, frente al norte de Europa más frío y, en general, con peores recursos agrícolas.

El comienzo de la Edad Media se sitúa tradicionalmente en el año 476 con la caída del Imperio romano de Occidente. Este acontecimiento fue seguido por sucesivos intentos de unificación y conquista, que sumieron al continente en numerosos conflictos y guerras durante la Edad Media, como la Guerra de los Cien Años (que duró más de un siglo). Esto, junto con la influencia sobre el continente de nuevos grupos, como los mongoles llegados por las estepas, o el surgimiento del islam, creándose una barrera que dividió dos culturas y el Mediterráneo, y con los choques en esta frontera, moldeó esta época en el continente.

La Edad Moderna marca, para Europa, el inicio de procesos que mucho después darán lugar a la globalización, y es el tiempo en el que los conflictos bélicos se hicieron cada vez más desastrosos, como la llamada Guerra de los Treinta Años. Durante este período, las armas de fuego europeas, debido a los constantes conflictos entre potencias europeas, mejoraron notablemente. Si bien Europa tenía una población muy inferior a China o India, y también una productividad menor (hacia 1500, Oriente Medio, China e India concentraban el 60 % de la producción mundial), los recursos provenientes de América y su tecnología militar serían un factor importante a partir de la segunda mitad del siglo XIX, cuando diversas potencias europeas tuvieron choques armados con China e India.

Un factor que influiría decisivamente en la expansión de la cultura europea (religión, lengua, etc.) fue la llamada expansión europea que tuvo una primera oleada en el siglo XVI en América y en dos oleadas posteriores hizo que gran parte de Asia, África y Oceanía también estuvieran controladas políticamente por países europeos.

Los procesos económicos y el desarrollo científico y tecnológico se aceleraron en desmedro de otros continentes de manera mucho más notoria durante la Edad Contemporánea, produciendo tensiones por competencias que desencadenaron más guerras (como las Guerras Napoleónicas y las Guerras Mundiales). Hoy los procesos tendentes a la unificación se procuran pacíficamente, tal es el caso de la Unión Europea, cuyo origen se remonta a la Declaración Schuman de 1950.

Europa es el continente que ha tenido más influencia política, militar y económica en la historia del mundo en los dos últimos siglos. Sin embargo, antes del siglo XIX, China e India eran demográficamente y económicamente más importantes que Europa: hacia 1500, Oriente Medio, India y China concentraban cerca del 60 % de la producción mundial; y poco antes de 1800, el 80 %.[15]​[16]​

Europa, el segundo continente más pequeño del mundo tras Oceanía, tiene una extensión de 10 530 751 km², representando el 7 % de las tierras emergidas.

Hablando estrictamente en términos de ciencia geográfica contemporánea, Europa, como Oceanía, dejan de estar categorizadas como continentes y son consideradas macro-unidades geográficas (MUG); ya que en efecto, en el caso de Europa esta macrounidad geográfica es una prolongación occidental del continente eurasiático. Caracteriza a Europa, tanto en lo geográfico (con mucha incidencia en lo climático como en su geografía humana), la elevada cantidad media de costas marítimas y oceánicas debida a la presencia de abundantes penínsulas, golfos, mares interiores e islas. Esto y el influjo de la corriente del Golfo y la proximidad de los desiertos cálidos de África y Asia determinan que en Europa prepondere, pese a las latitudes, un clima templado excepcionalmente benigno para la habitabilidad humana. Por otra parte la abundancia de costas e hidrovías ha permitido y permite el tránsito de poblaciones y luego su establecimiento desde fines del pleistoceno (cuando los Homo sapiens substituyeron a los Homo neandertalensis).

También es Europa, si se la considera de modo tradicional como un continente, el continente más llano, con una altura media de 230 metros. La máxima expresión de estas planicies es La gran llanura del Norte, que se extiende 2000 km desde las costas atlánticas francesas hasta los montes Urales, la frontera física más oriental con Asia. Los puntos más altos son el monte Elbrus (Rusia) en Europa oriental (5642 m), el Shkhara (Georgia) (5204 m) y el Mont Blanc (Italia-Francia) en Europa occidental (4807 m).

Al sur, Europa está separada del continente africano por el mar Mediterráneo, frontera que se reduce a unos pocos kilómetros en el estrecho de Gibraltar, al sureste los límites con Asia también están dados por el Mediterráneo y sus mares subsidiarios, el mar de Mármara y el mar Negro. Si bien se observa, el mar Mediterráneo y su cuenca más que un límite (según los momentos históricos) es un nexo de unión con los otros "continentes" (las macrounidades geográficas de Asia y África), resultando los verdaderos límites culturales y étnicos las extensas regiones desérticas que se ubican al otro lado del Mediterráneo. Considerando a Islandia como parte de Europa y a Groenlandia como parte de América, se puede observar que las distancias entre Europa y el continente americano son también bastante exiguas.

Entre los golfos de Europa destacan el golfo de Vizcaya (Francia y España), el de Cádiz (España y Portugal), el de Dardanelos y el del Bósforo (Turquía), el de Mesina (Italia) y el de Oresund (Dinamarca y Suecia), entre otros.

Sus principales penínsulas son la Escandinava (Suecia, Noruega), ibérica (España, Portugal, Andorra y Gibraltar), itálica (Italia, San Marino y Ciudad del Vaticano), balcánica (Grecia, Albania, Bulgaria, Macedonia del Norte, Serbia, Croacia, Montenegro, Bosnia-Herzegovina, Eslovenia y Rumania); además de las penínsulas de Kola (Rusia), Jutlandia (Dinamarca), Bretaña (Francia) y Crimea (disputado entre Rusia y Ucrania).

Sus principales islas son Gran Bretaña, Islandia e Irlanda.

Europa está dividida políticamente en cincuenta estados soberanos, ocho estados con limitado reconocimiento, seis territorios dependientes y tres regiones autónomas integradas en la Unión Europea. En el continente se hallan algunos de los estados-nación más antiguos del mundo.

Todos los estados europeos soberanos son miembros de pleno derecho de las Naciones Unidas (ONU), y se encuentran entre sus filas 12 países europeos como fundadores.
A nivel económico, los Estados europeos forman parte del Fondo Monetario Internacional (salvo Andorra, el Vaticano, Mónaco y Liechtenstein), aunque algunos no cumplen con el artículo VIII como son Bosnia y Herzegovina y Albania. En el caso de la Organización Mundial del Comercio se integran todos salvo el Principado de Mónaco y San Marino, además de países como Andorra, Azerbaiyán, Bielorrusia, Bosnia y Herzegovina, la Ciudad del Vaticano y Serbia que tienen el estatus de observadores.
El potencial económico de Europa hace que cuatro países europeos (Alemania, Francia, Italia y Reino Unido) estén entre las principales economías del mundo, estas potencias suelen unirse en los llamados G-7, G-12 y G-20. Recientemente, se unió la Unión Europea como miembro permanente.

En materia de justicia y seguridad todos los países europeos están integrados en la INTERPOL. En el caso de la Corte Penal Internacional cuatro países no han firmado ni ratificado el Estatuto de Roma (Rusia, Azerbaiyán, Turquía y Vaticano). Mientras que son tres los firmantes que aún no lo han ratificado (Ucrania, Armenia y Mónaco). Rusia, por su parte, firmó el Estatuto, pero recientemente retiró su firma. En el resto de países se acepta la jurisdicción del Corte Penal Internacional para juzgar casos de crímenes contra la humanidad.

En caso del Movimiento de Países No Alineados (1961) solo dos naciones europeas (Bielorrusia y Azerbaiyán) forman parte de esta organización en favor de la neutralidad.

Existen otras organizaciones de tipo transcontinental como la OCDE en 1961 (países americanos, europeos y asiáticos) a la que se adhirieron 24 países europeos. Como organizaciones transcontinentales entre Europa y América encontramos la Organización para la Seguridad y la Cooperación en Europa (1975) para fomentar la democracia, los derechos humanos, el control de armas y la seguridad. También la organización militar OTAN (1949), a la que pertenecen 26 estados europeos (dos potencias nucleares como el Reino Unido y Francia). De igual modo la OTAN mantiene desde 1994 un programa para estrechar lazos con estados de la antigua Unión Soviética y otros países europeos, la Asociación para la Paz.
Rusia mantiene una alianza militar contrapuesta a la Organización del Atlántico Norte, y es la Organización del Tratado de la Seguridad Colectiva de la que forman parte 6 naciones (Rusia, Armenia y Bielorrusia por parte europea más Kazajistán, Tayikistán y Kirguistán).
A nivel de investigación destaca la Agencia Espacial Europea (1975), a la que pertenecen 22 estados europeos, que fomenta la exploración espacial y el programa de satélites europeos.

La mayoría de los países europeos dado su peso político y financiero están presentes en muchas organizaciones mundiales y regionales (fuera de Europa) en calidad de miembros permanentes o miembros observadores caso del Banco Asiático de Desarrollo (1966), con 48 países regionales y 19 no regionales, o el Banco Asiático de Inversión en Infraestructura, a iniciativa de la República Popular de China en 2014, que engloba a 31 Estados asiáticos y algunos europeos como España, Alemania o Francia.

La organización regional más antigua de Europa es el Consejo de Europa (1949), que es una organización internacional de ámbito regional destinada a promover, mediante la cooperación de los estados de Europa, la configuración de un espacio político y jurídico común en el continente, sustentado sobre los valores de la democracia, los derechos humanos y el Imperio de la ley. Constituido por el Tratado de Londres el 5 de mayo de 1949, la única que integra en su seno a todos los Estados europeos, con la salvedad de Bielorrusia, Kazajistán y la Ciudad del Vaticano, excluidos por ser sus regímenes políticos incompatibles con los principios que sustentan la pertenencia al Consejo.

El Consejo de Europa ha perdido peso en la actualidad, ya que la política europea viene marcada por la existencia de un ente al cual pertenecen 27 países de Europa. Se trata de la Unión Europea (creada en 1958 como Comunidad Económica Europea). La Unión Europea fomenta la integración de sus miembros mediante una serie de mecanismos como la cesión de competencias a la Unión Europea, supeditación de los Estados a leyes europeas, Justicia europea o política exterior única y consensuada. Además, los Estados europeos miembros participan a través de las instituciones comunitarias como el Consejo de la Unión Europea, la Comisión Europea o el Parlamento Europeo entre otras. La Unión trabaja en varios organismos regionales tanto para los Estados miembros como para los países no pertenecientes a la Unión Europea:

Aparte de ser un actor principal dentro del continente europeo, la Unión Europea, mantiene relaciones con otros organismos continentales: mantiene reuniones con países de Asia (ASEM), con la CELAC mediante las Cumbre CELAC-UE y con Estados de África, del Caribe y del Pacífico (ACP) para, a través de varios acuerdos (el más reciente Acuerdo de Cotonú del año 2000) luchar contra la pobreza junto a la Unión Europea que trabaja por medio del Fondo Europeo de Desarrollo. La Unión Europea trabaja a través de la firma de acuerdos económicos.

La principal alternativa europea en bloque económico a la Unión Europea fue la Asociación Europea de Libre Comercio (EFTA en inglés) fundada en 1960 como contrapeso a la CEE. Sin embargo con el paso del tiempo muchos de sus miembros abandonaron la organización para ingresar en la CEE (Reino Unido y Dinamarca en 1972, Portugal en 1986, Finlandia, Austria y Suecia en 1991) dejando el bloque con cuatro miembros (Suiza, Liechtenstein, Noruega e Islandia). A nivel político encontramos el Estado de la Unión (1997) una unión entre Rusia y Bielorrusia.

Existen otros organismos de carácter regional como el NB8 (8 Nórdico-Báltico), organismo de cooperación entre Estados nórdicos y bálticos. Estas dos áreas tienen además sus propias organizaciones como el Consejo Nórdico (1952) y la Asamblea Báltica (1990). Una de las más antiguas es la unión política-económica de Bélgica, Países Bajos y Luxemburgo, conocida como Benelux, que fue fundada en 1948.

Las islas británicas (compuestas por Irlanda y el Reino Unido) están integradas en la Unión Europea (aunque el Reino Unido ha iniciado el proceso para abandonar la Unión). Sin embargo, están fuera del Espacio Schengen. Pero ambas naciones mantienen desde 1923 un Common Travel Area que permite a los ciudadanos de ambos países moverse en las islas con un control mínimo de pasaportes.

En la zona oriental de Europa existen también otras organizaciones regionales como el Grupo de Visegrado (1991) para fomentar la cooperación entre Hungría, Polonia, la República Checa y Eslovaquia. O la Cooperación Económica del Mar Negro (1992) con países miembros ribereños del mar Negro más Albania, Serbia, Moldavia, Azerbaiyán y Grecia. Destaca también el Acuerdo centroeuropeo de libre cambio (CEFTA) fundado en 1992 para los países orientales de Europa similar a la EFTA y a la CEE. En su máximo esplendor estuvo compuesto por catorce estados pero actualmente lo conforman seis (cuatro de la Antigua Yugoslavia más Moldavia y Albania), otros estados la abandonaron para ingresar en la Unión Europea:

En 2013, fue un grupo informal llamado EuroMed cuyo ámbito es de crear una alianza del sur para representar la singularidad de sus miembros. Los miembros son: Chipre, Croacia, Eslovenia, España, Francia, Grecia, Italia, Malta y Portugal.

Rusia tienen una fuerte influencia en países europeos de la antigua órbita soviética, de ello se desprende la creación de la Comunidad de Estados Independientes (1992), que incluye a Azerbaiyán, Bielorrusia, Moldavia y Armenia. Finalmente, para contrarrestar esta influencia rusa, se creó en 1997 GUAM entre cuatro países relacionados con la Comunidad de Estados Independientes: Georgia (antiguo miembro), Ucrania (Estado observador), Azerbaiyán, y Moldavia.

Grupos regionales según la ONU

Grupos regionales según The World Factbook

Unión Europea y países candidatos

Mapa con los miembros de la UE y de la OTAN

Cronología de adhesiones a la OTAN

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). para el año 2016.
Esta tabla recoge los Estados europeos con una independencia de facto como naciones con un limitado reconocimiento (o irreconocimiento) internacional. Ninguno es miembro de la Organización de las Naciones Unidas:

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). para el año 2016.
Estos territorios dependen de otras naciones europeas en diversas materias. Solo el territorio de Gibraltar pertenecen a la Unión Europea, el resto de territorios están catalogados como países y territorios de ultramar (o PTU), las dependencias y territorios de ultramar de los Estados miembros de la Unión Europea que no forman parte de la Unión, sino que tiene un estatuto de asociados a los Estados miembros desde el Tratado de Lisboa. En el caso de las bases soberanas de Acrotiri y Dhekelia en Chipre, no forman parte de la UE, aunque a los ciudadanos que residen y trabajan en ellas se les aplica la legislación de la UE, siempre que se traten de ciudadanos de la UE. En este caso se considera que se encuentran en territorio chipriota, aplicándose por tanto las leyes de este país.

Datos de superficie y población consultados en The World Factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). para el año 2016.
Estos territorios que gozan de gran autonomía y algunos forman parte de la Unión Europea como territorio metropolitano, como es Åland. Por su parte, las Islas Azores es una región autónoma que también pertenece a la Unión Europea integrada en la Región Ultraperiférica de la Unión Europea, por lo que el derecho de la Unión se aplica con excepciones en ese territorio. Las Islas Feroe no son parte de la Unión Europea, así como los daneses que viven en ellas no tienen la ciudadanía europea. Las islas tampoco forman parte del Área Schengen, aunque no se precisan controles fronterizos entre ambos.

Datos de superficie y población consultados en The World factbook (enlace roto disponible en Internet Archive; véase el historial, la primera versión y la última). para el año 2016.
En el siglo XIX, se realiza la primera integración moderna de la economía de varios estados europeos a través de la Unión Aduanera de Alemania. Durante esa época se consolida el poderío económico europeo y Europa a finales del siglo XIX llegó a ser la macrorregión con mayor renta per cápita del mundo, siendo actualmente un continente donde se colocan la mayor parte de países de renta alta.

Aun siendo la renta media europea alta, existe una gran disparidad en la riqueza económica de los distintos países europeos, así, mientras en las cinco principales economías el PIB per cápita supera los 50 000 USD por persona, Moldavia apenas sobrepasa los 2000 USD.

Si bien existieron en Europa desde la antigüedad grandes imperios que eran potencias militares y sus sociedades tenían un grado de especialización y complejidad económica superiores a otras partes del planeta, ni durante la Edad Media ni durante la Edad Moderna la economía europea fue superior a la de otras regiones de Asia. Durante la Edad Moderna, Asia aún siguió siendo el continente económicamente más productivo. Hacia 1500, Oriente Medio, India y China concentraban cerca del 60 % de la producción mundial, y poco antes de 1800 el 80 %. Durante el siglo XVIII, los textiles de India se exportaban extensivamente a Francia e Inglaterra. Y gran cantidad de productos industriales chinos estaban presentes, tanto en la América colonial desde el siglo XVII, como en Europa. Se estima que un 75 % de la plata extraída por los españoles en América acabó en China a cambio de la compra de productos manufacturados en China.[15]​[16]​ La revolución industrial europea alteró este equilibrio, y mediante conquista militar gran parte de Asia y África pasaron a estar controladas por potencias europeas, durante la segunda y tercera oleadas de la Expansión europea.

Durante el siglo XX, Europa y algunas de sus excolonias en América pasaron a ser las regiones más productivas y con mayor renta per cápita en el mundo. Aunque en los años 1920, Estados Unidos superaría ya a los países europeos más desarrollados en renta per cápita. Aun así, Europa ha seguido siendo uno de los principales polos económicos del mundo hasta el siglo XXI, con niveles de renta solo igualados por Estados Unidos, Canadá, Australia, Nueva Zelanda, Japón y Corea del Sur. Sin embargo, la nueva realidad de la economía mundial se ha consolidado en el transcurso de las tres últimas décadas, marcada principalmente por la desintegración de la Unión Soviética y el vertiginoso crecimiento de la República Popular China y otras regiones de Asia.

Uno de los factores más notorios de la dinámica económica europea desde 1970 fue la materialización de la unidad económica de buena parte de Europa. Así, la dinámica económica del continente ha estado supeditada al funcionamiento de la Unión Europea. En la actualidad, veinte estados europeos comparten una misma moneda, el euro (€). Otra de las particularidades de la economía europea es el hecho de que varios estados de poca extensión territorial, sin mayores recursos naturales y sin poseer costas, cuentan con economías prósperas y con un elevado nivel de vida. Tal es el caso de Andorra, Luxemburgo, Suiza o Liechtenstein, así como Mónaco, aunque este último posee costas en el Mediterráneo.

La dinámica de la economía europea se ha visto afectada en los últimos cincuenta años por dos cambios importantes en la economía mundial. El primero de ellos ha sido la globalización. Por ella, gran parte de la actividad industrial ha pasado a Extremo Oriente, y por eso Europa ha basado un porcentaje cada vez más mayor de su economía en sectores de alta tecnología y servicios financieros. Esto ha provocado en gran parte la financiarización de la economía productiva, que por una serie de malos diseños y mala regulación desencadenó la Gran Recesión de 2008 que fue especialmente aguda en muchas regiones de Europa y tuvo efectos políticos y el crecimiento del rechazo a la profundización de la unión política en la Unión Europea. Así en 2016, se aprobó la salida del Reino Unido de la Unión Europea, hecho que dio credibilidad al euroescepticismo en otros países europeos.

La población europea actual es, en su inmensa mayoría, fenotípicamente caucásica, y está dividida en tres grandes grupos:

También viven otros grupos de población que no pertenecen a estos tres mayoritarios como los judíos, los albaneses, los gitanos, los turcos, los lapones, etc.

Fueron varios los grupos étnicos que, a lo largo de los siglos, invadieron el continente europeo, entre ellos destacamos a los íberos, celtas, germanos, vikingos (germánicos), latinos o romanos (itálicos), etruscos (itálicos), helénicos, eslavos, etc. Considerados luego autóctonos de dicho continente, a los cuales se suma la migración desde el continente asiático: fenicios, árabes, judíos y gitanos entre otros.

En la actualidad existen otros tipos de inmigrantes, entre ellos los asiáticos del lejano oriente y los provenientes de África, Centroamérica y Sudamérica.

Con respecto a la situación demográfica, destaca el hecho de cómo en el continente europeo la mayor parte de sus habitantes corresponde a una población adulta entre los 30 y 50 años de edad, con un envejecimiento progresivo y una marcada disminución de la población juvenil. Esta situación ya resulta preocupante en varios países europeos, como Alemania, Austria, Francia, Suiza, Bélgica, Países Bajos, España, los países escandinavos, Dinamarca, Grecia y el Reino Unido, en los que se produce una pirámide poblacional invertida con escasa población juvenil y sobre todo infantil. Este fenómeno también se da en Europa Oriental, donde, en la década de 1990, la caída del comunismo provocó un colapso de la natalidad, ya de por sí baja dentro de los países comunistas europeos, junto con un brusco aumento de la mortalidad. En los últimos años, la caída de la natalidad en el antiguo bloque comunista se ha amortiguado, lo que ha permitido una recuperación de unos índices de natalidad más próximos a la estabilización de la población en la actualidad (1,5 hijos por mujer).[21]​ En países como Irlanda, Italia, Portugal e Islandia, el proceso de envejecimiento también se instaló en la sociedad, lo que provoca que la población de estos países decrezca anualmente y ni la inmigración logra cubrir esta problemática. Junto con Japón (Asia), estos últimos encabezan en los países con las demografías más envejecidas del mundo. Sin embargo la situación con respecto a la diferencia entre la natalidad y mortalidad ya se ha equilibrado últimamente.

Otro rasgo característico de la demografía europea es la elevada tasa de inmigración. En este aspecto destaca España en los últimos años: de tener una población extranjera inferior a los 100 000 habitantes en 1999, se ha pasado a varios millones, ya por encima del 10 % de la población y se ha convertido en el primer receptor europeo de inmigración, superando incluso a los países que tradicionalmente fueron los receptores de la inmigración, como Alemania, Francia o el Reino Unido. En el caso de España, se pasó de 39 millones de habitantes en 1999, y con una pronunciada caída de la población desde los 42 millones que se había dado en los años previos a 1999, a 45 millones en 2006, sin incluir la inmigración ilegal; este hecho sirvió al gobierno español para aumentar el peso dentro del parlamento europeo, al recibir más escaños por su población.

Los problemas asociados al envejecimiento de la población pueden ser resumidos en dos partes, un menor crecimiento económico por los desequilibrios del sistema social y por un rasgo de menor innovación dentro de las sociedades envejecidas, y el mantenimiento del sistema de pensiones, cuya balanza de pagos queda seriamente dañado cuando el número de pensionistas supera al de trabajadores. Por estas dos razones, por considerarse un complemento de los sistemas de ayuda internacional, y por la política de hechos consumados, algunos gobiernos europeos han apoyado la inmigración en épocas de bonanza económica o para paliar los problemas antes citados.

En la mayor parte de los países del continente las mujeres superan a los hombres en cantidad, excepto Andorra, Albania, Islandia y la colonia británica de Gibraltar. En estos países, la población masculina es mayoritaria, si bien existen ciertas variaciones entre Europa Occidental y Europa Oriental. Las personas de sexo masculino inferior a 65 años son mayoría principalmente en países como Alemania, Austria, Bélgica, Dinamarca, Francia, Grecia, Hungría, Irlanda, Reino Unido, Países Bajos, los países escandinavos y otros. Además en estos países, la cantidad porcentual de varones se encuentra en crecimiento y con el paso del tiempo podrían igualar o superar a las mujeres. En países como España, Italia y Portugal, la población de ambos sexos se encuentra casi equilibrada, sobre todo en la etapa juvenil, seguida de la de adultez y de la tercera edad. En la mayor parte de los países de Europa Oriental, como en la antigua Unión Soviética y el resto de los Balcanes, las mujeres menores de 65 años y mayores de 64 años son mayoría, y los varones son minoría por varias décimas porcentuales. Esto se debe a las guerras desatadas en los Balcanes, pues los varones son forzados al servicio militar obligatorio, lo cual ha provocado que declinen bastante. Su crecimiento solo dependerá entonces del número de nacimientos.

En Europa se presenta más influyente la comunidad LGBT que en cualquier otra región del mundo. Según un estudio del Eurobarómetro, en la mayoría de los países de Europa Occidental entre el 60 y 91 % de los ciudadanos por diferente país acepta la homosexualidad y el matrimonio igualitario, entre los cuales figuran Países Bajos, Suecia, Dinamarca, España, Irlanda, Bélgica, Luxemburgo, Francia y Reino Unido, con una cifra cercana o superior al 75 %. En el lado opuesto, en los países de Europa Oriental menos del 55 % por cada país tiene la misma opinión, siendo Lituania, Eslovaquia, Rumanía, Letonia y Bulgaria donde la cifra es menor al 25 %.[27]​ Diversas encuestas han estimado que en varios países europeos la tasa de homosexuales y lesbianas son un segmento visible a pesar de ser minoría; por ejemplo, estos componen el 22 % de los neerlandeses, el 14 % de los franceses[28]​y el 8,5 % de los británicos.[29]​ De igual forma, en Europa están los primeros países donde el matrimonio entre personas del mismo sexo se hizo legal, existiendo actualmente como tal en Países Bajos, Bélgica, España, Noruega, Suecia, Islandia, Portugal, Francia, Irlanda y el Reino Unido. En otros de los países occidentales y algunos orientales existe solamente la unión civil.[30]​

Solo el 3 % de todas las lenguas del mundo es autóctona de Europa, por lo que el continente es una de las regiones con menos diversidad lingüística del mundo.[31]​ Aun así Europa es una región con cierta diversidad lingüística, ya que existe un elevado número de lenguas regionales, que pertenecen a diferentes grupos filogenéticos. Entre las lenguas europeas están:

Según datos de Ethnologue [32]​ las cinco lenguas maternas más habladas en Europa son el ruso, el alemán, el francés, el inglés y el italiano. Sin embargo, por el número total de hablantes (lengua materna más segunda lengua), las cinco lenguas más habladas en Europa son el inglés, el francés, el ruso, el alemán y el español. Asimismo, las cinco lenguas europeas más habladas en el mundo (lengua materna más segunda lengua) son el inglés, el español, el francés, el ruso y el portugués.

Además los procesos migratorios que cobraron importancia a partir de la segunda mitad del siglo XX, hicieron que en Europa se instalaran importantes minorías de personas cuyas lenguas tienen un origen alóctono. Entre las lenguas alóctonas destacan las lenguas de África (en particular de los grupos semítico, bereber) y Níger-Congo) y las lenguas de Asia (en particular las indoiranias y las siníticas, que también son las más numerosas de la propia Asia). Por países, se aprecia que los países europeos con un pasado histórico colonial destacado tienden a recibir inmigrantes procedentes de sus excolonias, por lo que también se hablan un cierto número de lenguas minoritarias habladas en las excolonias.

Conjuntos religiosos:

Los católicos son mayoritarios en 23 países,[cita requerida] los protestantes en 13, [cita requerida] los ortodoxos en 11[cita requerida], los musulmanes suníes en tres (Albania, Bosnia-Herzegovina, Turquía) más la República Turca del Norte de Chipre y los musulmanes chiíes en Azerbaiyán.

Existen minorías religiosas dentro de estos grandes conjuntos:

Hablar de cultura europea es difícil porque en Europa se han ido sucediendo muchas culturas (y, a menudo han asimilado contribuciones no europeas) durante miles de años. Una definición de la cultura de Europa debe necesariamente tener en cuenta los límites geográficos del continente.

Premio Nobel

Festival de la Canción de Eurovisión (ABBA)

Academia de Cine Europeo

Liga de Campeones de la UEFA

Agencia Espacial Europea

Desde la antigüedad, muchas de las principales corrientes arquitectónicas se desarrollaron en toda Europa, hasta más allá de sus fronteras. En el primer milenio antes de Cristo, los griegos fundaron colonias por todo el Mediterráneo, seguidos por los romanos; exportaron su arquitectura, su escultura y su literatura a los países que ocuparon. En la periferia de estos territorios se desarrollaron movimientos artísticos originales, en contacto con las civilizaciones celta, ibérica... Pero el lugar de esta cultura es más importante en el mundo mediterráneo (el Mare Nostrum) que en la propia Europa (el arte grecorromano está particularmente bien representado en África del Norte y Oriente Medio). En el norte, los arquitectos están influidos por la cultura celta, que tuvo un notable dominio de los metales y el cobre.

Sin embargo, la civilización romana sobrepasó la costa mediterránea después de la Guerra de las Galias, alcanzó el Rin y los límites de Escocia. Las invasiones del final del Imperio romano perturbaron la situación: el arte grecorromano se extinguió con la decadencia de las grandes ciudades, mientras que se difundió un arte de inspiración germánica, más áspero y rústico, relacionado con el arte celta. Sin embargo, puesto que el Imperio bizantino permaneció en el Este, sus cánones arquitectónicos, y el uso del mosaico se desarrolló en Italia; este país permaneció abierto a las influencias bizantinas hasta la toma de Constantinopla por parte de los turcos islámicos procedentes de Asia en 1453, aunque los mismos invasores turcos otomanos desarrollaron "su propio arte islámico" a partir del arte bizantino con obras como la Mezquita Azul debida al converso Sinan, quien en gran medida se basó en la iglesia bizantina medieval de Santa Sofía.

Europa es la cuna del fútbol moderno.[34]​ A mediados del siglo XIX se crearon las reglas en Inglaterra, y unos años más tarde se extendió por todo el continente gracias a estudiantes, ingenieros, marineros, soldados y empresarios. Entre las organizaciones futbolísticas europeas se incluyen la Liga de Campeones de la UEFA, la Liga Europa de la UEFA, la Liga Europa Conferencia de la UEFA, la Recopa de Europa, la Supercopa de Europa y la Copa Intertoto.

Lo sublime es una categoría estética, derivada principalmente de la célebre obra Περὶ ὕψους (Sobre lo sublime) del crítico o retórico griego Longino (o Pseudo-Longino), y que consiste fundamentalmente en una "grandeza" o, por así decir, belleza extrema, capaz de llevar al espectador a un éxtasis más allá de su racionalidad, o incluso de provocar dolor por ser imposible de asimilar. El concepto de lo "sublime" fue redescubierto durante el Renacimiento y gozó de gran popularidad durante el Barroco, durante el siglo XVIII alemán e inglés, y sobre todo durante el primer Romanticismo.

Según el concepto original de Longino, lo sublime, que se resume en la composición digna y elevada, se funda en cinco causas o fuentes, tanto innatas como de técnica perteneciente sobre todo a las figuras de dicción y metafóricas del lenguaje. Lo sublime es una elevación y excelencia en el lenguaje de que se sirvieron prosistas y poetas que han alcanzado la inmortalidad (1.4). Se trata de  una "grandeza" de estilo cuya doctrina básica perviviría durante toda la Edad Media identificándose en el Virgilio superior de la Eneida. Dice Longino que lo sublime, usado en el momento oportuno, pulveriza como el rayo todas las cosas y muestra en un abrir y cerrar de ojos y en su totalidad los poderes del orador (1.4); que es grande realmente solo "aquello que proporciona material para nuevas reflexiones" y hace difícil, más aún imposible, toda oposición y "su recuerdo es duradero e indeleble" (7.5). "Nada hay tan sublime como una pasión noble, en el momento oportuno, que respira entusiasmo como consecuencia de una locura y una inspiración especiales y que convierte a las palabras en algo divino" (8.4). Siguiendo la tradicional oposición retórica virtud/vicio, explica Longino cómo "lo sublime reside en la elevación, la amplificación en la abundancia" (15.12, ed. esp. García López).

En sentido técnico, "sublime" es una calificación que la retórica antigua estableció en el marco de su Teoría de los estilos como designación del más elevado o grande de estos. El concepto longiniano de "grandeza", de raíz neoplatónica, tiene su gran precedente de sentido más estético que retórico en el diálogo Fedro de Platón, donde se conceptúa la "elevación", relativa a la "manía" y al conjunto de la gama platónica de la inspiración. Esta tradición conduce, en términos retóricos pero asimismo de proyección estética, a san Agustín, donde se cristianiza. Lo sublime, ya asociado también por Longino al "silencio" en sentido elocutivo, adquiere mediante este último término un desarrollo específicamente contemplativo y transcendental en el régimen de la mística europea y, especialmente, española (Juan de la Cruz, Teresa de Jesús, Francisco de Osuna...). Esta es la base del moderno desarrollo kantiano, fundado en la "infinitud" y la "suspensión".

En la estética oriental, y particularmente en la zen, donde no se distingue lo natural de lo artificial, se denomina a lo sublime yūgen. La traducción de la palabra depende del contexto. En los escritos filosóficos chinos yugen significa "oscuro", "profundo" y "misterioso". En el criticismo de la Waka (poesía japonesa), yugen ha sido usado para describir la sutil profundidad de las cosas que pueden ser vagamente referidas en los poemas. También se puede referir al nombre de un estilo de poesía, uno de los diez estilos ortodoxos delineados por Fujiwara no Teika en sus tratados. En otros tratados de teatro nō por Zeami Motokiyo yugen por el contrario se refiere a la gracia y elegancia del vestido y el comportamiento de las damas de la corte.[1]​

En teatro, yugen se refiere a la interpretación de Zeami como "elegancia refinada" en la actuación de nō, pero también significa "un profundo y misterioso sentido de la belleza del universo... y la triste belleza del sufrimiento humano”.[2]​

El tratado de Longino Sobre lo sublime y el concepto mismo permanecieron escasamente identificados  durante la Edad Media. Los amigos neoplatónicos de Miguel Ángel Buonarroti hablaban de su terribilità. Su gran notoriedad e influencia se alcanza en el siglo XVI, después de que Francesco Robortello publicase una edición de la obra clásica en Basilea en 1554, y Niccolò da Falgano otra en 1560. A partir de estas ediciones originales, las traducciones en lenguas vernáculas proliferaron.

Durante el siglo XVII, los conceptos de Longino sobre la belleza gozaron de gran estima, y fueron aplicados al arte barroco. La obra fue objeto de decenas de ediciones durante ese siglo. La más influyente de ellas se debió a Nicolas Boileau-Despréaux (Tratado de lo sublime o de las maravillas en la oratoria, 1674), que situó nuevamente al tratado y al concepto en el centro del debate crítico de la época. La difundida versión de Boileau no es técnicamente relevante ni de especial comprensión del concepto, si bien contribuye a difundir un concepto retórico que “eleva, rapta, transporta” y se dirige al sentimiento más que a la razón. Durante este periodo todavía había quien consideraba De lo sublime una obra demasiado primitiva como para ser aceptable por el civilizado hombre moderno.

La recuperación moderna del concepto de lo sublime se produjo notablemente en el Reino Unido, en el siglo XVIII, dentro de la filosofía empirista. Ya Anthony Ashley Cooper, 3er conde de Shaftesbury, y John Dennis, tras un viaje por los Alpes, expresaron su admiración por las formas sobrecogedoras e irregulares de la naturaleza exterior, apreciaciones estéticas que Joseph Addison sintetizó en su revista The Spectator (1711) en una serie de artículos titulados Pleasures of the Imagination.

En Los placeres de la imaginación, Addison introdujo el gusto por cosas que estimulan la imaginación, distinguiendo tres cualidades estéticas principales: grandeza (sublimidad), singularidad (novedad) y belleza. También creó una nueva categoría, lo “pintoresco”, aquel estímulo visual que aporta una sensación tal de perfección que pensamos que debería ser inmortalizado en un cuadro. Addison relacionó la belleza con la pasión, desligándola de la razón: la belleza nos afecta de forma inmediata e instantánea, como un golpe, actuando de forma más rápida que la razón, por lo que es más poderosa. Al retomar el concepto de lo sublime esbozado por Longino, lo elevó de categoría retórica a general, trasladándolo del lenguaje a la imagen.[3]​

Esta obra de Addison, en la que el concepto de grandeza se une al de sublimidad, junto con la obra de Edward Young Night Thoughts (1745), suelen considerarse como los puntos de partida de Edmund Burke a la hora de escribir su A Philosophical Inquiry into the Origin of Our Ideas of the Sublime and Beautiful ("Una investigación filosófica sobre el origen de nuestras ideas de lo sublime y lo bello") (1756). La importancia de la obra de Burke radica en que fue el primer filósofo en argüir que lo sublime y lo bello son categorías que se excluyen mutuamente, del mismo modo en que lo hacen la luz y la oscuridad. La belleza puede ser acentuada por la luz, pero tanto una luz demasiado intensa como la total ausencia de luz son sublimes, en el sentido de que pueden nublar la visión del objeto. La imaginación se ve así arrastrada a un estado de horror hacia lo "oscuro, incierto y confuso". Este horror, sin embargo, también implica un placer estético, obtenido de la conciencia de que esa percepción es una ficción.

Burke describió lo sublime como un temor controlado que atrae al alma, presente en cualidades como la inmensidad, el infinito, el vacío, la soledad, el silencio, etc. Calificó la belleza como “amor sin deseo”, y lo sublime como “asombro sin peligro”. Así, creó una estética fisiológica, ya que para Burke la belleza provoca amor y lo sublime temor, que pueden sentirse como reales. Introdujo igualmente la categoría de lo “patético”, emoción igualable al placer como sentimiento, que proviene de experiencias como la oscuridad, el infinito, la tormenta, el terror, etc. Estos sentimientos producen una “purgación”, recogiendo de nuevo la teoría de la “catarsis” de Aristóteles.[5]​

Immanuel Kant publicó en 1764 el breve Beobachtungen über das Gefühl des Schönen und Erhabenen ("Observaciones sobre el sentimiento de lo bello y lo sublime"), solo verdaderamente desarrollado más tarde en su Crítica del Juicio (1790). Kant investigó el concepto de lo sublime, definiéndolo como “lo que es absolutamente grande” o solo comparable a sí mismo, lo cual vendría a sobrepasar al contemplador causándole una sensación de displacer, y puede darse únicamente en la naturaleza, ante la contemplación acongojante de algo cuya mesura sobrepasa nuestras capacidades. El sublime kantiano es en el sujeto, si bien ha de mantener concordancia con la naturaleza.

Así, lo bello es una tranquila contemplación, un acto reposado, mientras que la experiencia de lo sublime agita y mueve el espíritu, causa temor, pues sus experiencias nacen de aquello que es temible, y se convierte en sublime a partir de la inadecuación de nuestras ideas con nuestra experiencia. De tal manera, para sentir lo sublime, a diferencia de para sentir lo bello, es menester la existencia de una cierta cultura: el hombre rudo, dice Kant, ve atemorizante lo que para el culto es sublime. El poderío de esta experiencia estética invoca nuestra fuerza, y la naturaleza es sublime porque eleva la imaginación a la presentación de los casos en que el ánimo puede hacer para sí mismo sensible la propia sublimidad de su destinación, aún por sobre la naturaleza. De tal modo, Kant interpretó la naturaleza como fuerza, y en ella está lo sublime:

Para Kant lo sublime es la ilimitación de magnitud o de fuerza: así como la belleza es forma, lo finito y limitado, lo sublime es lo informe, infinitud. La belleza comporta gusto, lo sublime atracción. La sublimidad es, en cierto modo, el punto donde la belleza pierde las formas, un superlativo de la belleza. Lo sublime es “aquello absolutamente grande”, aquello no imaginable. Es lo que gusta inmediatamente pero por la resistencia que opone al interés de los sentidos: una música muy alta, un sabor muy fuerte, un olor muy intenso. Kant distinguió un sublime “matemático” (del intelecto) y otro “dinámico” (de los sentidos) o del poder; el matemático se opone a la comprensión, mientras que el dinámico puede amenazar nuestra integridad física (por ejemplo, una tormenta de mar).[6]​

De hecho y en sentido estricto, la formación de la teoría de lo sublime es esencialmente anterior al Romanticismo. El concepto se incorporó a la cultura artística prerromántica y romántica desde sus orígenes, tanto en Reino Unido como en Alemania. La concepción panteísta de algunos de los primeros románticos, o la visión arrebatada y violenta de la naturaleza propia del Sturm und Drang, se corresponden muy bien con los últimos estadios de lo sublime tal y como los definió Schopenhauer.

Johann Christoph Friedrich Schiller, tras Kant el más importante pensador de esta categoría, compuso, entre otros elementos importantes relativos al concepto, dos ensayos fundamentales (De lo sublime, 1793, y Sobre lo sublime, 1801). Cabría decir que distingue tres fases: “sublime contemplativo”, el sujeto se enfrenta al objeto, que es superior a su capacidad; “sublime patético”, peligra la integridad física; y “superación de lo sublime”, en que el hombre vence moralmente, porque es superior intelectualmente. Schiller, que se sobrepone a Kant mediante la reconfiguración de la relación bello/sublime y conduce este último a teoría de la tragedia ampliamente desarrollada, se mantiene básicamente kantiano cuando piensa que el sentimiento de lo sublime es un sentimiento mixto "compuesto por un sentimiento de tristeza, que en su más alto grado se expresa a modo de escalofrío, y por un sentimiento de alegría, que puede llegar hasta el entusiasmo y, si bien no cabe sea entendido precisamente como gozo, las almas refinadas lo prefieren con mucho a cualquier placer"

Schiller conduce la teoría de lo sublime, como en general todo el núcleo de su pensamiento, a una teoría de la libertad. Según Schiller, y esto es muy importante para la concepción de lo sublime, el arte ofrece todas las ventajas de la naturaleza y ninguno de sus inconvenientes. En general, es de notar que el pensamiento poskantiano, sobre todo a partir de Herder, centró el problema sobre la dificultad de la radical distinción bello/sublime.

El pensamiento romántico alemán propiamente dicho, comienza sobre lo sublime, tras Herder, con Schleiermacher, Schelling y Jean Paul Richter, quien muy avanzadamente conduce el "humorismo" a "sublime destruido". Por su parte, Hegel, que acepta la base kantiana, sin embargo historiza y traslada lo sublime al mundo originario del arte simbólico anterior a la cultura clásica griega. Esto ha de entenderse sobre todo en razón de que hegelianamente el arte, como la religión, queda confinado al pasado y constituye una realidad conclusa, es decir sin futuro, a diferencia de lo propuesto por Kant, referible tanto al pasado como al futuro.

El antihegeliano Arthur Schopenhauer hizo una lista de las etapas intermedias desde lo bello hasta lo más sublime en su El mundo como voluntad y representación (capítulo 39). Para este filósofo, el sentimiento de lo bello nace simplemente de la observación de un objeto benigno. El sentimiento de lo sublime, en cambio, es el resultado de la observación de un objeto maligno de gran magnitud, que podría destruir al observador. Las fases entre uno y otro sentimiento serían por tanto las siguientes:

Si el prerromanticismo había sido temprano en algunos países, sobre todo en Inglaterra, el romanticismo, fuera de Alemania fue en distinto grado un fenómeno de expansión más tardía. En Francia, el mayor valedor del concepto de lo sublime fue Victor Hugo, tanto en sus poesías como en el prefacio a su obra de teatro Cromwell, donde definió lo sublime como una combinación de lo bello y lo grotesco, opuesta a la idea clásica de perfección. Además, tanto El jorobado de Notre Dame (en Nuestra Señora de París), como muchos de los elementos de Los Miserables pueden ser considerados propiamente dentro de la categoría de lo sublime. En Italia, para la teoría de lo sublime son de considerar sobre todo las obras de Martignoni y Tommaseo.

En España, con el antecedente ya lejano de Benito Jerónimo Feijoo en su ensayo El no sé qué (1734),[7]​ José Luis Munárriz tradujo las Lecciones sobre la Retórica y las Bellas Letras de Hugh Blair (Madrid, 1798-1799, 4 vols., segunda edición Madrid, 1804, y otras varias), publicada primitivamente en 1783, donde exponía la estética de lo sublime del prerromanticismo inglés; la asumió además el humanista y poeta prerromántico Francisco Sánchez Barbero en los "Principios sobre lo bello y el gusto" de sus Principios de retórica y poética (1805) apoyándose además en los trabajos de Esteban de Arteaga; Gustavo Adolfo Bécquer se extendió sobre lo sublime en sus Cartas literarias a una mujer (1860-1861) y después el gran filólogo romántico Manuel Milá y Fontanals (Principios de Estética y Estética y teoría literaria, 1869) ahondó también teóricamente en esta categoría.

Las últimas décadas del siglo XIX, invadidas por las múltiples estéticas del posromanticismo, vieron el nacimiento de la Kunstwissenschaft o "ciencia del arte", un movimiento que intentaba discernir las leyes de la apreciación estética y alcanzar un acercamiento científico a la experiencia estética. Pero los pensadores poshegelianos, y especialmente Vischer, habían ya conducido hacia una compleja inversión de la categoría de lo sublime. A comienzos del siglo XX, el neokantiano alemán Max Dessoir, que fundó la revista Zeitschrift für Ästhetik und allgemeine Kunstwissenschaft, publicó su Ästhetik und allgemeine Kunstwissenschaft, en la que distinguía cinco formas estéticas básicas: lo bello, lo sublime, lo trágico, lo feo y lo cómico. La experiencia de lo sublime implicaba para Dessoir un olvido del propio yo, en el que el miedo es sustituido por una sensación de bienestar y seguridad al enfrentarse a un ser superior. Esta sensación es similar a la experiencia trágica: la "conciencia trágica" es la capacidad de lograr un estado exaltado de la conciencia, logrado a partir de la aceptación del sufrimiento inevitable destinado a todos los seres humanos, y de las oposiciones irresolubles de la vida. 

Lo sublime también se encontraba en la base del modernismo, por cuanto intentaba reemplazar a lo meramente bello mediante la liberación del observador de las limitaciones de su condición humana. Fue relevantemente tratado por un buen número de pensadores, entre los cuales cabría recordar a George Santayana y Nicolai Hartmann. Por otra parte, es preciso constatar cómo el siglo XX ha definido una persistente e intensa tendencia a la aminoración y pérdida de transcendentalidad de la categoría, convertida ahora en categorización por antonomasia de una vida y un arte cuya común intranscendencia ya había quedado sellada en la época de la Vaguardia histórica. Así se creó un "sublime de bolsillo" (todavía transcendental, en el margo de la poesía) y, finalmente, por ejemplo, un "sublime del día" o hasta de la política.
En la obra del teórico posmoderno Jean-François Lyotard, lo sublime apunta a una aporía de la razón: indica el límite de nuestras capacidades conceptuales y revela la multiplicidad e inestabilidad del mundo postmoderno. Fredric Jameson da a la categoría "sublime" un sentido distinto de Kant, más próximo a la concepción de Burke, de estupor y horror, para describir la experiencia estética del hiperrealismo, al que considera el arte del capitalismo tardío. Encuentra en el hiperrealismo el síntoma de un mundo dominado por la imagen, en que es posible no distinguir la verdad de la falsedad, en que la vida diaria de la ciudad es alienante, en que la vista se deleita con imágenes convertidas en mercancía: la pobreza urbana es mostrada con brillantes superficies, y hasta los automóviles destruidos brillan con una especie de resplandor alucinatorio. (También lo denomina “sublime Camp”).
Es cierto que también ha existido alguna teoría reciente asimiladora elevada de sublime y "universalidad", pero salvo contadísimas excepciones, la fuerza de la trivialización o aminoración categorial ha sido la tendencia dominante que ha perpetuado el siglo XX y ha alcanzado al XXI, en correspondencia con la evolución de las artes.

Lo sublime tuvo gran relevancia en el romanticismo: los románticos tenían la idea de un arte que surge espontáneamente del individuo, destacando la figura del “genio” –el arte es la expresión de las emociones del artista–. Se exalta la naturaleza, el individualismo, el sentimiento, la pasión, una nueva visión sentimental del arte y la belleza que conlleva el gusto por formas íntimas y subjetivas de expresión, como lo sublime. También otorgaron un nuevo enfoque a lo oscuro, lo tenebroso, lo irracional, que para los románticos era tan válido como lo racional y luminoso. Partiendo de la crítica de Rousseau a la civilización, el concepto de belleza se alejó de cánones clásicos, reivindicando la belleza ambigua, que acepta aspectos como lo grotesco y lo macabro, que no suponen la negación de la belleza, sino su otra cara. Se valoró la cultura clásica, pero con una nueva sensibilidad, valorando lo antiguo, lo primigenio, como expresión de la infancia de la humanidad. Asimismo, se revalorizó la Edad Media, como época de grandes gestas individuales, en paralelo a un renacer de los sentimientos nacionalistas. El nuevo gusto romántico tuvo especial predilección por la ruina, por lugares que expresan imperfección, desgarramiento, pero a la vez evocan un espacio espiritual, de recogimiento interior.[8]​

En arte, lo sublime corrió en paralelo con el concepto de lo pintoresco, la otra categoría estética introducida por Addison: es un tipo de representación artística basada en unas determinadas cualidades como serían la singularidad, irregularidad, extravagancia, originalidad o la forma graciosa o caprichosa de determinados objetos, paisajes o cosas susceptibles de ser representadas pictóricamente. Así, sobre todo en el género del paisaje, en el arte romántico se aúnan sublime y pintoresco para producir una serie de representaciones que generen nuevas ideas o sensaciones, que agiten la mente, que provoquen emociones, sentimientos. Para los románticos, la naturaleza era fuente de evocación y estímulo intelectual, elaborando una concepción idealizada de la naturaleza, que perciben de forma mística, llena de leyendas y recuerdos, como se denota en su predilección por las ruinas. El paisaje romántico cobró predilección por la naturaleza grandiosa: grandes cielos y mares, grandes cumbres montañosas, desiertos, glaciares, volcanes, así como por las ruinas, los ambientes nocturnos o tormentosos, las cascadas, los puentes sobre ríos, etc. Sin embargo, no solo el mundo de los sentidos proporciona una visión sublime, también existe una sublimidad moral, presente en acciones heroicas, en los grandes actos civiles, políticos o religiosos, como se podrá ver en las representaciones de la Revolución francesa. Igualmente, existe la sublimidad pasional, la de la soledad, la nostalgia, la melancolía, la ensoñación, el mundo interior de cada individuo.[3]​

Los románticos encontraron cierta sublimidad –con efectos retroactivos– en la arquitectura gótica o en la “terribilità” de Miguel Ángel, que para ellos era el genio sublime por excelencia.[9]​ Sin embargo, el arte sublime se debe circunscribir al realizado en los siglos XVIII y XIX, sobre todo en Alemania y Reino Unido. Dos de los más grandes representantes de lo sublime, entendido como grandeza y como sentimiento desbordante, como un sublime moral más que físico, fueron William Blake y  Johann Heinrich Füssli. Blake, poeta y pintor, ilustraba sus propias composiciones poéticas con imágenes de desbordante fantasía, personales e inclasificables, mostrando una imagen paroxística de lo sublime por el carácter épico, místico y apasionado de los personajes y las composiciones, de movimiento dinámico y exacerbado, de influencia miguelangelesca, como en su poema simbólico Jerusalén (1804-1818) –Blake elaboraba a la vez imagen y texto, como en las miniaturas medievales–. Füssli, pintor suizo afincado en Gran Bretaña, realizó una obra de temática basada en lo macabro y lo erótico, lo satírico y lo burlesco, con una curiosa dualidad, por una parte los temas eróticos y violentos, por otra una virtud y sencillez influida por Rousseau, pero con una personal visión trágica de la humanidad. Su estilo era imaginativo, monumental, esquemático, con cierto aire manierista influido por Miguel Ángel, Pontormo, Rosso Fiorentino, Parmigianino y Domenico Beccafumi. El sentido de lo sublime en Füssli se circunscribe al ámbito emocional, psíquico, más que al físico: es la sublimidad del gesto heroico, como en Juramento en el Rütli (1779); del gesto desolado, como en El artista desesperado ante la grandeza de las ruinas antiguas (1778-80); o del gesto terrorífico, como en La pesadilla (1781).[10]​

Quizá el más prototípico artista de lo sublime fue el alemán Caspar David Friedrich, que tenía una visión panteísta y poética de la naturaleza, una naturaleza incorrupta e idealizada donde la figura humana tan solo representa el papel de un espectador de la grandiosidad e infinitud de la naturaleza –obsérvese que generalmente las figuras de Friedrich aparecen de espaldas, como dando paso a la contemplación de la gran vastedad del espacio que nos ofrece–. Entre sus obras destacan: Dolmen en la nieve (1807), La cruz en la montaña (1808), El monje junto al mar (1808-1810), Abadía en el robledal (1809), Arco iris en un paisaje de montañas (1809-1810), Acantilados blancos en Rügen (1818), El caminante sobre el mar de nubes (1818), Dos hombres contemplando la luna (1819), Océano glacial (Naufragio de la “Esperanza”) (1823-1824), El gran vedado (1832), etc.[11]​

Otro nombre de relevancia es el de Joseph Mallord William Turner, paisajista que sintetizó una visión idílica de la naturaleza influida por Poussin y Lorrain, con una predilección por los fenómenos atmosféricos violentos: tormentas, marejadas, niebla, lluvia, nieve, o bien fuego y espectáculos de destrucción. Son paisajes dramáticos, perturbadores, que provocan sobrecogimiento, dan sensación de energía desatada, de tenso dinamismo. Cabe destacar los profundos experimentos realizados por Turner sobre cromatismo y luminosidad, que otorgaron a sus obras un aspecto de gran realismo visual. Entre sus obras destacan: El paso de San Gotardo (1804), Naufragio (1805), Aníbal cruzando los Alpes (1812), El incendio de las Casas de los Lores y de los Comunes (1835), Negreros tirando por la borda a muertos y moribundos (1840), Crepúsculo sobre un lago (1840), Lluvia, vapor y velocidad (1844), etc.

También cabría citar como paisajistas enmarcados en la representación de lo sublime a John Martin, Thomas Cole y John Robert Cozens en el Reino Unido; Ernst Ferdinand Oehme y Carl Blechen en Alemania; Caspar Wolf en Suiza; Joseph Anton Koch en Austria; Johan Christian Dahl en Noruega; Hubert Robert y Claude-Joseph Vernet en Francia; y Jenaro Pérez Villaamil en España.[12]​

La agricultura (del latín agri ‘campo’ y cultūra ‘cultivo’, ‘crianza’)[1]​[2]​ es el conjunto de actividades económicas y técnicas relacionadas con el tratamiento del suelo y el cultivo de la tierra para la producción de alimentos. Comprende todo un conjunto de acciones humanas que transforma el medio ambiente natural.

Las acciones relacionadas son las que integran el llamado sector agrícola. Todas las actividades económicas que abarca dicho sector tienen su fundamento en la explotación de los recursos que la tierra origina, favorecida por la acción del ser humano: alimentos vegetales como cereales, frutas, hortalizas, pastos cultivados y forrajes; fibras utilizadas por la industria textil; cultivos energéticos etc.

La agricultura[3]​ también incluye una demanda global del ramo y el servicio de la alimentación mundial depende en gran medida del clima y de las técnicas para poder hacer la tierra fértil, conserva su origen en la propiedad privada y en la explotación de la tierra entregada a familias para poder establecerse. Es una actividad de gran importancia estratégica como base fundamental para el desarrollo autosuficiente y dinero de las naciones. La ciencia que estudia la práctica de la agricultura es la agronomía.

La agricultura comenzó una vez que las personas plantaron hierbas por sus semillas (o granos) en el Cercano Oriente, en Guangdong en China y en Latinoamérica; y tal vez plantaron verduras de raíz en Perú e Indonesia, también. El Creciente Fértil del sudoeste asiático, Egipto e India fueron los lugares donde se desarrollaron inicialmente la siembra y cosecha hidráulica de plantas que habían sido recogidas previamente en la naturaleza. El desarrollo independiente de la agricultura se produjo en el norte y sur de China, en el Sahel de África, en Nueva Guinea y en varias regiones de las Américas. Los ocho cultivos llamados fundadores del Neolítico de la agricultura, marcas de almidón en implementos de piedra que se encontraron en Nueva Guinea sugieren que el camote se ha cultivado ahí al menos desde hace 30 000 años; Las castañas de agua y los frijoles pudieron haber sido cultivados cerca de la Cueva del Espíritu, en Tailandia desde el año 11,000 a 7500 a.C.

El desarrollo primigenio de la agricultura en el creciente fértil se suele fechar hace alrededor del 9500 a. C., tras la última glaciación y muy probablemente como consecuencia de ella. Las comunidades de cazadores-recolectores del Oriente Medio se sedentarizaron y empezaron a domesticar animales y plantas salvajes de los que ya se alimentaban con el objetivo de proveerse de una fuente estable de alimento sin tener que viajar en su busca. La revolución neolítica subsecuente conllevó enormes cambios en la forma de vida de los seres humanos y llevó eventualmente a la aparición de la civilización" la revolución en las mismas áreas unos pocos milenios después. Alrededor del año 9000 a.C., algunas personas abandonaron el viejo modo de vida de cazar animales y recolectar frutos para establecerse y cultivar. Los expertos llaman a este gran cambio la "la revolución neolítica ".El farro y la cebada se cultivaron en el Cercano Oriente alrededor del año 8000 a.C.; borregos y cabras se domesticaron en este lugar poco después.

En el año 7000 a. C., la naciente agricultura llegó a Egipto. Por lo menos desde 7000 a. C., en el subcontinente indio se cultivó trigo y cebada, como lo demuestran excavaciones arqueológicas en Mehrgarh en Baluchistán, en lo que hoy es Pakistán.

En el año 6000 a. C., la agricultura campesina se atrincheró en las orillas del Nilo. Esto debido al poco desarrollo aún de las técnicas de riego. Durante este tiempo, la agricultura se desarrolló de forma independiente en el Lejano Oriente, con el arroz, en lugar de trigo, como cultivo principal. Los agricultores de China e Indonesia lograron domesticar el taro o papa china (Colocasia esculenta) y el frijol mung (Vigna radiata), la soja y el azuki (Vigna angularis). Como complemento a estas nuevas fuentes de hidratos de carbono, una red de pesca altamente organizada en los ríos, lagos y las costas del océano en estas áreas trajo consigo grandes volúmenes de proteínas esenciales. En conjunto, estos nuevos métodos agrícolas y de pesca originaron un auge de la población humana que empequeñeció todas las expansiones anteriores y que continúa en la actualidad.

El arado jalado por bueyes se utilizó desde el 5000a.C. Los chinos usaron arados de mano aun antes; En 5000 a. C., los sumerios habían desarrollado las principales técnicas agrícolas, incluyendo el cultivo intensivo de la tierra a gran escala, el monocultivo, técnicas de riego y el uso de mano de obra especializada, particularmente a lo largo de la vía acuática ahora conocida como el canal de Shatt al-Arab, del delta de Golfo Pérsico a la confluencia de los ríos Tigris y Éufrates.

La domesticación de especies silvestres: uros y muflones en ganado vacuno y ovino, respectivamente, dio paso a la utilización a gran escala de animales para comida/fibra y como bestias de carga. El pastor se unió al agricultor como un proveedor esencial para las sociedades sedentarias y seminómadas. El maíz, la mandioca y el arrurruz fueron domesticadas por primera vez en el continente americano y se remontan al 5200 antes de Cristo (A.C).

El inicio de la agricultura se encuentra en el período Neolítico, cuando la economía de las sociedades humanas evolucionó desde la recolección, la caza y la pesca a la agricultura y la ganadería. Las primeras plantas cultivadas fueron el trigo y la cebada. Sus orígenes se pierden en la prehistoria y su desarrollo se gestó en varias culturas que la practicaron de forma independiente, como las que surgieron en el denominado Creciente Fértil (zona de Oriente Próximo desde Mesopotamia al Antiguo Egipto), las culturas precolombinas de América Central, la cultura desarrollada por los chinos al este de Asia, etc.

Marcas de almidón en implementos de piedra que se encontraron en Nueva Guinea sugieren que el camote se ha cultivado ahí al menos desde hace 30 000 años; Se produce una transición, generalmente gradual, desde la economía de caza y recolección de productos agrícolas. Las razones del desarrollo de la agricultura pudieron ser debidas a cambios climáticos hacia temperaturas más templadas; también pudieron deberse a la escasez de caza o alimentos de recolección, o la desertización de amplias regiones. A pesar de sus ventajas, según algunos antropólogos, la agricultura significó una reducción de la variedad en la dieta, creando un cambio en la evolución de la especie humana hacia individuos más vulnerables y dependientes de un enclave que sus predecesores.

La agricultura y la dedicación de las mujeres a una maternidad }es cuando una mujer tiene un bebe y lo cuida asta su último aliento [4]​ permitieron una mayor densidad de población que la economía de caza y recolección por la disponibilidad de alimento para un mayor número de individuos. Con la agricultura las sociedades van sedentarizándose y la propiedad deja de ser un derecho solo sobre objetos móviles para trasladarse también a los bienes inmuebles, se amplía la división del trabajo y surge una sociedad más compleja con actividades artesanales y comerciales especializadas, los asentamientos agrícolas y los conflictos por la interpretación de linderos de propiedad dan origen a los primeros sistemas jurídicos y gubernamentales.

En los primeros tiempos de Roma se cultivaban principalmente cereales, leguminosas y hortalizas, pero en la época de la expansión republicana e imperial la agricultura incluía, además del trigo (el pan fue siempre la base de la alimentación) los otros dos elementos de la llamada tríada o trilogía mediterránea.

El campesino trabajaba con su familia, en un modelo literariamente idealizado de vida sencilla (base de los valores morales, familiares y públicos, y de la participación en la res publica); pero con la expansión territorial, la continuidad del esfuerzo bélico, que exigía un prolongado servicio militar de los ciudadanos, arruinó las pequeñas explotaciones en beneficio del modo de producción esclavista. En ese sistema se incluía la mayor parte de la producción agrícola, tanto la de los modestos lotes de tierras repartidos a soldados veteranos como los grandes latifundios en manos de la aristocracia senatorial. En la lenta transición del esclavismo al feudalismo, a partir de la crisis del siglo III, se sustituyeron los esclavos por siervos, y el Imperio se ruralizó, pasando las villae rurales a ser centros autosuficientes, en perjuicio de las decadentes ciudades.

A lo largo de la Edad Media europea, surgen importantes innovaciones tecnológicas que aportarán algunos elementos positivos al trabajo de los campesinos. Las principales innovaciones en la agricultura medieval se debieron al mayor dinamismo del modo de producción feudal, que suponía para los siervos un mayor incentivo en la mejora de la producción que para los esclavos. Las Partidas de Alfonso X de Castilla definen a los campesinos dentro de la sociedad estamental como los que labran la tierra e fazen en ella aquellas cosas por las que los hombres han de vivir y de mantenerse. Este campesinado activo fue la fuerza fundamental del trabajo en la sociedad medieval.

La introducción del uso de arados pesados (con ruedas y vertedera) permitió un cultivo más profundo de los suelos del norte de Europa (se incorporó a lo largo del siglo XI en las regiones al norte de los Alpes, mientras que los suelos frágiles de la zona mediterránea seguían vinculados al arado romano). Los molinos hidráulicos (posteriormente los de viento introducidos desde Persia) incrementaron de forma importante la productividad del trabajo, al igual que la mejora paulatina de los aperos agrícolas, como nuevos tipos de trillos, hoces y guadañas.

El cambio del buey por el caballo como animal de tiro fue el resultado de dos avances tecnológicos —el uso de la herradura y el desarrollo de la collera— que permitían al caballo tirar de mayores cargas más fácilmente. Esto aumentó la eficiencia del transporte por tierra, tanto para el comercio como para las campañas militares, y sumado a la mejora general de la red de carreteras aumentó las oportunidades comerciales para las comunidades rurales mejor comunicadas. En algunas zonas con tierras especialmente fértiles, se introdujo la rotación de cultivos de tres hojas (rotación trienal, asociando un cereal de primavera o una leguminosa a un cereal de invierno), lo que reducía al 33 en vez de al 50 % la necesidad de barbecho frente al sistema de año y vez, aumentando la producción y haciéndola más diversificada. La posibilidad de abonado, estaba restringida a la disponibilidad de ganadería asociada, que, en las zonas y periodos en que se incrementó, tuvo un importante impacto en la vida campesina, aunque no siempre positivo para los agricultores, cuyos intereses estaban en contradicción con los de los ganaderos, habitualmente de condición privilegiada (el Concejo de la Mesta y asociaciones ganaderas similares en los reinos cristianos peninsulares). El ejemplo de los monasterios, especialmente de la Orden benedictina expandidos por toda Europa occidental (Cluny y Císter), extendió prácticas agrícolas, de gestión de las propiedades y de industria alimentaria. En zonas de Europa meridional (la Sicilia y la España musulmanas), los árabes introdujeron mejoras agrícolas, especialmente en sistemas de regadío (norias de Murcia, acequias de Valencia), el aprovechamiento de las laderas (bancales de las Alpujarras), zonas inundables (arroz) y el cultivo intensivo de huertas, con la generalización de los frutales mediterráneos (naranjos, almendros) y todo tipo de verduras, que caracterizarán el estereotipo de la alimentación de los campesinos sometidos de estas zonas, de origen musulmán, frente a los conquistadores cristianos (villano harto de ajos llamaba Don Quijote a Sancho).

Estos cambios causaron un crecimiento, tanto en la variedad como en la cantidad de las cosechas, que tuvo efectos importantes en la dieta de la población. El campo fue el gran protagonista en la Plena Edad Media europea. Los recursos que aportaba la agricultura y la ganadería eran la base de la economía y la tierra era el centro de las relaciones sociales, siendo la distribución de sus excedentes la que permitió la revolución urbana que se vivió entre los siglos XI y XIII, cumbre del periodo denominado óptimo medieval, beneficiado por un clima especialmente benigno. La tasa de crecimiento promedio interanual de la población europea durante el período 1000-1300 fue de 0,2 %. Entre las causas de la reducción de la tasa de mortalidad que permitió ese crecimiento, leve pero sostenido, se ha sugerido la mejora en la alimentación producto de la incorporación del octavo aminoácido, gracias al consumo de la lenteja.[5]​

La expansión agrícola de las tierras cultivables se hizo a costa de la reducción de la superficie del bosque y de la incorporación de tierras marginales y aunque contribuyó al crecimiento de la producción de alimentos, inevitablemente conducía a las consecuencias negativas de la ley de los rendimientos decrecientes, lo que estuvo entre las causas lejanas o precondiciones de la crisis del siglo XIV. A pesar de los progresos, la agricultura medieval manifestó siempre signos de precariedad debido a la imposibilidad de realizar la inversión productiva de los excedentes (extraídos en forma de renta feudal por la nobleza y el clero) y su estrecha dependencia de las condiciones naturales.

Durante el Antiguo Régimen los países del sur y este de Europa prolongaron el sistema económico feudal, especialmente en la agricultura, pudiéndose hablar de una refeudalización evidente desde la crisis del siglo XVII, en que se reafirmó la posición predominante de los señores frente a los campesinos, que seguían siendo la inmensa mayoría de la población, pero que no tenían posibilidad de iniciar la acumulación del capital necesaria para la transformación agraria. En cambio, en la Europa noroccidental, especialmente en Países Bajos e Inglaterra, los cambios sociales y políticos (revolución burguesa) se vieron acompañados en el campo por una revolución agrícola previa a la Revolución Industrial del siglo XVIII, que intensificó los cultivos, aumentando los rendimientos gracias a mejoras técnicas y productivas (rotación de cultivos de cuatro hojas de Waasland; aperos de Jethro Tull) y a la introducción de nuevos cultivos.[6]​

La integración de la economía mundial tras la era de los descubrimientos permitió un intercambio de cultivos a nivel planetario: productos del Viejo Mundo, tanto de zonas templadas como el trigo y la vid, como de zonas cálidas como la caña de azúcar, el algodón y el café, fueron introducidos con éxito en América; mientras que productos del Nuevo Mundo como el maíz, la patata, el tomate, el pimiento y el tabaco diversificaron la agricultura europea y del resto de los continentes. Ya en época industrial, la explotación del caucho, restringida inicialmente a la silvicultura amazónica, también se acabó extendiendo a otras zonas ecuatoriales a pesar de todo el cuidado que se puso en impedirlo.

La ideología del liberalismo económico propugnó la liberación del mercado de tierras y la imposición de la propiedad privada sobre ellas, con distintas manifestaciones según los países (enclosures en Inglaterra desde el siglo XVIII; en España supresión de mayorazgos y señoríos desde las Cortes de Cádiz, desamortización de Mendizábal en 1836). La formación de mercados nacionales unificados implicaba la unificación de los pesos y medidas, y la liberalización de los precios frente al anterior proteccionismo mercantilista, tarea que el despotismo ilustrado había iniciado desde supuestos fisiócratas a mediados del siglo XVIII. La supresión de la tasa del trigo en España en 1765 estuvo entre las causas del motín de Esquilache, a partir de lo cual la lenta tramitación de una Ley Agraria no llegó a resultados efectivos (Informe de Jovellanos, 1795). En el Imperio austríaco se produjo la abolición de la servidumbre (José II, 1785), que en el Imperio ruso no llegó hasta 1861 (reforma de Alejandro II). En Francia, la Revolución de 1789 suprimió los derechos feudales, proporcionando una base de pequeños propietarios pero con suficiente capacidad de capitalización, muy implicados con su tierra, que caracterizó desde entonces la vitalidad y especial fuerza social y política del campo francés. En Inglaterra, el predominio de los terratenientes y la gentry en el Parlamento logró mantener hasta bien entrado el siglo XIX el proteccionismo de las Corn Laws para evitar un descenso en el precio del trigo, en perjuicio de los industriales que patrocinaron la Escuela de Mánchester. Lo que sí se había producido es la drástica reducción de la población activa agraria ante cada vez mayor productividad del trabajo. La falta de expectativas de trabajo en el campo para una población creciente (explosión demográfica)y la ruptura de las redes de solidaridad tradicionales en las parroquias rurales (Poor Laws, desaparición de los comunales —en España con la desamortización de Madoz, 1855—) condujo a un imparable éxodo rural que alimentó los suburbios de las ciudades industriales.

El uso de abonos químicos (fosfatos, nitratos, etc.) la mecanización y los estudios científicos de la edafología y la ingeniería agrícola transformaron la agricultura, a finales del siglo XIX, en una actividad similar a la industrial en cuanto a su conexión con la ciencia y tecnología. No obstante, la dependencia de la climatología y la periódica irrupción de plagas (hambre irlandesa de 1845-1849, con afectación de la patata, filoxera desde 1863, con afectación de la vid) produjo periódicas crisis agrícolas.

La división del mundo en países desarrollados y subdesarrollados tuvo en la agricultura uno de sus aspectos: los primeros caracterizados por una agricultura especializada y de mercado con altos rendimientos (incluso en los denominados países nuevos donde la presión de la población sobre la superficie es menor); mientras que en los segundos se produjo una división por zonas entre una agricultura de subsistencia de explotaciones familiares con tecnología tradicional y sometida a la presión del crecimiento demográfico, y una agricultura de plantación de monocultivos destinados al mercado internacional, que también presiona sobre los cada vez más reducidos espacios naturales (deforestación).

La revolución verde de la segunda mitad del siglo XX significó un salto cualitativo en la tecnificación de la agricultura en todo el mundo, basándose en mejoras tecnológicas avanzadas como las semillas de alto rendimiento, que a finales de siglo XX experimentó un nuevo impulso con la biotecnología (OGM). Simultáneamente, la evolución generalizada hacia una agricultura de mercado produjo la cada vez mayor dependencia de los plaguicidas y el abonado intensivo, con graves problemas medioambientales como la contaminación de suelos y acuíferos y una drástica reducción de la biodiversidad; a lo que se ha pretendido responder con el planteamiento de una denominada agricultura sostenible. El total de la superficie global dedicada a las actividades agrarias se sitúa desde finales de los 50 entre el 35% y el 40%.[7]​

Siglo XX, especialmente con la aparición del tractor, las exigentes tareas de sembrar, cosechar y trillar pueden realizarse de forma rápida y a una escala antes inimaginable. Según la Academia Internacional de Ingeniería de Estados Unidos. La mecanización agraria es uno de los 20 mayores logros de la ingeniería del siglo XX.
A principios del siglo XX, en Estados Unidos se necesitaba un granjero para alimentar de 2 a 5 personas, mientras que hoy, gracias a la tecnología, los agroquímicos y las variedades actuales, un granjero puede alimentar a 130 personas. El costo de esta productividad es un gran consumo energético, generalmente de combustibles fósiles.

La difusión de la radio y la televisión (medios de comunicación), así como de la informática, son de gran ayuda, al facilitar informes meteorológicos, estudios de mercado, etc.

Además de comida para humanos y sus animales, se produce cada vez con más amplia utilidad tales como flores, plantas ornamentales, madera, fertilizantes, pieles, cuero, productos químicos (etanol, plásticos, azúcar, almidón), fibras (algodón, cáñamo, lino), combustible (biodiésel, el propio etanol, que ahora ya se está obteniendo del maíz), productos biofarmacéuticos, y drogas tanto legales como ilegales (tabaco, marihuana, opio, cocaína). También existen plantas creadas por ingeniería genética que producen sustancias especializadas (como, por ejemplo, el maíz transgénico, que, al igual que la obtención de etanol, está modificando la economía de los cultivos de esta planta y la vida de las comunidades que de ella siguen dependiendo).

La manipulación genética, la mejor gestión de los nutrientes del suelo y la mejora en el control de las semillas han aumentado enormemente las cosechas por unidad de superficie, a cambio estas semillas se han vuelto más sensibles a plagas y enfermedades, lo que conlleva una necesidad de estos últimos mayor por parte del agricultor; Prueba de ello es el resurgimiento de antiguas variedades, muy resistentes a las enfermedades y plagas, por su rusticidad. Al mismo tiempo, la mecanización ha reducido la exigencia de mano de obra. Las cosechas son generalmente menores en los países más pobres, al carecer del capital, la tecnología y los conocimientos científicos necesarios.

La agricultura moderna depende enormemente de la tecnología y las ciencias físicas y biológicas. La irrigación, el drenaje, la conservación y la sanidad, que son vitales para una agricultura exitosa, exigen el conocimiento especializado de ingenieros agrónomos. La química agrícola, en cambio, trata con la aplicación de fertilizantes, insecticidas y fungicidas, la reparación de suelos, el análisis de productos agrícolas, etc.

Las variedades de semillas han sido mejoradas hasta el punto de poder germinar más rápido y adaptarse a estaciones más breves en distintos climas. Las semillas actuales pueden resistir a pesticidas capaces de exterminar a todas las plantas verdes. Los cultivos hidropónicos, un método para cultivar sin tierra, utilizando soluciones de nutrientes químicos, pueden ayudar a cubrir la creciente necesidad de producción a medida que la población mundial aumenta.

Otras técnicas modernas que han contribuido al desarrollo de la agricultura son las de empaquetado, procesamiento y mercadeo. Así, el procesamiento de los alimentos, como el congelado rápido y la deshidratación han abierto nuevos horizontes a la comercialización de los productos y aumentado los posibles mercados.

La mayoría de las personas en situación de pobreza alrededor del mundo dependen de la agricultura. Por ello, la seguridad alimentaria y la agricultura están siendo promovidas por gobiernos y organismos de desarrollo como una estrategia para favorecer a las personas en situación de pobreza y estimular el crecimiento económico. Estas intervenciones agrícolas incluyen tecnología, habilidades y el entorno regulatorio. Se ha planteado analizar el impacto de varias de estas intervenciones como, por ejemplo, la titulación de tierras, la capacitación y tecnología, las escuelas de campo para agricultores, el pago por servicios ambientales y la gestión forestal descentralizada.

Un análisis de cinco revisiones sistemáticas concluyó, entre varios resultados, que las reformas de titulación de tierras tienen efectos positivos sobre la productividad agrícola y en los ingresos de aquellos beneficiados. Asimismo, las innovaciones tecnológicas impactan positivamente en la seguridad alimentaria familiar, mientras que las escuelas de campo para agricultores demostraron ser eficaces en aumentar la producción agrícola. A pesar de que también se obtuvieron otros resultados, todavía queda pendiente comprender por qué algunos programas son más eficaces que otros, por lo que es recomendable realizar más investigaciones que estudien la gama completa de impactos relevantes y evalúen resultados sociales más amplios.[8]​

Sector agrícola es el sector de la economía que produce  productos agrícolas (materias primas de origen vegetal).

No debe confundirse con el sector agrario (que incluye también la ganadería y las demás actividades económicas del campo) ni con el sector primario (que incluye otros sectores productores de materias primas, como la minería).

Habitualmente se utiliza la expresión para identificar los intereses sectoriales de las empresas agrícolas o, genéricamente, de la totalidad de los habitantes de zonas agrícolas, puesto que son determinantes en la vida económica de la mayor parte de las regiones rurales. En la política de determinados países, especialmente en los Estados Unidos, funcionan como un lobby o grupo de presión.

Producto agrícola es la denominación genérica de cada uno de los productos de la agricultura, la actividad humana que obtiene materias primas de origen vegetal a través del cultivo. No se consideran productos agrícolas estrictamente los procedentes de la explotación forestal. Menos habitual es la distinción con los productos procedentes de la recolección, que en algunos casos es todavía una actividad económica estimable (por ejemplo, la recolección de setas –que propiamente no son vegetales, sino hongos–).

Según el destino que se de al producto, puede hacerse una división entre productos agrícolas alimentarios y productos agrícolas industriales. De los alimentarios, los más importantes (por ser la base de la alimentación humana y de la ganadería), destacan los cereales (trigo, arroz, maíz,tomate etc.); la patata y otros tubérculos; legumbres; las plantas oleaginosas (olivo, girasol, soja, colza); la vid y otras plantas susceptibles de producir distintas bebidas alcohólicas; las plantas azucareras; y los productos hortofrutícolas. De los industriales, imprescindibles para muchos procesos industriales, destacan las materias primas para la industria textil, como el algodón, el lino textil, el esparto, etc.; y otros de gran importancia económica, como el caucho y el tabaco. Las plantas tintóreas, que fueron de gran importancia hasta la Revolución Industrial, han sido sustituidas por tintes químicos. La producción de biocombustibles a partir de restos vegetales o cultivados expresamente para ello ha sido objeto de gran desarrollo en los últimos años.

No debe confundirse producción agrícola con producción agraria, que incluye, además de los productos de la agricultura, los de las demás actividades agrarias, especialmente la ganadería. Otro concepto confluyente es el de la totalidad de los productos del campo o productos rurales (lo rural). Estrictamente, la producción rural también incluye los productos de la industria rural, especialmente los de la industria alimentaria local o tradicional y los de la artesanía rural.

Tampoco se debe confundir con la aportación del sector primario a la producción total (PIB o PNB según cómo se considere), que suele dividirse en los tres sectores de la economía, puesto que el sector primario incluye, además, la pesca.Para que los agricultores puedan producir alimentos hacen uso de dos recursos naturales importantes:  
el suelo y el agua. Esta labor a menudo la realizan,  en áreas donde la topografía es montañosa con altas pendientes donde se requieren prácticas para el manejo de la escarnecía y control de erosión.  El suelo y el agua son también recursos vitales para toda actividad humana. Por tal razón el agricultor debe conocer las prácticas recomendadas para el uso efectivo y la conservación de estos recursos esenciales.

Los tipos de agricultura pueden dividirse según muchos criterios distintos de clasificación:

Según su dependencia del agua:

Según la magnitud de la producción y su relación con el mercado:

Según se pretenda obtener el máximo rendimiento o la mínima utilización de otros medios de producción, lo que determinará una mayor o menor huella ecológica:

Según el método y objetivos:

El impacto ambiental de la agricultura es el efecto que las diferentes prácticas agrícolas tienen sobre el medio ambiente. El impacto ambiental de la agricultura varía de acuerdo a los métodos, técnicas y tecnologías utilizadas, y la escala de la producción agrícola. La agricultura en general impacta sobre el suelo, el agua, el aire, la biodiversidad, las personas, las plantas y su diversidad genética, la calidad de la comida y los hábitats.

La agricultura contribuye al incremento de gases de efecto invernadero por la liberación de CO2 relacionado con la deforestación, la liberación de metano del cultivo de arroz, la fermentación entérica en el ganado y la liberación de óxido nitroso de la aplicación de fertilizantes.[10]​ Todos estos procesos juntos componen el 54% de emisiones de metano, aproximadamente el 80% de emisiones de óxido nitroso, y casi todas la emisiones de dióxido de carbono relacionados con el uso de tierras.[11]​ La agricultura industrial es la principal contribuyente de metano y óxido nitroso a la atmósfera terrestre.[12]​ Además, la agricultura industrial impacta en el ambiente debido al uso intensivo de agroquímicos, la contaminación del agua y la aparición de zonas muertas, la degradación del suelo, la producción de desechos y la contaminación genética.

El sector agropecuario es uno de los principales emisores de gases de efecto invernadero, que junto con los efectos del uso de tierras, están entre las principales causas del calentamiento global.[13]​ Además de ser un importante usuario de tierras y consumidor de combustibles fósiles, la agricultura y la ganadería contribuyen directamente a las emisiones de gases de efecto invernadero por medio de las técnicas empleadas para el cultivo de granos y monocultivos, y la cría de ganado.[14]​ El sistema agroalimentario global actual es responsable de cerca de la mitad (entre 44 % y 57 %) de todas las emisiones de gases con efecto de invernadero producidas por actividades humanas.[15]​ Esta cifra se compone de la contribución de las emisiones agrícolas —las emisiones producidas en los campos de cultivo— de entre el 11 y el 15 %; un 15-18 % producidas por el cambio en el uso del suelo y la deforestación ocasionada por la agricultura; entre un 15 y 20 % de emisiones proveniente del procesamiento y el empacado de los productos agrícolas y entre un 3.5 y 4.5 % proveniente de los desechos.

La concentración parcelaria, la deforestación y el drenaje de marismas para la explotación agrícola reducen la superficie disponible para la vida silvestre y fragmentan los hábitat naturales. Los plaguicidas y herbicidas destruyen gran número de insectos y plantas no deseadas, por lo que afectan a especies más grandes que ven reducidas sus fuentes de alimentos. Estas formas de vida que se ven afectadas pueden ser importantes recicladores de nutrientes del suelo, polinizadores de cultivos y predadores de insectos dañinos. Es decir, la pérdida de biodiversidad comienza con la fase de preparación de la tierra para el desarrollo agrícola y continúa después. Esta pérdida de biodiversidad es una constante que no se reduce ni siquiera en aquellos países que valoran y protegen la naturaleza. Asimismo, la degradación de la tierra, la salinización y el exceso de extracción de agua provocados por la agricultura afectan a la base de su propio futuro.[17]​

Muchos de estos problemas van agotando y desertizando el suelo, obligando a abandonar unos terrenos para arar otros nuevos que, a su vez, se agotan, creando un círculo vicioso que va destruyendo el entorno. Un ejemplo claro es la progresiva deforestación de la selva del Amazonas.

La agricultura es también una importante fuente de contaminación del aire y de gases de efecto invernadero. El amoniaco es una de las causas principales de la lluvia ácida que daña los árboles, acidifica los suelos, los lagos y los ríos, perjudicando la biodiversidad. Las emisiones de amoniaco procedentes de los fertilizantes minerales representan aproximadamente el 16% y la combustión de biomasa y residuos de cultivos el 18%. Asimismo, la combustión de biomasa de plantas provoca la emisión de otros potentes contaminantes del aire, como dióxido de carbono, óxido nitroso y partículas de humo. Los seres humanos son responsables aproximadamente del 90% de la combustión de biomasa, principalmente por la quema deliberada de vegetación forestal, asociada con la deforestación, y residuos de pastos y cultivos para favorecer el crecimiento de nuevos cultivos y destruir hábitats de insectos dañinos. El cultivo de arroz es otra fuente agrícola importante de metano, que representa aproximadamente una quinta parte del total de las emisiones.[17]​

La agricultura, especialmente la agricultura intensiva o industrial, tiene un impacto negativo sobre el suelo. Entre los problemas frecuentes se pueden mencionar:

Los pesticidas, herbicidas y otro tipo de agroquímicos afectan de varias maneras al ecosistema. Algunas de estos problemas incluyen:

Las maquinarias son elementos que se utilizan para dirigir la acción realizada por las fuerzas de trabajo a base de energía; por su parte en el campo agrícola, los mecanismos a motor que se emplean en estas labores aligeran la producción y mejoran las técnicas de cultivo. Entre las máquinas agrícolas más utilizadas en las labores del campo se mencionan:

Los equipos agrícolas son un grupo de aparatos diseñados para abrir surcos en la tierra, desmenuzar, fumigar y fertilizar en el suelo.

Las herramientas agrícolas son instrumentos que se utilizan para labrar la tierra, cargar arena, deshierbar, remover la tierra, abrir zanjas, transportar abono o material, etc. Son muchas y muy variadas las herramientas agrícolas, entre las que se mencionan:

La diferencia es que las maquinarias se encargan de remover la tierra, mientras que los equipos se encargan de ayudar al terreno, de deshacerse de lo que no debería estar en la tierra, y las herramientas ayudan a transportar y excavar para sembrar un nuevo cultivo.

La importancia que existe en:

La política agraria es muy compleja debido a la necesidad de equilibrar la ecología, las necesidades del país y los problemas sociales de quienes viven del campo.

La agricultura es un tema clave en la lucha por la justicia global. A pesar de existir un exceso de comida en los mercados mundiales, que hace que los precios caigan de forma continuada, aún no se ha resuelto el problema del hambre en el mundo. La rápida pérdida de tierras cultivables y la disminución de la cantidad de agua dulce disponible, de la que un 70 % se utiliza para la agricultura, son hoy una de las principales causas de la pobreza. La lucha contra el hambre que sufren 800 millones de seres humanos no es posible sin una profunda reforma de la política agraria global.

Los países ricos protegen a sus agricultores, bien a través de subvenciones a la producción, bien a través de fuertes aranceles a los productos extranjeros. Esto causa que los agricultores de países pobres se vean incapaces de competir en igualdad, por lo que actualmente existe una gran oposición por parte de muchos sectores a estos apoyos.

Las patentes otorgadas a las compañías que desarrollan nuevos tipos de semillas por ingeniería genética han permitido que se licencien a los agricultores las semillas de forma muy similar a la utilizada para licenciar software. Esto ha cambiado la balanza de poder en favor de los fabricantes de semillas, que pueden ahora dictar términos y condiciones antes imposibles. Debido a que si el agricultor no accede a las demandas de la compañía, esta no le vende la semilla. Esto ha hecho que muchos les acusen de biopiratería, ya que muchas de estas empresas se dedican a investigar las propiedades de las plantas, partiendo de conocimientos milenarios. Dándose la paradoja de que al patentar estos conocimientos, obligando a los pueblos de los que han aprendido dicho conocimiento, a pagarles por su uso.

Con objeto de impulsar las exportaciones de productos agrícolas, diversos organismos gubernamentales publican estudios económicos por productos y por países, a través de internet. Entre otros, se encuentran el FAS del Departamento de Agricultura de los Estados Unidos (USDA), Agricultura y Agroalimentario Canadá (AAFC), Austrade y NZTE, que representan cuatro de los países más importantes a nivel de exportación de productos agrícolas. La Federación de Asociaciones de Comercio Internacional publica estudios de FAS y AAFC, así como de otros organismos no gubernamentales, en su página web globaltrade.net.[18]​

La filosofía de la agricultura es, a grandes rasgos, una disciplina dedicada a la crítica sistemática de los marcos filosóficos (o cosmovisiones éticas) que son la base para las decisiones relativas a la agricultura.[19]​

Muchos de estos puntos de vista también se utilizan en la toma de decisiones sobre el uso del suelo en general. En el uso diario, puede definirse como la búsqueda de la sabiduría asociada a la agricultura, como uno de los componentes que fundan la civilización.[20]​

La agricultura se representa del mismo modo que a la diosa Ceres, coronada de espigas, con un arado al lado y un arbusto que empieza a florecer. Algunas veces tiene un cuerno de la abundancia lleno de toda clase de frutos y ambas manos sobre una pala o azada. Otros la pintan apoyada sobre el Zodiaco, para significar que las estaciones arreglan los trabajos de la agricultura y revestida de un ropaje verde, símbolo de la esperanza.

En algunas medallas es representada con una mujer que tiene echados a sus pies un león y un toro, el uno emblema de la tierra y el otro, de la labranza. En una piedra grabada de la biblioteca del Vaticano, se ve representada la Agricultura por Psiquis, apoyándose, en una pala como trabajo en que el alma encuentra lugar para la meditación. El genio de la Agricultura se simboliza por medio de un niño desnudo, de una fisonomía risueña y coronado de adormideras; en una mano tiene un manojo de espigas y en la otra un racimo de uvas.[21]​



Los pueblos germánicos o germanos son un histórico grupo etnolingüístico de los pueblos originarios del norte de Europa que se identifican por el uso de las lenguas germánicas (un subgrupo de la familia lingüística indoeuropea que se diversificaron a partir de una lengua original —reconstruible como idioma protogermánico— en el transcurso de la Edad de Hierro). En términos historiográficos son tanto un grupo de entre los pueblos prerromanos (en las zonas germanas al oeste del Rin —provincias de Germania Superior e Inferior— en que se estableció una fuerte presencia del Imperio romano y fueron romanizados) como un grupo de pueblos bárbaros (exteriores al limes del Imperio), situados al este del Rin y al norte del Danubio (Germania Magna); precisamente el que protagonizó las denominadas invasiones germánicas que provocaron la caída del Imperio romano de Occidente al instalarse en amplias zonas de este: suevos, vándalos, godos (visigodos y ostrogodos), francos, burgundios, turingios, alamanes, anglos, sajones, jutos, hérulos, rugios, lombardos, catos, téncteros, etc. Los vikingos protagonizaron posteriormente una nueva oleada expansiva desde Escandinavia (la zona originaria de todo este grupo de pueblos), que afectó a las costas atlánticas (normandos) y a las estepas rusas y Bizancio (varegos). 

Algunos pueblos germánicos como los francos y visigodos se fusionaron con la población romana dominante demográficamente en las zonas que ocuparon de Europa suroccidental (galo-romanos, hispano-romanos); mientras que otros se convirtieron en la base etnográfica de las actuales poblaciones de Europa central y noroccidental (escandinavos o nórdicos –la mayor parte de los países nórdicos: daneses, suecos, noruegos, islandeses, y los isleños de las Islas Feroe, con excepción de bálticos, fineses y lapones–, alemanes –en el sentido del ámbito lingüístico alemán, que incluye a los austriacos, la mitad de los suizos y otros grupos de habla alemana de la Europa central y oriental desde Francia hasta el Cáucaso–, las poblaciones de habla neerlandesa –noroeste de Alemania, Países Bajos y norte de Bélgica– y anglosajona). En la Europa oriental los pueblos germánicos se vieron desplazados por otros (especialmente los pueblos eslavos y los magiares), para pasar posteriormente a protagonizar una nueva fase expansiva.

Las migraciones de los pueblos germánicos se extendieron por toda Europa durante la Antigüedad tardía (Völkerwanderung) y la Edad Media (Ostsiedlung). Estos términos historiográficos se concibieron y utilizaron de forma no neutral, sino como justificación del expansionismo alemán hacia el este en la edad contemporánea (Drang nach Osten).

También en el ámbito religioso se produjo una fusión de los elementos germánicos y romanos: algunos ya habían sido cristianizados bajo credo arriano en Oriente en el siglo IV, y otros continuaban con las religiones nórdicas tradicionales. La conversión al catolicismo de suevos, visigodos y francos en el siglo VI fue clave para su éxito en la formación de sus respectivos reinos germánicos. Hacia el siglo XI todos los pueblos germánicos, inclusive los escandinavos, estaban incluidos en el ámbito de la cristiandad latina.

Las lenguas germánicas se convirtieron en dominantes a lo largo de las fronteras romanas (Austria, Alemania, Países Bajos, Bélgica e Inglaterra), pero en el resto de las provincias romanas occidentales los germanos adoptaron el latín, que se estaba transformando en las diferentes lenguas romances. Actualmente, las lenguas germánicas se hablan en gran parte del mundo, representadas principalmente por el inglés, el alemán, el neerlandés, el afrikáans (hablado en Sudáfrica y Namibia) y las lenguas escandinavas.

Germani, plural nominativo del adjetivo germanus, es el etnónimo con el que los romanos se referían a los habitantes de la extensa e indefinida zona que conocían con el topónimo Germania, desde la Galia hasta la Sarmatia. Tal nombre no se usó en la literatura latina hasta Julio César, quien lo adoptó a partir del vocablo que los galos usaban para designar a los pueblos de la orilla occidental del Rin, y que en las lenguas goidélicas probablemente significa "vecino".

El término parece haberse empleado previamente en la inscripción de Fasti capitolini para el año 222 (DE GALLEIS INSVBRIBVS ET GERM[ANEIS]) donde simplemente se refiere a pueblos "asociados", como los relativos a los galos. Por otra parte, puesto que las inscripciones se levantaron solo en 17-18 a. C., la palabra puede ser una adición posterior al texto. Otro de los primeros que citan el nombre, Posidonio (alrededor del 80 a. C., también es una fuente dudosa, ya que solo sobrevive en una cita de Ateneo (alrededor del 190 d. C.); la mención de germani en este contexto pudo ser más probablemente introducido por Ateneo y no provenir del texto inicial de Posidonio.[1]​

El escritor que al parecer introdujo el término en el corpus de la literatura clásica fue Julio César, en De bello Gallico (50 a. C.). Él usaba germani para designar a dos agrupaciones de pueblos diferentes: los germanos transrenanos (trans Rhenum), el conjunto de pueblos claramente no galos de la Germania transrhenana ("el otro lado del Rin" —Transrenania, la que posteriormente se denominó Germania Magna, interna—[2]​ o bárbara[3]​); y los germanos cisrenanos (cis Rhenum), un grupo difuso de pueblos del noreste de la Galia (la Germania cisrhenana, "este lado del Rin" —Cisrenania, donde se establecerían en la Germania Superior y la Germania Inferior, es decir, la Germania romana—[4]​), que no puede ser claramente identificado como celta ni como germánico.[5]​
El vocablo germani puede ser un préstamo de un exónimo celta aplicado a las tribus germánicas, sobre la base de una palabra que tanto puede significar "vecino" como "hombres de los bosques" (refiriéndose a la densidad de los bosques que cubrían casi todo el territorio de Germania). Tácito sugiere que el nombre podría ser el de una tribu que cambió su nombre después de que los romanos lo adaptaron, pero no hay pruebas de ello.

La sugerencia de derivar el nombre del término goidélico para "vecino" invoca al gair antiguo irlandés, ger galés ("cerca") o gearr irlandés moderno ("atajo, corto" –una corta distancia–); de raíz protocelta *gersos, más relacionada con el griego antiguo chereion ("inferior") o el inglés gash ("cuchillada" o "brecha"). La raíz protoindoeuropea pudo haber sido de la forma *khar-, *kher-, *ghar-, *gher- ("corte"), de la que también provendría la hitita kar- y la griega kharakter ("grabar", que da la palabra latina character y la española "carácter"). Otras posibilidades serían la palabra céltica que significa "ruidoso" o las germánicas gar y ger —también la céltica gae— que significan "lancero".[6]​

Al parecer, las tribus germánicas no tenían un endónimo (autodesignación) que incluyera a todos los pueblos que se identificaban a sí mismos como provenientes de un tronco común (por identificación lingüística o por identificación ancestral). No obstante, el término latino suevi (suevos), con el que en los últimos siglos del Imperio se designó a uno de los grandes grupos de entre los pueblos germánicos, se utilizaba de forma casi indistinta en los textos de César con germani; y sí que tiene una clara etimología germánica: *swē-ba- ("auténtico").

Lo que sí existía era un término para referirse a la totalidad de los pueblos no germanos, y que los germanos aplicaban principalmente a aquellos con los que tenían algún contacto (fueran celtas, romanos u otros pueblos del Imperio, como los griegos), a los que llamaban *walha- (walhoz en singular, walhaz en plural), raíz de la que derivan los topónimos de Gales, Valonia y Valais.[7]​

Tratando de identificar un término vernáculo contemporáneo y la nación asociada a un nombre clásico, desde el siglo X en adelante la literatura latina medieval usó el adjetivo teutonicus, originalmente aplicado a los teutones (el pueblo germánico antiguo derrotado por los romanos en la batalla de Aquae Sextiae —102 a. C.—) para referirse a todo lo relativo al Regnum Teutonicum o Francia orientalis ("Francia Oriental", la parte oriental del antiguo Imperio carolingio, tal como se dividió en los tratados de Verdún y de Mersen —años 843 y 870, respectivamente—). Específicamente, la Orden Teutónica o de los Caballeros Teutones fue una orden militar de gran importancia en la Europa Oriental.

El uso de las palabras castellanas "teutón" y "teutónico" no se limita al pueblo antiguo o a la orden militar, sino que se extiende de forma genérica a lo germánico o a lo alemán. El Diccionario de la lengua española recoge ese uso como un coloquialismo.[8]​

En castellano, portugués y francés, el gentilicio de Alemania (alemán, alemão, allemand)[9]​ se deriva del nombre del pueblo germánico de los alamanes, cuya etimología puede relacionarse con all (todo) y mann (hombre).[10]​

En lengua alemana, el gentilicio de Alemania es deutsch («alemán»),[11]​ una palabra derivada de una raíz genérica del germánico antiguo: *þiuda-, que significa «pueblo». La misma raíz aparece en muchos nombres de persona, como Thiud-reks, y también en el etnónimo de los suecos de un cognado del inglés antiguo Sweo-ðēod y nórdico antiguo: Suiþióð. Además þiuda- aparece en los términos Angel-ðēod («pueblo anglosajón») y Gut-þiuda («pueblo gótico»). El adjetivo derivado de este sustantivo, *þiudiskaz («popular»), fue utilizado posteriormente (el primer uso registrado es del año 786) para referirse a la theodisca lingua, «lengua del pueblo» (lengua vulgar), por oposición a la lengua latina.

Muchos idiomas modernos emplean palabras derivadas de este origen para el gentilicio de Alemania: la sueca/danesa/noruega tysk, las neerlandesas duits y diets (esta última se refiere al nombre histórico para el neerlandés medio o neerlandés, el antiguo significado en alemán), la italiana tedesco y la española «tudesco» (que se ha restringido en la práctica a su uso como arcaísmo). En cambio, en inglés, Dutch se aplica al gentilicio de Holanda (o, por extensión, al de los Países Bajos), usándose German para el gentilicio de Alemania. El despectivo boche se utilizó en el contexto histórico de las guerras mundiales del siglo XX, pero su etimología parece provenir de alboche, una combinación particular en argot francés.[12]​

Las pruebas arqueológicas muestran a los pueblos germánicos como originarios de la zona de Escandinavia y, a lo largo de un proceso secular, extendiéndose hacia el sur y el este por la Europa Central y Oriental.

En un contexto cultural de sociedades cazadoras-recolectoras se sitúan el maglemosiense y la cultura Fosna-Hensbacka (VII milenio a. C.), y posteriormente la cultura de Kongemose (VI milenio a. C.). Del V milenio a. C. al III milenio a. C. se desarrollaron las culturas neolíticas de la zona (Ertebølle, cultura de la cerámica perforada, cultura de los vasos de embudo) que en su última fase, según la hipótesis del sustrato germánico (nordwestblock),[13]​ habrían recibido el impacto cultural de lo indoeuropeo (cultura de la cerámica cordada).

En el II milenio a. C. se desarrolló la Edad del Bronce nórdica. En el I milenio a. C., las culturas de la Edad del hierro, como Wessenstedt y Jastorf, significaron ya el paso de lo protoindoeuropeo a lo protogermánico (Ley de Grimm). El endurecimiento climático que se produjo desde el 850 a. C., que se intensificó a partir del 760 a. C., desencadenó un proceso migratorio hacia el sur. La cultura material de esa época pone en estrecha relación a los protogermanos con las culturas de Hallstatt y Elp, en el ámbito cultural celta, forjando lo que se ha denominado Edad del hierro prerromana de Europa septentrional.[14]​

La zona norte de Europa fue visitada probablemente por viajeros griegos, como los que dieron origen al periplo massaliota (siglo VI a. C., recogido posteriormente en la Ora Maritima); pero el único testimonio de tales viajes es el periplo de Piteas por el mar del Norte, incluyendo la enigmática Thule (siglo IV a. C.). Su descripción de los pueblos que habitaban la zona fue la fuente prácticamente única que pudieron manejar los autores griegos posteriores, como Estrabón y Diodoro Sículo, o los romanos como Plinio el Viejo, que tendieron a citarlo con escepticismo.[15]​ De hecho, los textos griegos antiguos no dejan constancia de la existencia de algún grupo de pueblos con el nombre de "germanos", y suelen referirse a los habitantes de la Europa más al norte de la zona mediterránea con etnónimos genéricos como galos y escitas; aunque algunas referencias de Heródoto a los cimerios podrían referirse a algún grupo relacionado con lo que posteriormente se conoció como pueblos germanos.

En cuanto a los romanos, tuvieron conocimiento de dos de estos pueblos, cuando los cimbros y los teutones entraron en Helvecia y la Galia. Los romanos no se enfrentaron con ejércitos, sino con pueblos enteros que desplazaban a los celtas. A pesar de esto, no utilizaron el término germano hasta tiempos de Julio César.[5]​

En 112 a. C. las tribus invasoras derrotaron en la batalla de Noreya a los romanos comandados por el cónsul Cneo Papirio Carbón. Entonces los cimbros se establecieron en el territorio de los celtas alóbroges. Solicitaron a los romanos permiso para establecerse allí, pero estos se negaron, por lo que tuvieron que pelear de nuevo. En el año 109 a. C. volvieron a vencer al ejército romano, esta vez al mando de Marco Junio Silano en el sur de la Galia. Sin embargo, los cimbros no invadieron la península itálica y durante un tiempo se mantuvieron alejados de la esfera de influencia de Roma. El rey cimbro Boiorix derrotó en 105 a. C. en la batalla de Arausio a los romanos bajo las órdenes del procónsul Quinto Servilio Cepión y el cónsul Cneo Malio Máximo, perdiendo unos 80 000 hombres.

Los cimbros decidieron no invadir Italia y se desplazaron a Hispania, mientras que los teutones se quedaron en el sur de la Galia. En el año 103 a. C., los cimbros regresaron a la Galia —expulsados de Hispania por los celtíberos— y se aliaron con los teutones, decididos a conquistar Roma. En vista de lo grande de los ejércitos, decidieron separarse y reunirse en el valle del Po. Esto demostró ser un grave error, ya que en el año 102 a. C. los teutones fueron aniquilados por el cónsul Cayo Mario en la batalla de Aquae Sextiae, donde cayeron 100 000 teutones. Los cimbros sí lograron llegar al valle del Po, pero se encontraron con los ejércitos unidos de los cónsules Quinto Lutacio Cátulo y Mario. En la Planicie de Raudine se libró la batalla de Vercelas, que concluyó con la muerte del rey Boiorix y más de 60 000 cimbros.

La pacificación de los germanos obtenida por Cayo Mario se mantuvo por casi cinco décadas, hasta que Julio César inició la guerra de las Galias.

Los suevos habían cruzado el Rin y expulsado a los celtas, quienes pidieron ayuda a los romanos en 58 a. C. y César acudió, con el secreto deseo de anexar la Galia a Roma. César los derrotó y los envió de vuelta al este del Rin.

Nuevas tribus germánicas cruzaron el Rin en el 55 a. C., pero César las expulsó y construyó un puente sobre el río, que utilizó para perseguir a sus enemigos y derrotarlos. En el 53 a. C. cruzó de nuevo el Rin para seguir combatiendo a los germanos, pero estos lo evadieron y César regresó sin presentar batalla.

La política expansiva romana en Germania sufrió en tiempos de Augusto la gran humillación de la batalla del bosque de Teutoburgo (año 9), en la que el caudillo querusco Arminio, encabezando una coalición de pueblos germanos, exterminó a tres legiones comandadas por Publio Quintilio Varo. Uno de los miembros de la familia imperial, Germánico (que se ganó su cognomen por esta acción), fue el encargado de pacificar la zona (batalla de Idistaviso, año 16). A partir de entonces se prefirió seguir una política de contención, creando una frontera fortificada, el Limes Germanicus («límite» o «frontera»), a lo largo del Rin y el Danubio.

La conquista romana de Germania llevó a la organización de dos provincias en el territorio germano bajo dominio romano al oeste del Rin: Germania Superior y Germania Inferior. Germania Magna, al otro lado del Rin y el Danubio, quedó sin ocupar. A partir de asentamientos indígenas previos, campamentos romanos o de colonias, surgieron ciudades como Augusta Treverorum (Tréveris), Colonia Claudia Ara Agrippinensium[16]​ (Colonia), Mogontiacum (Maguncia), Noviomagus Batavorum (Nimega) o Castra Vetera (Xanten). La provincia alpina de Raetia (al sur de Rin y el alto Danubio) y las danubianas del Nórico y Panonia no tenían como sustrato indígena a pueblos germánicos, sino a los reti[17]​ (de clasificación incierta —itálicos o celtas—) y los panonios[18]​ (vinculados a los celtas y los ilirios).

El conocimiento de los romanos sobre los germanos fue intensificándose con el tiempo, como demuestra el mayor detalle con que los recogen progresivamente las principales fuentes historiográficas: Tácito en el año 98 redacta su Germania (De origine ac situ Germanorum, "Origen y territorio de los germanos").[19]​ Nombra unos cuarenta pueblos, identificándolos como pertenecientes a varios grupos dentro de los germanos occidentales, descendientes de Mannus (ingaevones —de Jutlandia y las islas adyacentes—, hermiones —del Elba— y istvaeones —del Rin—). A estos grupos hay que añadir los germanos septentrionales (de la península escandinava) y los germanos orientales (del Oder y el Vístula). Plinio el Viejo, en Naturalis Historia[20]​ (hacia el año 80), clasifica a los germanos en cinco confederaciones: ingvaeones, istvaeones, hermiones, vandili (vándalos) y peucini (bastarnos), de cada una de las cuales precisa los pueblos que las componen. Claudio Ptolomeo, en su Geographia (hacia el año 150), nombra a sesenta y nueve pueblos germánicos.

Las guerras marcomanas del siglo II incrementaron los contactos entre romanos y germanos. Aun así, el léxico de origen germánico que se usa por los autores latinos hasta el siglo V es muy escaso: en César urus y alce, en Plinio ganta y sapo (oca, jabón), en Tácito framea (lanza), en Apicio melca y en Vegecio burgus (castillo —castellum parvum quem burgum vocant—, que tendrá una extensa utilización posterior como sufijo en topónimos).[21]​

Desde la crisis del siglo III, y especialmente en la anarquía militar (235-285), Roma estuvo sumida en un periodo de caos y guerras civiles. Las fronteras, debilitadas, no fueron un obstáculo para la penetración de los germanos, que simultáneamente se desplazaban de forma paulatina en busca de nuevas tierras, presionados por su propia demografía. En esa época llegaban quizá a los 6 millones de personas, un millón de las cuales se desplazaron hacia el este, la actual Ucrania. Los que emigraron hacia el sur y el oeste, "invadiendo" el Imperio romano, divididos en pequeños grupos, en total llegarían a unas doscientas mil.[22]​

Las provincias occidentales del Imperio sufrieron una primera oleada de invasiones simultáneamente a la crisis socioeconómica que se manifestaba en las rebeliones campesinas (bagaudas).[23]​ En Oriente fueron los godos quienes inicialmente protagonizaron la principal amenaza. Divididos en grupos de godos orientales (ostrogodos) y de godos occidentales (visigodos), se introdujeron al sur del Danubio en los Balcanes y obtuvieron todo tipo de concesiones de las autoridades imperiales: en el año 376 se les concede su entrada pactada, pero al sentirse defraudados en sus expectativas, se dedicaron al saqueo, consiguiendo incluso vencer al ejército imperial de Valente en la batalla de Adrianópolis (378). Esto puso a los godos en una posición extraordinariamente ventajosa, que obligó al nuevo emperador, Teodosio, a concederles un foedus para su asentamiento en la Tracia (382).[22]​ Su prolongada presencia dentro de las fronteras les permitió asimilar rasgos de la civilización romana, como la religión, adoptando el arrianismo (una de las versiones del cristianismo que, posteriormente, en el Concilio de Constantinopla de 381, fue condenada como herética). El proceso de aculturación incluso significó la adquisición de la ciudadanía romana por muchos de los considerados bárbaros, o su acceso a altos cargos de la administración romana y del ejército; pero no la asimilación, ni la disminución de la conflictividad. Todo lo contrario: en el 410 los visigodos de Alarico I saquearon la propia ciudad de Roma, obteniendo un mítico botín.

El invierno particularmente frío del año 406 permitió cruzar el Rin helado a grupos masivos de suevos y vándalos (junto con los alanos, un pueblo no germánico, sino iranio). Los emperadores de la época recurrieron a ficciones jurídicas como otorgarles el permiso de ingreso, bajo las condiciones teóricas de que deberían actuar como colonos y trabajar las tierras, además de ejercer como vigilantes de frontera; pero el hecho fue que la decadencia del poder imperial impedía cualquier tipo de dominio. Los invasores no encontraron obstáculo en su avance hacia las ricas provincias meridionales de Galia e Hispania. Los vándalos incluso cruzaron el estrecho de Gibraltar, tomando las provincias africanas y amenazando las rutas marítimas del Mediterráneo occidental. El imperio tuvo que recurrir a los visigodos, los más romanizados de entre los germanos, para intentar recuperar algún tipo de control sobre las provincias occidentales. Los visigodos, en efecto, se impusieron sobre los invasores, pero únicamente para establecerse a su vez como un reino independiente (reino de Tolosa, 418) justificado en la figura jurídica del foedus.

Una nueva invasión fue protagonizada por Atila, el rey de los hunos (un enigmático pueblo o confederación de pueblos, cuyo desplazamiento secular hacia el oeste estuvo probablemente en el origen del movimiento inicial de los germanos). Tras acosar al Imperio romano de Oriente, que solo le enfrentó mediante una política de apaciguamiento, se dirigió a Occidente, donde una inestable coalición de romanos y germanos le venció en la batalla de los Campos Cataláunicos (451).

Después de la descomposición del imperio de Atila, nuevas oleadas invasoras se establecieron los territorios que ya solo de nombre podían considerarse provincias romanas: desde mediados del siglo V (batalla de Guoloph, 439, batalla del Monte Badon, 490) anglos, sajones y jutos desembarcaban en la Britania posromana, inicialmente como mercenarios para proteger a los britanos de escotos y pictos y luego como conquistadores;[24]​ a comienzos del siglo VI los francos tomaron las Galias, venciendo a losv isigodos en la (batalla de Vouillé (507), cerca de Poitiers, batalla en la que murió el rey Alarico II, desplazando a los visigodos a Hispania, dando origen al Reino Visigodo de Toledo. Por otro lado en la península itálica, la ficción de la pervivencia del Imperio había dejado existir desde 476, cuando los hérulos de Odoacro destituyeron al último emperador romano, Rómulo Augústulo. Su dominio fue breve, pues se vieron acometidos a su vez por sucesivas invasiones instigadas por el emperador de Oriente (Zenón): en 487 y 488 la de los rugios de Feleteo y Federico, que logran rechazar; y finalmente la de los ostrogodos de Teodorico el Grande, que los derrotan en Aquilea, Verona (489) y el río Adda (490), quedando sitiado Odoacro en Rávena hasta su asesinato a manos del propio Teodorico (493).[25]​

Tanto visigodos como francos obtuvieron el extraordinario beneficio que suponía la aplicación extensiva del concepto de hospitalitas (la asignación al huésped de la tercera parte del patrimonio del anfitrión), lo que en la práctica significó cederles la tercera parte de las tierras que ocupaban en las Galias. Los hérulos de Odoacro exigieron lo mismo en Italia, y ante la respuesta negativa de las autoridades romanas, optaron por aclamar a su jefe como "rey de Italia".

Durante todo el siglo V, el ejército romano y, en gran medida, la dirección política del Imperio occidental, estuvieron en manos de personalidades de origen germano: Estilicón (de origen vándalo, fue clave durante el imperio de Honorio), Aecio (de oscuro origen —godo o escita— fue el artífice de la coalición anti-Atila), Ricimero (mitad suevo, mitad visigodo, llegó a proclamar tres emperadores —Mayoriano, Libio Severo y Olibrio—), Gundebaldo (burgundio, sobrino de Ricimero, proclamó a su vez otro emperador —Glicerio—), Orestes (depuso a Julio Nepote e impuso como emperador a su propio hijo, Rómulo Augusto) y Odoacro (habitualmente designado como hérulo, pero cuya concreta nacionalidad se ignora —pudo ser también rugio, godo, esciro o incluso huno—, depuso a Rómulo Augusto e hizo devolver las insignias imperiales a Zenón —emperador de oriente—, quedando como único poder de hecho en Italia).[26]​

Los distintos pueblos germánicos se asentaron en diferentes zonas del antiguo Imperio romano de Occidente, fundando reinos en los que los germanos pretendieron inicialmente segregarse como una élite social separada de la mayoría de la población local. Con el tiempo, los más estables de entre ellos (visigodos y francos) consiguieron la fusión de las dos comunidades en los aspectos religioso, legislativo y social. Fueron reinos germánicos desde el siglo V hasta finales del VIII: el Reino Suevo (409-585), el Reino Visigodo de Tolosa (476-507), el Reino Visigodo de Toledo (507-711), el Reino Franco (481-843)y el Reino Ostrogodo (493-553).

La diferencia cultural y de grado de civilización entre los pueblos germánicos y el Imperio romano era muy notable, y su contacto produjo la asimilación por los germanos de muchas de las costumbres e instituciones romanas, como el Derecho Romano, que siguió aplicándose como fundamento del derecho visigodo en el Liber Iudiciorum, también llamado Lex Visigothorum o Código de Recesvinto, mientras que se conservaron otras leyes y costumbres propias de sus antiguas tradiciones e instituciones, formando así la cultura que se desarrolló en la Europa medieval y que es la base de la actual civilización occidental.[27]​

Sin duda el rasgo más definitorio de los germanos es el lenguaje, ya que el concepto es ante todo etnolingüístico. No obstante, aunque las lenguas germanas antiguas eran cercanas entre sí, los germanos no hablaban la misma variante, sino variedades diferentes derivadas del protogermánico.

Aunque aparentemente compartían una lengua ancestral común, al momento de su avance sobre el interior europeo ya tenían varios dialectos: el protonórdico, los dialectos germanos occidentales y los dialectos germanos orientales.

No tenían alfabeto (el rúnico de los escandinavos se usaba solo para fines religiosos), por lo que no hay registros escritos de su historia hasta su encuentro con los romanos.

La traducción parcial de la Biblia del obispo Ulfilas (el Codex Argenteus —un evangeliario—) es el primer texto escrito en una lengua germánica (el gótico). Para su escritura creó los caracteres de un "alfabeto ulfilano", precedente del posterior "alfabeto gótico".

La arquitectura germánica hasta finales del siglo VIII destaca en dos pueblos que sobresalen culturalmente sobre los demás al estar más romanizados, ostrogodos en la península itálica y visigodos en Hispania. Sus construcciones, tienen una fuerte influencia del antiguo Imperio Romano.

El Mausoleo de Teodorico el Grande es un antiguo monumento de Ravena (Italia). Fue construido en el 520 D.C.

El Palacio de Teodorico el Grande, también en Ravena, fue construido con dos plantas, es de estilo romano y de él solo se conserva la primera crujía y la fachada, tiene una composición simétrica basada en arcos y columnas monolíticas de mármol, que fueron reutilizadas de edificios romanos anteriores. Los capiteles son de distintos tipos y tamaños para adaptarse a las diferentes alturas de los fustes.[28]​Los ostrogodos restauraron edificios romanos, algunos han llegado hasta nosotros gracias a ellos.

Han dejado construcciones religiosas que han sobrevivido a la conquista musulmana de la península ibérica, por estar alejadas de los núcleos urbanos, pues era frecuente reutilizar los sillares, para construir murallas, castillos, etc. desmontando los edificios visigodos existentes hasta el año 711. Es característico de la arquitectura visigoda el arco de herradura, que más tarde sería adoptado por los musulmanes. Se pueden citar iglesias como: San Pedro de la Nave, en la localidad de El Campillo (Zamora), del siglo VII, la iglesia de Santa María de Melque, en San Martín de Montalbán (Toledo) la iglesia de San Juan, en Baños de Cerrato (Palencia) o la cripta de San Antolín, en la catedral de Palencia, son algunos ejemplos y tienen como característica común la utilización del arco de herradura, después asimilado por los constructores musulmanes. En arquitectura visigoda civil es destacable la que fue ciudad de Recópolis en Zorita de los Canes (Guadalajara). El conjunto está considerado «uno de los yacimientos más trascendentes de la Edad Media al ser la única ciudad de nueva planta construida por iniciativa estatal en los inicios de la Alta Edad Media en Europa» según Lauro Olmo Enciso, catedrático de arqueología de la Universidad de Alcalá[29]​.Se han identificado los restos de un complejo palatino, de una basílica visigoda, viviendas, y talleres de artesanía.

Las fíbulas aquiliformes (en forma de águila) que eran utilizadas como broche o imperdible, de oro, bronce y vidrio para unir la vestimenta, solas o por pares, destacan las encontradas en Alovera[30]​ (Guadalajara), de la época visigoda. También se han encontrado fíbulas ostrogodas en Italia.

Coronas y cruces votivas de los reyes visigodos de Hispania, halladas en el S XIX en el denominado Tesoro de Guarrazar, situado en la localidad de Guadamur, muy cerca de Toledo. Actualmente las piezas están repartidas entre el Museo Cluny de París, la Armería del Palacio Real y el Museo Arqueológico Nacional, ambos en Madrid. Son Coronas y cruces votivas que varios reyes de Toledo ofrecieron en su día como exvoto, es decir como ofrenda del rey terrenal al Rey Celestial, y eran destinadas a colgar sobre el altar de los templos. La corona del rey Recesvinto llama la atención por su trabajo de orfebrería y su belleza, con letras colgantes que forman una frase en latín: RECCESVINTHVS REX OFFERET (El rey Recesvinto la ofreció), estas piezas representan la más alta muestra de orfebrería visigoda así como una de las más importantes muestras del arte visigodo en Hispania,

Las placas y hebillas de cinturón encontradas en España, símbolo de rango y distinción de las mujeres visigodas, con decoración en pasta vítrea con incrustaciones de piedras preciosas, algunas piezas contienen excepcionales incrustaciones de lapislázuli de estilo bizantino.[31]​ así como vidrio u otros materiales. En las necrópolis visigodas, también se encontraron pulseras de diferentes metales, collares de perlas y pendientes, con incrustaciones de vidrio de color.

La mitología nórdica era en lo esencial compartida por la totalidad de los pueblos germánicos, lo que permitió incluso su recreación historicista durante el romanticismo. La estructura en tríada y otros rasgos comunes a las religiones de otros pueblos antiguos permitieron a los estudiosos de la historia de las religiones (especialmente Georges Dumezil) emparentar las religiones germánicas primitivas con otras religiones indoeuropeas.

El ritual funerario más extendido era la cremación, sustituida por la inhumación a medida que se produjo la cristianización.

El contacto con el Imperio romano, cristianizado a partir del siglo IV (Edicto de Milán, 313, Edicto de Tesalónica, 380) produjo la cristianización de los godos y otros pueblos germánicos; principalmente a partir del arrianismo, diferenciado del catolicismo y considerado como herejía en ese mismo periodo (entre el Primer Concilio de Nicea, 325, y el Primer Concilio de Constantinopla, 381). Esa diferenciación tuvo el efecto de intensificar la separación social entre los germanos y la población de las partes del Imperio que ocupaban (hispano-romanos, galo-romanos, etc.), dificultándose incluso los matrimonios mixtos. La conversión al catolicismo se produjo inicialmente en el reino de los francos (Clodoveo I, entre 496 y 506), el de los suevos (Carriarico, 560) y el de los visigodos (Recaredo, 587).

Los reinos anglosajones de Gran Bretaña fueron cristianizados a partir de la evangelización de monjes irlandeses (San Columba, monasterio de Iona, 563) y romanos (Agustín de Canterbury, conversión de Ethelberto de Kent, 597 a 601);[32]​ que también pasaron a la Europa continental. La gran popularidad de la leyenda de Santa Úrsula y las once mil vírgenes ilustraba la dificultad de la cristianización de los pueblos germanos de la Europa central, que se fue produciendo paulatinamente (San Bonifacio, monasterio de Fulda, 742).[33]​ Hacia el siglo XI ya se habían cristianizado incluso los reinos escandinavos; todo ello en el espacio de la cristiandad latina, mientras que los varegos que formaron los estados rusos se incorporaron a la cristiandad oriental.

Además de la lengua, la religión y otros aspectos culturales, existían muchos rasgos sociales y políticos comunes, ampliamente extendidos entre todos los pueblos germánicos.

Aunque tradicionalmente se les asocia con el concepto de "barbarie", tal como se definió por las ciencias sociales en construcción durante los siglos XVIII y XIX (como un estadio intermedio entre los conceptos de "salvajismo" y "civilización"); también es muy común la utilización del no menos genérico concepto de lo "tribal" para designar su organización política y social.

Los germanos eran pastores y agricultores seminómadas, cuyos asentamientos, de estructura urbanística propia de aldeas, eran poco duraderos.[34]​ Con anterioridad a la época de las invasiones, se encontraban muy lejos de constituir ningún tipo de estructura política que pudiera denominarse Estado. Todos se regían por formas de jefatura más o menos identificables con una monarquía electiva. El rey o "jefe de la tribu" (king, kuningaz), con funciones eminentemente militares, era elegido coyunturalmente (no de forma vitalicia) por una asamblea de guerreros (thing, althing, witenagemot), que era la realmente soberana a la hora de administrar justicia, pactar la paz o declarar la guerra. El rey no dejaba de ser un primus inter pares, y todos los guerreros se consideraban sus iguales, e iguales entre sí, al menos en teoría. No obstante, la estratificación social por la riqueza hacía evidente la diferenciación de clases con marcadas desigualdades económicas y sociales, que el atesoramiento, el botín de guerra, el incremento del comercio a larga distancia de productos de lujo (esclavos, caballos, vino, madera, ámbar, telas, cerámica, metales, orfebrería, joyas y armas) e incluso el uso de la moneda romana no hacía más que incrementar. El comercio romano-germano ha sido definido como englobando tres sistemas económicos: el espacio económico romano (monetario y de mercado), la zona intermedia (con economía monetaria limitada y un mercado rudimentario), que se extendería unos doscientos kilómetros más allá del limes, y la zona sin mercado o con mercado no monetario, en las regiones más alejadas. De Roma se importaba bronce, vidrio, objetos de prestigio y monedas de oro y plata; mientras que entre las exportaciones germanas había jabón, pieles, carros y textiles.[35]​

Ninguno de los pueblos germánicos tuvo antes de las invasiones un código legislativo de derecho escrito, sino costumbres y prácticas de derecho consuetudinario muy similares entre sí y que, además de quedar reflejadas en textos latinos o en la codificación que se realizó en los reinos germánicos del sur de Europa, se mantuvieron durante siglos en los pueblos nórdicos.

La organización política era bastante simple, pero se fue sofisticando a medida que se conformó una nobleza enriquecida, definida por la exclusividad de acceso a los puestos de mando (asamblea de guerreros, mandos militares) y de entre la que se nombraban los reyes. El resto de los hombres libres (véase yeomen), que retenían el derecho a portar armas y formaban parte del ejército, practicaban la agricultura, la ganadería, la caza y otras actividades cotidianas. La presencia de esclavos varió según el contexto histórico. La situación social de los pueblos conquistados era muy diferente, existiendo situaciones de vasallaje o semilibertad.

Las distintas "tribus" o "pueblos" germanos, independientes, ocasionalmente se confederaban para la guerra contra enemigos comunes (fueran germanos o no germanos); y muy a menudo también se producían escisiones entre facciones que guerreaban entre sí.

[36]​

Algunas tribus, como los francos salios, establecieron relaciones de clientela con los romanos, sirviendo ocasionalmente en sus ejércitos. Estas relaciones sentaron la base del futuro régimen feudal, y los dominios que establecieron fueron el origen de los reinos medievales y los actuales países europeos.

Los pueblos germánicos se convirtieron en un mito historiográfico y en el soporte de ideologías justificativas de todo tipo, tanto favorables (germanofilia, germanismo, pangermanismo) como desfavorables (germanofobia, antigermanismo).

Como disciplina científica, el germanismo o los estudios germánicos[37]​ han constituido una parte importante en la controvertida construcción histórica, desde finales del siglo XVIII, de ciencias sociales como la filología (Jacob Grimm —Deutsches Wörterbuch, Deutsche Mythologie—,[38]​ Rasmus Christian Rask —Undersøgelse om det gamle Nordiske eller Islandske Sprogs Oprindelse—, Henry Sweet[39]​ y Matthias Lexer[40]​) y la antropología (teoría indoeuropea, de interpretación desviada hacia el racismo —nordicismo o mito ario—).

Estudios literarios anteriores son Historia de gentibus septentrionalibus (Olaus Magnus, 1555) y las primeras ediciones impresas de la Gesta Danorum (obra del siglo XIII de Saxo Grammaticus, publicada en 1514) y otros poemas medievales alemanes (Melchior Goldast,[41]​ 1603, que posteriormente estudió el monacato benedictino alemán). Por la misma época (finales del XVI y comienzos del XVII) se emprendieron estudios y publicaciones de los textos del inglés antiguo (Robert Cotton, Cotton Manuscripts,[42]​ y Peder Hansen Resen,[43]​ Edda Islandorum, 1665).[44]​

Desde la genética de poblaciones (disciplina cuya aplicación a las ciencias sociales ha de hacerse con especial cuidado de no caer en explicaciones simplistas de identificar inexistentes "razas humanas", según advierten sus propios científicos)[45]​ se ha sugerido que las migraciones de los pueblos germánicos pueden detectarse en la distribución actual del linaje masculino representado por el haplogrupo del cromosoma Y denominado I1;[46]​ que descendería de un ancestro común más reciente localizable probablemente en el territorio de la actual Dinamarca hace de cuatro mil a seis mil años.[47]​[48]​



Un valle (del latín vallis) es una llanura entre montañas o alturas, una depresión de la superficie terrestre entre dos vertientes, con forma inclinada y alargada, que conforma una cuenca hidrográfica en cuyo fondo se aloja un curso fluvial.

En un relieve joven predominan los valles «en V», característico de los valles fluviales: las vertientes, poco modeladas por la erosión, convergen en un fondo muy estrecho. Por el contrario, un estado avanzado de la erosión de lugar a la de valles aluviales, de fondo plano y amplio, constituidos por depósitos aluviales entre los cuales puede divagar el curso de agua. Los valles en U, característicos de los valles o "artesas" glaciales, tienen sus paredes muy abruptas y el fondo cóncavo. En ciertos casos, al retroceder un antiguo glaciar, el lecho de uno de sus afluentes queda a mucha altura por encima del de aquel y desemboca en su vertiente, a menudo, formando saltos de agua. Un tercer tipo de valles es en forma de cuna o batea: son amplios, de suave pendiente y superficiales.[1]​ 

Cuando un río es capturado por otro o cuando su lecho es cerrado por morrenas u otro tipo de depósitos, queda más abajo un valle muerto o río decapitado, que ya no tiene un curso de agua. En otros casos, un valle no tiene salida natural, por cerrarlo una contrapendiente, y las aguas que por él discurren penetran en el suelo y prosigue su curso por una red subterránea. Esos valles ciegos son propios de los terrenos cársicos. Asimismo, en muchas regiones áridas los ríos no puede salir de su cuenca hidrográfica, discurriendo por valles endorreicos. Un valle puede haber sido íntegramente excavado en un terreno sedimentario por su curso de agua, pero por lo general, este se abre paso por depresiones de origen tectónico. Según sean estas, se tiene un valle de fractura, de fosa, de ángulo de falla, etc. Un valle longitudinal está orientado paralelamente a los pliegues de una cordillera, en tanto que un valle transversal es perpendicular a ellos.

Geomorfológicamente existen diferencias entre los valles angostos y los valles amplios, las principales se describen a continuación.

Las corrientes de agua que generalmente ocupa la parte más baja del valle se presenta completamente confinada y fuertemente controlada para la migración lateral, de esta manera los procesos de ajuste del cauce se presentan directamente en el fondo del cauce modificando la pendiente e inclusive incisando el lecho;[2]​ asociado a estos procesos se puede presentar inestabilidad de orillas y deslizamientos. La cercanía de las montañas al cauce hacen que estos valles sean generalmente poco atractivos para desarrollos urbanísticos.

El desarrollo de los valles encañonados está íntimamente relacionado con la intensidad de los procesos geomorfológicos que forman el valle y con la geología, concretamente con la composición litológica o con las fallas. Los materiales de los valles varían desde roca firme hasta suelos residuales en la forma de coluviones, flujo de escombros entre otros materiales depositacionales.[2]​

La localización de los valles encañonados es más frecuente en las partas altas de la cuenca hidrográfica donde los ríos tienen poco caudal, las pendientes son altas y las paredes del valle frecuentemente muestran roca firme sin cobertura. Cuando las montañas que circundan el valle son muy resistentes a la meteorización y a la erosión, el valle presenta una configuración encañonada aún en el recorrido medio de la cuenca. frecuentemente se encuentran cataratas y rápidos en los cursos de agua que drenan estos valles.

Aunque generalmente se cree que los valles en V son perfectamente simétricos, lo cierto es que también presentan cierto grado de asimetría, siendo la pared o ladera de la izquierda de mayor pendiente o inclinación que la de la derecha (tratándose del hemisferio Norte).

Ejemplos de valle en forma de V en Svaneti, Montañas del Cáucaso.

Los valles amplios están asociados en los famosos ríos de planicie (maduros y viejos) donde el cauce ocupa una parte reducida del valle ya que la planicie aluvial es amplia. En esta se pueden observar rasgos del paisaje que no se encuentran en los valles encañonados como son: terrazas aluviales, diques, naturales, madreviejas, cauces abandonados, complejos de orillares.[3]​ Al igual que la planicie, el cauce también es amplio presentándose relaciones ancho profundidad mayores de 10.[4]​ La llanura está sujeta a inundaciones recurrentes, por lo que esta no es estática ni estable.

La llanura está compuesta, generalmente, por sedimentos no consolidados que se erosionan rápidamente durante inundaciones y crecidas del río. El canal de un río puede cambiar de posición en la amplia llanura de inundación y esta, a su vez, es modificada periódicamente por las inundaciones, a medida que el canal se desplaza de un lugar a otro.

Durante los períodos de aguas normales o bajas, el río que corre por el valle, queda confinado a su cauce y no se derrama sobre la planicie de inundación. La planicie de inundación creada por la erosión lateral y por el retroceso gradual de las paredes del valle, se llama planicie de inundación erosional y se caracteriza por una delgada cubierta de grava, arena y limo de unos cuantos decímetros o pocos metros de espesor. De otro lado, bajo el fondo de muchos valles amplios se encuentran depósitos de grava, arena y limo que alcanzan 100 o más metros de espesor. Estos gruesos depósitos se forman cuando las condiciones variantes fuerzan al río a dejar caer su carga a lo ancho del fondo del valle; esta planicie de inundación formada por la construcción del fondo del valle o agradación, se llama planicie de inundación de agradación.

Las planicies de inundación de este tipo son mucho más comunes que las erosionales y normalmente tienden a encontrarse en el curso inferior de los ríos. Ambas planicies de inundación, la erosional y la de agradación, muestran formas como meandros, trenzamientos, bordos naturales, depósitos de sedimentos, cauces divagantes.

Ejemplos de valle en forma de U en Acatlán (Hidalgo), Valle de Acatlán.

Tanto en el pasado como en el presente, varios procesos han actuado y están actuando en el sentido de profundizar y ensanchar los valles, aunque las evidencias de dicha acción, se pueden perder o debilitar con el tiempo. Si se dejara a una corriente en libertad de alcanzar por sí misma su nivel-base, erosionaría el lecho directamente hacia abajo, formando un abismo de paredes verticales en el proceso.[1]​ 

Pero como la corriente no es el único agente que trabaja en la formación del valle, las paredes de la mayoría de los valles se inclinan hacia arriba y hacia fuera del fondo del valle. Con el tiempo aun las paredes de las gargantas más abruptas se inclinarán hacia fuera con relación al eje de sus valles. Conforme una corriente corta hacia abajo y profundiza su cauce dentro de la superficie del terreno, la meteorización, el escurrimiento y los movimientos en masa entran en juego, desgastando constantemente las paredes del valle, haciéndolas retroceder, apartándose entre sí. 

El material bajo la influencia de la gravedad, es arrastrado de las paredes del valle abajo y descargado en la corriente, para ser movido adelante rumbo finalmente a los océanos. 

El resultado es un valle cuyas paredes se ensanchan afuera y arriba, desde la corriente, para formar un perfil transversal típico.

La velocidad con que las paredes del valle son reducidas y los ángulos que adoptan, dependen de varios factores, siempre pensando en tiempos geológicos, es decir millones de años. Si las paredes están hechas de material sin consolidar (que es vulnerable a la erosión y al movimiento en masa), la velocidad será rápida; pero si las paredes están constituidas de roca resistente, la velocidad de erosión será muy lenta, y las paredes podrán levantarse casi verticalmente desde el fondo del valle. Además de cortar hacia abajo en su cauce, una corriente corta también de lado a lado, o lateralmente en sus bordos. 

En las primeras etapas de ensanchamiento del valle, cuando la corriente está todavía por encima de su nivel-base, predomina la erosión hacia abajo. Posteriormente, a medida que la corriente se aproxima a su nivel-base, la erosión hacia abajo va siendo cada vez menos importante; en esta etapa se destina a la erosión de sus bordos una proporción más grande de la energía de la corriente. Como esta oscila de un lado a otro, forma sobre el fondo del valle una planicie de inundación que tiende a ensancharse siempre y el valle se hace cada vez más amplio.

La filosofía (del griego antiguo φιλοσοφία 'amor a la sabiduría' derivado de φιλεῖν [fileîn] 'amar' y σοφία [sofía] 'sabiduría';[1]​ trans. en latín como philosophĭa)[2]​ es una disciplina académica y un conjunto de reflexiones y conocimientos de carácter trascendental que, en un sentido holístico, estudia la esencia, las causas primeras y los fines últimos de las cosas.[3]​ Trata de responder a una variedad de problemas fundamentales acerca de cuestiones como la existencia y el ser (ontología y metafísica), el conocimiento (epistemología y gnoseología), la razón (lógica), la moral (ética), la belleza (estética), el valor (axiología), la mente (fenomenología, existencialismo, filosofía de la mente), el lenguaje (filosofía del lenguaje) y la religión (filosofía de la religión).[4]​[5]​[6]​ A lo largo de la historia, muchas otras disciplinas han surgido a raíz de la filosofía, y a su vez es considerada la base de todas las ciencias modernas por muchos autores.[7]​ La disciplina ha existido desde la Antigüedad en Occidente y Oriente, no solo como actividad racional sino también como forma de vida. La historia de la filosofía nos permite comprender su evolución, desarrollo e impacto en el pensamiento.

El término probablemente fue acuñado por Pitágoras.[8]​Al abordar los problemas, la filosofía se distingue del misticismo, el esoterismo, la mitología y la religión por su énfasis en los argumentos racionales sobre los argumentos de autoridad,[9]​ y de la ciencia porque generalmente realiza sus investigaciones de una manera no empírica,[10]​ sea mediante el análisis conceptual,[11]​ los experimentos mentales,[12]​ la especulación u otros métodos a priori, aunque sin desconocer la importancia de los datos empíricos. Históricamente, la filosofía abarcaba todos los cuerpos de conocimiento y un practicante era conocido como «filósofo». Desde la época del filósofo griego Aristóteles hasta el siglo XIX, la «filosofía natural» abarcaba la astronomía, la medicina y la física; por ejemplo, el término se menciona en la obra Principios matemáticos de la filosofía natural (1687) de Isaac Newton.[13]​

La filosofía occidental ha influido sobre otras ramas del conocimiento humano, por ejemplo, en el ámbito de la ciencia, la religión y la política.[14]​[15]​ Muchos filósofos importantes fueron a la vez grandes científicos, teólogos o políticos y algunas nociones fundamentales de estas disciplinas todavía son objeto de estudio filosófico. Esta superposición entre disciplinas se debe a que la filosofía es una disciplina muy amplia. En el siglo XIX, el crecimiento de las universidades de investigación modernas llevó a la filosofía académica y otras disciplinas a profesionalizarse y especializarse.[16]​[17]​ Desde entonces, varias áreas de investigación que tradicionalmente formaban parte de la filosofía se han convertido en disciplinas académicas separadas, como la psicología, la sociología, la biología, la lingüística y la economía.

Hoy en día, los principales subcampos de la filosofía académica incluyen la metafísica, que se ocupa de la naturaleza fundamental de la existencia y la realidad; epistemología, que estudia la naturaleza del conocimiento y las creencias; la ética, que se ocupa del valor moral; y lógica, que estudia las reglas de inferencia que permiten deducir conclusiones a partir de premisas verdaderas.[18]​[19]​ Otros subcampos notables incluyen la filosofía de la ciencia, la filosofía de la tecnología y la filosofía política. 

La Conferencia General de la Unesco proclamó el Día Mundial de la Filosofía cada tercer jueves del mes de noviembre de cada año.[cita requerida]

Inicialmente, el término se refería a cualquier rama de conocimiento.[8]​ En este sentido, la filosofía está estrechamente relacionada con la religión, las matemáticas, las ciencias naturales, la educación y la política.[20]​ Además, los antiguos filósofos no diferenciaban la teoría de la práctica cotidiana, por lo que su discurso filosófico formaba parte integral y preparatoria de su modo de vida, y viceversa.[21]​

En la sección trece de Vidas, opiniones y sentencias de los filósofos más ilustres, la historia de la filosofía más antigua que se conserva (siglo III), Diógenes Laercio presenta una división en tres partes de la investigación filosófica griega antigua:[22]​

En Contra los lógicos el filósofo pirronista Sexto Empírico detalló la variedad de formas en que los filósofos griegos antiguos habían dividido la filosofía, y señaló que Platón, Aristóteles, Jenócrates y los estoicos estuvieron de acuerdo en esta división en tres partes.[26]​ El filósofo escéptico académico Cicerón también siguió esta división en tres partes.[27]​

Esta división no está obsoleta, pero ha cambiado: filosofía natural se ha dividido en las diversas ciencias naturales, especialmente la física, astronomía, química, biología, y cosmología; filosofia moral ha dado a luz las ciencias sociales, sin dejar de incluir la teoría de valor (p.ej. ética, estética, filosofía política, etc.); i filosofía metafísica ha dado paso a las ciencias formales como la lógica, matemáticas i filosofía de la ciencia, sin dejar de incluir epistemología, cosmología, etc. Por ejemplo, en Principios matemáticos de la filosofía natural (1687) de Newton, ya que está clasificado como un libro de física, utiliza el término filosofía natural como se entendía en ese momento, abarcando disciplinas como astronomía, medicina y física que luego se asoció con las ciencias.[28]​

La metafísica (del latín metaphysica, y este del griego μετὰ [τὰ] φυσικά, «más allá de la naturaleza»)[31]​ es la rama de la filosofía que estudia la naturaleza, estructura, componentes y principios fundamentales de la realidad.[32]​[33]​[34]​ Esto incluye la clarificación e investigación de algunas de las nociones fundamentales con las que comprendemos el mundo, como entidad, ser, existencia, objeto, propiedad, relación, causalidad, tiempo y espacio. Junto con la lógica y la epistemología, la metafísica es la rama más básica de la filosofía. Ha sido estudiada por filósofos como Platón, Aristóteles, Agustín, Boecio, Aquino, Leibniz, Locke, etc.[35]​

Antes del advenimiento de la ciencia moderna, muchos de los problemas que hoy pertenecen a las ciencias naturales eran estudiados por la metafísica bajo el título de filosofía natural.[36]​[37]​ Hoy la metafísica estudia aspectos de la realidad que son inaccesibles a la investigación empírica. Según Immanuel Kant, las afirmaciones metafísicas no se pueden dar en juicios sintéticos a priori, lo que excluye que la metafísica pueda constituirse en ciencia positiva al estilo de la física o las matemáticas.[38]​ Esto dará lugar en el siglo XX a la lectura heideggeriana de la metafísica occidental como ontoteología y, por lo tanto, a la necesidad de repensar la cuestión del ser desde el origen mismo de los pensadores presocráticos. Aristóteles designó la metafísica como «ciencia primera».[39]​ En la química se asume la existencia de la materia y en la biología la existencia de la vida, pero ninguna de las dos ciencias define la materia o la vida; solo la metafísica suministra estas definiciones básicas.[40]​

La ontología es la parte de la metafísica que se ocupa de investigar qué entidades existen y cuáles no, más allá de las apariencias.[41]​[42]​ La metafísica tiene dos temas principales: el primero es la ontología, que en palabras de Aristóteles es la ciencia que estudia al ser en cuanto tal. El segundo es la teleología, que estudia los fines como causa última de la realidad. Existe, sin embargo, un debate que sigue aún hoy sobre la definición del objeto de estudio de la metafísica, y sobre si sus enunciados tienen propiedades cognitivas.

La gnoseología (del griego γνωσις, gnōsis, «conocimiento» o «facultad de conocer», y λόγος, logos, «razonamiento» o «discurso»), también llamada teoría del conocimiento,[44]​ es la rama de la filosofía que estudia la posibilidad, el origen o medios, la naturaleza o esencia, y la fenomenología del conocimiento.[45]​[46]​

La gnoseología no estudia los conocimientos particulares, como pueden ser los conocimientos de la física, de la matemática o del entorno inmediato, sino la naturaleza del conocimiento en general. Muchas ciencias particulares tienen además su propia filosofía, como por ejemplo la filosofía de la física, la filosofía de la matemática, la filosofía de la historia, etc. Otras disciplinas también se ocupan del conocimiento en general, pero desde otros puntos de vista. La psicología estudia los aspectos de la vida mental implícitos en el conocer, la lógica estudia la corrección o incorrección de los razonamientos que pueden implicar nuevos conocimientos, y la ontología o metafísica estudia la naturaleza de los objetos que se pueden conocer.

La epistemología, del griego ἐπιστήμη ─epistḗmē («conocimiento»)─ y λόγος ─lógos («estudio»)─, es la rama de la filosofía que estudia el conocimiento en general, su naturaleza, posibilidad, alcance y fundamentos.

Algunos autores distinguen a la epistemología, estudio del conocimiento científico, de la gnoseología, estudio del conocimiento en general.[49]​ Otros, en cambio, consideran que el término «epistemología» ha ido ampliando su significado y lo utilizan como sinónimo de «teoría del conocimiento», sobre todo en el mundo anglosajón.

La epistemología estudia las circunstancias históricas, psicológicas y sociológicas que llevan a la obtención del conocimiento científico y los criterios por los cuales se lo justifica o invalida, así como la definición clara y precisa de los conceptos epistémicos más usuales, tales como verdad, objetividad, realidad o justificación. Algunas de las preguntas que pretende responder la epistemología son ¿Cómo conocemos?, ¿Cuáles son las fuentes del conocimiento?, ¿Cómo diferenciamos lo verdadero de lo falso? y ¿Cuáles son los tipos de conocimiento?. El debate no se centra en un conocimiento específico, sino en la forma en como conocemos.

Generalmente, los debates en la epistemología se agrupan en torno a cuatro áreas centrales:

La lógica es una rama de la filosofía[50]​ de carácter interdisciplinario, entendida como la ciencia formal que estudia los principios de la  demostración y la inferencia válida,[51]​ las falacias, las paradojas y la noción de verdad.[52]​

La lógica se divide en varias categorías según su campo de estudio. La lógica filosófica estudia el concepto y la definición, la enunciación o proposición y la argumentación utilizando los métodos y resultados de la lógica moderna para el estudio de problemas filosóficos. La lógica matemática estudia la inferencia mediante sistemas formales como la lógica proposicional, la lógica de primer orden y la lógica modal. La lógica informal se enfoca en el desarrollo lingüístico de los razonamientos y sus falacias. La lógica computacional es la aplicación de la lógica matemática a las ciencias de la computación.

Los orígenes de la lógica se remontan a la Edad Antigua, con brotes independientes en China, India y Grecia. Desde entonces, la lógica tradicionalmente se considera una rama de la filosofía, pero en el siglo XX la lógica ha pasado a ser principalmente la lógica matemática, y por lo tanto ahora también se considera parte de las matemáticas, e incluso una ciencia formal independiente.

No existe un acuerdo universal sobre la definición exacta o los límites de la lógica.[53]​[54]​[55]​ Sin embargo, el ámbito de la lógica (interpretada en sentido amplio) incluye:

La ética o filosofía moral es la rama de la filosofía que estudia la conducta humana,[56]​[57]​ lo correcto y lo incorrecto,[58]​[59]​ lo bueno y lo malo,[59]​la moral,[60]​ el buen vivir,[61]​ la virtud, la felicidad y el deber. La ética contemporánea se suele dividir en tres ramas o niveles: la metaética estudia el origen, naturaleza y significado de los conceptos éticos, la ética normativa busca normas o estándares para regular la conducta humana, y la ética aplicada examina controversias éticas específicas.[62]​[63]​

Ética y moral son conceptos muy relacionados que a veces se usan como sinónimos, pero tradicionalmente se diferencian en que la ética es la disciplina académica que estudia la moral.[60]​La ética no inventa los problemas morales, sino que reflexiona sobre ellos.[64]​ Las acciones relevantes para la ética son las acciones morales, que son aquellas realizadas de manera libre, ya sean privadas, interpersonales o políticas.[65]​ La ética no se limita a observar y describir esas acciones, sino que busca determinar si son buenas o malas, emitir juicio sobre ellas y así ayudar a encauzar la conducta humana.[66]​

El estudio de la ética se remonta a los orígenes mismos de la filosofía en la Antigua Grecia, y su desarrollo histórico ha sido amplio y variado. A lo largo de la historia ha habido diversas maneras de entender la ética y distintas propuestas morales orientadoras de la vida humana.

La estética (del griego αἰσθητική [aesthetic], ‘sensación’, ‘percepción’, y este de[aísthesis], ‘sensación’, ‘sensibilidad’, e -ικά [-icá], ‘relativo a’) es la rama de la filosofía que estudia la esencia y la percepción de la belleza y el arte.[68]​[69]​

Algunos autores definen la estética de manera más amplia, como el estudio de las experiencias estéticas y los juicios estéticos en general, y no solo los relativos a la belleza.[70]​ Cuando juzgamos algo como «bello», «feo», «sublime» o «elegante» (por dar algunos ejemplos), estamos haciendo juicios estéticos, que a su vez expresan experiencias estéticas.[70]​ La estética es el dominio de la filosofía, estudiando el arte y cualidades como la belleza; asimismo es el estudio de estas experiencias y juicios que suceden día a día en las actividades que realizamos, produciendo sensaciones y emociones ya sean positivas o negativas en nuestra persona. La estética busca el por qué de algunas cuestiones, por ejemplo, por qué algún objeto, pintura o escultura no resulta atractivo para los espectadores; por lo tanto el arte lleva relación con la estética ya que busca generar sensaciones a través de una expresión.

En otra acepción, la estética es el estudio de la percepción en general, sea sensorial o entendida de manera más amplia. Estos campos de investigación pueden coincidir, aunque no necesariamente es lo mismo.

La estética estudia las más amplias y vastas historias del conocimiento isabelino, así como las diferentes formas del arte. La estética, así definida, es el campo de la filosofía que estudia el arte y sus cualidades, tales como la belleza, lo eminente, lo feo o la disonancia. Es la rama de la filosofía que estudia el origen del sentimiento puro y su manifestación, que es el arte, se puede decir que es la ciencia cuyo objeto primordial es la reflexión sobre los problemas del arte, la estética analiza filosóficamente los valores que en ella están contenidos.

Desde que en 1750 (en su primera edición) y 1758 (segunda edición publicada) Alexander Gottlieb Baumgarten usara la palabra «estética» como ‘ciencia de lo bello, misma a la que se agrega un estudio de la esencia del arte, de las relaciones de ésta con la belleza y los demás valores’. Algunos autores han pretendido sustituirla por otra denominación: «calología», que atendiendo a su etimología significa ciencia de lo bello (kalos, ‘bello’).

La filosofía política es la rama de la filosofía que estudia cómo debería ser la relación entre las personas y la sociedad,[72]​ e incluye cuestiones fundamentales acerca del gobierno, la política, las leyes, la libertad, la igualdad, la justicia, la propiedad, los derechos, el poder político, la aplicación de un código legal por una autoridad, qué hace a un gobierno legítimo, qué derechos y libertades debe proteger y por qué, qué forma debe adoptar y por qué, qué obligaciones tienen los ciudadanos para con un gobierno legítimo (si acaso alguna), y cuándo lo pueden derrocar legítimamente (si alguna vez).[73]​[74]​ Mientras la ciencia política investiga cómo fueron, son y serán los fenómenos políticos, la filosofía política se encarga de teorizar cómo deberían ser dichos fenómenos.[72]​[75]​

En un sentido vernacular, el término «filosofía política» a menudo refiere a una perspectiva general, o a una ética, creencia o actitud específica, sobre la política que no necesariamente debe pertenecer a la disciplina técnica de la filosofía.[76]​ Charles Blattberg, que define la política como «responder a los conflictos con el diálogo», sugiere que las filosofías políticas ofrecen consideraciones filosóficas de ese diálogo.[77]​

La filosofía política tiene un campo de estudio amplio y se conecta fácilmente con otras ramas y subdisciplinas de la filosofía, como la filosofía del derecho y la filosofía de la economía.[72]​ Se relaciona fuertemente con la ética en que las preguntas acerca de qué tipo de instituciones políticas son adecuadas para un grupo depende de qué forma de vida se considere adecuada para ese grupo o para los miembros de ese grupo.[72]​ Las mejores instituciones serán aquellas que promuevan esa forma de vida.[72]​

En el plano metafísico, la principal controversia divisora de aguas es acerca de si la entidad fundamental sobre la cual deben recaer los derechos y las obligaciones es el individuo, o el grupo.[72]​ El individualismo considera que la entidad fundamental es el individuo, y por lo tanto promueven el individualismo metodológico.[72]​ El comunitarismo enfatiza que el individuo es parte de un grupo, y por lo tanto da prioridad al grupo como entidad fundamental y como unidad de análisis.[72]​

Los fundamentos de la filosofía política han variado a través de la historia. Para los griegos la ciudad era el centro y fin de toda actividad política. En la Edad Media toda actividad política se centraba en las relaciones que debe mantener el ser humano con el orden dado por Dios. A partir del Renacimiento la política adopta un enfoque básicamente antropocéntrico. En el mundo moderno y contemporáneo surgen y conviven muchos modelos, que van desde los totalitarismos hasta los sistemas democráticos participativos (entre los cuales existen muchas variantes).

La filosofía del lenguaje es la rama de la filosofía que estudia el lenguaje en sus aspectos más generales y fundamentales, como la naturaleza del significado y de la referencia, la relación entre el lenguaje, el pensamiento y el mundo, el uso del lenguaje (o pragmática), la interpretación, la traducción y los límites del lenguaje.

La filosofía del lenguaje se distingue de la lingüística en que se sirve de métodos no-empíricos (como experimentos mentales) para llegar a sus conclusiones.[78]​ Además, en la filosofía del lenguaje generalmente no se hace diferencia entre el lenguaje hablado, el escrito o cualquiera otra de sus manifestaciones, sino que se estudia aquello que es común a todas ellas. Por último, los lingüistas en general estudian el lenguaje con fines descriptivos, analizando sus formas, niveles y funciones. En cambio, el enfoque de los filósofos del lenguaje es más abstracto y desligado de la descripción práctica de los lenguajes particulares.

La semántica es la parte de la filosofía del lenguaje (y de la lingüística) que se ocupa de la relación entre el lenguaje y el mundo.[79]​ Algunos problemas que caen bajo este campo son el problema de la referencia, la naturaleza de los predicados, de la representación y de la verdad.[79]​ En el Crátilo, Platón señaló que si la conexión entre las palabras y el mundo es arbitraria o convencional, entonces es difícil entender cómo el lenguaje puede permitir el conocimiento acerca del mundo.[79]​ Por ejemplo, es evidente que el nombre «Venus» pudo haber designado cualquier cosa, aparte del planeta Venus, y que el planeta Venus pudo haberse llamado de cualquier otra forma. Luego, cuando se dice que «Venus es más grande que Mercurio», la verdad de esta oración es convencional, porque depende de nuestras convenciones acerca de lo que significan «Venus», «Mercurio» y el resto de las palabras involucradas. En otro lenguaje, esas mismas palabras podrían, por alguna coincidencia, significar algo muy distinto y expresar algo falso. Sin embargo, aunque el significado de las palabras es convencional, una vez que se ha fijado su significado, parece que la verdad y la falsedad no dependen de convenciones, sino de cómo es el mundo. A este «fijar el significado» se lo suele llamar interpretación, y es uno de los temas centrales de la semántica.

Un problema ulterior en esta dirección es que si una interpretación se da en términos lingüísticos (por ejemplo: «Venus es el nombre del segundo planeta a partir del Sol»), entonces queda la duda de cómo deben interpretarse las palabras de la interpretación. Si se las interpreta por medio de nuevas palabras, entonces el problema resurge, y se hace visible una amenaza de regresión al infinito, de circularidad, o de corte arbitrario en el razonamiento (tal vez en palabras cuyo significado sea supuestamente autoevidente). Pero para algunos este problema invita a pensar en una forma de interpretación no lingüística, como por ejemplo el conductismo o la definición ostensiva.

La filosofía de la mente es la rama de la filosofía (en particular de la filosofía analítica) que estudia la mente, incluyendo las percepciones, sensaciones, emociones, fantasías, sueños, pensamientos y creencias.[82]​ Uno de los problemas centrales de la disciplina es determinar qué hace que todos los elementos de esta lista sean mentales, y otros no.[83]​ Además de las cuestiones ontológicas acerca de la naturaleza de los estados mentales, la filosofía de la mente estudia cuestiones epistemológicas en torno a la cognoscibilidad de la mente.

Tanto para la fenomenología como para la filosofía analítica, un candidato importante para ser una condición necesaria, aunque no suficiente, de todo fenómeno mental es la intencionalidad.[84]​ La intencionalidad es el poder de la mente de ser acerca de, de representar, o suplir cosas, propiedades o estados de cosas.[84]​ Por ejemplo, uno no recuerda simplemente, sino que recuerda algo, y tampoco quiere en abstracto, sino que quiere algo determinado. La propuesta de algunos filósofos es que todo lo que sea mental está «dirigido» hacia algún objeto, en el sentido más general de objeto, y que por lo tanto la intencionalidad es una característica necesaria, aunque no suficiente, de lo mental.

Otra característica importante y controversial de lo mental son los qualia, o propiedades subjetivas de la experiencia.[85]​ Cuando uno ve una nube, se pincha un dedo con un alfiler, o huele una rosa, experimenta algo que no se puede observar desde fuera, sino que es completamente subjetivo. A estas experiencias se las llama «qualia». Parte de la importancia de los qualia se debe a las dificultades que suscitan al fisicalismo para acomodarlos dentro de su concepción de lo mental.[85]​ Algunos neurocientíficos como Antonio Damasio, Gerald Edelman, Vilayanur Ramachandran y Rodolfo Llinás han abordado esta temática de la filosofía de la mente y sostienen que los qualia existen y no son eliminables y reemplazables por otra cosa como conductas o propiedades objetivas del cerebro observadas en imágenes de resonancia magnética.[86]​[87]​
[88]​[89]​[90]​[91]​[92]​[93]​

La filosofía de la mente se relaciona con la ciencia cognitiva de varias maneras.[94]​ Por un lado, las filosofías más racionalistas pueden considerarse como parte de las ciencias cognitivas.[94]​ En cambio, otras filosofías más naturalistas que dan énfasis a la biología y neurociencia critican a la ciencia cognitiva por suponer que lo mental es intelectual (lógico) o computacional o por equiparar a los seres vivos a artefactos mecánicos. Por ejemplo, algunos críticos señalan que la ciencia cognitiva descuida muchos factores relevantes para el estudio de lo mental, entre ellos las emociones, la conciencia, el cuerpo y el entorno.[86]​[87]​[94]​

La filosofía de la naturaleza, a veces llamada filosofía natural o cosmología fue el estudio filosófico de la naturaleza y el universo físico que era dominante antes del desarrollo de la ciencia moderna. Se considera el precursor de lo que hoy conocemos como las ciencias naturales y física hasta mediados del siglo XIX.

Problemas como los del determinismo o indeterminismo, causalidad, finalismo, orden y probabilidad, especificidad de la vida, etc., eran considerados argumentos propios de la filosofía de la naturaleza la cual debería ser independiente de la propiamente dichas las ciencias e investigaciones empíricas y teóricas. Similarmente se hablaba de "teología natural" o "racional" como la investigación filosófica referida a Dios basados en la razón y la experiencia ordinaria de la naturaleza.[95]​ La filosofía natural trató cuestiones que pocas ciencias naturales se han planteado, como la existencia de un mundo inmaterial. Por esta cuestión, la cosmología se interpenetra con la "psicología" como estudio del alma.[96]​

La filosofía de la ciencia es la rama de la filosofía que investiga el conocimiento científico y la práctica científica. Se ocupa de saber, entre otras cosas, cómo se desarrollan, evalúan y cambian las teorías científicas, y de saber si la ciencia es capaz de revelar la verdad de las «entidades ocultas» (o sea, no observables) y los procesos de la naturaleza. Son filosóficas las diversas proposiciones básicas que permiten construir la ciencia. Por ejemplo:

Si bien estos supuestos metafísicos no son cuestionados por el realismo científico, muchos han planteado serias sospechas respecto del segundo de ellos[97]​ y numerosos filósofos han puesto en tela de juicio alguno de ellos o los tres.[98]​ De hecho, las principales sospechas con respecto a la validez de estos supuestos metafísicos son parte de la base para distinguir las diferentes corrientes epistemológicas históricas y actuales. De tal modo, aunque en términos generales el empirismo lógico defiende el segundo principio, opone reparos al tercero y asume una posición fenomenista, es decir, admite que el hombre puede comprender la naturaleza siempre que por naturaleza se entienda "los fenómenos" (el producto de la experiencia humana) y no la propia realidad.

En pocas palabras, lo que intenta la filosofía de la ciencia es explicar problemas tales como:

La filosofía de la ciencia comparte algunos problemas con la gnoseología —la teoría del conocimiento— que se ocupa de los límites y condiciones de posibilidad de todo conocimiento. Pero, a diferencia de esta, la filosofía de la ciencia restringe su campo de investigación a los problemas que plantea el conocimiento científico; el cual, tradicionalmente, se distingue de otros tipos de conocimiento, como el ético o estético, o las tradiciones culturales.

Algunos científicos han mostrado un vivo interés por la filosofía de la ciencia y algunos como Galileo Galilei, Isaac Newton y Albert Einstein, han hecho importantes contribuciones. Numerosos científicos, sin embargo, se han dado por satisfechos dejando la filosofía de la ciencia a los filósofos y han preferido seguir haciendo ciencia en vez de dedicar más tiempo a considerar cómo se hace la ciencia. Dentro de la tradición occidental, entre las figuras más importantes anteriores al siglo XX destacan entre muchos otros Platón, Aristóteles, Epicuro, Arquímedes, Boecio, Alcuino, Averroes, Nicolás de Oresme, Santo Tomas de Aquino, Jean Buridan, Leonardo da Vinci, Raimundo Lulio, Francis Bacon, René Descartes, John Locke, David Hume, Emmanuel Kant y John Stuart Mill.

La filosofía de la religión es una rama de la filosofía que tiene por objeto de estudio a la religión, la espiritualidad, en el marco del liberalismo religioso, como una manifestación humana consciente y reflexiva sobre el sentido trascendente de la existencia y el mundo,[99]​ lo que incluye sus argumentos sobre la naturaleza, la existencia de Dios, el problema del mal, dando cuenta de su universalismo en tanto que ha prevalecido considerablemente en la historia de las culturas humanas, como también sobre la relación entre la religión y otros sistemas de valores como la ciencia.[100]​

Se advierte la distinción entre la filosofía de la religión y la filosofía religiosa,[101]​ dado que la última alude a un saber que se considera inspirado y guiada por su Dios y su religión, como pueden ser las filosofías judía, cristiana e islámica.[102]​[103]​

La filosofía utiliza varios métodos de investigación. En general se distingue del método científico por ser a priori, es decir que se realiza sin recurrir a la experiencia (aunque también existe la filosofía experimental).[106]​ Algunos son comunes a la ciencia, como los experimentos mentales y el método axiomático, otros no, como la duda metódica y la mayéutica.

Un argumento (del latín argumentum) es la expresión oral o escrita de un razonamiento o idea[107]​ mediante el cual se intenta probar, refutar o incluso justificar una proposición o tesis.[108]​[109]​
Las cualidades fundamentales de un argumento son la consistencia y coherencia; entendiendo por tal el hecho de que el contenido de la expresión, discurso u obra adquiera un sentido o significado que se dirige a un interlocutor con finalidades diferentes:

Es por tanto un discurso dirigido:

En lógica, una falacia (del latín fallacia ‘engaño’) es un argumento que parece válido, pero no lo es.[111]​[112]​ Algunas falacias se cometen intencionalmente para persuadir o manipular a los demás, mientras que otras se cometen sin intención debido a descuidos o ignorancia. En ocasiones las falacias pueden ser muy sutiles y persuasivas, por lo que se debe poner mucha atención para detectarlas.[113]​

Que un argumento sea falaz no implica que sus premisas o su conclusión sean falsas ni que sean verdaderas. Un argumento puede tener premisas y conclusión verdaderas y aun así ser falaz. Lo que hace falaz a un argumento es la invalidez del argumento en sí. De hecho, inferir que una proposición es falsa porque el argumento que la contiene por conclusión es falaz es en sí una falacia conocida como argumento ad logicam.[114]​

El estudio de las falacias se remonta por lo menos hasta Aristóteles, quien en sus Refutaciones sofísticas identificó y clasificó trece clases de falacias.[111]​ Desde entonces se han agregado a la lista cientos de otras falacias y se han propuesto varios sistemas de clasificación.[115]​

El razonamiento deductivo o deducción es un argumento donde la conclusión se infiere necesariamente de las premisas.[116]​ Tradicionalmente se consideraba, y en muchos casos todavía se considera, que la deducción es un método de razonamiento «top-down», o que «va de lo general a lo particular». Esto, en oposición a la inducción, que sería un método «bottom-up», o que «va de lo particular a lo general». 

En su definición lógica formal, una deducción es una secuencia finita de fórmulas, de las cuales la última es designada como la conclusión (la conclusión de la deducción), y todas las fórmulas en la secuencia son, o bien axiomas, o bien premisas, o bien inferencias directas a partir de fórmulas previas en la secuencia por medio de reglas de inferencia.[116]​[117]​ En resumen es comprender/entender(deducir) algo con base en un argumento.

El razonamiento inductivo o inducción es una forma de razonamiento en que la verdad de las premisas apoyan la conclusión, pero no la garantizan. Un ejemplo clásico de razonamiento inductivo es:

En principio, podría ser que el próximo cuervo que se observe no sea negro. En contraste a los razonamientos deductivos, los razonamientos inductivos tienen la ventaja de ser ampliativos, es decir que la conclusión contiene más información de la que hay contenida en las premisas. Dada su naturaleza ampliativa, los razonamientos inductivos son muy útiles y frecuentes en la ciencia y en la vida cotidiana. Sin embargo, dada su naturaleza falible, su justificación resulta problemática. ¿Cuándo estamos justificados en realizar una inferencia inductiva, y concluir, por ejemplo, que todos los cuervos son negros a partir de una muestra limitada de ellos? ¿Qué distingue a un buen argumento inductivo de uno malo? Estos y otros problemas relacionados dan lugar al problema de la inducción, cuya vigencia e importancia continúa desde hace siglos.

La lógica inductiva estudia las maneras de medir la probabilidad de que una conclusión sea verdadera, así como las reglas para construir argumentos inductivos fuertes. A diferencia de los razonamientos deductivos, en los razonamientos inductivos no existe acuerdo sobre cuándo considerar un argumento como válido. De este modo, se hace uso de la noción de «fuerza inductiva» que hace referencia al grado de probabilidad de que una conclusión sea verdadera cuando sus premisas son verdaderas. Así, un argumento inductivo es fuerte cuando es altamente improbable que su conclusión sea falsa si las premisas son verdaderas.[118]​

Tradicionalmente se consideraba, y en muchos casos todavía se considera, que la inducción es un método «bottom-up», o que «va de lo particular a lo general». Es decir, es una modalidad de razonamiento que, a partir de premisas que contienen datos particulares o individuales, obtiene conclusiones generales. Por ejemplo, a partir de la observación repetida de objetos o eventos de la misma índole se establece una conclusión general para todos los objetos o eventos de dicha naturaleza.[119]​[120]​[121]​ Esto, en oposición a la deducción, que sería un método «top-down», o que «va de lo general a lo particular».

Sin embargo, esa definición ha caído en desuso. Si con estas definiciones de deducción e inducción se quiere decir que en un argumento inductivo válido las premisas son siempre todas afirmaciones particulares y la conclusión es una afirmación general (esto es, cuantificacional).[122]​[123]​ Lo anterior, es dado porque es posible tanto enunciar proposiciones inductivas en forma «deductiva»[124]​ como de manera que no corresponden formalmente a lo que clásicamente se consideraba razonamiento inductivo.[125]​ Cuando en este método se parte de algunos casos, la inducción se denomina «incompleta»; por el contrario, cuando se enumeran todas las cosas para llegar a una conclusión general, esta inducción se conoce como «completa».

Consecuentemente, en el presente, «mucho de la inferencia sintética o contingente ahora se toma como inductiva, algunas autoridades van tan lejos como a considerar toda inferencia contingente como inductiva.»[126]​ Véase Juicios analíticos y sintéticos y Peirce en La inducción como probabilidad más abajo.

El razonamiento abductivo (del latín abdūctiō y esta palabra de ab, desde lejos, y dūcere, llevar) es un tipo de razonamiento que, a partir de la descripción de un hecho o fenómeno, ofrece o llega a una hipótesis que explica las posibles razones o motivos del hecho mediante las premisas obtenidas. Charles Sanders Peirce la llama una conjetura.[129]​ Esa conjetura busca ser, a primera vista, la mejor explicación, o la más probable. Sin embargo, la abducción y la inferencia a la mejor explicación son dos tipos de razonamientos distintos, aunque existen autores que lo discuten. 

Aristóteles investigó los razonamientos abductivos en sus Primeros analíticos (II, 25). Según Aristóteles, los razonamientos abductivos son silogismos en donde las premisas solo brindan cierto grado de probabilidad a la conclusión.[130]​

Una analogía (del griego αναλογíα, ana ‘reiteración o comparación’ y logos ‘estudio’) es una comparación o relación entre varias cosas, razones o conceptos; comparar o relacionar dos o más seres u objetos a través de la razón; señalando características generales y particulares comunes que permiten justificar la existencia de una propiedad en uno, a partir de la existencia de dicha propiedad en los otros.[131]​

En el aspecto lógico, permite comparar un objeto con otros, en sus semejanzas y en sus diferencias.[132]​ 
Una analogía permite la deducción de un término desconocido a partir del análisis de la relación que se establece entre dos términos desconocidos.

Un experimento mental es un recurso de la imaginación empleado para investigar la naturaleza de las cosas. En su sentido más amplio es el empleo de un escenario hipotético que nos ayude a comprender cierto razonamiento o algún aspecto de la realidad. Existe una gran variedad de experimentos mentales y se utilizan en campos tan variados como la filosofía, el derecho, la física y la matemática. Sin embargo, todos emplean una metodología racional independiente de consideraciones empíricas, en el sentido de que no se procede por observación o experimentación física (otra forma de realizar la misma distinción sería entre lo a priori y lo a posteriori).

En filosofía, los experimentos mentales se utilizan por lo menos desde la Antigüedad clásica, algunos filósofos presocráticos, y eran igualmente bien conocidos en el derecho romano. Varias teorías o posturas filosóficas se fundan en los resultados de experimentos mentales: el dilema del tranvía en ética, la habitación china y la tierra gemela en filosofía del lenguaje, el cerebro en una cubeta y el cuarto de Mary en filosofía de la mente, etc.

La especulación (del latín speculari, observar) es una forma filosófica de pensar para ganar conocimiento yendo más allá de la experiencia o práctica tradicional y enfocándose en la esencia de las cosas y sus primeros principios. La especulación es la actividad intelectual que permite la resolución dialéctica de las contradicciones en una unidad de orden superior. El término figura en un lugar crucial en la filosofía de Georg Wilhelm Friedrich Hegel, para el cual este procedimiento de resolución (la Aufhebung o superación) constituía la esencia del pensamiento filosófico.

La mayéutica  (del griego μαιευτικóς, maieutikós, «perito en partos»; μαιευτικη´, maieutiké, «técnica de asistir en los partos»[133]​) es el método aplicado por Sócrates a través del cual el maestro hace que el alumno, por medio de preguntas, descubra conocimientos.[134]​

Como la partera, Sócrates lleva a cabo tres funciones principales o fundamentales: despierta y apacigua los dolores del parto, conduce bien los partos difíciles y provoca, si es necesario, el aborto; el proceso es doloroso debido a las crueles interrogantes del método socrático, pero esto desencadena la iluminación, en la que la verdad parte desde el mismo individuo.[135]​ La invención de este método del conocimiento se remonta al siglo IV a. C. y se atribuye por lo general al Sócrates histórico en referencia a la obra Teeteto, de Platón.

La mayéutica es la segunda de las fases del método socrático. La primera es la llamada ironía socrática, en la que el maestro simula ignorancia sobre la materia a tratar ensalzando inicialmente las cualidades de su interlocutor para, después, hacer comprender a este que lo que creía saber en realidad no lo sabe y que su conocimiento estaba basado en prejuicios o costumbres.[136]​ 

La duda metódica es un método y principio para llegar a una base de conocimiento cierto, desde donde partir y cómo fundamentar otros conocimientos del mundo. René Descartes populariza este método en el siglo XVII. No obstante, son notables y numerosos los escritos y filósofos anteriores que coinciden en formulaciones similares, no solo en su contenido, sino también con evidentes similitudes formales, que sugieren fuertemente que los pudo haber tomado como fuente de consulta e inspiración en su propia filosofía.

Descartes expone que su objetivo es encontrar verdades seguras, tangibles y fácticas de las cuales no sea posible dudar en absoluto, verdades evidentes que permitan fundamentar la edificación del conocimiento con absoluta garantía. El primer problema planteado es cómo encontrarlas y, para resolverlo, expone el método de la duda.

En este método la cuestión preliminar y fundamental es la de decidir por dónde empezar la búsqueda. La respuesta y el primer momento de este proceso de búsqueda del conocimiento verdadero es la llamada duda metódica. La duda metódica consiste en descartar cualquier supuesto no seguro, del que se pueda dudar. Si esta existe, este supuesto podría ser verdadero o falso. No permitiría construir sobre él el conocimiento.[138]​

Ser es el más general de los términos. Con la palabra «ser» se intenta abarcar el ámbito de lo real en sentido ontológico general, esto es, la realidad por antonomasia, en su sentido más amplio: «realidad radical». El Ser es, por lo tanto, un trascendental, aquello que trasciende y rebasa todos los entes sin ser él mismo un ente, es decir, sin que ningún ente, por muy amplio que sea y se presente, lo agote. Dicho de otro modo: el Ser desborda y supera dialécticamente el mundo de las formas, el mundus asdpectabilis, trasladándose en otro contexto, «más allá del horizonte de las formas», más allá de toda la "morfología cósmica".[141]​

La pregunta por el ser no corresponde solamente a Occidente: ya los filósofos antiguos de China desarrollaron independientemente posiciones acerca del ser.  Laozi en el siglo VI a.C. hace la distinción entre ser y no-ser. Luego, las escuelas neo-taoístas (Wang Bi, Guo Xiang, etc.) harán prevalecer el no-ser sobre el ser.

La tradición distingue dos tipos de enfoques distintos al concepto de ser:

La causalidad es la "relación que se establece entre causa y efecto. Se puede hablar de esa relación entre acontecimientos, procesos, regularidad de los fenómenos y la producción de algo".[142]​

No existe una única definición comúnmente aceptada del término "causa". En su acepción más amplia, se dice que algo es causa de un efecto cuando el último depende del primero; o, en otras palabras, la causa es aquello que hace que el efecto sea lo que es. Esto se puede dar de muchos modos diversos y, por ello, no es extraño que a un efecto correspondan multitud de causas.

Dos condiciones necesarias pero no suficientes para que A sea causa de B son:

El uso de la palabra verdad abarca asimismo la honestidad, la buena fe y la sinceridad humana en general; también el acuerdo de los conocimientos con las cosas que se afirman como realidades: los hechos o la cosa en particular;[144]​ y, finalmente, la relación de los hechos o las cosas en su totalidad en la constitución del Todo, el Universo.[145]​

Las cosas son verdaderas cuando son «fiables», fieles porque cumplen lo que ofrecen.[146]​[147]​

El término no tiene una única definición en la que estén de acuerdo la mayoría de los estudiosos y las teorías sobre la verdad continúan siendo ampliamente debatidas. Hay posiciones diferentes acerca de cuestiones como:

Este artículo procura introducir las principales interpretaciones y perspectivas, tanto históricas como actuales, acerca de este concepto.

La pregunta por la verdad es y ha sido objeto de debate entre teólogos, filósofos y lógicos a lo largo de los siglos considerándose un tema concerniente al alma y al estudio de una llamada psicología racional dentro del campo de la filosofía.

En la actualidad es un tema de investigación científica así como de fundamentación filosófica:[148]​

La moral es el conjunto de costumbres y normas que se consideran "buenas" para dirigir o juzgar el comportamiento de las personas en una comunidad.[152]​ También es la diferenciación de intenciones, decisiones y acciones entre las que se distinguen como propias (correctas) y las impropias (incorrectas).[153]​ Se distingue de la ética en que ésta es una moral transcultural o universal, aunque suelen confundirse. La moral permite distinguir cuáles acciones son buenas y cuáles son malas con criterios objetivos. Otra perspectiva la define como el conocimiento de lo que el ser humano debe hacer o evitar para conservar la estabilidad social.[154]​

El término «moral» tiene un sentido opuesto al de «inmoral» (contra la moral) y «amoral» (sin moral). La existencia de acciones susceptibles de valoración moral está fundamentada en el ser humano, como sujeto de actos voluntarios. Abarca la acción de las personas en todas sus manifestaciones, además de que permite la introducción y referencia de los valores.

Los conceptos y creencias sobre la moral llegan a ser considerados y codificados de acuerdo a una cultura, religión, grupo, u otro esquema de ideas, que tienen como función la regulación del comportamiento de sus miembros. La conformidad con dichas codificaciones también puede ser conocida como moral y se considera que la sociedad depende del uso generalizado de esta para su existencia. En la práctica, suelen ser conductas morales basadas, no en planteamientos religiosos, sino coherentes con un determinada antropología. Pueden llegar a darse situaciones equívocas si se pretende negar valor ético a comportamientos que tengan su origen en la religión.[155]​

Hay diversas definiciones y concepciones de lo que significa moral, lo que ha sido tema de discusión y debate a través del tiempo. Múltiples opiniones concuerdan en que el término representa aquello que permite distinguir entre el bien y el mal[156]​ de los actos, mientras que otros dicen que son solo las costumbres las que se evalúan virtuosas o perniciosas.

La belleza se describe comúnmente como una cualidad de los objetos que hace que estos sean placenteros de percibir. Tales objetos incluyen paisajes, puestas de sol, seres humanos u obras de arte. Belleza es una noción abstracta ligada a numerosos aspectos de la existencia humana. La belleza se estudia dentro de la disciplina filosófica de la estética, además de otras disciplinas como la historia, la sociología y la psicología social. 
La belleza se define como la característica de una cosa que a través de una experiencia sensorial (percepción) procura una sensación de placer o un sentimiento de satisfacción.[157]​ Proviene de manifestaciones tales como la forma, el aspecto visual, el movimiento y el sonido, aunque también se la asocia, en menor medida, a los sabores y los olores. En esta línea y haciendo hincapié en el aspecto visual, Tomás de Aquino define lo bello como aquello que agrada a la vista (quae visa placet).[158]​

La percepción de la «belleza» a menudo implica la interpretación de alguna entidad que está en equilibrio y armonía con la naturaleza, y  puede conducir a sentimientos de atracción y bienestar emocional. Debido a que constituye una experiencia subjetiva, a menudo se dice que «la belleza está en el ojo del observador».[159]​ Aunque tal relativismo es exagerado y suele asociarse a cosmovisiones y modas, lo concreto es que existen objetos y seres que dan la impresión de belleza ya desde su objetividad natural porque se corresponden con los requisitos naturales del homo sapiens, por ejemplo: el sabor dulce es preferido al sabor amargo porque el amargo suele corresponder a tóxicos, lo mismo que la fragancia de muchas flores se prefiere naturalmente en gente psíquicamente sana al hedor pútrido.

Un problema filosófico es una cuestión cuyo planteamiento teórico se presenta sin aparente respuesta o cuya solución es cuestionable. Los problemas tienden a expresar temas de recurrente dominio de la filosofía como: la sabiduría, el hombre, la verdad, el conocimiento, la moral, el arte, la reflexión, la conciencia, la lógica, la realidad, la ciencia, el sentido de la vida, etc. Estos problemas se pueden presentar en dilemas o paradojas.

El dilema de Eutifrón es planteado en el diálogo Eutifrón de Platón. Sócrates pregunta a Eutifrón: «¿Es el piadoso (τὸ ὅσιον) amado por los dioses porque es piadoso, o es piadoso debido a que es amado por los dioses?».[160]​

El trilema de Münchhausen o trilema de Agripa es un ataque a la posibilidad de lograr una justificación última para cualquier proposición, incluso en las ciencias formales como la matemática y la lógica. 

Un trilema es un problema que admite sólo tres soluciones, todas las cuales parecen inaceptables. El argumento discurre así: cualquiera que sea la manera en que se justifique una proposición, si lo que se quiere es certeza absoluta, siempre será necesario justificar los medios de la justificación, y luego los medios de esa nueva justificación, etc. Esta simple observación conduce sin escape a una de las siguientes tres alternativas (los tres cuernos del trilema):[162]​

El problema de Gettier es un problema en gnoseología moderna que surge al presentar contraejemplos a la definición clásica de conocimiento como «creencia verdadera justificada» y que obligan a modificar la definición.

Desde al menos el Teeteto de Platón,[163]​ la gnoseología contaba con una definición generalmente satisfactoria del conocimiento proposicional: si S es un sujeto y p una proposición, entonces S sabe que p si y sólo si:

Por ejemplo, Newton sabe  que de alguna manera  tiene una manzana si y sólo si:

Sin embargo, en 1963, Edmund Gettier publicó un artículo de tres páginas titulado ¿Es el conocimiento creencia verdadera justificada? en el que argumentó que la definición clásica no es suficiente. Gettier mostró que hay casos en los que una creencia verdadera justificada puede fallar en ser conocimiento. Es decir, hay casos en los que los tres requisitos se cumplen, y sin embargo intuitivamente nos parece que no hay conocimiento. Retomando el ejemplo anterior, podría ser que Newton crea que tiene una manzana y esté justificado en ello (por ejemplo, porque parece una manzana), pero que sin embargo la manzana sea de cera. En ese caso, según la definición clásica, Newton no posee conocimiento, porque falta que sea verdad que tiene una manzana. Pero supongamos también que dentro de la manzana de cera hay otra manzana, más pequeña, pero real. Entonces Newton cumple con los tres requisitos: Newton cree que tiene una manzana; Newton está justificado en su creencia; y de hecho tiene una manzana. Sin embargo, intuitivamente nos parece que Newton no posee conocimiento, sino que solamente tuvo suerte (lo que se llama suerte epistémica).

A partir de la definición de Platón del conocimiento como «creencia verdadera y justificada»,[166]​ disponer de una justificación adecuada para la inducción es requisito indispensable para que tales «creencias» constituyan conocimiento válido o legítimo.

La RAE define «inducir» en su sentido filosófico, como «extraer, a partir de determinadas observaciones o experiencias particulares, el principio general que en ellas está implícito». Esas «extracciones» son de dos tipos:

Dado que ambas son utilizadas, ya sea explícita o implícitamente, en forma generalizada para proponer hipótesis —ya sea formales o no— a partir de observaciones empíricas, su cuestionamiento pone en duda una gran parte, si es que no la totalidad, del conocimiento humano. El problema adquiere especial relevancia en el ámbito científico, dado que generalmente se pensaba que las asunciones necesarias para formular leyes científicas requieren tanto generalizaciones como expectativas de que eventos en el futuro continuarán exhibiendo los mismos comportamientos que en el pasado.[168]​ Esto se expresa generalmente como el principio de simetría[169]​[170]​[171]​ o principio de invariancia[172]​ (véase también principio de Curie[173]​ y teorema de Noether).

Consecuentemente Alfred North Whitehead describió la inducción como «el rompecabezas (the despair) de la filosofía»[174]​ y el filósofo C. D. Broad sugirió: «La inducción es la gloria de la ciencia, y el escándalo de la filosofía».[175]​

El problema del ser y el deber ser (también llamado ley de Hume, la guillotina de Hume y a veces confundido con la falacia naturalista) es un problema en metaética acerca de la posibilidad de deducir oraciones normativas a partir de oraciones descriptivas. Las oraciones descriptivas son aquellas que dicen lo que es el caso (por ejemplo «los emperadores son crueles») mientras que las oraciones normativas son aquellas que dicen lo que debe ser el caso («los emperadores deben ser crueles»).

Claro que así como se puede pedir justificación para las oraciones normativas, se puede pedir justificación para las oraciones descriptivas. Pero esto es otro problema, que puede encontrar otras respuestas. Las oraciones descriptivas se pueden (quizás) justificar a partir de la investigación empírica. Así por ejemplo, el valor de verdad de la oración «los emperadores son crueles» se puede determinar haciendo una investigación histórica. Sin embargo, no sucede lo mismo con la oración «los emperadores deben ser crueles». La verdad o falsedad de esta oración se debe determinar por otros métodos, y si se descarta la posibilidad de probar su verdad a través de una deducción a partir de premisas verdaderas, entonces vale preguntar si hay algún otro camino.

El abismo que separa a los hechos de los deberes no tiene nada que ver con el contenido de las proposiciones descriptivas de las que se parte. Lo mismo da que se trate de proposiciones metafísicas, científicas o de la vida cotidiana. El error se encuentra en el procedimiento, no en el punto de partida. La ambigüedad inadvertida empírico-normativa de ciertos términos conduce a falacias lógicas tales como: «La esencia de la sexualidad es la procreación. Por lo tanto, la anticoncepción no está permitida, porque no refleja la naturaleza de la sexualidad».

En filosofía del espíritu y ciencia cognitiva, el problema mente-cuerpo es el problema de explicar la relación entre la mente (alma para algunos autores) y la materia: cómo es que estados mentales o subjetivos (ej. sensaciones, creencias, decisiones, recuerdos) explican a, interactúan con, o bien supervienen de las sustancias y procesos del mundo de objetos estudiado por la ciencia.[183]​ Se trata por lo tanto de un problema ontológico; mientras que el problema de otras mentes puede ser entendido como su homólogo epistémico.

El problema fue  descrito por René Descartes en el siglo XVII, y por los filósofos aristotélicos, en la filosofía de Avicena, y en las anteriores tradiciones asiáticas.[184]​[185]​[186]​ Una variedad de ontologías han sido propuestas; la mayoría de ellas dualistas (como la cartesiana) o monistas. El dualismo sostiene una distinción entre las esferas material y mental; pudiendo llegar a ser esta última algo sobrenatural. El monismo sostiene que existe solo una realidad, sustancia o esencia unificadora en cuyos términos todo puede ser explicado.

El problema mente-cuerpo está estrechamente ligado a la intencionalidad, la causalidad mental, el problema difícil de la consciencia, el del libre albedrío, el de la significación de los símbolos, el de la identidad del individuo, el problema de otras mentes, etc.

El problema del mal o también, paradoja de Epicuro, es estudiado en filosofía de la religión, en teodicea y en metafísica como el problema que resulta al considerar la compatibilidad entre la presencia del mal y del sufrimiento en el mundo con la existencia de Dios omnisciente, omnipresente, omnipotente y omnibenevolente.  El argumento del mal afirma que debido a la existencia del mal, o Dios no existe o no tiene alguna de las tres propiedades mencionadas. Los argumentos para sostener lo contrario se les conoce tradicionalmente como teodiceas. Además de la filosofía de la religión, el problema del mal también es importante en los campos de la teología y ética. El problema del mal se puede expresar de la siguiente forma:

A menudo se formula de dos formas: el problema del mal lógico y el problema del mal evidencial.[198]​ La versión lógica del argumento intenta demostrar deductivamente una imposibilidad lógica en la coexistencia entre Dios y el mal,[199]​ mientras que la evidencial sostiene inductivamente que dado que existe el mal en el mundo, es improbable que exista un dios omnipotente, omnisciente y perfectamente bueno.[200]​ Esta versión inductiva del argumento es más popular que su versión deductiva. El problema del mal también se ha extendido a los seres vivos no humanos, incluidos el sufrimiento animal provocado por la naturaleza y la crueldad animal humana.[201]​ 

Existe una amplia variedad de respuestas al problema del mal y se clasifican en: refutaciones, defensas y teodiceas. Hay además muchas discusiones sobre el mal y problemas relacionados en otros campos filosóficos, tales como la ética secular[202]​[203]​[204]​ o la ética evolucionista,[205]​[206]​ pero en el sentido ordinario el "problema del mal" se trata dentro del contexto teológico.[198]​[200]​

La filosofía antigua es el período de la historia de la filosofía occidental que corresponde a la Edad Antigua. Comprende la filosofía griega (presocrática y helenística) y la filosofía romana.[213]​ Duró más de 1100 años, desde alrededor del año 600 a. C. (con Tales de Mileto) hasta el siglo VI d.C., cuando los últimos neoplatónicos estaban activos. Sus principales ubicaciones fueron la antigua Grecia y el Imperio Romano.

La filosofía de la antigüedad fue limitada geográficamente en el Mediterráneo. Los filósofos de la antigüedad pueden dividirse a grandes rasgos en diferentes grupos. Primero, los filósofos anteriores a Sócrates, llamados «presocráticos» (alrededor del 600 - 400 a.C.) y conocidos por dar «el paso del mito al logos». Luego, el período clásico griego, que comienza con Sócrates (alrededor del 500 - 300 a. C.). Platón, alumno de Sócrates, y Aristóteles, alumno de Platón, se convirtieron en dos de los filósofos más importantes e influyentes, conocidos como los «socráticos mayores». Otros contemporáneos fueron los sofistas y los «socráticos menores» (megáricos, cínicos y cirenaicos).[214]​ Finalmente, la filosofía del período helenístico siguió al período clásico, seguida por la filosofía de la antigüedad tardía, que incluyen a los epicúreos, los estoicos, los escépticos y los neoplatónicos.

Otras tradiciones filosóficas importantes de la antigüedad fueron la filosofía china y la filosofía india, influyentes fueron las culturas del judaísmo, el antiguo Egipto, el Imperio Persa y Mesopotamia. En las regiones del Creciente Fértil, Irán y Arabia surgió la literatura filosófica de los libros sapienciales y que hoy domina la cultura islámica. La literatura sapiencial temprana del Creciente Fértil era un género que buscaba instruir a las personas sobre la acción ética, la vida práctica y la virtud a través de historias y proverbios. En el Antiguo Egipto, estos textos eran conocidos como sebayt («enseñanzas») y son fundamentales para nuestra comprensión de la filosofía del Antiguo Egipto. La astronomía babilónica también incluyó muchas especulaciones filosóficas sobre la cosmología que pudieron haber influido en los antiguos griegos. 

La filosofía judía y la filosofía cristiana son tradiciones religio-filosóficas que se desarrollaron tanto en Oriente Medio como en Europa, que comparten ciertos textos judaicos primitivos (principalmente el Tanaj) y creencias monoteístas. Los pensadores judíos como los Geonim de las Academias Talmúdicas en Babilonia y el filósofo Maimónides estudiaban la filosofía griega e islámica. Más tarde, la filosofía judía estuvo bajo fuertes influencias intelectuales occidentales e incluye las obras de Moisés Mendelssohn, quien marcó el comienzo de la Haskalá (también conocida como la ilustración judía), el existencialismo judío y el judaísmo reformista.

La filosofía renacentista, o filosofía del Renacimiento, es la filosofía que se desarrolló principalmente entre los siglos XV y XVI, comenzando en Italia y avanzando hacia el resto de Europa.

En el Renacimiento, la filosofía todavía era un campo muy amplio que abarcaba los estudios que hoy se asignan a varias ciencias distintas,[216]​ así como a la teología. Teniendo eso en cuenta, los tres campos de la filosofía que más atención y desarrollo recibieron fueron la filosofía política, el humanismo y la filosofía natural.[216]​

En la filosofía política, las rivalidades entre los estados nacionales, sus crisis internas y el comienzo de la colonización europea de América renovaron el interés por problemas acerca de la naturaleza y moralidad del poder político, la unidad nacional, la seguridad interna, el poder del Estado y la justicia internacional.[216]​ En este campo destacaron los trabajos de Nicolás Maquiavelo, Jean Bodin y Francisco de Vitoria.[216]​

El humanismo fue un movimiento que enfatizó el valor y la importancia de los seres humanos en el universo,[216]​ en contraste la filosofía medieval, que siempre puso a Dios y al cristianismo en el centro. Este movimiento fue, en primer lugar, un movimiento moral y literario, protagonizado por figuras como Erasmo de Róterdam, Santo Tomás Moro, Bartolomé de las Casas y Michel de Montaigne.[216]​

La filosofía de la naturaleza del Renacimiento quebró con la concepción medieval de la naturaleza en términos de fines y ordenamiento divino, y comenzó a pensar en términos de fuerzas, causas físicas y mecanismos.[216]​ Hubo además un retorno parcial a la autoridad de Platón por sobre Aristóteles, tanto en su filosofía moral, en su estilo literario como en la relevancia dada a la matemática para el estudio de la naturaleza.[216]​ Nicolás Copérnico, Giordano Bruno, Johannes Kepler, Leonardo da Vinci y Galileo Galilei fueron precursores y protagonistas en esta revolución científica, y Francis Bacon proveyó un fundamento teórico para justificar el método empírico que habría de caracterizar a la revolución. Por otra parte, en la medicina, el trabajo de Andreas Vesalius en anatomía humana revitalizó la disciplina y brindó más apoyo al método empírico.[216]​ La filosofía de la naturaleza renacentista tal vez se explica mejor por dos proposiciones escritas por Leonardo da Vinci en sus cuadernos:

De manera similar, Galieo basó su método científico en experimentos, pero también desarrolló métodos matemáticos para su aplicación a problemas de física, un ejemplo temprano de física matemática. Estas dos formas de concebir el conocimiento humano formaron el fondo para el inicio del empirismo y el racionalismo, respectivamente.[216]​

La filosofía moderna es aquella filosofía desarrollada durante la edad moderna y asociada con la modernidad. No es una doctrina concreta o escuela (por lo que no debe ser confundida con movimientos específicos como el Modernismo), a pesar de que muchos autores de esta era comparten ciertos supuestos comunes, lo cual ayuda para distinguirla de filosofía anterior y posterior.[219]​

El siglo XVII marca el inicio de la filosofía moderna, mientras que el comienzo del siglo XX marca aproximadamente su fin. Cuánta parte del Renacimiento debería ser incluido como parte de la filosofía moderna es un asunto controvertido: el Renacimiento Temprano es a menudo considerado menos moderno y más medieval comparado al Alto Renacimiento más tardío.[220]​ También se debate si la modernidad ha acabado o no en el siglo XX y si ha sido reemplazada por la posmodernidad. Cómo uno decide estas cuestiones determina el alcance del uso del concepto de «filosofía moderna». Otro de estos usos es datar la filosofía moderna desde la «Era de la Razón», donde la filosofía sistemática se hizo común, lo cual excluye a Erasmo de Róterdam y a Nicolás Maquiavelo como «filósofos modernos». Otra forma es fecharla, de la misma forma que la mayoría del período moderno está fechado, desde el Renacimiento. Para algunos, la filosofía moderna terminó en 1800 con el surgimiento del hegelianismo y del idealismo. Una visión general tendría entonces a Erasmo de Róterdam, Francis Bacon, Nicolás Maquiavelo y Galileo Galilei como representantes del auge del empirismo y del humanismo.

Durante los siglos XVII y XVIII, las figuras importantes en filosofía de mente, epistemología y metafísica se podían dividir aproximadamente en dos grupos principales. El racionalismo, dominante en Francia y Alemania, que argumentaba que todo conocimiento tiene que empezar de ideas innatas en la mente. Racionalistas importantes fueron René Descartes, Baruch Spinoza, Gottfried Leibniz, y Nicolás Malebranche. El empirismo, por otro lado, defendió que el conocimiento siempre empieza por la experiencia sensorial que recibimos a través de los sentidos. Figuras importantes de esta línea de pensamiento fueron David Hume, John Locke y George Berkeley. La ética y la filosofía política generalmente no se subsume dentro de estas categorías, aunque todos estos filósofos trabajaron en la ética en sus estilos distintivos propios. Otras figuras importantes en filosofía política son Thomas Hobbes y Jean-Jacques Rousseau.

A fines del siglo XVIII, Immanuel Kant estableció un sistema filosófico innovador que pretendía reconciliar el racionalismo y el empirismo. Tuviera o no razón, la disputa filosófica continuó. Kant influyó fuertemente en las obras filosóficas alemanas a principios del siglo XIX, comenzando así la tradición del idealismo alemán. El tema característico del idealismo fue que el mundo y la mente deben entenderse de acuerdo a las mismas categorías. El idealismo alemán culminó con el trabajo de Georg Wilhelm Friedrich Hegel, quien, entre muchas otras cosas, dijo que «lo real es racional; lo racional es real».

La filosofía contemporánea es el período actual de la historia de la filosofía. Por extensión, se llama también con este nombre a la filosofía producida por filósofos que aún están vivos. Es el período que sigue a la filosofía moderna, y su inicio se suele fijar a finales del siglo XIX o principios del siglo XX.

Las tradiciones filosóficas más significativas y abarcadoras del siglo XX fueron la filosofía analítica en el mundo anglosajón, y la filosofía continental en la Europa continental.[231]​ El siglo XX también vio el surgimiento de nuevas corrientes filosóficas, como el positivismo lógico, la fenomenología, el existencialismo, el postestructuralismo, y el materialismo filosófico.

La filosofía oriental o filosofía asiática incluye las diversas filosofías de Asia del Sur y Asia Oriental, incluida la filosofía china, la filosofía india, la filosofía budista (dominante en el Tíbet, Bhután, Sri Lanka y el Sudeste Asiático), la filosofía coreana y la filosofía japonesa.[233]​[234]​

La filosofía persa o filosofía iraní[237]​[238]​[239]​[240]​[241]​ se remonta a tiempos  de tradiciones filosóficas y pensamientos que se originaron en la antigua persia con raíces indo-iraníes y fueron influenciadas considerablemente por las enseñanzas de Zoroastro. La cronología de la materia y de la ciencia de la filosofía comienza con los indo-iraníes, que datan este evento a 1500 a. C.[242]​ La filosofía de Zaratustra ingresó a influir la tradición occidental a través del Judaísmo, y por lo tanto en el platonismo medio.[242]​

La filosofía india (en sánscrito: darśana, «enseñanza»)[244]​ es la descripción de la filosofía oriental que abarca las filosofías, visiones del mundo y enseñanzas [245]​ que surgieron en la antigua India. Estos incluyen seis sistemas ortodoxos (shad-darśana) Sankhya, Yoga, Nyaya, Vaisheshika, Mimamsa y Vedanta.[246]​

Las tradiciones de la filosofía india se clasifican generalmente como ortodoxas o heterodoxas, āstika o nāstika,[247]​ dependiendo de si aceptan la autoridad de los Vedas y si aceptan las teorías de Brahman y Atman.[248]​[249]​ Las escuelas ortodoxas, propiamente hinduistas generalmente incluyen Nyaya, Vaisheshika, Samkhya, Yoga, Mīmāṃsā y Vedanta, y las escuelas heterodoxas comunes son Jainismo, Budismo, Ajñana, Cārvāka, Ajivika y Lokaiata.

Algunos de los primeros textos filosóficos que sobreviven son los Upanishads del período védico posterior (1000-500 a. C.). Los conceptos filosóficos indios importantes incluyen dharma, karma, samsara, moksha y ahimsa. Los filósofos indios desarrollaron un sistema de razonamiento epistemológico (pramana) y lógica e investigaron temas como la metafísica, la ética, la hermenéutica y la soteriología. La filosofía india también cubrió temas como la filosofía política como se ve en el Arthashastra (siglo IV a. C.) y la filosofía del amor como se ve en el Kama Sutra.

Las seis escuelas ortodoxas comunes surgieron entre el comienzo de la era común y el Imperio Gupta.[250]​ Estas escuelas hindúes desarrollaron lo que se ha llamado la «síntesis hindú» fusionando elementos brahmánicos y elementos heterodoxos del budismo y el jainismo.[251]​ El pensamiento hindú también se extendió hacia el este llegando al imperio indonesio Srivijaya y el Imperio jemer camboyano. Estas tradiciones se agruparon más tarde bajo el nombre Hinduismo. El Hinduismo con sus diferentes denominaciones es la religión dominante en Asia del Sur. Así, el hinduismo es una categorización de distintos puntos de vista intelectuales o filosóficos, más que un conjunto de creencias rígidas,[252]​ y con cerca de mil millones de seguidores es la tercera religión más grande del mundo, después del cristianismo y el islam.[253]​

Desarrollos posteriores incluyen el desarrollo del Tantra y las influencias islámicas. El budismo desapareció en su mayoría de la India después de la conquista musulmana en el subcontinente indio, sobreviviendo en las regiones del Himalaya y el sur de la India.[254]​ El período moderno temprano vio el florecimiento de Navya-Nyāya (la «nueva razón») bajo filósofos como Raghunatha Siromani (circa 1460-1540) que fundó la tradición, Jayarama Pancanana, Mahadeva Punatamakara y Yashovijaya (quien formuló una respuesta jainista).[255]​

En la historia de la filosofía india se pueden distinguir tres grandes períodos. El primer período es el del vedismo, que transcurrió aproximadamente desde el siglo XV a. C. hasta el siglo VIII a. C. Durante este primer período se desarrollaron los primeros textos védicos, en particular el Rig vedá, el Sama vedá, el Iáyur vedá y el Átharva vedá, así como el sistema de castas. El segundo período es aquel del brahmanismo, aproximadamente desde el siglo VIII a. C. hasta el siglo V a. C. En este período se incorporaron los Upanishads al conjunto de textos sagrados. Y el tercer período es el del hinduismo propiamente, que comienza aproximadamente en el siglo V a. C. y aún continúa.

La filosofía china es la descripción de la filosofía oriental que comprende la suma de escuelas filosóficas creadas en China. Tiene una historia de varios miles de años y su inicio se suele establecer en el siglo XII a. C. con la escritura del I Ching (El libro de los cambios), un compendio antiguo sobre adivinación que introdujo alguno de los términos fundamentales de la filosofía china. Sin embargo, la tradición oral se remonta a épocas neolíticas.

La historia de la filosofía china se puede dividir en cuatro períodos. El primero vio venir las primeras doctrinas de la dinastía Shang acerca de lo cíclico, así como el I Ching (el Libro de los cambios). El segundo período es el de la filosofía china clásica, conocido por la variedad y cantidad de escuelas que se formaron. Entre ellas destacaron el confucianismo, el taoísmo, el moísmo, el legalismo y la Escuela de los Nombres. El tercer período comenzó cuando la dinastía Qin adoptó como filosofía oficial el legismo, persiguiendo además a los confucianistas y moistas. Luego la dinastía Han impuso al confucianismo y taoísmo como doctrinas oficiales, y su influencia continuaría hasta el siglo XX. El último período, el de la modernidad, se caracteriza por la importación e incorporación de la filosofía occidental.

Durante la dinastía Zhou occidental y los siguientes períodos después de su caída, florecieron las cien escuelas del pensamiento (siglo VI a 221 a. C.).[257]​[258]​ Este período se caracterizó por importantes desarrollos intelectuales y culturales y vio el surgimiento de las principales escuelas filosóficas de China: el confucianismo, el legalismo y el taoísmo, así como numerosas otras escuelas menos influyentes. Estas tradiciones filosóficas desarrollaron teorías metafísicas, políticas y éticas como Tao, Yin y yang, Ren y Li que, junto con el budismo chino, influyeron directamente en la filosofía coreana, la filosofía vietnamita y la filosofía japonesa (que también incluye la tradición sintoísta nativa). El budismo comenzó a llegar a China durante la dinastía Han (206 a. C.-220 d. C.) a través de una transmisión gradual a través de la Ruta de la Seda, y mediante influencias nativas desarrollaron distintas formas chinas (como Zen) que se extendieron por toda la esfera cultural de Asia Oriental. Durante las dinastías chinas posteriores, como la dinastía Ming (1368-1644), así como en la dinastía coreana de Joseon (1392-1897), un renacimiento del neoconfucianismo dirigido por pensadores como Wang Yangming (1472-1529) se convirtió en la escuela de pensamiento dominante, y fue promovido por el estado imperial.

La filosofía japonesa es la descripción de la filosofía oriental que se origina a partir del desarrollo cultural de Japón, a través del proceso religioso e histórico que surgió del pensamiento chino, manteniéndose hasta el período Heian, del cual se inicia el pensamiento japonés y al igual que el primero, se orienta a los asuntos de sabiduría práctica.

La filosofía budista es la descripción de la filosofía oriental que comprende la suma de las investigaciones filosóficas de las varias escuelas budistas. La principal preocupación del budismo siempre fue la liberación del sufrimiento (nirvana)[262]​ y el camino hacia esa liberación, que consiste en acción ética (sīla), meditación y sabiduría (prajña, saber «las cosas como realmente son», sct. yathābhūtaṃ viditvā). Los budistas indios buscaron esta comprensión no solo a partir de las enseñanzas del Buda, sino a través del análisis filosófico y la deliberación racional.[263]​ Los pensadores budistas en India y posteriormente en Asia oriental han cubierto temas filosóficos tan variados como fenomenología, ética, ontología, epistemología, lógica y filosofía del tiempo en su análisis de este camino.

El budismo temprano se basó en evidencia empírica obtenida por los órganos de los sentidos (ayatana)[264]​ y el Buda parece haber mantenido una distancia escéptica de ciertas preguntas metafísicas, negándose a responderlas porque no eran conducentes a la liberación. Los puntos particulares de la filosofía budista han sido a menudo objeto de disputas entre diferentes campos filosóficos budistas. Estas disputas dieron lugar a varias escuelas llamadas Abhidharma, y a las tradiciones Mahayana de Prajnaparamita (perfección de la sabiduría), Madhyamaka (camino medio) y Yogacara (práctica de yoga).

La filosofía budista comienza con el pensamiento de Gautama Buddha (circa siglos VI y IV a. C.) y se conserva en los primeros textos budistas como las Nikayas del Canon Pali. El pensamiento budista es transregional y transcultural. Se originó en la India y luego se extendió a Asia oriental, el Tíbet, Asia central y el Sudeste Asiático, desarrollando tradiciones nuevas y sincréticas en estas diferentes regiones. Las diversas escuelas del pensamiento budistas son la tradición filosófica dominante en el Tíbet y en países del sudeste asiático como Sri Lanka y Birmania.

La principal preocupación del budismo es la soteriología, definida como la libertad desde dukkha (inquietud).[265]​ Debido a que la ignorancia sobre la verdadera naturaleza de las cosas se considera una de las raíces del sufrimiento (dukkha), la filosofía budista se ocupa de la epistemología, la metafísica, la ética y la psicología.[266]​ Los textos filosóficos budistas también se deben entender dentro del contexto de las prácticas meditativas que se supone que producen ciertos cambios cognitivos.[267]​ Los conceptos innovadores clave incluyen las Cuatro Nobles Verdades, Anatta (no-yo) una crítica de una identidad personal fija, la transitoriedad (Anicca) de todas las cosas y un cierto escepticismo sobre las preguntas metafísicas.[268]​

Después de la muerte de Buda, varios grupos comenzaron a sistematizar sus principales enseñanzas y desarrollaron sistemas filosóficos denominados Abhidharma.[269]​ Los filósofos de Mahayana como Nagarjuna y Vasubandhu desarrollaron las teorías de shunyata (vacuidad de todos los fenómenos) y «vijnapti-matra» (solo apariencia), una forma de fenomenología o idealismo trascendental.[270]​ La escuela de Dignāga o escuela de pramāṇa promovió una forma de epistemología y lógica. A través del trabajo de Dharmakirti, esta tradición de lógica budista se ha convertido en el principal sistema epistemológico utilizado en la filosofía y el debate de budismo tibetano.[271]​

Según el profesor de filosofía budista Jan Westerhoff, las principales escuelas indias desde 300 a. C. hasta 1000 d. C. fueron:[272]​

La filosofía islámica es el conjunto de doctrinas relacionadas con la vida, el universo, la ética, la sociedad y demás cuestiones fundamentales vinculadas al mundo islámico.

La tradición islámica actual combina algunos pensamientos del neoplatonismo y del aristotelismo con otros conceptos que fueron insertados mediante el desarrollo del Islam. Ciertos filósofos de peso como el árabe al-Kindi y los persas al-Farabi y Avicena, así como Ibn Tufail y Averroes, originarios de la península ibérica, precisaron algunas interpretaciones de Aristóteles que fueron después absorbidas por los intelectuales judíos y cristianos.
La historia de la filosofía islámica contiene ejemplos significativos de otros filósofos que abordaron un gran número de cuestiones que terminaron por influenciar al escolasticismo medieval de Europa, entre ellos se encuentran Al-Ghazali y Mulla Sadra.

La filosofía africana es la filosofía producida por la gente africana, filosofía que presenta visiones del mundo, ideas y temas africanos, o filosofía que utiliza diferentes métodos filosóficos africanos. El pensamiento africano moderno se ha ocupado de la etnofilosofía, con definir el significado mismo de la filosofía africana y sus características únicas y lo que significa ser africano.[273]​

Durante el siglo XVII, la filosofía etíope desarrolló una sólida tradición literaria como ejemplifica Zera Yacob. Otro de los primeros filósofos africanos fue Anton Wilhelm Amo (c. 1703–1759) que se convirtió en un respetado filósofo en Alemania. Diferentes ideas filosóficas africanas incluyen Ujamaa, la idea Bantú de 'Fuerza', Negritud, Panafricanismo y Ubuntu. El pensamiento africano contemporáneo también ha visto el desarrollo de la filosofía profesional y de la filosofía africana, la literatura filosófica de la diáspora africana que incluye corrientes como el existencialismo negro de  afrodescendientes estadounidenses. Algunos pensadores africanos modernos han sido influenciados por el Marxismo, la literatura afroamericana, teoría crítica, teoría crítica de la raza, postcolonialismo y feminismo.

El pensamiento filosófico indigenoamericano consiste en una amplia variedad de creencias y tradiciones entre diferentes culturas. Entre algunas de las comunidades de nativos americano de los Estados Unidos, hay una creencia en un principio metafísico llamado el 'Gran Espíritu' (Siux: wakȟáŋ tȟáŋka; Algonquino: gitche manitou). Otro concepto ampliamente compartido fue el de orenda ('poder espiritual'). Según Whiteley (1998), para los nativos americanos, "la mente está críticamente informada por la experiencia trascendental (sueños, visiones, etc.) así como por la razón."[274]​ La práctica para acceder a estas experiencias trascendentales se denomina chamanismo. Otra característica de las cosmovisiones indígenas estadounidenses fue su extensión de la ética a animales y plantas no humanos.[274]​[275]​

En Mesoamérica, la filosofía azteca fue una tradición intelectual desarrollada por individuos llamados Tlamatini ('los que saben algo')[276]​ y sus ideas se conservan en varios Códices mexicas. La cosmovisión azteca postuló el concepto de una energía o fuerza universal llamada Ometéotl ('Energía Cósmica Dual') que buscaba una forma de vivir en equilibrio en un mundo en cambio constante.

La teoría de Téotl puede verse como una forma de panteísmo.[277]​ Los filósofos aztecas desarrollaron teorías de metafísica, epistemología, valores y estética. La ética azteca se centró en buscar tlamatiliztli ('conocimiento', 'sabiduría') que se basó en la moderación y el equilibrio en todas las acciones como en el proverbio Nahua "el bien medio es necesario."[277]​

La civilización inca también tenía una clase de élite de filósofos-eruditos denominados los Amawtakuna que fueron importantes en el sistema de educación inca como profesores de religión, tradición, historia y ética. Los conceptos clave del pensamiento andino son Yanantin i Masintin que involucran una teoría de “opuestos complementarios” que ve las polaridades (como masculino/femenino, oscuro/claro) como partes interdependientes de un todo armonioso.[278]​

Muchos debates filosóficos que comenzaron en la antigüedad todavía se debaten hoy. El filósofo británico Colin McGinn afirma que no hubo progreso filosófico durante ese intervalo.[279]​ El filósofo australiano David Chalmers, por el contrario, ve el progreso en la filosofía similar al de la ciencia.[280]​ Mientras tanto, Talbot Brewer, profesor de filosofía en la Universidad de Virginia, sostiene que el "progreso" es el estándar equivocado para juzgar la actividad filosófica.[281]​

Desde la antigüedad se ha tenido conocimiento de mujeres que se han dedicado a la filosofía a lo largo de la historia, pero mucho de su legado no ha sido tan estudiado hasta nuestros días. Existen testimonios de mujeres filósofas al menos desde la Grecia antigua y un número relativamente pequeño de ellas fueron consideradas como tal en las épocas antigua, medieval, moderna y contemporánea, especialmente durante los siglos XX y XXI, apenas hay mujeres filósofas que hayan entrado en el canon filosófico occidental.[282]​[283]​ La mujer y la filosofía siempre se ha mantenido en un completo tabú y según estudios posteriores algunos filósofos occidentales atribuían al hombre un carácter racional y a la mujer un potencial más emotivo e intuitivo. De esta opinión fueron Aristóteles, Séneca, Tomás de Aquino, Rousseau, Hegel, Schopenhauer y Nietzsche.

En la filosofía antigua en Occidente, mientras que la filosofía académica era del dominio de filósofos masculinos como Platón y Aristóteles, filósofas como Hiparquia de Maronea (activa hacia el año 325 aC), Areta de Cirene (activa en el siglo V-IV aC) y Aspasia de Mileto (470-400 aC) mantuvieron también actividad durante este período. Una notable filósofa medieval fue Hipatia (siglo V). Filósofas modernas destacadas fueron Mary Wollstonecraft (1759-1797) y Margaret Fuller (1810-1850). Entre las filósofas contemporáneas influyentes están Ayn Rand (1905-1982), Susanne Langer (1895-1985), Hannah Arendt (1906-1975), Simone de Beauvoir (1908-1986), María Zambrano (1904-1991), Mary Midgley (1919), Mary Warnock (1924-2019), Celia Amorós (1944), Julia Kristeva (1941), Patricia Churchland (nacida en 1943), Susan Haack (nacida en 1945) y Amelia Valcárcel (1950).

A principios del siglo XIX, algunas universidades del Reino Unido y Estados Unidos comenzaron a admitir a las mujeres, dando lugar a nuevas generaciones de mujeres académicas. Sin embargo, investigaciones del Departamento de Educación de los Estados Unidos realizados a finales de los años 1990 del siglo XX indicaban que la filosofía era uno de los campos más desiguales en las humanidades con respecto a la presencia de varones y mujeres.[284]​ Las mujeres constituían apenas el 17% del estudiantado en la Facultad de filosofía.[285]​ En 2014, Inside Higher Education describió la filosofía "... con una historia propia en la disciplina de la misoginia y acoso sexual" de las mujeres estudiantes y profesoras.[286]​ Jennifer Saul, profesora de filosofía en la Universidad de Sheffield, declaró en 2015 que las mujeres "... están dejando la filosofía después de haber sido acosadas, agredidas o haber sufrido represalias".[287]​

A principios de los años noventa, la Asociación Filosófica Canadiense afirmó que existe un desequilibrio de género y sesgo de género en el campo académico de la filosofía.[288]​ En junio de 2013, un profesor de sociología estadounidense declaró que "de todas las citas recientes en cuatro prestigiosas revistas de filosofía, las mujeres representan sólo el 3,6% del total". Los editores de la Enciclopedia de Stanford de la Filosofía han trasladado su preocupación sobre la subrepresentación de las mujeres filósofas,[288]​y reclaman a editores y escritores garantizar que se incluyan las contribuciones de las mujeres filósofas. Según Eugene Sun Park, "la filosofía es predominantemente blanca y predominantemente masculina, esta homogeneidad existe en casi todos los aspectos y en todos los niveles de la disciplina".[283]​Susan Price sostiene que el "... canon filosófico sigue dominado por los hombres blancos -la disciplina que ... todavía sigue al mito de que el genio está ligado al género."[289]​ Según Saul," la filosofía, la más antigua de las humanidades, es también la más masculina (y la más blanca). Si bien otras áreas de las humanidades se acercan a la paridad de género, la filosofía es en realidad más abrumadoramente masculina incluso que las matemáticas."[290]​

La invención del término «filosofía» se suele atribuir al pensador y matemático griego Pitágoras de Samos,[8]​ aunque no se conserva ningún escrito suyo que lo confirme.[292]​ Según la tradición, hacia el año 530 a. C., el general León trató de sabio (σοφóς: sofos) a Pitágoras, el cual respondió que él no era un sabio, sino alguien que aspiraba a ser sabio, que amaba la sabiduría, un φιλο-σοφóς.

Según Pitágoras, la vida era comparable a los juegos olímpicos, porque en ellos encontramos tres clases de personas: las que buscan honor y gloria, las que buscan riquezas, y las que simplemente buscan contemplar el espectáculo, que serían los filósofos.

Años más tarde, Platón agregó más significado al término cuando contrapuso a los filósofos con los sofistas. Los filósofos eran quienes buscaban la verdad, mientras que los sofistas eran quienes arrogantemente afirmaban poseerla, ocultando su ignorancia detrás de juegos retóricos o adulación, convenciendo a otros de cosas infundadas o falsas, y cobrando además por enseñar a hacer lo mismo. Aristóteles adoptó esta distinción de su maestro, extendiéndola junto con su obra a toda la tradición posterior.[294]​

El texto más antiguo que se conserva con la palabra «filosofía» se titula Tratado de medicina antigua, y fue escrito hacia el año 440 a. C. Allí se dice que la medicina «moderna» debe orientarse hacia la filosofía, porque solo la filosofía puede responder a la pregunta «¿qué es el hombre?».[292]​

Alfanio hace a la filosofía hija de la experiencia y de la memoria. Se representa como una mujer de aspecto grave en actitud retórica y con la frente majestuosa ceñida de una preciosa diadema. Está sentada en un sillón de mármol blanco en cuyos brazos hay esculpidas las imágenes de fecunda naturaleza. Esta figura simbólica tiene dos libros, en uno de los cuales se lee naturalis y en el otro moralis. Rafael, autor de esta idea, ha querido con ella indicarnos los cuatro elementos, objeto de las investigaciones filosóficas, valiéndose de los diversos colores que ha dado a los ropajes con que la viste. El manto de color azul que cubre las espaldas, designa el aire; la túnica encarnada, el fuego; el ropaje de azul celeste que cubre sus rodillas, el agua; y el de color amarillo que le llega basta los pies, la tierra. Dos genios que coloca cerca de la figura principal sostienen esta inscripción Causarum cognitio: el conocimiento de las causas.[295]​

Boecio en el retrato que ha tratado de la filosofía le pone en una mano algunos libros y en la otra un cetro. En el extremo de su ropaje hay una letra griega y en el estómago otra que designan, la primera la teoría y la segunda la práctica, para dar a entender que la filosofía debe ser activa y especulativa. Luego, finge que esta imagen simbólica se le ha presentado bajo los rasgos de una mujer que con rostro radiante y ojos llenos de fuego anuncia algo de divino: que su talla parece igual a la de la especie humana y finalmente, que algunas veces levanta la cabeza hacia los cielos y se oculta a la vista de los débiles mortales.[295]​Cochin la representa como una mujer hermosa, reflexiva, vestida sencillamente, con un cetro en una mano y un libro en la otra, la hace trepar un monte áspero y pedregoso, haciéndola apoyar en el freno de la razón.[295]​Bernard Picart en un asunto alegórico pinta la armonía de la religión con la filosofía, su figura simbólica tiene diferentes atributos, los cuales caracterizan las cuatro partes. Está coronada de estrellas para designar la física y un cetro que lleva en su mano izquierda indica la moral; dos genios colocados cerca de ella: el uno lleva una serpiente mordiéndose la cola —símbolo de la eternidad—, y esto anuncia la metafísica; el otro, una piedra de toque para expresar la lógica, cuyo objeto es el de distinguir lo verdadero de lo falso.[295]​

La gastronomía mexicana  es el conjunto de platillos y técnicas culinarias de México que forman parte de las tradiciones y vida común de sus habitantes, enriquecida por las aportaciones de las distintas regiones del país, que deriva de la experiencia del México prehispánico con la cocina europea, entre otras. El 16 de noviembre del año 2010, la gastronomía mexicana fue reconocida como Patrimonio Cultural Inmaterial de la Humanidad por la Unesco.[1]​[2]​

La cocina mexicana ha sido influida y ha influido a su vez a cocinas de otras culturas, como la española, francesa, italiana, africana, del Oriente Medio y asiática. Es testimonio de la cultura histórica del país: muchos platillos se originaron en el México prehispánico y otros momentos importantes de su historia. Existe en ella una amplia gama de sabores, colores, olores, texturas e influencias que la convierten en un gran atractivo para nacionales y extranjeros: México es famoso por su gastronomía.

La base de la cocina mexicana actual deriva en gran parte de la cocina existente en la época prehispánica, con un uso preponderante del maíz, frijol, chile, jitomate, tomate verde, calabaza, aguacate, cacao, cacahuate, amaranto, vainilla, nopal, agave, cactáceas, hierbas y condimentos (epazote, hoja santa, pápalo, quelites), diversas aves como el guajolote y variedad de mamíferos, peces e insectos. Mientras que múltiples ingredientes se han adaptado a la cocina mexicana a través del intercambio cultural que trajo el Virreinato de Nueva España y los siglos subsecuentes, que introdujeron ingredientes europeos, mediterráneos, asiáticos y africanos como es el trigo, arroz, café, comino, hierbabuena, laurel, orégano, perejil, cerdo, res, pollo, cebolla, limón, naranja, plátano, caña de azúcar, cilantro, canela, clavo, tomillo y pimienta; muchos de los cuales han sido ampliamente adoptados e incluso históricamente cultivados en México, como es el caso del café y el arroz.

México aportó al mundo productos sin los cuales no sería posible entender la gastronomía mundial. Entre ellos el maíz, frijol, chile, aguacate, vainilla, cacao, jitomate, calabaza, chayote, zapote, mamey, papaya, guayaba, nopal y guajolote.[3]​

La diversidad es la característica esencial de la cocina mexicana, y es la comida regional uno de sus aspectos fundamentales. Cada estado mexicano y región poseen sus propias recetas y tradiciones culinarias.[4]​ Ejemplos de comidas regionales son platillos como el caldillo duranguense (Durango), cochinita pibil (yucateca), el mole oaxaqueño, el mole poblano y el chile en nogada (Puebla), los múltiples tipos de pozole, el cabrito (coahuilense y neoleonense), el pan de cazón campechano, el churipo y las corundas (región purépecha) o el menudo (jalisciense, michoacano, sinaloense, sonorense y chihuahuense). Ciertamente, hay creaciones gastronómicas que surgieron localmente y que por su calidad, aceptación y difusión se han vuelto emblemáticas de la cocina mexicana en lo general. En los mercados de cada sitio se muestra esta diversidad, y la actividad por las mañanas comienza con típicos desayunos como molletes dulces o salados, chilaquiles y/o huevos al gusto y bebidas con leche, café, chocolate y jugos, hasta platillos únicos de cada región.

En el conjunto inmenso de cocinas regionales, se caracterizan todas ellas por un componente indígena básico en sus ingredientes y algunas técnicas comunes de preparación de los alimentos. El común denominador en muchas cocinas es el uso de maíz, chile y frijol, acompañados de jitomate en sus diversas formas, aunque no es un determinante.

El acto de cocinar en México es considerado una de las actividades más importantes de la vida diaria. En la mayor parte del país, especialmente en las zonas rurales, los alimentos se consume en el hogar tomando como base los ingredientes locales.

La gastronomía mexicana siempre ha sido calificada como una cocina de gran influencia barroca, resultado de un mestizaje culinario, representando la visión que los mexicanos tienen del mundo. De esta forma, la región del norte de México es de clima más seco, ofrece una cocina más bien austera, de sabores sencillos, fuertemente basada en la recolección y el mestizaje, pero que destaca por el nivel de endemismos florísticos. En cambio, en el sureste y otras zonas tropicales, se da una amplia diversidad de sabores con una cantidad hasta ahora desconocida de platillos y recetarios locales.

En México se acostumbra hacer tres comidas al día: el “desayuno”, la “comida” y la “cena”, si bien al tratarse del país industrializado donde la gente más labora,[5]​ esto ha ido cambiando y ahora cada familia aplica un horario acorde a sus necesidades. En término medio, el «desayuno» se hace entre las 7 y 10 de la mañana, la «comida» entre las dos y tres de la tarde, y la cena después de las 8 de la noche. El «desayuno» suele ser algo más abundante que en otros países y varía según la región, desde los huevos preparados de distintas formas, solos o acompañados con frijoles, chilaquiles, tortas y quesadillas, hasta platillos más complejos hechos con carne o guisos y generalmente como bebida jugos, leche, o café. La comida principal es la comida y suele involucrar el platillo más elaborado del día, generalmente acompañada de aguas frescas o bebidas regionales, aunque los refrescos han ganado terreno los últimos años. La «cena» varía de acuerdo a costumbres personales desde una comida sencilla acompañada de pan dulce, café, té, chocolate o bebidas regionales, hasta platillos también complejos o algún recalentado. En México el «almuerzo» es el alimento ligero que algunos consumen antes del mediodía, generalmente entre las diez y las doce o después del desayuno, si bien puede resultar confuso ya que si la persona retrasa el desayuno suele llamarle almuerzo o incluso en algunas regiones a cualquier desayuno se le llama así, mientras que la «merienda» es el alimento ligero entre la comida y la cena, o aquella cena que se suele hacer más temprano de lo habitual.

La cocina en México también cumple funciones rituales y festivas determinantes, tales como la instalación del altar de muertos o la fiesta de quince años. La comida suele representar claramente la estructura social del país.

Una de las características de la gastronomía mexicana es que aunque hace distinción entre la cocina cotidiana y la alta cocina, estos pueden consumirse en cualquier momento y ser adecuado. Así, aunque existen platillos típicos festivos como el mole o los tamales, estos pueden consumirse cualquier día del año si así se desea, lo mismo en una casa particular que en un restaurante lujoso o en una pequeña fonda sin un valor ritual especial; y a la vez darle ese valor ritual cuando se requiera.

Para la fiesta del Día de Muertos, platillos festivos se ponen en altares y se cree que los parientes muertos que los visitan comen la esencia de la comida, y si esta es ingerida por las familias más tarde, ha perdido el sabor. Cualquier festividad o ceremonia como una boda, la fiesta de quince años, fiestas o reuniones familiares suele ser un pretexto para la preparación de platillos tradicionales, que incluyen mole, barbacoa, carnitas, mixiotes, birria o distintos platillos según la región. En ceremonias suelen estar preparados para alimentar a unos quinientos invitados, requiriendo grupos de cocineros profesionales o simplemente organizados por la familia. La cocina está fuertemente diseñada para vincular familias y comunidades.

Algunos platillos muy complejos suelen ser exclusivos de algunas épocas del año, por ejemplo en Navidad y los últimos días del año, la cena pasa a ser la comida más importante y varía desde platillos regionales más complejos (tamales con alguna distinción, pozole y otros platos), romeritos, pavo de Nochebuena o bacalao, acompañados de buñuelos y otros postres y dulces, se rompen piñatas rellenas de frutas y dulces, y como bebidas recetas más autóctonas, como ponche de frutas navideño, champurrado y atole. Este patrón de “celebrar lo tradicional” se repite en otras fiestas del año. Al tratarse de un país mayormente católico, los pueblos y comunidades suelen tener días y fiestas patronales, en los que se come de forma especial. Las ferias locales son otro ejemplo de festividad local, prácticamente cualquier población y ciudad celebra una, donde la gente da un lugar especial a la gastronomía.

Aún es muy frecuente que en México la gastronomía se adapte para celebrar la Cuaresma. Durante este período se elaboran platos con ingredientes sencillos y vegetales cotidianos como la papa, calabacita y flor de calabaza, camote, ejotes, chiles poblanos, aguacate, nopal y otros ingredientes y productos indígenas, aumenta el consumo de frijoles, lentejas, habas, garbanzos, huitlacoche, charales, sardinas, camarones y toda clase de pescados frescos o secos. Platillos como el pipián o las tortitas de camarón con nopales son propiamente típicos de estas fechas y puede ser más complicado encontrarlos durante el resto del año.[6]​

En últimos años se han vuelto frecuentes los festivales gastronómicos, que van desde lo local hasta ferias nacionales, entre las más reconocidas se encuentra:

La profesionalización del trabajo culinario en México es una virtud ampliamente valorada por la sociedad, rindiéndole a aquel que la práctica una jerarquía especial. Salvo excepciones, como las del "taquero", este dominio sigue siendo predominantemente femenino: es común ver al frente de las cocinas de restaurantes y fondas a mujeres que, al adquirir el grado de excelencia, son nombradas "mayoras", denominación que en el Virreinato se le daba a las jefas de las cocinas de las haciendas y que ahora sería equivalente al chef europeo. En efecto, múltiples autores han calificado la cocina mexicana, como una cocina matriarcal.

La comida callejera es muy variada, y va desde los antojitos hasta cualquier tipo de cocina tradicional. Se come igual porciones pequeñas o medianas que funcionan como el "tentempié" mexicano y pueden ser consumidas en pocos minutos, hasta comidas completas o tan complejas que solo pueden ser consumidas ahí. Destacan los tacos de cualquier tipo que los mexicanos no solo clasifican por el contenido, sino por la tortilla, la región y el modo de preparación (al pastor, de bistec, de arrachera, de adobada, de chorizo, de longaniza, de "cabeza", de lengua, de lechón, de canasta o sudados, dorados, ahogados, de carnitas, placeros, de barbacoa, de suadero, de cecina, de pepena, de pescado, de langosta, laguneros, potosinos, de chicharrón, mineros, de escamoles, de chapulines, de jumiles, de pito, de coetlas, de fritada de cabrito, de pejelagarto, de nata, codzitos o de nada y una interminable lista), quesadillas, pambazos, tamales, huaraches, sopes, tortas y cemitas. Desde finales del siglo XX, la influencia de la comida rápida estadounidense se ha dejado ver en la comida de muchos puestos de la calle, que ofrecen hamburguesas y perros calientes.

Uno de los atractivos de la comida callejera es la satisfacción del hambre o el antojo espontáneo sin toda la connotación social y emocional de comer en casa, y claramente también influenciado por la carga laboral común del mexicano, que desea seguir comiendo platillos complejos de la gastronomía mexicana sin contar con el tiempo para prepararlos. Aunque los clientes a largo plazo suelen generar una relación de amistad con un vendedor elegido.

En las zonas urbanas, debido a la integración de las mujeres a la fuerza laboral y a la influencia del estilo de vida occidental (principalmente de los Estados Unidos), se ha ido perdiendo la tradición de cocinar en casa. Sin embargo, se considera que las fondas (una versión mexicana de los bistró franceses, lugares donde comer fuera a mediodía de forma económica) son un reservo urbano de las recetas tradicionales.

Los primeros habitantes del territorio actual de México al inicio de la Etapa Lítica eran nómadas, sobrevivían de la recolección, caza y la pesca y contaban con una tecnología lítica que fue mejorando constantemente a lo largo de milenios. De esta época data la invención del molcajete, el metate y otros instrumentos asociados al aprovechamiento de semillas, así como el desarrollo de utensilios de sílex y obsidiana.

La agricultura en Mesoamérica nació en varios puntos a la vez. Los Valles Centrales de Oaxaca y el Valle de Tehuacán, sitio de las arcaicas culturas mixteca y zapoteca, fueron uno de los escenarios donde se desarrolló por primera vez. Se calcula que la ocupación humana en estos dos sitios pudo haberse dado de forma continua desde el 10,000 a.C. En Coxcatlán, Puebla y en otros sitios de Chiapas se han localizado restos de maíz fósil que datan de alrededor del año 5000 a. C. La zona sureste del país ahora es rica en sitios arqueológicos donde se da fe de la recolección y domesticación del maíz, chile, calabaza, chía, amaranto, agave, jitomate, tomatillo, y múltiples hiervas de forma progresiva entre el 8000 a 2000 a.C. Estos productos permitieron el nacimiento del sedentarismo en América, que permitió una mayor especialización en su manejo y preparación.

A su vez, pueblos de Oasisamérica, en el noroeste de México y la Gran Cuenca del suroeste de Estados Unidos continuaron viviendo de la recolección y aproximadamente hasta el siglo VII lograron el total sedentarismo. Esto abrió una puerta de comercio intenso entre los pueblos del norte con los del centro y sur de México que continuó hasta el Virreinato, permitiendo un intenso intercambio de productos culinarios.

Se ha encontrado evidencia del uso del cacao por la cultura mokaya desde el 1900 a.C. La importancia de este fruto fue tanta, que las posteriores culturas olmeca y maya le dieron valor de manjar, llegando a ser uno de los productos más valorados y elevarlo a título de moneda. Entre los animales que sirvieron de alimento a la cultura olmeca (1500-500 a.C) se encuentran tlacuaches, monos, guajolotes, venados, tapires, peces, mariscos y aves acuáticas. Toda esta forma de alimentación se pone de manifiesto en su cerámica y escultura.

La cultura maya continuó el cultivo de maíz, frijol y calabaza, el cual mejoraron y complementaron con una amplia variedad de plantas, cultivadas en jardines o recolectadas en la selva. Hay evidencia de la primera nixtamalización del maíz en algún momento entre el 1500 a 1200 a.C. Los mayas molían semillas de algodón para la producción de aceite de cocina y crearon cultivos de prestigio de maíz, algodón, vainilla y cacao. Comenzaron a utilizar al perro para la caza de animales mayores como venados, y domesticaron al pato criollo y al guajolote. Las abejas fueron domesticadas para obtener miel y cera que era usada para elaborar guisos y bebidas fermentadas; el nivel de desarrollo de la apicultura por los mayas fue tal, que conocían la variación del sabor de la miel en función de las flores que consumían las abejas.[7]​

Las grandes ciudades del periodo Clásico como Teotihuacán, Cholula, Monte Albán, Calakmul y Tikal crearon una escena histórica única. Al convertirse en grandes urbes permitieron unificar el conocimiento gastronómico de muchas culturas en las ciudades y abrir la puerta para el intercambio masivo de productos. Muchas culturas se especializaron en la producción de ciertos productos o alimentos, que solían ser valorados por otras culturas.

La llegada y desarrollo de los mexicas al Valle de México generó un nuevo mundo de posibilidades gastronómicas, quienes aprendieron y reinventaron todas las técnicas culinarias de los milenios de desarrollo de las otras culturas con nuevos conocimientos técnicos y agrícolas. Son varios los factores que permitieron el desarrollo tan especializado de la cocina mexica, entre ellos desarrollarse sobre una zona lacustre altamente fértil, encontrarse en el centro de Mesoamérica donde convivían varias culturas que permitieron un gran intercambio de productos, y posteriormente su dominio militar que les trajo tributo en especies y productos.

El maíz desempeñaba un papel muy importante en la economía mexica, pues sirvió durante un tiempo como moneda. Cintéotl era la energía del maíz, y al dios Huitzilopochtli se le veneraba o se le ofrendaba cañas de maíz. Se cultivaba un inestimable número de variedades. Otros alimentos importantes fueron el chile, la sal, los frijoles y las diferentes variedades de grano de amaranto y chía. Las tortillas o tlaxcalli y los tamales o tamalli tenían tamaños, composiciones, adornos y formas especiales, como mariposas y rayos. Hacían toda clase de ofrendas y ritos funerarios donde la gastronomía jugaba un papel muy importante, durante el culto a sus dioses se ofrecía maíz, chía, y toda clase de plantas y hierbas, maíz tostado o izquitl (antecedente de las palomitas de maíz), solo o revuelto con miel y harina de semillas de amaranto.

El amaranto o huautli era considerado por los mexicas como un alimento especial de tipo espiritual, le daban formas de rodelas, espadas o dioses y lo comían hecho pasta con miel que puede equivaler al dulce de alegría actual. Comúnmente, al finalizar las fiestas se dividían figuras de este alimento y se les comía a manera de comunión, hecho que escandalizó a los frailes por su parecido con los ritos cristianos, de ahí que el cultivo del amaranto se prohibiera durante el Virreinato, sin llegar a extinguirse. También consumían diversos hongos y setas, especialmente el huitlacoche, un hongo parásito que crece en la mazorca del maíz. La calabaza y sus pepitas secas o tostadas, los jitomates y tomatillos eran un ingrediente común.

La dieta mexica incluía animales como pavos, palomas, patos, faisanes, jabalí americano, tuzas, conejos y liebres, iguanas, tortugas, ajolotes, ranas, camarones, pescados, insectos y crustáceos lacustres como los acociles. Una raza de perro, el xoloitzcuintle, era criado para la alimentación. Por su condición lacustre, los pescadores eran un sector importante. Entre los insectos figuraban varios tipos de hormiga y el ahuautle (la hueva del insecto axayáctl).

El atole y el pulque fueron las bebidas más comunes entre la sociedad, además de bebidas que eran fermentadas de maíz, miel, cactáceas o frutos. La élite de la sociedad se enorgullece de no beberlas ya que preferían diversas bebidas preparadas con cacao. El xocolātl era uno de los mayores lujos disponibles y se convirtió en la bebida de gobernantes, guerreros y nobles. Fue condimentado con vainilla, miel y una interminable lista de hierbas y especias, entre ellas chiles.

Los mexicas acostumbraron dos comidas por día, aunque los obreros podían tomar tres, una en la madrugada, otra aproximadamente a las 9 de la mañana y una a las 3 de la tarde. Esto es similar a la costumbre que se tenía en la Europa contemporánea, pero no está claro si la ingesta de atole era considerado como una comida o no, ya que podían tomarlo a cualquier hora. De acuerdo a su calendario de 18 meses (con 20 días cada uno), los mexicas tenían una fiesta mensual para honrar a sus deidades, en las que se hacían ofrendas de alimentos distintos a los del consumo cotidiano.

Una ceremonia muy importante de la élite mexica, era la celebración de banquetes. Antes de un banquete los sirvientes presentaban olorosos rollos de tabaco y flores con las que los huéspedes cubrían su cabeza, manos y cuello. Antes de empezar la comida cada huésped apartaba un poco de comida y la dejaba en el suelo como ofrenda a la diosa Tlaltecuhtli. Los cigarros y las flores pasaban de la mano izquierda del siervo a la mano derecha del invitado al igual que los platos con los alimentos, esto era una imitación del momento en que un guerrero recibía su átlatl, flechas y escudo. Las flores entregadas recibían diferentes nombres según la mano con que se entregaban; Las "flores espada" pasaban de la mano izquierda a la derecha y las "flores escudo" pasaban de la derecha ala izquierda. Al comer, los invitados sostenían sus tazones llenos de salsa con la mano derecha y luego sumergían tortillas o tamales con la izquierda. La comida concluía al servirse el chocolate, el cual era servido en una jícara y un palillo para agitarlo. Hombres y mujeres comían separados en los banquetes. Los anfitriones más ricos a menudo recibían invitados nobles o militares en habitaciones alrededor de un pequeño patio abierto en el que también solían presentarse bailes, algunas fiestas comenzaban a medianoche; algunos invitados bebían chocolate o comían hongos alucinógenos para poder hablar de sus experiencias y visiones a los demás huéspedes. Antes del amanecer, los invitados comenzaban a cantar y a quemar y enterrar ofrendas en el patio para asegurar la buena suerte de los hijos. Las flores, cigarros y alimentos restantes eran dados a los ancianos y pobres que habían sido invitados, o a los sirvientes.

Tras la Conquista, los españoles introdujeron una variedad de alimentos y técnicas de la cocina europea. La cocina española en aquella época ya era una mezcla de ingredientes debido a ocho siglos de conquista musulmana. El objetivo original de la introducción era reproducir su cocina casera, pero naturalmente esto no pudo ser posible por múltiples razones, la abrumadora población que existía en México, que en el Valle de México pudo haber ascendido a 1 millón y medio de habitantes y en la península de Yucatán hasta de 8 a 10 millones a su llegada; así como la distancia: muchos ingredientes españoles no estaban disponibles o se convirtieron en caros.

Los españoles optaron por introducir productos y técnicas de forma progresiva en las zonas dominadas, creando cultivos de nuevos productos, y en este proceso, como sucedió en los conventos, ambas cocinas se mezclaron. Mientras los españoles llevaron a Europa un “nuevo mundo” de productos que crearon conmoción en el siglo XVI y XVII europeo, incorporaban en la Nueva España alimentos como el aceite de oliva, arroz, el trigo, la avena, cebollas, ajo, orégano, cilantro, canela, clavo y muchas otras hierbas y especias. Introdujeron nuevos animales domesticados, como cerdos, vacas, pollos, cabras y ovejas para carne y leche, desplazando en algunos sitios muchas carnes prehispánicas e insectos. El queso se convirtió en el producto lácteo más importante.

Entre las múltiples técnicas culinarias introducidas destacaba el añejamiento, el destilado de bebidas en alambiques y la técnica de cocción para la creación de frituras. Entre los nuevos productos que se comenzaron a cultivar en México de forma exitosa se encontraba el arroz, el café y la caña de azúcar, la cual transformó completamente la producción de dulces y bebidas e inició la tradición de producción de frutas locales en almíbar.

Tras la Independencia de México, la economía quedó fuertemente dañada. Algunas carnes, especialmente la de res, dejaron de ser un alimento básico debido a su alto precio. Nacen recetas para exaltar el patriotismo mexicano, como son los chiles en nogada que presentaba los colores de la nueva nación. Progresivamente tras la retirada de España, el país recibió influencia de nuevos países lo que diversificó lo que se servía en las mesas.

En este siglo el mercado local se consolidó como centro principal de venta de una gran variedad de ingredientes para la elaboración de comida. El mercado desde entonces ha fungido como centro importante para la economía, porque mientras trae a las comunidades nuevos productos, compra el producto familiar local activando la economía.

Llegaron a México técnicas francesas por medio de manuales y recetarios. Los manuales dieron la pauta para comenzar a crear un sincretismo entre la cocina europea y mexicana. Los mexicanos adoptaron el uso de manuales para difundir trucos y recetas. El primer manual apareció en 1831, el cual determinó la técnica de cocina, la limpieza y las costumbres culinarias de los hogares. En 1866 se publicó "El libro de las familias" y en 1896 apareció "El cocinero de las familias" que incluía comidas hispanoamericanas y mexicanas. La mayoría de los platillos eran de Yucatán y Veracruz, que se convirtieron en gastronomías favoritas de muchos habitantes. Con el tiempo existirían muchos libros más que elevarían al tamal como uno de los platillos especiales mexicanos.

En las zonas urbanas el pulque fue uno de los principales acompañantes de la mesa mexicana de todos los grupos sociales. La condesa y cronista de Maximiliano y Carlota, Paula Kolonitz, escribió "El vino y la cerveza se beben poco, pero el pulque jamás falta a la mesa de los ricos".[8]​

La élite en los tiempos de Porfirio Díaz prefería una cocina europea, sin dejar de lado la mexicana, mientras que el resto de la población se concentraba en platillos populares. La segunda intervención francesa en México (1864-1867) terminó estableciendo un gusto general por la comida francesa, que se vio reflejado en la alta sociedad, que comenzó a imitar recetas y a producir postres más refinados. Entre 1876 y 1910 se publicaron numerosos libros de cocina con tendencia europea como: El cocinero mexicano en Francia, "El arte novísimo de cocinar" y el "Manual de la cocina francesa", que revolucionaron la cocina familiar de la clase media.

Este refinamiento de la cocina mexicana nuevamente creó un estilo visible en platillos actuales como es el acompañamiento con papas a la francesa, crepas que envuelven guisos mexicanos, pasteles cubiertos de crema o en la fabricación de hojaldre.

El siglo XX inicia con la Revolución Mexicana y un nuevo golpe a la economía del mexicano, nace la canasta básica y muchas recetas se vuelven exclusivas de las clases altas mientras que la cocina tradicional vuelve a fortalecerse. En el México post-revolucionario y tras un segundo aire del nacionalismo mexicano, nace el gusto aristocrático por esta cocina tradicional.[8]​ Nacen las empresas refresqueras e industrias productoras de alimentos como Bimbo. En las ciudades la clase media y alta seguía manteniendo activos bares y reposterías. La cocina de este siglo está marcada por una sociedad con cada vez más carga laboral y la adhesión de la mujer a nuevos trabajos: el orden de las comidas se alteró, se perdió la costumbre de la siesta después de comer y surge el término “lunch” o lonche para referirse a los desayunos más rápidos.[8]​

A partir de la década de 1950 crece el número de restaurantes para la clase media y conceptos como “Cocina Internacional” se vuelven frecuentes. Nacen las enchiladas suizas, los club sándwich, el desayuno tipo americano, el buffet, la paella, la comida rápida estadounidense y las parrillas al estilo americano y sudamericano. A la vez que fondas y sitios alejados de la modernidad se convirtieron verdaderos reservorios de la cocina antigua; así pues era común que los habitantes de zonas urbanas viajaran a sitios rurales para buscar la comida tradicional con familiares o procurar productos.[8]​ Además de que el turismo comienza a tomar forma.

Se desarrollan las heladerías y neverías. Muchas tortillerías se tecnifican. Una nueva inmigración española encuentra su nicho en las panaderías. Gracias a la importación de Estados Unidos de productos como el tequila, las destileras maduran y se desarrollan. Muchas industrias de envasado de frutas y verduras crecen. Los aparatos electrodomésticos simplifican la labor de muchas cocinas.

Surgen autoridades de la cocina mexicana como Diana Kennedy, que a través de 8 libros que se convirtieron en best-seller en países angloparlantes, el mundo comienza a estudiar y darle un mayor estatus a la gastronomía mexicana. Las televisoras y revistas presumen darle la mejor receta a las amas de casa. Nace un gusto generalizado por la cocina tradicional y el deseo de preservarla. A finales del siglo XX se empieza a dar importancia a la imagen del chef. Comienzan a aparecer escuelas de gastronomía incluyendo en sus apartados estudios de la cocina mexicana. Se reconoce a la cocina mexicana como una de las más complejas y completas del mundo.

En el siglo XXI muchas costumbres del siglo XX siguen siendo parte de la costumbre del mexicano: encontrar tamales en alguna esquina, la venta de panes en bicicleta o en canastos grandes, carritos de camotes con silbato, venta ambulante de raspados y helados, venta de antojitos en la calle con masa cocida al comal al momento, venta de quesos frescos por la mañana, puestos de jugos de fruta de la temporada y un enorme ambulantaje gastronómico.[8]​

Nace la “Nueva Cocina Mexicana” a veces llamada “Alta Cocina Mexicana”, y chefs como Alejandro Ruiz, Benito Molina, Bricio Domínguez, Daniel Ovadía, Elena Reygadas, Enrique Olvera, Jair Téllez, Jorge Vallejo, José Burela,  Paulina Abascal, Patricia Quintana, Ricardo Muñoz Zurita y Roberto Ruiz, entre otros, han alcanzado fama internacional, posicionando a las gastronomías regionales mexicanas como unas de las de mayor prestigio y presentando recetas innovadoras, que a la vez luchan por preservar las técnicas e ingredientes tradicionales, es común encontrar en sus restaurantes insectos, hongos y hierbas que buscan enaltecer los productos mexicanos. En Tijuana y otros lugares en Baja California, ha surgido la cocina fusión con el nombre de Baja Med, combinando los ingredientes típicos de la cocina mexicana y frescos cosechados en Baja California con la cocina mediterránea, como el aceite de oliva, y las asiáticas, como el limoncillo (hierba limón).[9]​ En este marco, en 2010 se nombra a la cocina tradicional mexicana "Patrimonio Cultural Inmaterial de la Humanidad", resaltando el arte culinario mexicano como uno de los más elaborados y destacando a las pequeñas comunidades mexicanas por sus esfuerzos de preservación de la cocina tradicional como un medio de desarrollo sostenible.[10]​

Entre los utensilios empleados destacan comales, metates, molcajetes y tejolotes, tazones, jícaras de guaje, canastos y "bules" (ollas y cántaros) para transportar el agua, todo tipo de tejidos y un complejo desarrollo de la alfarería.
Muchas de las técnicas de preparación de los alimentos prehispánicos persisten hoy en día y se enlistan abajo, aunque, como sucede con otras comidas del mundo, cada platillo o bebida puede tener propias técnicas de preparación.

Los ingredientes de la cocina mexicana varían de acuerdo a la región. Por ejemplo en la costa, los mariscos son un ingrediente determinante. Las carnes de pollo, res y puerco son siempre populares. Las salsas son un elemento común, y probablemente los chiles, el jitomate y la cebolla sus ingredientes más frecuentes. La comida se sazonada de formas muy variables. Las listas de ingredientes básicos varían siempre, una vez que los elementos principales siempre son los autóctonos y regionales, pero es indiscutible el papel del maíz, del chile y del frijol en estas cocinas.

La sociedad mexicana siempre ha buscado consumir maíz cultivado en el mismo país, así que es el cereal más sembrado en todo el territorio, llegando a cultivarse más de cuarenta y dos tipos diferentes.[11]​ A su vez, cada uno de estos tipos presenta diversas variedades, llegando a un aproximado de más de tres mil tipos de la especie, según el Centro Internacional de Mejoramiento de Maíz y Trigo. Las características de cada raza varían de acuerdo con las condiciones del suelo, la humedad relativa del medio ambiente, la altitud e incluso de la forma en que se cultiva. Aunque algunas de las evidencias más antiguas del cultivo del maíz sugieren que su domesticación se produjo en varios focos del país al mismo tiempo, es probable que este proceso estuviera ligado a los pueblos de habla otomangues. Como sea, el maíz sigue siendo la base de la mayoría de las cocinas mexicanas, y quedan exceptuadas algunas tradiciones gastronómicas del norte de México, donde el maíz disputa al trigo el lugar como cereal básico.

La forma principal en que se consume el maíz en México es la tortilla, pero es un insumo igualmente necesario para la preparación de casi todos los géneros de tamales, atoles y antojitos. La tortilla se utiliza en una gran gama de los platillos como chilaquiles, tostadas, quesadillas, tacos, chalupas, gorditas, huaraches y gran variedad de antojitos mexicanos. El elote (fruto del maíz) también se consume de innumerables formas, maduro y fresco (elote) o bien, tierno y fresco (xilote). La gastronomía mexicana tiene inmensamente arraigado al maíz como elemento principal de un gran número de recetas, cada parte de la planta es utilizada con algún fin, y existen múltiples técnicas para manipular cada una de estas porciones. En el siguiente esquema se explica de una forma muy básica, la forma en que se mezclan las técnicas más comunes de preparación del maíz con sus usos más frecuentes, sin profundizar en otros usos, como son los industriales, la producción de harinas refinadas o bebidas fermentadas; pero sirve para esquematizar la importancia que tiene el maíz en la cultura.


Entero = asado (elote asado, a las brazas o tatemado) o hervido (elote cocido), se preparan variadamente condimentados

Rebanado = útil en sopas, moles y guisos

Granos enteros = presente en sopas, pozoles, guisos, ensaladas y esquites

Granos molidos o desmenuzados = útil en tamales, atoles, pasteles, sopas y postres.

Tostados y molidos = útil en pinoles, atoles y galletas.

Tortillas y antojitos de tortilla = tacos, enchiladas, chilaquiles, etc.)

Antojitos hechos de masa = chalupas, gorditas, tlacoyos, sopes, huaraches, etc.

Tamales

Bebidas

Para espesar atoles, sopas, salsas, pozole, etc.

Nixtamalizados sin moler = útil en menudos, pozoles, sopas, etc.

Molidos = útil para hacer harina de maíz

Palomitas (frituras)

Huitlacoche = útil en sopas y en rellenos

Hojas de mazorca verdes o secas = útiles para envolver y cocinar carnes y tamales

Cabellos (pelos) de la mazorca = preparados como té diurético

Resto de la planta = utilizada como forraje, abono y para usos industriales

Es uno de los ingredientes más representativos de la gastronomía mexicana y un fruto asociado a la identidad, funciona como fruto, verdura, guarnición, especia y como ingrediente de salsas y platillos mayores, incluso como bebida; sus orígenes se remontan, según algunos historiadores, a fechas tan lejanas como el 6000 a.C. y, según las más recientes investigaciones, su domesticación no fue un hecho atribuible a una sola cultura y en un solo momento; se dio a lo largo de la región conocida como Mesoamérica en diferentes etapas. Su conocimiento y uso están registrado en los códices, en donde se le menciona también como medicina ritual, pues sanaba algunos de los aspectos relacionados con la salud del alma, ahuyentaba a los "malos espíritus" y rectificaba las actitudes de los niños "malcriados" por medio de su humo. Fray Bartolomé de las Casas mencionó: "Sin el chile, los mexicanos no creen que están comiendo",[12]​ frase que se conserva en el colectivo social actual como "si no pica, no sabe". Desde tiempos antiguos también ha habido en el chile cierta asociación a la sexualidad. Fray Bernardino de Sahagún consigna que durante las festividades de Macuilxóchitl "Señor de las flores, de la danza, de los juegos y del amor", los hombres y las mujeres que tomaban parte en la celebración se sometían durante cuatro días a un riguroso ayuno y se abstenían, como medida precautoria, de comer chile y quien rompía el ayuno era castigado por este dios, que hacía padecer al transgresor enfermedades "en las partes secretas". En todo caso, la prohibición de comer chile durante los ayunos rituales continúa siendo una práctica común entre algunos pueblos indígenas.  Es discutido el número de especies y subespecies de chile consumidas en México, lo que es cierto, es que el mexicano común otorga diferentes atributos a cada tipo, prefiriendo algunos tipos para platillos especiales (como es el caso de los chiles rellenos, donde por tradición se usan chiles más grandes y poco picantes), otros valorados por su nivel de picante (como el chile habanero), y otros por su sabor o uso en comidas más dulces. Es también evidente los usos sumamente distintos que se le da como fruto fresco o como fruto seco, recibiendo nombres distintos en cada caso a pesar de que se trate de una misma especie. Los principales productores de chile fresco son Puebla, Hidalgo, Veracruz, Sonora, Guerrero y México; mientras que más del 50 % del chile seco se produce en Zacatecas.[13]​ Entre las principales variedades de chile fresco consumidas son: jalapeño, serrano, poblano, habanero, morrón, manzano, de árbol, tabasco, chiltepín y piquín  mientras que como fruto seco destaca el chile chipotle, guajillo, pasilla, ancho, cascabel y de árbol (seco).

A la vaina se le llama "ejote" (del náhuatl exotl) a las semillas se les llama "frijoles". Fue domesticado a la par del maíz. En el país se consumen más de 50 variedades distintas, desde el frijol blanco ("aluvias") hasta múltiples variedades negras y cafés ("bayos"). Es la leguminosa favorita de la comida mexicana, presente en múltiples antojitos, como acompañamiento en platillos mayores, o incluso como plato principal. Existen platillos complejos en torno al frijol, como también se ha convertido en el ingrediente principal de la canasta básica del mexicano por su bajo costo.

De origen asiático y traído desde el siglo XVI, el arroz un cereal muy presente en las mesas mexicanas. Dado que es más versátil que el trigo, el arroz puede constituir cualquiera de los tres tiempos de la comida, y es un acompañante frecuente de otros platillos.[14]​ La manera más extendida de consumir arroz en México es arroz a la mexicana, que no es sino un arroz frito y luego cocido en salsa de jitomate. Sin embargo, las variedades de arroz seco son muchas: lo hay blanco —saborizado en algunas regiones con un tomate verde y cebolla—, verde —con chile poblano—, amarillo —con azafrán—, negro —con caldo de frijoles negros—, y además puede ser acompañado con verduras, especialmente en la forma conocida como «arroz a la jardinera». El arroz también es ampliamente usado en repostería.

De herencia europea, entre los cereales, destaca el trigo. Disputa al maíz la condición de cereal principal. Está asociado principalmente con la confección de panes —aunque también existe la tortilla de harina—, ya sea blancos o dulces. El pan blanco (bolillo, telera, birote) es elemento esencial de platillos comunes como las tortas mexicanas. Mientras que el pan de dulce —que se puede encontrar bajo innumerables formas— es acompañante ideal de las bebidas calientes que se suelen servir en el desayuno o en la merienda.

Las verduras que alimentaron a los antiguos mexicanos fueron sobre todo los quelites (quilitl), plantas aún inmaduras de diferentes familias botánicas (amarantáceas, quenopodáceas, crucíferas), plantas tiernas que se «cocían en olla» o se comían crudas; en estas familias se incluyen los quintoniles, los cenizos, los huauhzontles, las verdolagas y una planta denominada mexixiquilitl, que se parece al berro. Los romeritos son plantas también muy importantes que han sido empleadas en diferentes guisados, sobre todo durante la Cuaresma y la Navidad. 

Los principales frutos que se consumen en México son el aguacate, cacahuate, guayaba, jitomate, limón, mango, naranja, papaya, piña, pitahaya, pitaya, plátano, sandía, tamarindo, tejocote, tomatillo, tuna y zapote. 

Los vegetales y leguminosas elementales son: la cebolla, lechuga, chayote, ajo, calabazas, zanahoria, pepino, frijol, garbanzo, lentejas y guisantes.

Una amplia diversidad de condimentos son utilizados para agregar aroma y sabor a los guisos, salsas y sopas. 
Entre las especias originarias elementales se encuentran: el achiote, el epazote, la hoja de aguacate, la hoja santa, el pápalo, la pimienta tabasco, los quelites y la vainilla. Estas plantas también poseen con propiedades medicinales.

Se incorporaron otros condimentos de origen europeo, asiático y africano como la albahaca, el anís, el azafrán, la canela, el clavo de olor, el comino, la hierbabuena, la mejorana, el laurel, la menta, el orégano, el perejil, la pimienta, el tomillo, entre otros. Destaca el amplio uso del cilantro, del cual se utilizan sus hojas en multitud de recetas para dar un sabor característico a muchos platillos.

Acompañan infinidad de platillos y bebidas. El nopal ha sido un ingrediente común de la cocina mexicana desde hace milenios por su cualidad como planta de recolección común. El nopal se encuentra en el escudo de México y se considera un símbolo nacional; por lo tanto su consumo es considerado por la sociedad como un símbolo de lo mexicano, presente en frases como "nada más mexicano que el nopal". Su fruto, la tuna, obtuvo una gran popularidad a nivel mundial.

Las cactáceas son representativas de la cocina de las zonas áridas del país y sus frutos como la pitaya y pitahaya. La domesticación de las especies de agave es tan antigua como la del maíz y se ha documentado su origen en el Valle de Tehuacán y en los Valles Centrales de Oaxaca desde hace milenios: su uso está representado por el mezcal y el tequila, el aguamiel, el pulque, jugos y jarabes dulces (muy necesarios al igual que la miel para crear bebidas prehispánicas cuando no se contaba con la caña de azúcar), miel, vinagre, aguardiente y atoles, también es el sitio idóneo para la producción de gusanos de maguey blancos, gusanos de maguey rojos y sal de gusano; su uso es esencial para la producción del mixiote, guisos, postres, algunos tipos de azúcar, saborizante de tamales y pan, como levadura, condimento y en la preparación de ciertos tipos de barbacoa. El mezquite sigue siendo importante alimento en el Norte del país y su uso fue común en tiempos prehispánicos, cuando los chichimecas fabricaban pan de mezquite con la harina proveniente del fruto de la vaina. También es comestible la vaina del huizache.

Los lácteos y quesos tienen una historia que comienza con el Virreinato de la Nueva España, llega al territorio la fabricación de quesos, la cual progresivamente fue modificada para adaptarse a los gustos europeos, indígenas y criollos. Esta mezcla dio lugar a una serie de variedades de quesos mexicanos, la mayoría hechos de leche de vaca y otros cuantos de cabra. Existen entre 20 a 40 tipos de queso mexicano dependiendo cómo se le clasifique, además de las variaciones regionales y de producción casera que existen en cada poblado. Destacan el queso oaxaca, el queso panela, queso manchego mexicano (que es un queso blando hecho de vaca o cabra, distinto al queso manchego español hecho de oveja), queso chihuahua (de influencia menonita), queso cotija, queso bola, queso añejo de Zacazonapan, queso Zacatecas, queso cabra de Perote, queso de poro de Balancán y el queso crema chiapas, También existe producción de queso de tipo meramente europeo, principalmente en Guanajuato. Con el advenimiento de la producción de quesos industriales, hoy muchos de los quesos regionales están en riesgo de desaparecer, especialmente las variedades locales de queso ranchero, requesón y panela. Otros derivados lácteos como yogur artesanal, o el jocoque (un producto cuya base es la leche fermentada), también son comunes en la mesa de algunas zonas del país, especialmente en Sinaloa, los Altos de Jalisco y en la Comarca Lagunera.

El cacao es un ingrediente distintivo de la gastronomía mexicana y su uso se remonta a la época pre-olmeca (2000-1200 a.C.). 
En sus inicios durante la época precolombina, se elaboraba una bebida a base de los granos de cacao, agua y diferentes variedades de chiles. Actualmente el chocolate se utiliza como condimento para la preparación de diferentes variedades de moles, dulces, repostería, y en la preparación de bebidas a base de maíz.

La cocina mexicana consta de una variedad enorme de carnes, como res, cerdo, venado, conejo, armadillo, tuza, iguana, tapir, pecarí (jabalí americano), y aves como la codorniz, el pollo, variedades de pato, guajolote y faisán, por mencionar algunas; si bien el consumo de otros animales monteses o silvestres se ha reducido con el tiempo ya sea por su escasez o la prohibición de su consumo, pasando a ser rarezas o comida exclusiva de pequeñas zonas del país, como es el caso del ajolote mexicano, el tlacuache,el armadillo ,la tortuga caguama, el manatí, el mono y la ardilla.

México ocupa el decimocuarto puesto en mayor longitud de costas, las cuales están bañadas por dos océanos diferentes, el Pacífico y el Atlántico, por lo tanto no es sorprendente que el pescado y los mariscos sean protagonistas en las recetas de cocina mexicanas. Por otro lado, el país está bañado por tres importantes vertientes hidrográficas, por lo que también es común el consumo de peces, crustáceos y caracoles lacustres.

Entre los pescados mayormente consumidos por el mexicano se encuentra el atún, la sardina, el huachinango, la mojarra, la trucha, el róbalo, las carpas mexicanas, entre una innumerable lista. En algunas poblaciones cercanas a las playas, principalmente en el norte de Nayarit, las familias se reúnen a la orilla del mar para asar en parrillas de varas de mangle la liza recién pescada. Las amas de casa de todo el país acostumbran freír el pescado y después guisarlo de alguna forma, así es que por la demanda durante las mañanas es frecuente la venta de todos estos pescados en cualquier mercado.

México es uno de los principales exportadores de pulpo y calamar y también son comunes en su gastronomía. También son populares los platos con langosta, langostino y cangrejo, si bien su costo los ha limitado. Algunos moluscos como los ostiones suelen comerse vivos, y otros como el callo de hacha son altamente valorados en ceviches. Las sopas de mariscos son uno de los platillos que más consumen las familias costeras. El camarón es uno de los ingredientes más importantes de algunas gastronomías locales, y se prepara en sopas, asado, frito, cocido, empanizado, en empanadas, albóndigas, tamales y romeritos.

Los charales (una especie de pez de agua dulce exclusiva de los lagos del centro de México) se adquiere frecuentemente en los mercados, se puede preparar de diversas formas, ya sea seco y frito (con sal y limón), cubierto de chile seco, empanizado, frito con huevo o con ajo. Además se pueden preparar como otros tipos de comida, como puede ser el omelette o tortitas fritas en salsa verde. Incluso se venden en hojas de maíz, como tamales, estos han sido previamente asados a las brasas de leña.

Como un ejemplo de la riqueza y diversidad de la comida mexicana, puede citarse el consumo de los insectos.[15]​ México es el país del mundo en el que se ha documentado mayor variedad de especies consumidas.[16]​[17]​  Le entomofagia muestra la adaptación de la cocina tradicional a una gran variedad de ambientes o recursos y es un reflejo de la biodiversidad del país; además de que los insectos han favorecido otras necesidades humanas como: ropa, medicina, control de plagas y desintegración de desechos orgánicos.

En México, la entomofagia ha sido una práctica común desde la época prehispánica, como lo prueba el Códice Florentino, escrito por fray Bernardino de Sahagún, en donde se describe el consumo de 96 especies de insectos comestibles.[15]​ A la fecha los insectos siguen consumiéndose en todo el país (aunque en mayor medida en las regiones centrosur, oriente, sureste y suroeste del país), y llegan a considerarse, en casos como los escamoles un manjar de alto precio.

Según la fuente que se consulte, se han contabilizado entre 504 a 549 especies de insectos comestibles en el país (una tercera parte de las especies culinarias del mundo),[18]​[19]​[20]​ la mayor parte en los estados de Hidalgo, Oaxaca, Guerrero, Estado de México, Ciudad de México, Yucatán, Puebla y Veracruz mientras que en el norte de México se consumen en menor variedad. En múltiples grupos indígenas se cuenta con varios términos para clasificar a los artrópodos, a los que no siempre conocen con el término en español insecto, lo que ha dificultado su conteo.

Los insectos son altos en nutrientes, las culturas prehispánicas comían una enorme variedad sin percatarse de su gran aporte en proteína, ácidos grasos poliinsaturados, hierro, aminoácidos esenciales, calcio, fósforo, magnesio, vitaminas del complejo B y vitaminas A, C y D.[15]​

Entre los insectos mayormente consumidos destacan los escamoles, los gusanos de maguey, hormigas chicatanas, axayácatl y su ahuautle, chapulines y jumiles, entre otros.[15]​ Todos estos pueden ser consumidos en su forma natural ya que tienen un sabor agradable sin embargo ya cocinados son aún más deliciosos y pueden apreciarse mejor sus sabores. Se preparan tostados, asados, en tacos, en salsas, incluso hay restaurantes en México que los sirven en omelettes. Aunque son una excelente opción para la dieta humana, aún se tienen problemas para su promoción dado que no existe una Norma Oficial Mexicana que regularice sus procesos de producción, recolección y distribución.[cita requerida]

Los insectos comestibles en México se producen según la época del año; por ejemplo, los chinicuiles (la larva de una mariposa que se encuentra en el maguey) se "cosechan" durante las primeras lluvias del año, cuando comienza a llover, estos salen debajo del maguey y es en ese momento son recolectados. Actualmente hay poblaciones en donde estos insectos son criados colocando en una olla de barro una capa de chinicuiles vivos y otra de tortillas de maíz. En cuanto a los gusanos de maguey, estos crecen dentro de las pencas (la hoja del maguey), por lo que cada una debe ser destruida ya que es aproximadamente un gusano por penca, esto es un ejemplo del por qué muchas comidas de insectos pueden llegar a ser caras o comerse solo por temporadas.

El ahuautle (del náhuatl ahuautli o ahuahutli, o en su forma españolizada ahuautle o ahuahutle [a'wautle]) es la hueva de un insecto llamado axayáctl, se obtiene a través de la colocación en la orilla de los lagos de unos tules (tejidos abiertos hechos con este fin) o antiguamente hojas de mazorca o un atado de zacate y/o ramas secas atadas a una estaca. Las 6 especies de axayácatl se encuentran en peligro, son endémicas del Valle de México y su parentesco con las chinches hace que ahora no resulte muy atractivo para el consumidor común, miedo que se pierde al comerlo pues es considerado un manjar e incluso usado en la comida gourmet. Su hábitat se ha visto reducida por la desecación y contaminación de los cuerpos acuáticos, y la demanda, ocasionan que su precio sea muy elevado. Como pasa con múltiples insectos su nivel proteínico es más alto que el de la carne roja, con menor cantidad de ácidos grasos de mala calidad y resulta una excelente alternativa alimentaria por lo simple y económico de su cultivo, lo que puede llevar a conservar la especie.

Algunos estudios han comprobado que 100 gramos de este alimento (cualquier especie) aporta 80% de proteína. Los insectos contienen sales minerales, algunos son muy ricos en calcio, albergan vitaminas del grupo B y son una fuente importante de magnesio; además, en estado de larva, proporcionan calorías de gran calidad, y son ricos en ácidos grasos poliinsaturados.

Los escamoles (del náhuatl azcatl, "hormiga", y molli, "guiso") son larvas de hormiga que pueden encontrarse de manera silvestre en lugares áridos o boscosos, dependiendo del tipo de hormiga, aunque predomina la hormiga güijera o escamolera (Liometopum apiculatum), endémica de Norteamérica. Para hallar los escamoles, se debe descubrir cuidadosamente el nido en que los insectos tejen una estructura de ramas y saliva llamada trabécula donde depositan sus huevos. Luego, deben removerse solo dos terceras partes y volverse a cubrir con nopales secos y algunas ramas para que las hormigas regeneren los huevos extraídos y no haya un impacto ambiental considerable. Los escamoles tienen entre tres y cuatro veces más proteínas que la carne y tienen un sabor delicado que resulta un reto respetar en los diferentes medios de preparación; por lo tanto, aquel que logra prepararlos es considerado un buen cocinero.

Los gusanos de maguey son especies de larvas de lepidópteros que se crían en las pencas (hojas) de las especies de la familia del Agave. El término en realidad es el nombre popular de 2 especies principales:

El gusano blanco de maguey es la larva de una mariposa que crece en las hojas, pencas y raíces del maguey. Es blanco (excepto la cabeza y las extremidades), tiene un sabor muy delicado lo que lo ha llevado a ser uno de los insectos de mayor prestigio gastronómico mundial. Sus modos de preparación son varios, pero predominantemente es frito. Se obtiene del centro del maguey después de las épocas de lluvia, por lo que la extracción de unos 3 o 4 animales (no se obtienen más) ocasiona la pérdida de la planta; conociendo que de por sí el maguey tarda varios años en llegar a la madurez para poder ser raspado para la obtención de aguamiel con la que se obtiene el pulque; por esta razón, son ingredientes altamente valorados por el mexicano. Se encuentra en el centro del país, pero su mayor uso se encuentra en el estado de Hidalgo.

El gusano rojo de maguey es una especie de lepidóptero ditrisio de la familia de los cósidos, es una larva de polilla endémica de Norteamérica, donde habita en zonas áridas y desérticas; se consume como alimento en el centro de México, principalmente en el Valle del Mezquital. Los chinicuiles son muchas veces fritos en mantequilla dentro de una olla de barro y consumidos en tacos o en salsas rojas. También se incluyen en botellas de mezcal.

Las chicatanas (del náhuatl tzicatanah "hormiga con bolsa"),[21]​ en algunas zonas del Norte de México simplemente tzicatana, o en Chiapas nucú y bitú (según se trate de hembra y macho, respectivamente), son consideras de alto valor en la cocina ya que se tiene que ser muy cuidadoso al atraparlas; son muy agresivas y sus mordeduras son dolorosas. Tienen unas pequeñas tenazas que aunque son pequeñas son muy fuertes. Durante uno de los amaneceres de algunas temporadas, quizá huyendo de la inundación de la abundante lluvia, dejan el hormiguero para ser capturadas por mucha gente que ha aguardado pacientemente el momento adecuado, el cual es difícil predecir porque suele suceder en cualquiera de las primeras lluvias. Las hormigas chicatanas son un género de las hormigas arrieras u hormigas cortadoras de hojas.

Su uso en la cocina se extendía en la época precolombina desde el sur de Estados Unidos hasta Colombia y se pone de manifiesto en varios platillos; sin embargo en México a la hormiga grávida o reina es a la que se le da el mayor valor culinario, presente en platillos del sur, como Guerrero, Oaxaca, Chiapas, Veracruz, Hidalgo, Yucatán. Dependiendo de la región se cocina asada, frita, en botana, en salsa picante, en salsa de tomate, con guaje y sazonada con sal y limón.

Los saltamontes, conocidos popularmente en México como chapulines, aportan cantidades significativas de vitaminas. Algunas especies tienen mayores proporciones de vitamina A que el hígado y la leche y también aportan minerales como zinc, magnesio o calcio. La recolección de estos chapulines de los campos de cultivo, en lugar de su erradicación mediante el empleo de pesticidas, no solo reduce la contaminación del suelo y del agua, sino que también proporciona otros alimentos de mejor calidad e ingresos adicionales. Se destacan entre todos por un contenido proteico de hasta el 77.13%.

Por su consistencia crujiente, similar al de otras frituras del país, su venta es popular como botana adobada y asada en mercados y puestos ambulantes del centro y sur de México; sin embargo su uso está presente en platillos más complejos, dada su relativa facilidad para prepararlo con casi cualquier aderezo, lo que lo vuelve un ingrediente perfecto en guisos para tacos y quesadillas.

Las bebidas alcohólicas que acompañan a la gastronomía mexicana pueden beberse ahora en casi todo el mundo. Una excepción tal vez sea el pulque, bebida prehispánica cuyos expendios, las casi extintas «pulquerías», solo pueden encontrarse en México y, en particular, en ciertos estados de la república. Son sitios populares donde antiguamente se rendía culto a Mayahuel (la diosa del pulque).

Las bebidas alcohólicas más conocidas fuera y dentro de México son: el mezcal (bebida fermentada del agave) cuyo aroma y sabor le hacen inconfundible, y el tequila, licor nacional —aperitivo, en su origen— que se toma de múltiples maneras, a veces acompañado de sal y limón o junto con sangrita (bebida picante con jugo de naranja o de jitomate). El tequila recibió la denominación de origen el 13 de octubre de 1977, fecha a partir de la cual se incrementó la calidad en su producción y por consiguiente aumentó su proyección internacional. Con esta denominación de origen, solo 181 municipios entre 5 estados mexicanos: Jalisco, Nayarit, Michoacán, Guanajuato, Hidalgo y Tamaulipas pueden producir la bebida. El mezcal también cuenta con denominación de origen, y solo puede ser producido en 9 estados de México: Durango, Zacatecas, San Luis Potosí, Tamaulipas, Guanajuato, Michoacán, Guerrero, Puebla y Oaxaca. Así pues, en el mundo, solo Tamaulipas, Guanajuato y Michoacán pueden producir las dos bebidas. Como quiera que sea, estas dos bebidas son altamente populares en el país y el extranjero.

El pulque (en idioma otomí conocida como ñogi, en lengua purépecha como urapi, y en náhuatl como meoctli) es una bebida fermentada elaborada a partir del mucílago -popularmente conocido en México como aguamiel-, del agave o maguey especialmente del maguey pulquero (Agave salmiana) o del Agave atrovirens. Además de tratarse de una bebida popular del centro del país por su sabor refrescante, su historia está cargada de misticismo, presente en muchas leyendas e historias de México. Existe un pulque fuerte y otros "curados", es decir, pulque al que se le han añadido frutas y jarabes (piña, fresa, limón, naranja), semillas (nuez, avellana, piñón) o granos y legumbres (avena, maíz tostado, apio, alfalfa, perejil).

El pulque es consumido en pulquerías, muy frecuentes en zonas urbanas. En el aspecto gastronómico, el pulque no solamente se consume como bebida, existen platillos mexicanos que presentan al pulque como ingrediente para crear un sazón distinto en los platillos. Es el elemento alcohólico indispensable de la tradicional salsa borracha, además de que forma parte de las recetas de varios tipos de carnes y caldos, un ejemplo es el pollo al pulque, que se prepara como pollo frito sazonado en caldo de pulque y servido en cazuelas de barro para guardar el sabor.

El sotol es un destilado de la planta de sotol, cereque o sereque (Dasylirion wheeleri), de la familia Asparagaceae, y que crece en el desierto del norte de México. La planta y el destilado son originarios del desierto de Sonora y desierto de Chihuahua, bebida que evolucionó a partir de otras bebidas de la misma planta que cultivaron los apaches. Desde el 2008 cuenta con denominación de origen, y solo puede ser producido en los estados de Chihuahua, Coahuila y Durango. Su sabor es parecido al del tequila, pero más fuerte a pesar de su textura muy suave.

El pox es un destilado de maíz criollo, endémico de Chiapas, caña de azúcar y trigo. Históricamente era un destilado de la panela sobre viejos alambiques caseros y era, en su mayoría, proveniente de San Cristóbal de las Casas y de San Juan Chamula (Los Altos de Chiapas). Hoy su producción se ha extendido por toda la zona.

Otra bebida típica, el tepache (del náhuatl tepiatl), se elabora a partir de la fermentación de la cáscara de la piña. La elaboración del tepache requiere de cuatro días: en los dos primeros se dejan reposar trozos de pulpa y cáscara de piña en una olla de barro con clavos y canela, después se le agrega una mezcla de cebada y piloncillo, previamente hervidos, los cuales se dejan fermentar otros dos días.

La charanda es una bebida alcohólica originaria del estado de Michoacán, obtenida por destilación y rectificación de mostos fermentados, preparados a partir de jugo de caña o de sus derivados. El proceso de elaboración es similar al de los rones y otros aguardientes de caña. Lo que hace especial a la charanda es la materia prima: caña de altura y la tierra de cultivo, así como, de modo preponderante, la calidad del agua que se utiliza, si bien genéricamente se puede considerar una especie de ron. Cuenta con denominación de origen exclusiva para Michoacán.

El tejuino, tesgüino, o izquiate, conocido como el manjar de los dioses huicholes, es una bebida color ámbar claro a base de maíz no fermentado y dulce de caña o piloncillo, más denso que ligero, se bebe solo o con limón y sal. Regularmente se bate con molinillo antes de beberse para levantar espuma. Lo consumen diversos grupos étnicos del noroeste, noreste, occidente y centro norte, y en menor proporción, del sur de México; como los yaquis y pilas de Sonora, los tarahumaras y tumbares de Chihuahua, Durango y Jalisco, los huicholes de Jalisco y Nayarit, y los zapotecas de Oaxaca. Si bien es típica su venta en estados como Jalisco y Nayarit, mientras que en otros sitios tiene un valor de festividad, como es el caso de Nochistlán, Zacatecas, donde se realiza toda una fiesta en torno a San Sebastián, el pinole y el tejuino.

El colonche, o nochol, es una bebida alcohólica de origen prehispánico, tan antigua como el mezcal (aproximadamente 2000 años) preparada a partir de la fermentación de la pulpa de la tuna, fruto del nopal; más específicamente de la tuna roja (llamada cardona). Esta bebida es muy popular en los estados de San Luis Potosí, Guanajuato y en algunas zonas de Querétaro y Zacatecas. Para su preparación, los frutos de nopal se pelan y se trituran para obtener el jugo, que se hierve durante 2 a 3 horas. Después de enfriar, el jugo se deja fermentar durante unos pocos días. El color del colonche es básicamente rojo con algunos tonos azulados. Su aroma es fresco, ligero y su consistencia un tanto viscosa. A veces se le agrega colonche viejo como iniciador de la fermentación, aunque también se suelen usar "tibicos". Esta bebida se consume en mayor medida en la época de producción de tunas, desde julio hasta finales de octubre; si bien es una bebida en peligro de desaparecer.[22]​

Son muy conocidas las cervezas de México por su calidad, por su sabor suave y delicado, y otras por su sabor fuerte e intenso; se suelen tomar frías y, en muchas ocasiones, acompañadas de un limón. En 2016, la industria reportó una producción de 105 millones de hectolitros, lo que convierte al país en el cuarto mayor productor del mundo, y el primer exportador de cerveza a nivel mundial (21.3 % de la cerveza exportada en todo el mundo), siendo sus principales destinos Estados Unidos, Australia y Reino Unido.

La cocina mexicana y la cerveza resultan ser una combinación bastante popular en México y otras partes del mundo, esto gracias a que la cerveza goza de una amplia gama de aromas, sabores, y referencias gastronómicas que la vuelven versátil ante muchos alimentos. Cervezas de buen cuerpo, oscuras, y con referencias a chocolate y/o café por ejemplo van con platillos de gran personalidad como el mole poblano. Mientras que platillos picantes lo hacen con una cerveza de aromas madereros y frutados que tengan al mismo tiempo liviandad y frescura. Aunque en la tradición culinaria se habla de los licores y destilados como digestivos, en el sentido estricto estos pueden ser una carga digestiva irritante, que lejos de favorecer la digestión, la puede empeorar. Esto no es así con el vino y la cerveza, que por su baja graduación funcionan como verdaderos aperitivos y digestivos. La cerveza es consumida con casi cualquier platillo de la gastronomía mexicana, pero comúnmente se hace durante la tarde, con el plato fuerte, o cuando se va a consumir pescado y mariscos.

La producción de vino en México llegó tras el Virreinato, que trajo a América la especie de uva más utilizada actualmente para la producción de vino, Vitis vinifera, si bien es importante saber que ya existía en toda la región la producción de bebidas por la fermentación de frutos, y 4 especies de uva endémicas de Norteamérica eran ampliamente conocidas. Los mexicas conocieron a la uva como acacholli, los purépechas le conocían como seruráni, los otomíes la llamaron obxi y los tarahumaras le decían úri. Hasta la fecha es discutido el término que se pudo utilizar para los fermentados de uva, pero gracias a esto no es difícil entender lo sencillo que fue para los locales adoptar la tradición vitivinícola.

El temprano desarrollo de la producción de vino y su desarrollo a través de los siglos lo han convertido en una actividad de suma importancia para la economía mexicana. Se distinguen como productores de vino: Aguascalientes, Baja California, Baja California Sur, Chihuahua, Coahuila, Guanajuato, Nuevo León, Querétaro y Zacatecas. Los vinos mexicanos han sido premiados en festivales internacionales, lo cual ha causado un fuerte impacto en el mercado exterior, si bien por su alta calidad, algunos son percibidos por sectores sociales como de precio elevado.

Una gran proporción de la mixología o coctelería mexicana encontrada en bares, clubes o cantinas se trata sobre un rescate gastronómico de recetas e ingredientes nacionales como el tequila o el mezcal. Hoy en día restaurantes y bares se dan a la tarea de preparar o reinventar mezclas que incluyan destilados o fermentados locales o regionales, refrescos o jugos mexicanos, así como ingredientes de la cocina mexicana como es el chile y el limón. También muchas de las copas utilizadas son de invención nacional, como es el caso de la copa bola (tongolele o chabela) o variantes de las copas internacionales modificadas por el arte mexicano del vidrio soplado. 

Algunos cocteles populares son:

Existen también una serie de bebidas que fueron ideadas a lo largo de la historia, y que rematan el universo de la gastronomía mexicana, algunas de ellas muy populares como el atole o la internacional "bebida de los dioses": el chocolate.

Las aguas frescas, son sin duda alguna las bebidas más utilizadas en la gastronomía mexicana durante el almuerzo o comida, aunque sin necesidad de acompañar algo se les puede conseguir en cualquier lugar del país, ya sea en un mercado, restaurante, centro comercial o parque sin importar la época del año. Son elaboradas a partir del zumo de frutas, agua y azúcar. Las más tradicionales pueden estar preparadas de frutos dulces (melón, papaya, sandía, mango, guayaba, coco o piña), frutas ácidas (limón, chía, lima, naranja, tamarindo, fresa, pepino, pitahaya, guanábana, changunga, tejocote, carambola, toronja, mandarina o kiwi) o de granos, flores u hojas (jamaica, horchata, alfalfa o cebada). Probablemente por su popularidad, merezcan una especial mención tres de estas aguas frescas: el agua de horchata, una bebida a base de arroz, leche y azúcar y que puede ser acompañada con canela; el agua de jamaica, una infusión hecha de cálices (sépalos) de la rosa de Jamaica muy popular en todo el mundo y que en México encontró su nicho especial como bebida refrescante; y el agua de chía, una bebida a base de agua, limón, azúcar y chía (Salvia hispanica), planta endémica de México y Centroamérica ampliamente cultivada en esta región en tiempos prehispánicos (en algunas bibliografías la mencionan tercera en importancia solo por detrás del maíz y el frijol), una vez hecha la mezcla el agua obtiene una ligera consistencia gelatinosa y un sabor agradable y refrescante, además a esta última bebida los mexicanos atribuyen múltiples beneficios a la salud.

Del maya chocolhaa, "bebida caliente", y náhuatl xocoatl o cacahuatl, "bebida de cacao", la mayoría de las culturas mesoamericanas hicieron bebidas de chocolate, incluidos los mayas y mexicas. Los mexicas premiaban a los mejores guerreros de la época otorgándoles el derecho de consumir libremente chocolate. También a los soldados se les otorgaban especies de bolitas hechas con polvo de cacao para que pudieran preparar su chocolate a lo largo de la guerra. La bebida actual resultó finalmente de la incorporación del azúcar en el siglo XVI.

Actualmente la bebida tiene variantes a lo largo del territorio, si bien suele servirse como bebida caliente durante el desayuno o la cena.  Existe una variante entre el chocolate y el atole llamada champurrado, elaborada a base de masa de maíz machacado, chocolate oscuro y agua con vainilla, hervidos hasta espesar. Por lo general se sirve acompañando otro plato típico de México, los tamales.

Del náhuatl atolli, "aguado", bebida preparada con maíz cocido, molido y diluido en agua y/o leche y hervido hasta darle cierta consistencia. Se prepara y sirve de esta forma, o en su preparación se mezcla con otros frutos o ingredientes, como es la guayaba, el arroz, fresa, pinole, vainilla, canela, piloncillo, entre muchos otros. Suele acompañar a platillos como los tamales. El atole sirve de base para otra bebida, el chilate.

Conocida popularmente como "bebida de los dioses", es una bebida preparada a base de maíz y cacao, tradicional del estado de Oaxaca. Los ingredientes principales del tejate son harina tostada de maíz, granos de cacao fermentados, semillas de mamey y flor de cacao también conocida como rosita de cacao, especialmente común en San Andrés Huayapam, Oaxaca. Su sabor es a la vez fresco y dulce, y suele servirse en jícaras.

El pozol o pochotl, (del náhuatll pozolli), es una bebida refrescante compuesta de masa de maíz cocido, cacao y granos de pochotl o ceiba. Es muy popular en los estados de Tabasco y Chiapas, así como en algunas zonas de Veracruz, Oaxaca y Centroamérica. La consistencia final es el de una bebida medianamente grumosa, que tiende a asentarse después de un par de minutos, quedando en el fondo del recipiente un residuo llamado en lengua maya "shish"  (castellanizado como "shishito" en Tabasco o "musú" o "motzú" en Chiapas), constituido de masa de maíz y cacao (o solo masa en el caso del pozol blanco). Entonces, al agitarlo nuevamente, con el característico movimiento elíptico que ofrecen las vasijas y jícaras en que se sirve, llamado "meneadito del pozol", este retoma su consistencia espesa y, como se dice popularmente "es una bebida comestible", haciendo alusión a que, al tiempo de tomarlo, se mastica el "shish", calmando de esta manera la sed y el hambre simultáneamente. En efecto, el pozol puede ser una bebida utilizada especialmente con fines alimenticios, por lo que es común su ingesta en sitios de trabajo o en campos de agricultura. En el estado de Yucatán se prepara una variante blanca sin cacao llamada localmente "pozole", que es consumida de varias maneras: con sal y chile habanero al natural (tanto fresco como agrio) o endulzada con azúcar y ralladura de coco.[23]​

México es el quinto productor de café del mundo, después de Brasil, Colombia, Indonesia y Vietnam; es el primer productor de café orgánico[24]​ y uno de los principales productores de café "gourmet". El café de olla es una de las formas más comunes de preparar el café en México. Se elabora calentando, en agua contenida en una olla de barro grande de boca angosta, granos de café gruesos e incluso enteros, que se mezclan en la forma adecuada con canela y piloncillo. Suele consumirse mucho en el campo o en los pueblos pequeños.

Destacan por la calidad de su café algunas regiones en Veracruz como Coatepec, Córdoba, Huatusco y Orizaba, la región del Soconusco en Chiapas y el café Pluma Hidalgo en Oaxaca, aunque también hay mención específica sobre la alta calidad del grano producido en Colima y Uruapan, Michoacán.

El Conservatorio de la Cultura Gastronómica Mexicana (CCGM) es una organización de la sociedad civil mexicana, que tiene como objetivo la salvaguardia de las raíces, la identidad y la continuidad de la gastronomía de México. Del mismo modo promueve y difunde los valores y las características de la cocina mexicana con el fin de que siga manteniendo su lugar como una de las más grandes y variadas en el mundo.

Con este propósito, el organismo, que también actúa como consultor de la Unesco, lanzó la iniciativa de creación del expediente técnico, para que esta institución mundial reconociera y considerara la cocina tradicional mexicana como Patrimonio Cultural Inmaterial de la Humanidad, cuestión que fue aprobada al incorporarse aquella en la lista de tal patrimonio en noviembre de 2010.[26]​[27]​

Entre los objetivos del CCGM, se encuentra el desarrollo de programas, proyectos y proponer acciones tales como ferias, foros y encuentros enfocados a:

Una denominación de origen es el nombre de un sitio que se utiliza para designar a un producto originario, cuyas cualidades y características se deben exclusivamente al medio geográfico. Hasta 2018, México posee 16 productos de reconocimiento mundial por sus características únicas y calidad especial. De estas, 13 son las que están relacionadas con la cocina mexicana o bebidas tradicionales. Estos productos son protegidos por el Instituto Mexicano de la Propiedad Intelectual, para su protección como bienes únicos y su difusión a nivel mundial.

Si bien existen platos nacionales (como el mole, los chiles en nogada o la cochinita pibil), muchos autores han definido la gastronomía mexicana más como un conjunto de cocinas regionales, que no es más que una prolongación de la´diversidad cultural, geográfica, histórica y étnica del país.

El Homo sapiens (del latín homo, «hombre», y sapiens, «sabio»), hombre o ser humano es una especie del orden de los primates perteneciente a la familia de los homínidos. También son conocidos bajo la denominación genérica de humanos. Los seres humanos poseen capacidades mentales que les permiten inventar, aprender y utilizar estructuras lingüísticas complejas, lógicas, matemáticas, escritura, música, ciencia y tecnología. Los seres humanos son animales sociales, capaces de concebir, transmitir y aprender conceptos totalmente abstractos.

Se considera Homo sapiens de manera indiscutible a los que poseen las características anatómicas de las poblaciones humanas actuales. Los restos más antiguos atribuidos a Homo sapiens, datados en 315 000 años, se encontraron en Marruecos.[1]​ Las evidencias más antiguas de comportamiento moderno son las de Pinnacle Point (Sudáfrica), con 165 000 años de antigüedad.

Pertenece al género Homo, que fue más diversificado y durante el último millón y medio de años incluía otras especies ya extintas. Desde la extinción del Homo neanderthalensis, hace 28 000 años, y del Homo floresiensis hace 12 000 años (debatible), Homo sapiens es la única especie conocida del género Homo que aún perdura.

Hasta hace poco, la biología utilizaba un nombre trinomial —Homo sapiens sapiens— para esta especie, pero más recientemente se ha descartado el nexo filogenético entre el neandertal y la actual humanidad,[2]​ por lo que se usa exclusivamente el nombre binomial. Homo sapiens pertenece a una estirpe de primates, los hominoideos. Aunque el descubrimiento de Homo sapiens idaltu en 2003 haría necesario volver al sistema trinomial, la posición taxonómica de este último es aún incierta.[3]​ Evolutivamente se diferenció en África y de ese ancestro surgió la familia de la que forman parte los homínidos.

Filosóficamente, el ser humano se ha definido y redefinido a sí mismo de numerosas maneras a través de la historia, otorgándose de esta manera un propósito positivo o negativo respecto de su propia existencia. Existen diversos sistemas religiosos e ideales filosóficos que, de acuerdo con una diversa gama de culturas e ideales individuales, tienen como propósito y función responder a algunas de esas interrogantes existenciales. Los seres humanos tienen la capacidad de ser conscientes de sí mismos, así como de su pasado; saben que tienen el poder de planear, transformar y realizar proyectos de diversos tipos. En función de esta capacidad, han creado diversos códigos morales y dogmas orientados directamente al manejo de estas capacidades. Además, pueden ser conscientes de responsabilidades y peligros provenientes de la naturaleza, así como de otros seres humanos.

El nombre científico es el asignado por el naturalista sueco Carlos Linneo (1707-1778) en 1758,[4]​ alude al rasgo biológico más característico: sapiens significa «sabio» o «capaz de conocer», y se refiere a la consideración del ser humano como «animal racional», al contrario que todas las otras especies, siendo la descripción que aportó para Homo sapiens fue simplemente: Nosce te ipsum («Conócete a ti mismo»). Es precisamente la capacidad del ser humano de realizar operaciones conceptuales y simbólicas muy complejas —que incluyen, por ejemplo, el uso de sistemas lingüísticos muy sofisticados, el razonamiento abstracto y las capacidades de introspección y especulación— uno de sus rasgos más destacados. Posiblemente esta complejidad, fundada neurológicamente en un aumento del tamaño del cerebro y, sobre todo, en el desarrollo del lóbulo frontal, es también una de las causas, a la vez que producto, de las muy complejas estructuras sociales que el ser humano ha desarrollado, y que forman una de las bases de la cultura, entendida biológicamente como la capacidad para transmitir información y hábitos por imitación e instrucción, en vez de por herencia genética. Esta propiedad no es exclusiva de esta especie y es importante también en otros primates.

Linneo clasificó al hombre y a los monos en un grupo que llamó antropomorfos, como subconjunto del grupo cuadrúpedos, pues entonces no reconocía signos orgánicos que le permitieran ubicar al ser humano en un lugar privilegiado de la escala de los vivientes. Años más tarde, en el prefacio de Fauna suecica, manifestó que había clasificado al hombre como cuadrúpedo porque no era planta ni piedra, sino un animal, tanto por su género de vida como por su locomoción y porque además, no había podido encontrar un solo carácter distintivo por el cual el hombre se diferenciara del mono; en otro contexto afirmó sin embargo que considera al hombre como el fin último de la creación. A partir de la décima edición de Systema naturae reemplazó a los cuadrúpedos por los mamíferos y como primer orden de estos, puso a los primates, entre los cuales colocó al hombre. Linneo tuvo el mérito de dar origen a un nuevo e inmenso campo epistemológico, el de la antropología, si bien se limitó a enunciarlo y no lo cultivó. A él tendrán que remitirse todos los científicos posteriores, tanto para retomar sus definiciones como para criticarlas. En 1758 se definió al Homo sapiens linneano como una especie diurna que cambiaba por la educación y el clima. 

Linneo no designó un holotipo para Homo sapiens, pero en 1959 William Stearn propuso al propio Linneo, padre de la moderna taxonomía, como lectotipo para la especie. Con posterioridad se difundió la idea de que había sido sustituido por Edward Cope, pero esta propuesta no llegó a formalizarse, así que siguen siendo los restos de Linneo enterrados en Uppsala el tipo nomenclatural -que debe considerarse simbólico- para la especie Homo sapiens.[5]​ 

En la actualidad existen defensores de incluir al ser humano, chimpancé (Pan troglodytes) y bonobo (Pan paniscus) en el mismo género, dada la cercanía filogenética, que es más estrecha que la que se encuentra entre otras especies animales que sí están agrupadas genéricamente.[6]​ Sin embargo, la inmensa mayoría de los especialistas no consideran correcto incluirlos dentro del mismo género, debido a que los linajes evolutivos que condujeron al ser humano y al chimpancé divergieron hace entre 6 y 10 millones de años y se diversificaron posteriormente, como argumenta Sandy Harcourt,[6]​ y debido a las significativas diferencias entre los planes corporales de ambas líneas, especialmente en la de los Hominina, que permiten justificar varios géneros (Ardipithecus, Paranthropus, Australopithecus u Homo).[7]​

El ser humano es un ser vivo, y como tal está compuesto por sustancias químicas llamadas biomoléculas, por células y realiza las tres funciones vitales: nutrición, relación y reproducción.[8]​

Además, el cuerpo es un organismo pluricelular; es decir, está formado por muchas células, entre las cuales existen diferencias de estructura y de función.[8]​

Por otra parte, el ser humano es un animal, pues tiene células eucariotas, es decir, presenta orgánulos celulares especializados en una función determinada y su material genético se encuentra protegido por una envoltura; y presenta nutrición heterótrofa; es decir, que para obtener su propia materia orgánica se alimenta de otros seres vivos.[8]​

En cuanto a su locomoción y movimiento, es uno de los más plásticos del reino animal, pues existe una amplia gama de movimientos posibles, lo que le capacita para actividades como el arte escénico y la danza, el deporte y un sinnúmero de actividades cotidianas. Asimismo destaca la habilidad de manipulación, gracias a los pulgares oponibles, que le facilitan la fabricación y uso de instrumentos.

La especie humana posee un notorio dimorfismo sexual en el nivel anatómico, siendo los hombres adultos más altos y más pesados que las mujeres en promedio, aunque se ha notado una «tendencia secular» al aumento de las tallas en ambos sexos (especialmente durante el siglo XX). 

El ser humano adulto contemporáneo promedio mide entre: 1,55 m. a 1,65 m. (mujeres), y entre 1,65 m. a 1,85 m. (hombres). El peso depende de la contextura del individuo y del sexo, generalmente rondando los 45 kg a 70 kg (mujeres), y 65 kg a 100 kg (hombres). Los cuerpos humanos difieren entre sí según la estatura, peso, raza, musculatura, nivel de grasa, entre otros.

La mente se refiere colectivamente a aspectos del entendimiento y conciencia que son combinaciones de capacidades como el raciocinio, la percepción, la emoción, la memoria, la imaginación y la voluntad. La mente, según la neurociencia, es un resultado de la actividad del cerebro.

El término pensamiento define todos los productos que la mente puede generar incluyendo las actividades racionales del intelecto y las abstracciones de la imaginación; todo aquello que sea de naturaleza mental es considerado pensamiento, bien sean estos abstractos, racionales, creativos, artísticos, etc. Junto con los cetáceos superiores (delfines y ballenas), los homininos de los géneros Gorilla y Pan, y los elefantes, alcanzan el mayor desarrollo y aun muchas de sus interacciones nos son desconocidas.

Los seres humanos, a diferencia del resto del reino animal, son los únicos con capacidad de razonar. Además poseen capacidades mentales que les permiten inventar, aprender y utilizar estructuras lingüísticas complejas, lógicas, matemáticas, escritura, música, ciencia y tecnología. Los seres humanos son animales sociales, capaces de concebir, transmitir y aprender conceptos totalmente abstractos.

El ser humano es un animal omnívoro.[9]​[10]​  En las primeras especies del género Homo, el paso de una alimentación eminentemente vegetariana a la inclusión de carne y grasas animales en la dieta no se debió a cuestiones culturales, sino a los desajustes metabólicos provocados por un mayor desarrollo cerebral.[9]​ Sin embargo, en el humano, una dieta demasiado rica en proteínas necesita el complemento de carbohidratos y grasas; de lo contrario pueden aparecer carencias nutricionales importantes que pueden incluso provocar la muerte.[9]​ Por ello, la alimentación del ser humano se basa en la combinación de carne con materia vegetal.[9]​

La especie humana es entre los seres vivos pluricelulares actuales una de las más longevas; se tienen documentados casos de longevidad que sobrepasan los cien años. Tal longevidad es un carácter genotípico que, sin embargo, debe ser coadyuvado por condiciones vivenciales favorables. En el Imperio romano, hacia el año 1 d. C., la esperanza de vida rondaba solo los 25 años, debido en gran parte a la elevada mortalidad infantil.[cita requerida] A principios del siglo XXI, la esperanza de vida global era de unos 70 años aproximadamente, siendo más elevada en países desarrollados y más baja en países subdesarrollados. 

Se supone que el ser humano, en óptimas condiciones, pueda vivir cien años o un poco más. Sin embargo a pesar del avance en la salud y calidad de vida en el último siglo, las costumbres humanas como el consumo de drogas, alcohol, azúcar, comidas chatarras, sedentarismo, estrés, enfermedades de todo tipo, exposición a elementos tóxicos, entre otros, disminuye los años de vida de los seres humanos. Se cree también que pueda ser genético.[11]​

La 'infancia' humana es una de las más prolongadas en comparación con otras especies cercanas, siendo la edad de la pubertad es aproximadamente a los once años en las niñas y a los trece años en los niños, aunque las edades varían según la persona.

Como todos los mamíferos, el ser humano tiene comportamientos reproductivos y sexuales. Pero a diferencia de la mayoría de ellos no tiene una época reproductiva estacional determinada, manteniendo actividad sexual y fertilidad en las hembras a lo largo de todo el año. Las mujeres tienen un ciclo de ovulación aproximadamente mensual, durante el cual producen óvulos y pueden ser fecundadas; en caso contrario tienen la menstruación, que es la eliminación a través de la vagina de los tejidos y sustancias relacionados con la producción de células sexuales.

Pero el comportamiento sexual humano no está únicamente supeditado a las funciones reproductivas, sino que, de modo similar a otros simios antropoides, tiene fines recreativos y sociales. En el contacto sexual se busca tanto el placer como la comunicación afectiva. Es una parte importante de las relaciones de pareja y también se considera importante en las necesidades psicológicas del individuo aunque no tenga una relación de pareja.

Cabe destacar la importancia del lenguaje simbólico en el Homo sapiens, que hace que los significantes sean los soportes del pensar o los pensamientos. En nuestra especie, el pensar humano, a partir de los tres años y medio de edad se hace prevalentemente simbólico.

Asociado con lo anterior, debe notarse que la especie humana es prácticamente la única que se mantiene en celo sexual continuo: es realmente destacable que en la especie humana no exista un estro propiamente dicho. En las mujeres existe un ciclo de actividad ovárica en virtud del cual existen cambios fisiológicos en todo su sistema reproductivo y del cual derivan ciertos cambios de conducta. Sin embargo, como en las mujeres la aceptación sexual no se circunscribe a una parte del ciclo reproductivo, no se debería usar el término "estro" o "celo" en el ser humano, dado que la aceptación sexual es independiente de su ciclo reproductivo. Ya entre chimpancés y, sobre todo, bonobos, se nota una conducta próxima.

Ahora bien; dada la dificultad de vivir solamente practicando relaciones sexuales, un "mecanismo" evolutivo compensatorio habría sido el de la sublimación –la cual se considera asociada a la existencia de un lenguaje y un pensar simbólicos–. Si se da una sublimación, esto parece significar que también se da una represión (en el sentido freudiano) que origina a lo inconsciente. El Homo sapiens es, en este sentido, un animal pulsional. Según la ley del reflejo condicional de Pavlov el Homo sapiens no se restringe a un "primer sistema de señales" (el de estímulo/respuesta y respuesta a un estímulo substitutivo), sino que el ser humano se encuentra en un nivel de "segundo sistema de señales". Este segundo sistema es, principalmente, el del lenguaje simbólico que permite una heurística, que es la capacidad para realizar de forma inmediata innovaciones positivas para sus fines.

Por otra parte, la especie humana es de las pocas, junto con el bonobo (Pan paniscus), en el reino animal que copula cara a cara, lo cual tiene implicaciones emocionales de gran relevancia para la especie.

Cabe anotar que con el surgimiento de la teoría de la inteligencia emocional, desde la psicología sistémica, el ser humano no debe reducirse a sus pulsiones, las cuales sublima o reprime, sino que se entiende como un ser sexuado, que vive esta dimensión en relación con la formación recibida en la familia y la sociedad. La sexualidad se forma entonces desde los primeros años y se va entendiendo como una vivencia procesual acorde a su ciclo vital y su contexto socio-cultural.

A diferencia de lo que ocurre en la mayor parte de las otras especies sexuadas, la mujer sigue viviendo mucho tiempo tras la menopausia. En las otras especies la hembra suele fenecer al poco tiempo de llegada la misma.

Por la indicada prematuración, la madurez sexo-genital es –en relación a otras especies– muy tardía entre los individuos de la especie humana. Actualmente en muchas zonas la menarquia está ocurriendo a los once años; esto significa que, aunque la madurez sexo-genital es siempre lenta en la especie humana, existe un adelantamiento de la misma respecto a épocas pasadas (del mismo modo suele darse una menopausia cada vez más tardía). Pero si la madurez sexo-genital es tardía en la especie humana, aun más suele serlo la madurez intelectual y, en especial, la madurez emotiva.

A lo largo de la historia se han ido desarrollando distintas concepciones míticas, religiosas, filosóficas y científicas respecto del ser humano, cada una con su propia explicación sobre el origen del hombre, trascendencia y misión en la vida.

Evolutivamente, en cuanto perteneciente al infraorden Catarrhini, Homo sapiens parece tener su ancestro, junto con todos los primates catarrinos, en un período que va de los 50 a 33 millones de años antes del presente (AP). Uno de los primeros catarrinos, quizás el primero, es Propliopithecus, incluyendo a Aegyptopithecus. En este sentido, el ser humano actual, al igual que primates del "Viejo Mundo" con características más primitivas, probablemente descienda de esa antigua especie.

En cuanto a la bipedestación, esta se observa en ciertos primates a partir del Mioceno. Ya se encuentran ejemplos de bipedación en Oreopithecus bambolii y la bipedestación parece haber sido común en Orrorin y Ardipithecus. Las mutaciones que llevaron a la bipedación fueron exitosas porque dejaban libres las manos para agarrar objetos y, particularmente, porque en la marcha un homínido ahorra mucha más energía andando sobre dos piernas que sobre cuatro patas, puede acarrear objetos durante la marcha y otear más lejos. Sin embargo, de remontarse la bipedestación a quizás a unos seis millones de años AP, la andadura o forma de marcha típica del humano se consolida aproximadamente hace al menos unos cuatro millones de años con Australopithecus. Previamente los primates antropoides apoyaban toda la planta del pie haciendo una flexión y descargando el peso en el calcáneo; en cambio, Australopithecus logra una marcha bípeda eficiente, pues se notan claramente los cambios anatómicos a nivel del pie, en especial del dedo gordo; también ajustando el ángulo del fémur con el cuerpo para el equilibrio, la cadera o pelvis cambia a más robusta, corta y cóncava (forma de cuenco); la columna pasó de ser un arco en forma de C a una forma de S y el agujero de la base del cráneo que conecta con la columna se desplazó hacia adelante[12]​ como dirigiéndose al centro de gravedad de la cabeza.

Hace 1.5 millones de años con Homo erectus o con Homo ergaster, la andadura moderna implica la existencia de un pequeño ángulo entre el dedo gordo y el eje del pie, así como la presencia del arco longitudinal de la planta y una distribución medial del peso (nótese que en las mujeres la andadura distribuye el peso más hacia las partes internas del pie debido a la mayor anchura de la pelvis).[13]​

Todos los cambios reseñados han sucedido en un periodo relativamente breve (aunque se mida en millones de años). Esto explica la susceptibilidad de nuestra especie a afecciones en la columna vertebral y en la circulación sanguínea y linfática (por ejemplo, el corazón recibe -relativamente- "poca" sangre).

Lo que denominamos propiamente «humano» es una referencia a la aparición de la capacidad de fabricar herramientas de piedra en un homínido bípedo, Homo habilis, considerado por la mayoría como la especie humana más primitiva, mostrando además incremento en la capacidad craneana con respecto a Australopithecus. Es así como se establece que hace unos dos millones y medio de años, con la aparición del género Homo, se toma como punto de inicio para el Paleolítico o Edad de Piedra. Mayor éxito evolutivo tendrá Homo erectus, quien logrará expandirse por todo Eurasia.

Probablemente cuando los ancestros de Homo sapiens vivían en selvas comiendo frutos, bayas y hojas, abundantes en vitamina C, pudieron perder la capacidad metabólica que tiene la mayoría de los animales de sintetizar en su propio organismo tal vitamina; ya antes parecen haber perdido la capacidad de digerir la celulosa. Tales pérdidas durante la evolución han implicado sutiles pero importantes determinaciones: cuando las selvas originales se redujeron o, por crecimiento demográfico, resultaron superpobladas, los primitivos homínidos (y luego los humanos) se vieron forzados a recorrer importantes distancias, migrar, para obtener nuevas fuentes de nutrientes. La pérdida de la capacidad de metabolizar ciertos nutrientes como la vitamina C habría sido compensada por una mutación favorable que permite a Homo sapiens una metabolización óptima (ausente en primates) del almidón, y así una rápida y "barata" obtención de energía, particularmente útil para el cerebro. Homo sapiens parece ser una criatura bastante indefensa, y como respuesta satisfactoria la única solución evolutiva que ha tenido es su complejísimo sistema nervioso central, espoleado principalmente por la búsqueda de nuevas fuentes de alimentación. Se ha sugerido la hipótesis de que la cefalización aumentó paralelamente al incremento de consumo de carne,[cita requerida] aunque dicha hipótesis no concuerda con el grado de cefalización desarrollada por los animales carnívoros. La habilidad humana para digerir alimentos con alto contenido de almidón podría explicar el éxito del Homo sapiens en el planeta, y sugiere un estudio genético.[14]​

Se denomina «humanos arcaicos», «Homo sapiens arcaico» o también «pre-sapiens», a un cierto número de especies de Homo que aun no son considerados anatómicamente modernos. Poseen hasta 600 000 años de antigüedad y tienen un tamaño cerebral cercano al de los humanos modernos. El antropólogo Robin Dunbar opina que es en esta etapa cuando aparece el lenguaje humano. La filiación de estos individuos dentro de nuestro género resulta aun controvertida.

Entre los humanos arcaicos están considerados Homo heidelbergensis, Homo rhodesiensis, Homo neanderthalensis y a veces Homo antecessor. En 2010 se ha añadido a estos el denominado «hombre de Denísova»,[15]​ y en 2012 el denominado «hombre del ciervo rojo» en China.[16]​ Ya que no son sapiens, algunos especialistas prefieren llamarlos simplemente arcaicos antes que H. sapiens arcaico.[17]​

Se denomina propiamente Homo sapiens o anatómicamente modernos a individuos con una apariencia similar a la de los humanos modernos. Estos humanos pueden clasificarse como premodernos, pues en ellos no se observa todavía el conjunto de características de un cráneo moderno, casi esférico, con la bóveda alta y la frente vertical.[18]​ La similitud se aprecia a nivel del esqueleto del cuerpo y cavidad craneana, pero esta similitud no es total pues el rostro aun mantiene características arcaicas como los arcos superciliares (grandes cejas) y prognatismo maxilar (proyección bucal), aunque menos desarrollados que en los neandertales.[19]​

Se considera dentro de este grupo a los restos de Florisbad en Sudáfrica (260 000 años),[20]​ los de Herto en Etiopía, que corresponde a Homo sapiens idaltu (160 000 años), los de Jebel Irhoud en Marruecos (315 000 años) y los de Skhul/Qafzeh al norte de Israel (100 000 años). También se considera anatómicamente modernos a los hombres de Kibish; sin embargo, estos se enmarcan mejor dentro de los humanos modernos.

Se considera Homo sapiens sapiens de forma indiscutible a los que poseen las características principales que definen a los humanos modernos: primero la equiparación anatómica con las poblaciones humanas actuales y luego lo que se define como "comportamiento moderno".

Actualmente, gracias a los análisis científicos, se sabe que en la genealogía de la evolución humana habría existido un antepasado común masculino y uno femenino, a los cuales se les nombró como sus símiles religiosos.

Los restos más antiguos son los de Omo I, llamados Hombres de Kibish, encontrados en Etiopía con 195 000 años, y restos en cuevas del río Klasies en Sudáfrica con 125 000 años y con indicios de una conducta más moderna.[21]​

Esta antigüedad coincide con lo estimado para la Eva mitocondrial, la cual está considerada la antecesora de todos los seres humanos actuales y de la que se cree que vivió en el África Oriental[22]​ (probablemente Tanzania) hace unos 200 000 años.

Por otra parte, la línea patrilineal nos lleva hasta el Adán cromosómico, quien nos confirma un origen para los humanos modernos en el África subsahariana y se le calcula unos 140 000 años de antigüedad.[23]​

Es casi seguro que la Eva mitocondrial y el Adán, los primeros Homo sapiens eran melanodérmicos, esto es, de tez oscura. Esto se debe a que la piel oscura es una excelente adaptación a la exposición solar alta de las zonas intertropicales del planeta Tierra; la tez oscura (por la melanina) protege de las radiaciones UV (ultravioletas) y obtiene de ellas por metabolismo un nutriente llamado folato, indispensable para el desarrollo del embrión y del feto; pero, a medida que las poblaciones humanas migraron a latitudes más allá de los 45º (tanto norte como sur) la melanina paulatinamente fue menos necesaria, más aun, en las cercanías de las latitudes de los 50º la casi total falta de este pigmento en la dermis, cabello y ojos ha sido una adaptación para captar más radiaciones U.V. —relativamente escasas en tales latitudes, salvo que se produzcan huecos de ozono—; en tales latitudes la tez muy clara posibilita una mayor metabolización de vitamina D a partir de las radiaciones UV.

La aparición del comportamiento humano moderno significó el más importante cambio en la evolución de la mente humana, dando lugar a que el ingenio creativo humano le permitiese dominar su entorno paulatinamente.

Las innovaciones que fueron apareciendo consisten en una gran diversidad de herramientas de piedra, en el uso de hueso, asta y marfil, en entierros con bienes funerarios y rituales, construcción de viviendas, diseño de las fogatas, evidencia de pesca, cacería compleja, aparición del arte figurativo y el uso de adornos personales.[24]​

Las evidencias más antiguas se encuentran en África; herramientas elaboradas hace 165 000 años se encontraron en la cueva de Pinnacle Point (Sudáfrica).[25]​ Restos de puntas de flechas y herramientas de hueso para pescar se encontraron en el Congo y tienen 90 000 años. Igualmente antiguos son unos símbolos sombreados con ocre rojo en costas al sur de África.[26]​

Según la teoría fuera de África, hubo una gran migración de África hacia Eurasia hace 70 000 años que produjo la paulatina dispersión por todos los continentes. Según los estudios genéticos y los descubrimientos paleontológicos, se estima que hace 60 000 años hubo una migración costera por el Sur de Asia, de pocos miles de años, que posibilitó la colonización posterior de Australia, Extremo Oriente y Europa.

En Occidente hubo un centro de expansión en el Medio Oriente que está relacionado con el hombre de Cromañón y la población temprana de Europa, probable causa de la extinción del hombre de Neandertal.

Según algunos estudios genéticos, en Europa hubo tres migraciones: la primera, proveniente del Asia Central hace 40 000 años que colonizó la Europa del Este. Una segunda oleada hace 22 000 años, proveniente del Oriente Medio, que se instaló en la Europa del sur y del oeste. El 80 % de los europeos actuales son descendientes de estas dos migraciones, que durante el transcurso del máximo glaciar de hace 20 000 años se refugiaron en la península ibérica y en los Balcanes, para volver a expandirse por el resto de Europa cuando llegó el clima favorable. La tercera migración se habría producido hace 9000 años, proveniente del Oriente Medio, durante el transcurso del Neolítico, y solo el 20 % de los europeos actuales llevan marcadores genéticos correspondientes a esos emigrantes.[29]​

Otros estudios dicen lo contrario, afirmando que en Europa el componente neolítico desde el Cercano Oriente es el más importante.[30]​ Lo cierto por ahora es que el acervo genético europeo prehistórico proviene mayoritariamente del Cercano Oriente, y una menor parte proviene de África, Asia Central y Siberia.

En Oriente la población es igualmente antigua. El pliegue epicántico de los párpados existente en gran parte de las poblaciones del Asia y de América, el pliegue que hace 'bridados' en su aspecto externo a los ojos, ha sido una especialización de poblaciones que durante las glaciaciones debieron pervivir en lugares con abundancia de nieve; los ojos vulgarmente llamados «rasgados» entonces fueron el modo de adaptación para que los ojos no padecieran un excesivo reflejo de la luz solar reflejada por la nieve.[cita requerida]

Sin embargo, una publicación de julio de 2019 en la revista Nature puso en tela de juicio las teorías e ideas previas acerca del momento del poblamiento de Europa por el Homo sapiens desde África. El hallazgo y datación de un cráneo de Homo sapiens de 210 000 años de antigüedad en Grecia significaría un poblamiento de Europa 60 000 años más temprano que lo que se suponía.[31]​[32]​

El lenguaje designa todas las comunicaciones basadas en la interpretación, incluyendo el lenguaje humano, pero la mayoría de las veces el término se refiere a lo que los humanos utilizan para comunicarse, es decir, a las lenguas naturales. El lenguaje es universal y es usado por naturaleza en las personas y en los animales. Sin embargo, filósofos como Martin Heidegger consideran que el lenguaje propiamente tal es solo privativo del hombre. Es famosa la tesis de Heidegger según la cual el lenguaje es la casa del ser (Haus des Seins) y la morada de la esencia humana. Este criterio es similar al de Ernst Cassirer, quien ha definido al Homo sapiens como el animal simbólico por excelencia; tan es así que es casi imposible suponer un pensamiento humano sin la ayuda de los símbolos, particularmente de los significantes que subyacen como fundamentos elementales para todo pensar complejo y que transcienda a lo instintivo.

Actualmente la especie humana muestra esta faceta hablando en torno a 6000 idiomas diferentes, si bien más del 50 % de los 7000 millones de personas que actualmente conforman la colectividad humana, sabe hablar al menos una de las siguientes lenguas: chino mandarín, español, inglés, francés, árabe, hindi, portugués, alemán, bengalí o ruso.

En muchas civilizaciones los seres humanos se han visto a sí mismos como diferentes de los demás animales, y en ciertos ámbitos culturales (como las religiones del Libro o buena parte de la metafísica del Occidente) la diferencia se asigna a una entidad inmaterial llamada alma, en la que residirían la mente y la personalidad, y que algunos creen que puede existir con independencia del cuerpo.

Posiblemente, la manifestación más clara de humanidad es el arte —en el sentido amplio del término—, que produce la cultura. Por ejemplo, los individuos de una determinada especie de ave fabrican un nido, o emiten un canto, cuyas características son específicas, comunes a todos los individuos de esa especie. En cambio, cada hombre puede imprimir a sus acciones los rasgos propios de su individualidad; por eso, cuando se analiza un cuadro, una forma de escribir, una manera de fabricar herramientas, etc., se puede deducir quién es su autor, su artífice, su artista.[cita requerida]

En 2011, en la revista Science, se publicó un trabajo de Francesco d'Errico, de la Universidad de Burdeos, donde afirma haber encontrado uno de los rastros más antiguos de un taller de pintura, en la cueva Blombos en Cape Coast, 300 km al este de Ciudad del Cabo. Este hecho muestra un modo sistemático para obtener pigmentos, pues reunir todos los elementos necesarios para una preparación de este tipo es indicativo de un elevado nivel de pensamiento, que se puede llamar pensamiento simbólico. "La capacidad de tener estos pensamientos es considerada un gran paso en la evolución humana, precisamente lo que nos diferenció del mundo animal".[33]​

Paralelamente, también es la única especie que dedica su tiempo y energía a algo aparentemente inútil desde el punto de vista puramente práctico. El arte es una de las manifestaciones de la creatividad humana, pero una manifestación vacía y negativa desde el punto de vista de la supervivencia. Si bien esta actividad es en principio dañina, en realidad es la herramienta con la cual el Homo sapiens desarrolla su cultura, unión y fuerza como pueblo.[aclaración requerida][cita requerida]

La ciencia (del latín scientĭa, 'conocimiento') es un sistema que organiza y construye el conocimiento a través de preguntas comprobables y un método estructurado que estudia e interpreta los fenómenos naturales, sociales y artificiales.[34]​ El conocimiento científico se obtiene mediante observación y experimentación en ámbitos específicos. Dicho conocimiento es organizado y clasificado sobre la base de principios explicativos, ya sean de forma teórica o práctica. A partir de estos se generan preguntas y razonamientos, se formulan hipótesis, se deducen principios y leyes científicas, y se construyen modelos científicos, teorías científicas y sistemas de conocimientos por medio de un método científico.[35]​

La ciencia considera y tiene como fundamento la observación experimental. Este tipo de observación se organiza por medio de métodos, modelos y teorías con el fin de generar nuevo conocimiento. Para ello se establecen previamente unos criterios de verdad y un método de investigación. La aplicación de esos métodos y conocimientos conduce a la generación de nuevos conocimientos en forma de predicciones concretas, cuantitativas y comprobables referidas a observaciones pasadas, presentes y futuras. Con frecuencia esas predicciones se pueden formular mediante razonamientos y estructurar como reglas o leyes generales, que dan cuenta del comportamiento de un sistema y predicen cómo actuará dicho sistema en determinadas circunstancias.

Una sociedad humana es aquella que se considera a sí misma, a los habitantes y a su entorno, todo ello interrelacionado con un proyecto común, que les da una identidad de pertenencia. Asimismo, el término connota un grupo con lazos económicos, ideológicos y políticos. Tal sociedad supera al concepto de nación-estado, planteando a la sociedad occidental como una sociedad de naciones, etc.

En relación con la capacidad para realizar grandes modificaciones ambientales, cabe decir que Homo sapiens es actualmente un poderoso agente geomorfológico; es en este y otros sentidos que el ser humano es actualmente el mayor superpredador y la especie más poderosa del planeta. Sin embargo, sigue siendo frágil ante posibles eventos cataclísmicos que pudieran afectar a su hábitat, como las glaciaciones.

Homo sapiens, por ser un animal muy vulnerable en el medio natural, es muy dependiente de la tecnología (ergo: es dependiente de la ciencia por primitiva que esta sea), así es que se dice de Homo sapiens que es homo faber.

Quizás, dado que todo sistema retroalimentado de forma natural llega a su fin, el fin de un ecosistema llega cuando la vida ha logrado evolucionar hasta lograr seres con un grado de conciencia capaz de programarse en función de la educación recibida y no según lo termodinámicamente sostenible.[cita requerida] La educación es, por tanto, la demostración evidente de si somos parte de un sistema aun mayor o intentamos independizarnos de todo, estableciendo nuestras formas de obtener nuestros recursos, sin tener en cuenta los ya establecidos por la propia naturaleza.

Por ejemplo, la naturaleza le dota de capacidades físicas para buscar alimentos en el medio que les rodea de una manera termodinámicamente eficaz. Los humanos establecen que lo mejor es racionalizar los medios que la naturaleza les da y replicarlos de forma industrial, aplicando procesos que no se dan de forma natural, aumentando el consumo energético por redundar algo que ya existe y ampliándolo a algo totalmente termodinámicamente innecesario, como es el hecho de que se le entregue alimento en casa, de intervenir los códigos genéticos de las especies alimentarias para hacerlas resistentes a enfermedades, de influir en qué alimentos contendrán semillas y cuáles no y un largo etcétera, que a día de hoy nos hace la vida más cómoda, pero que ignoran cómo les afectan esos cambios en su estructura genética y, por lo tanto, si su descendencia portará características fundamentales para sobrevivir a un medio natural o, por el contrario, nacerán y dependerán tan íntimamente del medio artificial que cualquier modificación a ese medio le incapacite de tal manera que provoque su extinción.[cita requerida]

Un aeropuerto es un aeródromo con terminal para el viaje de transporte aéreo en aeronave. Las funciones del aeropuerto son variadas, entre ellas el aterrizaje y despegue de aeronaves, embarque y desembarque de pasajeros, equipaje y mercancía, reabastecimiento de combustible y mantenimiento de aeronaves, así como lugar de estacionamiento para aquellas que no están en servicio. Los aeropuertos sirven para aviación militar, comercial o general.[1]​

Los aeropuertos se dividen en dos partes:

Un aeródromo es un área definida de tierra(que incluye todas sus edificaciones, instalaciones y equipos) destinado total o parcialmente a la llegada, partida o movimiento de aeronaves. Son aeropuertos aquellos aeródromos públicos que cuentan con servicios o intensidad de movimiento aéreo que justifiquen tal denominación. Aquellos aeródromos con vuelos provenientes del o con destino al extranjero, donde se presten servicios de sanidad, aduana, migraciones y otros, se denominan aeródromos o aeropuertos internacionales.[2]​

Los helipuertos se definen como aeródromos destinados a ser utilizados solamente por helicópteros, por tanto, le son aplicables las disposiciones relativas a la clasificación y a los procedimientos de autorización vigente para aeródromos.[3]​

Los hidroaviones aterrizan en superficies con agua, tales como el mar o lagos, de forma que necesitan tener zonas libres de obstáculos y aguas tranquilas. También existen helicópteros anfibios y RPAS que amerizan.

Un portaaviones, ejemplo de base aérea.

Helipuerto en las Cataratas del Niagara, Ontario, Canadá.

Terminal de pasajeros en el Aeropuerto internacional de Incheon, Incheon, Corea del Sur.

Según el tipo de actividad se distinguen los siguientes aeropuertos:

Según el tipo de vuelos se distinguen dos tipos:

Son aeropuertos de interés general:[4]​

Un aeropuerto nacional (también llamado aeropuerto de cabotaje o interno) es un aeropuerto que sirve solo vuelos nacionales, interiores a un mismo país, también llamados vuelos de cabotaje. Los aeropuertos nacionales carecen de oficinas de aduanas y de control de pasaportes y, por lo tanto, no pueden servir vuelos procedentes o con destino a un aeropuerto extranjero.

Estos aeropuertos tienen generalmente pistas cortas en las que solo pueden maniobrar pequeños aviones y donde operan vuelos de aviación general (es decir, no comercial, taxis aéreos, vuelos sanitarios, chárter, etc.). En muchos países carecen de controles de seguridad y escáneres de metal, pero poco a poco se han ido incorporando.

La mayoría de los aeropuertos municipales de Canadá y Estados Unidos son aeropuertos nacionales. En los aeropuertos internacionales canadienses existen terminales destinadas únicamente a los vuelos interiores. Por el contrario, algunos países pequeños carecen de aeropuertos nacionales públicos o incluso carecen de vuelos interiores, como, por ejemplo, Bélgica. También dentro de la categoría de aeropuertos nacionales se encuadran los aeródromos que sí poseen todos los sistemas de aproximación y balizamiento pero con operación de líneas aéreas locales.

Los aeropuertos nacionales o internos son a veces mal llamados aeropuertos domésticos, debido a una mala traducción del inglés domestic. Este es un extranjerismo incorrecto que se debe evitar.[5]​

En un aeropuerto, desde el punto de vista de las operaciones aeroportuarias, se pueden distinguir dos partes: el denominado lado aire y el llamado lado tierra. La distinción entre ambas partes se deriva de las distintas funciones que se realizan en cada una.

En el lado aire, la atención se centra en las aeronaves y todo se mueve alrededor de lo que estas necesitan. El principal componente de esta parte es la pista de aterrizaje, pero dependiendo del tipo de aeropuerto, puede que tenga calles de rodaje, plataformas de estacionamiento y hangares de mantenimiento. La plataforma (también conocida como apron del inglés) es el área destinada a dar cabida a las aeronaves mientras se llevan a cabo las operaciones de embarque y desembarque de pasajeros o mercancías, así como otras operaciones de atención a la aeronave (abastecimiento de combustible, mantenimientos menores, limpieza).

En el lado tierra, los servicios se concentran en el manejo de los pasajeros y sus necesidades. Su principal componente es la terminal (para un aeropuerto comercial de pasajeros) o las bodegas y terminal de carga (para un aeropuerto de carga). Usualmente todos los aeropuertos tienen ambos componentes. Es posible que un juego de pistas de aterrizaje sea también utilizado por aviones militares.

El volumen de pasajeros y el tipo de tráfico (regional, nacional o internacional) determinan las características que debe tener la infraestructura.

Un área importante en todo aeropuerto es la denominada torre de control, en la cual se encuentran los llamados controladores del tráfico aéreo o ATCOs (por sus siglas en inglés), encargados de dirigir y controlar todo el movimiento de aeronaves en el aeropuerto y en la zona aérea bajo su jurisdicción.

La pista es la parte más importante de un aeródromo pues permite a las aeronaves que están en tierra llegar a las velocidades necesarias para lograr la sustentación en el aire, y permite a aeronaves en vuelo, tomar tierra. La pista forma parte del lado aire de un aeródromo. Salvo contadas excepciones, toda pista permite operaciones de aterrizaje y de despegue de aeronaves. 

Las pistas necesitan ser lo suficientemente largas y anchas para que permitan operaciones de aterrizaje y despegue de aquellos aviones de mayor tamaño que operen el aeropuerto. Es decir, la pista será el limitante para los diferentes tipos de aeronaves que puedan aterrizar en ella.

En aeropuertos de alto tráfico existen las pista de carreteo, que son pistas auxiliares que agilizan el tráfico de aeronaves en tierra firme y aumentan el número máximo de operaciones que se pueden llevar a cabo.


Las cabeceras de las pistas de aterrizaje de los aeropuertos necesitan estar libres de cualquier obstáculo que pueda entorpecer o poner en riesgo la operación de aterrizaje/despegue de la aeronave. La línea de aproximación de aeronaves, por esta razón, necesita estar libre de torres y edificios.
Las pistas de aterrizaje y despegue deben orientarse de acuerdo al patrón de vientos de la región: para la seguridad de una operación de aterrizaje o despegue, la componente lateral del viento no debe superar una velocidad admisible para las aeronaves más pequeñas en el 95 % del tiempo; cuando suceden, crean turbulencias en la aeronave, aumentando las probabilidades de un accidente. En lugares donde la serie de vientos es tal que con una sola pista no se cumple tal reglamentación, debe construirse una segunda pista con su debida orientación.

Los aeropuertos de uso civil están diseñados para la atención de pasajeros que utilizan el avión como medio de transporte, para carga y correo aéreo. La mayoría de los aeropuertos operan los tres, pero muchos atienden principalmente o pasajeros o carga/correo, dadas ciertas circunstancias: 

El tamaño de un aeropuerto y la variedad de servicios que ofrece depende principalmente de la cantidad de vuelos que atiende el aeropuerto y el movimiento de tráfico aéreo, que incluye el movimiento de pasajeros, carga y correo aéreo. Naturalmente, los aeropuertos que mueven una gran cantidad de pasajeros, con un alto movimiento de aeronaves, tienden a ocupar una mayor superficie.

Son los edificios del lado tierra del aeródromo que permiten el manejo y control de pasajeros que embarcan o desembarcan aeronaves. Para los aeropuertos de pasajeros, las terminales tienen como función la conexión entre los modos de acceso, con el modo de transporte aéreo: Taxi, automóvil, autobuses, tren o metro.

Los centros aeroportuarios de gran o mediana categoría están bien equipados para la atención de aeronaves importantes, así como para el tráfico de pasajeros por el aeropuerto. En tales aeropuertos, hay áreas destinadas a la facturación, terminales separadas para el embarque (donde el pasajero espera su vuelo) y desembarque, servicios comerciales.

La configuración de la terminal está determinada por el tipo de tráfico (regional, nacional o internacional) y por la cantidad de viajeros. Los grandes aeropuertos tienen más de una terminal. Puede suceder que las ampliaciones hayan llevado a construir varios edificios para suplir la demanda.

Las terminales tienen las siguientes dependencias: vestíbulos de control, salas de embarque, bandas de equipajes, puertas de salida, zonas de esparcimiento, restaurantes, tiendas, bancos, cajas de cambio y aparcamiento de automóviles. Los aeropuertos internacionales tienen además controles migratorios (control de pasaportes y aduana. En la aduana, los pasajeros que salen o entran del país informan sobre el ingreso o salida de dinero y mercancías. 

Por las recientes amenazas terroristas, los controles de acceso a las aeronaves es muy estricto. Además de máquinas detectoras de metales y escáneres corporales, muchos aeropuertos poseen máquinas de rayos X para la detección de materiales peligrosos en el equipaje de los pasajeros.

Además, algunos aeropuertos de alto tráfico también ofrecen otros servicios comerciales que permiten incrementar los ingresos del operador del aeropuerto. Ofrecen al pasajero gran variedad de opciones, mientras espera; por ejemplo, almacenes, salas vip, centros de internet, zonas de juegos, lugares de culto religioso, museos, restaurantes, etc.

La forma de la terminal de pasajeros de un aeropuerto trata de maximizar el número de posiciones para el embarque de aeronaves, tratando de reducir las distancias de caminata de los pasajeros. Por esa razón, desde la parte central de los edificios, se desprenden corredores que permiten la conexión con varios aviones. Estos corredores se conocen como "espigones". Muy frecuentemente los pasajeros abordan aeronaves, no desde las posiciones en la terminal, sino en la plataforma.

Cuando las terminales de pasajeros están alejadas unas de otras o distantes de la terminal principal, entran en juego las líneas de autobuses y trenes especiales que conectan una terminal con otra, de modo que faciliten el movimiento de pasajeros y operarios entre todas las terminales.

Los aviones no son los únicos medios de transporte presentes en un área aeroportuaria: una amplia variedad de vehículos diferentes actúan dentro del aeropuerto, con una variada gama de servicios, como el transporte de pasajeros, transporte de carga, equipaje, limpieza de las aeronaves. Entre tales vehículos están: 

Los vehículos aeroportuarios se desplazan por el aeropuerto a través de avenidas destinadas a ellos. Existen otras pistas, dedicadas a la orientación de las aeronaves, en la plataforma de estacionamiento y en las taxiways (calles de rodaje). Además, cuentan con vehículos de emergencia que deben estar listos en todo momento para atender un percance o emergencia: camiones de bomberos, pipas de agua, ambulancias y vehículos de policía.

El servicio de mantenimiento de los aviones que operan en un aeropuerto es generalmente suministrado por la mayor aerolínea operativa en el aeropuerto o por compañías especializadas, en el caso de los aviones de pasajeros. Cabe resaltar que aunque muchos aeropuertos poseen servicios básicos de mantenimiento, solo parte de ellos ofrecen servicios más especializados y complejos. 

Durante el período en que la aeronave está estacionada en tierra se le realiza un chequeo a cargo de una empresa de manejo en tierra de aeronaves.

Los aeropuertos poseen generalmente un área designada especialmente al proceso de carga, con hangares destinados al almacenamiento de la carga a ser transportada y equipamientos necesarios para su manejo, así como personal especializado.

Los aeropuertos son administrados por el Estado, el municipio o por un privado a quién se le ha dado esta tarea en concesión. 

El concesionario o administrador del aeropuerto puede tener una concesión mixta; es decir, puede mantener solamente las terminales o solamente las pistas, en la mayoría de los casos se concesionan ambas áreas, para ello deberá contar con empresas tercerizadas o personal propio que se dedique a los rubros de limpieza, mantenimiento de las infraestructuras (ascensores, escaleras mecánicas, refrigeración, calefacción, energía primaria y secundaria, mobiliarios, sanitarios, etc.) corte de áreas verdes, descontaminación de áreas de movimientos (las pistas se contaminan con el desprendimiento del caucho de los neumáticos de las aeronaves al hacer contacto y frenar sobre los pavimentos), mantenimiento de Ayudas Visuales Luminosas (balizamiento de pistas y rodajes) y demás servicios operativos de ambos lados (aire y tierra). 

Cuando la demanda de pasajeros y carga lleva a que la infraestructura esté cerca de su capacidad total, pueden ser necesarios algunos cambios, como la expansión de las terminales de pasajeros o carga, nuevas pistas de carreteo, pistas de aterrizaje y despegue y aparcamientos. Cuando esto no es posible, se considera la construcción de un nuevo aeropuerto en la región. 

Los ingresos de un aeropuerto se clasifican en operacionales y no operacionales. Los operacionales de dan por las tasas cobradas por el aterrizaje de una aeronave y las tasas a pasajeros. Estas tasas son generalmente reguladas por el Estado o su autoridad de aviación civil. Los precios varían según el aeropuerto. Los ingresos no operacionales del aeropuerto son aquellos asociados a la renta generada por el aparcamiento de automóviles y motos y el alquiler de locales comerciales.

En lo concerniente a la seguridad aérea es conveniente distinguir entre dos conceptos que, en inglés, se denominan de forma diferente. Uno es la seguridad desde el punto de vista policial o de orden público (en inglés security) que afecta a las instalaciones relacionadas con el tráfico de mercancías y pasajeros; y el otro concepto es el de seguridad en el transporte y la navegación (safety) que afecta, principalmente, a la organización del trabajo de las personas relacionadas con la navegación aérea y al mantenimiento de las aeronaves y los aeropuertos.

La seguridad en los grandes aeropuertos de pasajeros es un asunto muy serio, y los controles en ellos se han incrementado notablemente tras los atentados del 11 de septiembre de 2001.

Las terminales de pasajeros muy concurridas hacen uso de máquinas de rayos X para la verificación de materiales peligrosos, detectores de metales para la detección de armas y animales entrenados en detectar explosivos en un pasajero, equipaje o carga. Los guardas jurados del aeropuerto también pueden realizar una inspección manual a los pasajeros o a su equipaje. Además de objetos considerados armas (armas de fuego, cuchillos, tijeras, etc), también están prohibidos los objetos que pongan en riesgo la integridad del vuelo, como mecheros, cortauñas, materiales inflamables o explosivos, etc.[7]​ También se realizan registros para evitar el tráfico de drogas. Problemas como la falta de presupuesto pueden hacer con que tales medidas de seguridad no se realicen como deberían, aumentando el riesgo de atentados o secuestros.

Otras cuestiones concernientes a la seguridad en los aeropuertos incluyen el área de aproximación de aterrizaje de aeronaves, no siempre libre de obstáculos (como, por ejemplo, el antiguo aeropuerto de Hong Kong, con montañas de gran altitud durante la aproximación), o la relación entre el número de operaciones de aterrizajes y despegues en un aeropuerto dado y el tamaño de su pista. Un factor muy importante en la seguridad operacional es el llamado control del peligro aviario y fauna; se denomina así al control que se realiza en las pistas y áreas de maniobras antes que aterrice o despegue una aeronave evitando que las turbinas u otra parte del avión succione o sea impactada por aves o fauna poniendo en peligro la fase del vuelo. Sobre esta materia existen métodos y asociaciones internacionales ya que los incidentes y accidentes causados por aves y todo tipo de fauna han costado a las industrias pérdidas en vidas humanas e importantes daños materiales.

En cuanto a la seguridad aérea en navegación (safety) es muy importante recalcar que es de suma importancia que los pasajeros conozcan cada una de las medidas tomadas por el personal para poder lograr un vuelo seguro. Una de las principales es la operación de las puertas utilizadas como salidas de emergencia, pues todo aquel pasajero que atiende correctamente a las medidas de seguridad que las sobrecargos a bordo dan, puede lograr una evacuación exitosa; la correcta utilización de las mascarillas en caso de una despresurización(pérdida de presión en cabina para atmósfera similar a la del suelo) pues si la colocación y la activación del sistema de oxígeno de la misma no es la adecuada, se corre el peligro de sufrir hipoxia. La constante utilización del cinturón de seguridad puede en mucha medida, prevenir algún accidente durante el vuelo como golpes en la cabeza, esguince cervical etc. (en caso de turbulencia) y la salida del mismo cuerpo a través del fuselaje dañado (en una despresurización). Es de suma importancia que quede claro que cuando los pasajeros de un vuelo, acatan al 100 % las instrucciones de los procedimientos llevados a cabo por la tripulación, se puede obtener un vuelo completamente seguro.

La edición radioayuda o radionavegación puede definirse como el conjunto de señales radioeléctricas, generalmente generadas en instalaciones terrestres y recibidas a bordo, que permiten a la flight traffic control guiarse.

Si bien el control de tráfico aéreo (ATC) o la asistencia de las aeronaves en tierra es importante e imprescindible, lo son en igual medida los sistemas de navegación que se encuentran en los aeropuertos. Estos sistemas electrónicos comúnmente llamados radioayudas, son sistemas electrónicos cuyo funcionamiento consiste en una emisión constante de ondas de radio, estas ondas son captadas por el avión que haya sintonizado la frecuencia de esa radioayuda, seguidamente los sistemas del avión traducen esas ondas en datos que son visualizados por la tripulación de cabina. 

Existen varios tipos de radio ayudas entre las que se encuentran el VOR, el ADF, el TACAN, el ILS, etc. 

Las torres de control organizan el movimiento de aeronaves en tierra y en el espacio aéreo cuando estas se aproximan al aeródromo, y autorizan operaciones de aterrizaje y despegue. Estas torres de control se sitúan en un lugar que permita una amplia visión del aeródromo, así como una amplia visión de aeronaves en aproximación. Varios aeródromos de pequeña dimensión y áreas de aterrizaje, así como algunos aeropuertos de mediana importancia, no poseen torre de control. En estos aeródromos solo se facilita servicio de información de vuelo y no de control.

Dado el intenso movimiento de aeronaves y de tráfico generado por el movimiento de personas de y al aeropuerto, los aeropuertos son fuente de dos tipos de contaminación: 

Tales problemas pueden causar alteraciones en la salud de los habitantes de los alrededores, como problemas de sueño o respiratorios. La construcción de un nuevo aeropuerto no es, generalmente, bien recibida por los habitantes que viven cerca del área escogida.[8]​

Los aeropuertos son las instalaciones en las que una aeronave puede realizar el despegue y el aterrizaje con mayor seguridad del mundo. Sin embargo, existen algunos casos en los que las pistas son más cortas de lo normal o con ubicaciones geográficas complicadas haciendo que estas maniobras puedan resultar muy complicadas de realizar. A continuación, se recoge una lista donde se ven reflejados los aeropuertos más peligrosos del mundo.[9]​

Un aeropuerto (aeródromo) militar permite las operaciones de aeronaves militares. Es operado por la fuerza aérea. Los aeródromos acuáticos (portaaviones) también permiten la aviación militar.

Los criterios de localización de los aeropuertos militares en el territorio son diferentes a los de los aeropuertos de aviación comercial o aviación general, ya que se localizan en lugares estratégicos para la defensa o el ataque, o que permitan el patrullaje de fronteras. 

El Holocausto[1]​ —también conocido en hebreo como השואה, Shoá, traducido como «La Catástrofe»—, conocido en la terminología nazi como «solución final» —en alemán, Endlösung— de la «cuestión judía»,[2]​ es el genocidio que tuvo lugar en Europa durante el transcurso de la Segunda Guerra Mundial bajo el régimen de la Alemania nazi.[3]​ Los asesinatos tuvieron lugar a lo largo de todos los territorios ocupados por Alemania en Europa.[4]​ 


La decisión nazi de llevar a la práctica el genocidio fue tomada entre finales del verano y principios del otoño de 1941[5]​ y el programa genocida alcanzó su punto culminante en la primavera de 1942 —desde finales de 1942, las víctimas eran transportadas regularmente en trenes de carga, especialmente conducidos a campos de exterminio donde, si sobrevivían al viaje, la mayoría eran asesinados sistemáticamente en las cámaras de gas—.[6]​ A cargo de su planificación, organización administrativa y supervisión estuvo Heinrich Himmler.[7]​ Por lo demás, fue la repetida retórica antisemita de Adolf Hitler la que incentivó la ejecución de las matanzas, que además contaron directamente con su aprobación.[8]​ De esta forma, entre 1941 y 1945, la población judía de Europa fue perseguida y asesinada sistemáticamente, en el mayor genocidio del siglo XX. Sin embargo, este exterminio no se limitó sólo a los judíos, sino que los actos de opresión y asesinato se extendieron a otros grupos étnicos y políticos.[9]​ Cada brazo del aparato del Estado alemán participó en la logística del genocidio, convirtiendo al Tercer Reich en un «Estado genocida».[10]​ Las víctimas no judías de los nazis incluyeron a millones de polacos, comunistas y otros sectores de la izquierda política, homosexuales, gitanos, discapacitados físicos y mentales y prisioneros de guerra soviéticos. 

Dada la dificultad para establecer cifras certeras, se ha tomado la cifra simbólica de seis millones de muertos en torno a la comunidad judía.[11]​[12]​ Se estima que, en total, un mínimo de once millones de personas murieron, de ellas, un millón habrían sido niños. Asimismo, de los judíos residentes en Europa antes del Holocausto, aproximadamente dos tercios fueron asesinados.[13]​ La maquinaria del Holocausto tenía una red de aproximadamente 42 500 instalaciones por toda Europa para confinar y matar a sus víctimas y contó con la participación directa de entre 100 000 y 500 000 personas para su planificación y ejecución.[14]​ Entre los métodos utilizados estuvieron la asfixia por gas venenoso (Zyklon B), los disparos, el ahorcamiento, los trabajos forzados, el hambre, los experimentos pseudocientíficos, la tortura médica y los golpes.[15]​ 

Por otro lado, a lo largo del Holocausto se produjeron episodios de resistencia armada contra los nazis. El ejemplo más notable fue el Levantamiento del Gueto de Varsovia de 1943, cuando miles de combatientes judíos mal armados se enfrentaron durante cuatro semanas a las SS. Se estima que entre 20 000 y 30 000 judíos participaron en Europa del Este en los movimientos partisanos creados durante la Segunda Guerra Mundial en los países ocupados por Alemania, que contaron con millones de guerrilleros.[16]​ Los judíos franceses también tuvieron gran actividad en la Resistencia francesa. En total, se produjeron alrededor de un centenar de levantamientos judíos armados.

La Unión Europea sancionó una ley que entró en vigor a finales de 2007 penando el negacionismo del Holocausto y de todos los demás crímenes nazis;[17]​ además, creó en 2010 la base de datos Infraestructura europea para la investigación del Holocausto (EHRI), destinada a reunir y unificar toda la documentación y archivos que conciernen al genocidio.[18]​ Por otro lado la ONU rinde homenaje a las víctimas del Holocausto desde 2005, habiendo fijado el 27 de enero como Día Internacional de la Memoria de las Víctimas del Holocausto, dado que ese día de 1945, el Ejército Rojo de la Unión Soviética liberó el campo de concentración de Auschwitz.[19]​

Los primeros en usar el término «Holocausto» fueron los historiadores judíos de finales de la década de 1950; la generalización de dicho término se produjo a finales de los años sesenta.[20]​

La palabra «holocausto» proviene de la traducción griega del texto masorético conocida como Versión de los setenta, en la que el término olokaustos (ὁλόκαυστος: de ὁλον, ‘completamente’, y καυστος, ‘quemado’) traduce una palabra hebrea que se refiere a un sacrificio consumido por el fuego.[21]​

También se utiliza para nombrarlo el término Shoá (Shoah o Sho'ah),[22]​ término proveniente del hebreo שואה y cuyo significado es «catástrofe».[23]​ La palabra forma parte de la expresión Yom ha-Sho'ah, con la que se nombra en Israel al día oficial de la Memoria del Holocausto.

En yidis para referirse al Holocausto se emplea la expresión hurb'n eiropa,[24]​ y ella posee el significado de «Destrucción [de las comunidades judías] de Europa», incluyendo esto también la cultura de las mismas.[25]​

En cuanto a la historia del uso del término «holocausto», desde el siglo XVI se empleó la expresión holocaust en el idioma inglés para catástrofes extraordinarias de incendios con gran cifra de víctimas. En el siglo XVIII la palabra adquiere un significado más general de muerte violenta de gran número de personas.[26]​

Antes del genocidio judío perpetrado por los nazis, Winston Churchill usó la expresión holocaust en su publicación El mundo en crisis en referencia al genocidio armenio en Turquía.[27]​

En relación al uso de la palabra holocausto para referirse al genocidio de aproximadamente seis millones de judíos europeos durante la Segunda Guerra Mundial,[28]​ en la entrada «Holocaust» de la Encyclopaedia Britannica (2007), la definición es la siguiente:


La persecución y el asesinato de los judíos no se desarrollaron exclusivamente en Alemania o en los distintos campos de exterminio, sino que también tuvieron lugar en Rusia, Europa Oriental y la península balcánica, donde los alemanes y sus colaboradores (austriacos, lituanos, letones, ucranianos, húngaros, rumanos, croatas y otros) llevaron a cabo múltiples matanzas de judíos en fosas, bosques, barrancos y trincheras.[30]​

En la posguerra y en la década de 1950 no hubo una toma de conciencia del hecho mismo del Holocausto. Los judíos eran considerados unas víctimas más de la Segunda Guerra Mundial, por lo que el Holocausto «está poco presente en el debate público, y los propios judíos no intentan introducirlo. Los sobrevivientes a menudo querían hablar, pero no se les escuchaba demasiado...», comenta Michel Wieviorka. Cuando se empieza a hablar en Occidente de la destrucción de los judíos de Europa es en la década de 1960 a raíz del proceso a Adolf Eichmann y es entonces cuando empieza difundirse el término Holocausto, aunque este no alcanzará a todas las capas de la población hasta la emisión en 1978 de la serie televisiva norteamericana Holocausto. Por su parte el término Shoah, utilizado en Israel, no se populariza en Occidente hasta la década de 1980, especialmente tras el estreno en 1985 del monumental documental de Claude Lanzmann Shoah. Después películas —como La lista de Schindler— y libros contribuyen a que el Holocausto esté presente en la conciencia colectiva. Según Michel Wieviorka, la toma de conciencia del Holocausto constituye «un gran escudo, aporta una barrera a toda expresión fuerte de antisemitismo».[31]​

La historiografía sobre el nazismo y el Holocausto ha discutido desde siempre el grado de diseño u organización previa con la que se llevó a cabo el genocidio y, asimismo, el grado de implicación de Hitler, tanto en lo que se refiere a si hubo una orden directa y explícita del mismo para que se iniciase, como en si hubo respaldos explícitos por su parte durante su ejecución.


En el estado actual de conocimientos, parece asentada la idea de que el Holocausto no se desarrolló siguiendo las directrices de ningún plan perfectamente definido; de hecho, no se tiene constancia de ningún documento que recogiese un diseño específico para el mismo. Así las cosas, se considera que


En cuanto al grado de responsabilidad directa de Hitler, Adolf Eichmann recordó, años después de terminada la guerra, que Heydrich le había comunicado que tenía una orden de Hitler para exterminar físicamente a los judíos.[34]​ En esta línea, hasta la década de 1970[35]​ se aceptaba que la «solución final» se había puesto en marcha a partir de una orden directa de Hitler. Sin embargo, en 1977 el historiador Martin Broszat dio un giro a esta visión de los hechos notando que Hitler no había dado ninguna «orden exhaustiva de exterminio general», sino que habían sido los «problemas para aplicar la deportación general», tras la invasión de la URSS, los que habían llevado a los dirigentes nazis a iniciar los asesinatos en masa de judíos en las regiones que estuviesen bajo su mandato. Solo retrospectivamente, esos asesinatos habrían sido notados por la dirección nazi y reconvertidos en un programa de exterminio más general y concienzudo.[36]​ En concreto,


Esta línea de interpretación sería respaldada desde 1983 por otro historiador, Hans Mommsen, quien ha insistido en la idea de que la Solución Final surgió a partir de los fragmentados procesos de toma de decisiones del nazismo, los cuales permitirían las iniciativas particulares al respecto y la acumulación de la radicalización de las mismas. Para él, está claro que Hitler conocía y aprobaba todo lo que sucedía, pero la improbabilidad de que pudiese haber una orden formal suya en relación al genocidio se compadece perfectamente con sus intentos explícitos de ocultar su responsabilidad personal y, subconscientemente, de suprimir la realidad circundante.[38]​

Con todo, ha habido historiadores, como Christopher R. Browning, que han mantenido la idea de una decisión concreta de Hitler, que habría tenido lugar durante el verano de 1941 y cuyo reflejo habría sido la orden de Göring a Heydrich por la que le instaba a preparar una solución total a la «cuestión judía» (otros historiadores, como Philippe Burrin, no veían detrás de este mandato la orden de Hitler). La aprobación del plan de exterminio por parte de Hitler habría ocurrido a finales de octubre o noviembre de ese año, una vez paralizada la invasión a la URSS.[39]​

Otras hipótesis al respecto han apuntado a enero de 1941 como fecha para una decisión de Hitler de exterminar a los judíos (Richard Breitman); a agosto de 1941, justo al conocerse la declaración de la Carta del Atlántico firmada por Roosevelt y Churchill (Tobías Jersak); a diciembre de ese mismo año (Christian Gerlach); e, incluso, a junio de 1942, justo después del asesinato de Reinhard Heydrich en Praga (Florent Brayard).

Son seguras, sin embargo, sus declaraciones justificativas del genocidio, especialmente concentradas durante los primeros meses de 1942, y con referencias directas que demuestran su conocimiento del mismo.[40]​

En las dos últimas décadas, y dado que además de que no se ha encontrado ninguna orden de Hitler relacionada con el Holocausto, «parece improbable que Hitler diera una orden única y explícita para ejecutar la Solución Final»,[41]​ la historiografía se ha decantado por la idea de que nunca se tomó una decisión única y específica de matar a los judíos de Europa.[42]​ Con todo, durante su proceso en Jerusalén en 1961, Adolf Eichmann confesó que durante la Conferencia de Wannsee (1942) «se estudiaron con rigor los [más efectivos] métodos para exterminar a todo el pueblo judío que vivía en Europa».[43]​

En relación a Hitler, cuyo papel principal habría sido el de una especie de árbitro entre los líderes nazis que fueron tomando las decisiones que desembocaron en el genocidio, el historiador Ian Kershaw ha hablado de su «autoridad carismática» como fuente del mecanismo psicológico mediante el cual sus subordinados trabajaban con


Así las cosas, su papel al respecto es menos evidente de lo que puede parecer a simple vista. Los historiadores no han llegado a ningún acuerdo claro en relación al grado de intervención directa de Hitler para dirigir la política de exterminio, lo que incluye el debate acerca de si hubo por su parte una orden o, incluso, si hubo necesidad de la misma.[45]​ Las dificultades al respecto radican, al parecer, en el estilo de liderazgo de Hitler, muy poco burocrático y que, desde que comenzó la guerra, fomentó el secretismo y el encubrimiento transmitiendo sus órdenes y deseos solo de forma verbal y en aquellos casos, sobre todo los más sensibles, en que era algo estrictamente necesario.[46]​

En lo que se considera "un punto de inflexión" y "un antes y un después en la vida judía en Europa", el discurso de Hitler en el Congreso alemán en el año 1939 (sobre el futuro de Europa y en particular sobre el destino del judaísmo europeo) parece despejar toda duda sobre quién ordenó el exterminio del pueblo judío: “Si los financieros judíos internacionales de dentro o fuera de Europa vuelven a llevar a las naciones a una guerra mundial…el resultado no será el triunfo del bolchevismo en el mundo y con ellos el triunfo del judaísmo, sino la aniquilación total de la raza judía en Europa”.[47]​

El Tercer Reich se impuso como uno de sus objetivos prioritarios la reestructuración racial de Europa. En ella, desempeñó un papel fundamental el antisemitismo, que se incardinó en


Además de esta ideología, la ejecución del genocidio tuvo como soporte a la sociedad alemana, la más moderna y con más nivel de desarrollo técnico de Europa, y que contaba con una burocracia organizada y eficiente.[49]​

El antisemitismo presente, en mayor o menor medida, en Europa Occidental y Estados Unidos, además de los problemas económicos derivados de la Gran Depresión, provocaron también «la desgana de los responsables políticos británicos y estadounidenses a la hora de realizar algún esfuerzo significativo de salvamento de judíos europeos durante el Holocausto».[50]​

El Partido nazi, que tomó el poder en Alemania en 1933, tenía entre sus bases ideológicas la del antisemitismo, profesado por una parte del movimiento nacionalista alemán desde mediados del siglo XIX. El antisemitismo moderno se diferenciaba del odio clásico hacia los judíos en que no tenía una base religiosa, sino presuntamente racial. Los nacionalistas alemanes, a pesar de que recuperaron bastantes aspectos del discurso judeófobo tradicional, particularmente del de Lutero, consideraban que ser judío era una condición innata, racial, que no desaparecía por mucho que uno intentara asimilarse en la sociedad cristiana. En palabras de Hannah Arendt, se cambió el concepto de judaísmo por el de judeidad.[51]​ Por otro lado, el nacionalismo sólo creía en el Estado nación caracterizado por la homogeneidad cultural y lingüística de su población. Considerados como nación perteneciente a otra raza, extranjera, inferior e inasimilable a la cultura alemana, los judíos solo podían ser segregados y excluidos del cuerpo social. Frente a la raza judía, extraña al pueblo germánico, colocaban los nazis a la raza aria, sosteniendo que solo esta última constituía la nación alemana, la única llamada a dominar Europa.[52]​

La primera cuestión era determinar quién era judío. Los nacionalistas alemanes no habían logrado establecer una línea divisoria clara entre judíos y no judíos; había en Alemania numerosas personas descendientes de judíos conversos que no tenían ya ninguna relación con la cultura judía, así como numerosas familias mixtas y sus descendientes. En este sentido, la primera preocupación de los nazis fue crear un criterio para basar la posterior segregación.

Las primeras leyes dirigidas contra los judíos no incorporaban todavía una definición del ser judío y se hablaba en general de «no arios». La definición finalmente adoptada fue la siguiente: judío era quien tuviera al menos tres abuelos judíos, fuera cual fuera la religión de la persona interesada. Quienes tuvieran dos o un solo abuelo judío, eran Mischlinge, es decir, medio judíos. Los primeros, con dos abuelos judíos, eran «Mischlinge de segundo grado» y podían ser reclasificados como judíos en función de complejas consideraciones (su religión o la de su cónyuge, por ejemplo). Podían también ser «liberados» de su condición y convertirse en arios en pago a los servicios prestados al régimen, o podían seguir siendo Mischlinge, con lo que estaban sometidos a ciertas restricciones en tanto que «no arios», pero no a las persecuciones dirigidas contra los judíos. Los Mischlinge de primer grado eran los que tenían un único abuelo judío y en general eran tratados como arios plenos. Los Mischlinge de uno u otro grado abundaban en Alemania y a menudo lograban ocultar su condición. El dirigente de las SS Reinhard Heydrich, El Carnicero de Praga, era Mischlinge de segundo grado, dato que fue ocultado celosamente por sus superiores nazis.[cita requerida]

Para el psicólogo social Harald Welzer, estudioso del comportamiento de las sociedades ante las catástrofes sociales, la irracionalidad de los motivos no influye en la racionalidad de la acción, cosa que se verificó en el Holocausto y también corrobora un enunciado de William Thomas: «Si las personas definen las situaciones como reales, éstas son reales en sus consecuencias».[53]​

Tras la Primera Guerra Mundial, el Imperio alemán (Deutsches Reich) se dotó de una Constitución que lo definía como una República, de ahí el nombre de República de Weimar con el que habitualmente se conoce a Alemania en el periodo que va de 1919 a 1933.[54]​

Desde un punto de vista sociológico, la República de Weimar se estableció


Hubo también, a partir de la guerra, un generalizado incremento de la violencia en Alemania, hasta el punto de que desde 1918 esta fue una de sus principales características: la violencia de la guerra total fue vista como un presagio de una nueva sociedad, dura y moderna, donde la virilidad y la crueldad serían factores esenciales. Muchos de los miembros de las unidades de Frikorps que habían continuado la lucha tras la Gran Guerra en Polonia y el Báltico, regresaron a Alemania y se integraron en grupos paramilitares como el en formación movimiento nazi, y fueron responsables entre 1919 y 1922 de más de 300 asesinatos políticos. La reacción de la judicatura, sobre todo en los casos en que las víctimas eran claramente izquierdistas, fue benevolente. Este estado de cosas, facilitó que el ciudadano medio viese con indulgencia la escalada de violencia que acompañó al nazismo en su llegada al poder entre 1930 y 1932. Así, cuando se produjeron el ataque nazi de 1933 contra la izquierda y las purgas en su propio movimiento al año siguiente, Hitler, que había admitido su responsabilidad, consiguió la aprobación generalizada y un aumento de popularidad.[56]​

A lo anterior hay que añadir un considerable caos económico y político, todo lo cual repercutió en que la derecha nacionalista empezase a perfilarse como enemiga de un régimen al que hacía responsable de la situación, incidiendo especialmente en determinadas consecuencias del tratado, como el reconocimiento por parte de Alemania de su culpabilidad de guerra, la pérdida de territorios, la reducción del ejército y la dependencia de préstamos extranjeros. Una inflación masiva en 1923 y el consecuente colapso monetario, que afectaron duramente a las clases trabajadora y media, redondearon un contexto ideal para el surgimiento de una oposición radical al régimen.

Simultáneamente, ya desde 1918, la económicamente fuerte población judía alemana (poco más de medio millón de personas) fue objeto de atención por una


En el contexto del interés global europeo por diversas teorías de raza seudocientíficas, desarrolladas mucho antes de la Primera Guerra Mundial y con el objeto de justificar la exclusión y represión de determinados sectores de la sociedad,[58]​ en 1923 se creó la primera cátedra de higiene racial en la Universidad de Múnich y en 1927, en Berlín, el Instituto Emperador Guillermo de Antropología, Herencia Humana y Eugenesia.[59]​

En general, los sentimientos antijudíos se recrudecieron con las crisis económicas y políticas que se desarrollaron entre 1918 y 1923. Por un lado, se empezó a asociar a los judíos con actividades subversivas por el papel desempeñado por diversos socialistas y comunistas judíos (Rosa Luxemburg, Kurt Eisner, Gustav Landauer, Eugen Leviné, Hugo Haase, etc.) en las frustradas revoluciones de 1918-1919. La mayoría de ellos terminarían siendo asesinados por miembros de la derecha nacionalista, incluido Walter Rathenau, el primer judío que había llegado al cargo de ministro de Asuntos Exteriores de Alemania.

Por otro lado, desde 1920 se experimentó una inmigración masiva de judíos polacos en Berlín. Sin trabajo y con dificultades para adaptarse por el idioma, se convirtieron en objetivo para las quejas xenófobas de muchos.

Así, el nuevo nacionalismo adoptó la violencia como un modo de alcanzar la salvación nacional. Desde principios de la década de 1920, una nueva generación de estudiantes universitarios bien preparados de clase media asimiló las ideas völkisch de nacionalismo racista extremo; ideas que, diez o quince años después de terminar sus estudios, cuando llegaron a los puestos más altos de las SS y la Policía de Seguridad, y a los puestos estratégicos del Estado y del partido, pondrían en práctica.[60]​

En definitiva, la sociedad de la República de Weimar se fue polarizando, tanto en las clases privilegiadas como en las populares, en dos grandes grupos: por un lado, aquellos que cerraron filas ante los entendidos como los valores tradicionales y auténticos de Alemania, y, por otro, aquellos que amenazaban con su modernidad a estos: el socialismo, el capitalismo y, especialmente, como cabeza de turco de estos dos, los judíos. Y, paulatinamente,


El recrudecimiento en Alemania del sentir antijudío, una constante histórica en Europa desde el origen del cristianismo, se hizo notar ya a finales del siglo XIX, cuando degeneró en antisemitismo. Fue durante ese siglo cuando algunos judíos intentaron resolver la marginalidad a la que les llevaba la observancia de las normas de su religión por medio bien de la asimilación al cristianismo, bien transformándose en una nueva clase de judíos.[62]​ La consecuencia fue una presencia social entre los no judíos que no pasó inadvertida para muchos de estos, lo que posibilitó la aparición de reacciones antisemitas incluso en medios intelectuales. Así, por ejemplo, en unos artículos de 1879 y 1880, el historiador nacionalista alemán Heinrich von Treitschke llegó a escribir que «los judíos son nuestra desgracia» (Die Juden sind unser Unglück), una frase que sería retomada más adelante como eslogan por parte de los nazis.[63]​ Y fue también en esos años cuando Wilhelm Marr acuñó los términos «antisemita» y «antisemitismo» y se hizo muy conocido con su ensayo La victoria del judaísmo frente al germanismo: desde un punto de vista confesional, en donde insistía en la peculiaridad racial, y no tanto religiosa, de los judíos, además de crear una organización llamada «Liga Antisemita», cuyo ideario era esencialmente antijudío.[64]​

Ya en el siglo XX, la culpabilización de los judíos como responsables de la derrota alemana en la Primera Guerra Mundial fue una actitud general entre los soldados que participaron en ella. El 25 de diciembre de 1918, por ejemplo, un grupo de veteranos creó la asociación Stahlhelm («Casco de acero»), de carácter nacionalista y antisemita.

Por su parte, los nacionalistas de derecha, los monárquicos conservadores y las viejas élites, atemorizados por la revolución de Octubre, asociaban el bolchevismo con el judaísmo y creían en la posibilidad de una conspiración judía. En cuanto a las clases medias y bajas, la creencia en que los judíos habían obtenido ganancias económicas a costa de la guerra y las reparaciones posteriores era también frecuentes. En general, existía un cierto malestar por la inmigración de judíos desde el Este (entre 1918 y 1933 la política antisemita del gobierno de Polonia había llevado a 60 000 judíos a emigrar a Alemania) y por la convicción de que el capital estaba en manos de judíos (aun así, en 1925 los judíos constituían apenas un 0,9 % de la población alemana, 564 379 personas).[65]​

Así las cosas, y teniendo en cuenta que justo tras la guerra ya se había convertido en un éxito de ventas el panfleto antisemita ruso Protocolos de los sabios de Sion,


El antisemitismo dio origen también a numerosas publicaciones antisemitas, tanto literarias como periódicas. Además de lecturas infantiles como la titulada No puedes fiarte de un zorro en un brezal ni del juramento de un judío, los libros de texto para niños presentaban a Hitler como un gran guerrero nórdico y describían a los no nórdicos como menos que humanos. El currículo insistía en la teoría de razas, especialmente con la introducción de la biología racial y seudocientífica.[67]​

En 1923 empezó a circular en Núremberg (donde entre 1922 y 1933 se profanaron alrededor de 200 tumbas judías, profanación que fue generalizada en todo el país en 1927) el periódico pronazi y antisemita Der Stürmer (El asaltante), que retomó la frase «Los judíos son nuestra desgracia» como eslogan. El 4 de julio de 1927, Goebbels publicó el número uno del también antisemita Der Angriff («El ataque»), con el objeto de mantener vivo el espíritu del partido nazi los años en que fue ilegal en Berlín. Constituido en órgano oficial del partido nazi, incitaba a la violencia contra los judíos.

En 1929 se creó, por un lado, la Liga de Médicos Alemanes Nacional-Socialistas, con el objeto de centralizar el interés en la eugenesia, y, por otro, la Liga para Luchar por la Cultura Alemana, una asociación antisemita y anti-bolchevique dirigida por Alfred Rosenberg que centró sus acciones en la lucha contra lo que él llamaba «arte degenerado».

En 1935 se inició la publicación de las revistas antisemitas Deutsche Wochenschau für Politik Wirtschaft, Kultur und Technik (Semanario alemán de política, economía, cultura y tecnología) y Zeitschrift für Rassenkunde (Revista de ciencia racial»), una publicación seudocientífica. En 1936, Goebbels fundó el Instituto del NSDAP para el Estudio del Tema Judío y se publicó la primera tirada de la revista Forschungen zur Judenfrage (Investigación sobre el Tema Judío), también de carácter seudocientífico. En julio de 1937, se inauguró en Múnich la exposición Entartete Kunst (Arte degenerado), una muestra de obras de arte consideradas inaceptables de autores judíos y no judíos, y en noviembre otra exposición titulada Der Ewige Jude (El eterno judío), en la que se asociaba a los judíos con el bolchevismo, además de mostrar sus características raciales tópicas: nariz ganchuda, labios grandes y frente inclinada.[68]​

Las zonas de mayor antisemitismo (en el siglo XIX, la violencia antijudía era habitual en ellas)[69]​ y, por tanto, más receptivas a las ideas nazis al respecto fueron Franconia, Hesse, Westfalia y otras partes de Baviera. Allí, los elementos de hostilidad arcaica hacia los judíos, se fusionaron a finales del XIX con las nuevas corrientes ideológicas del nacionalismo völkisch, el antisemitismo racial que fue la base del racismo nazi.

Con todo,


Posteriormente, cuando se vieron obligados a evitar el contacto social y económico con ellos, los alemanes desarrollaron, según la interpretación del historiador Ian Kershaw, una «indiferencia fatídica» hacia el destino de los judíos. Así, pues, la política antijudía llevada a cabo en los años previos al comienzo de la guerra contó con una amplia aprobación social por cuanto no afectaba a las experiencias diarias de la gran mayoría de la población.[71]​ Desde otro punto de vista, los historiadores Otto Dov Kulba y Aaron Rodrigue han preferido calificar de «complicidad pasiva» a la actitud de la ciudadanía alemana ante el trato dado a los judíos por parte del nazismo.

En general, la historiografía distingue entre la actitud durante los años anteriores a la guerra y la actitud durante la misma. Así, en la época previa la sociedad alemana mantuvo una amplia diversidad de puntos de vista sobre los distintos asuntos que la afectaban, fiel reflejo de la pluralidad de influencias de muy diversa índole que la afectaban. En este sentido, hubo variados obstáculos a la penetración ideológica nazi generalizada, sobre todo en asuntos relacionados con las esferas de interés de las iglesias de confesión cristiana y en las preocupaciones económicas del día a día, especialmente las relaciones laborales, respecto de las cuales se produjeron protestas colectivas y acciones de desobediencia civil. Respecto de la cuestión judía, se han señalado cuatro actitudes básicas:[72]​ violenta y agresiva, sobre todo por parte de los radicales nazis; de aceptación de las normas legales de discriminación y exclusión; crítica, por motivos morales, religiosos, humanistas, éticos, económicos e ideológicos, por parte de diversos sectores sociales; y de indiferencia.

Con todo, un periódico como Der Stürmer (El atacante), que recordaba las acusaciones medievales contra los judíos de asesinos rituales de niños cristianos y de utilizar la sangre de estos para ritos religiosos, llegó a tener unos 600 000 lectores.[73]​

Respecto del conjunto de la Iglesia cristiana, aunque


estuvo sujeta a la derrota reciente de Alemania en la primera guerra mundial, la inestabilidad del gobierno, el temor al comunismo, la persecución política y el terror desencadenados por los nazis y a la actitud ambivalente de algunos de sus líderes ante el racismo, dada la tradición cristiana de antijudaísmo que aún conservaba fuerzas a comienzos del siglo XX, por lo que las declaraciones públicas tajantes contra el antisemitismo no fueron unánimes como debieron y las declaraciones explícitas sobre los judíos fueron excepcionales. Así, en enero de 1933 el obispo de Linz, Gfollner, que consideraba que no se podía ser un buen católico siendo nazi,[75]​ indicaba en una de sus pastorales que era deber de los católicos el adoptar una «forma moral de antisemitismo».[76]​ Esta consideración antisemita fue rechazada el mismo año por la totalidad del episcopado católico austriaco, denunciando esa carta por despertar el odio y el conflicto.[77]​ En agosto de 1935 un pastor protestante conocido por su antinazismo, Martin Niemöller, afirmaba que la historia judía era siniestra y que los judíos llevarían por siempre una maldición por haber sido responsables de la muerte de Jesús;[78]​ el mismo pastor, recordaría en abril de 1937 la desgracia que suponía el que Jesús hubiera nacido como judío. A pesar de su antisemitismo, fue detenido el 1 de julio por su oposición al nazismo.

El rechazo de los nazis al origen semítico del cristianismo llevó a un choque con el cristianismo y dentro de este, la mayor oposición surgió en el catolicismo. Los católicos correspondían al 30 % de los habitantes de Alemania.[79]​ Su posición ante los gobiernos germanos no era favorable, incluyendo durante el Imperio Alemán que había caído al perder la primera guerra mundial en 1918 y no existía ningún concordato que diera estabilidad a las relaciones con el estado alemán. Las relaciones entre la Iglesia católica y los nazis eran muy malas, pero esto iba más allá de una preocupación nazi sobre la lealtad de los católicos al estado alemán que deseaban los nazis. El antisemitismo nazi chocaba inevitablemente con un Jesús judío y en el caso de los católicos con el papel de la Virgen María también judía, de los apóstoles, el primer papa y los primeros santos, todos judíos.

Representados en imágenes por los católicos, Jesús, María, Pablo de Tarso, Pedro y los apóstoles se convertían en un problema práctico para la idea nazi de que la raza judía era maligna por sí misma y debía ser eliminada. Existía una contradicción entre la idea de la supremacía de la raza aria y la enseñanza de que Israel es el pueblo de las promesas y que Abraham (un judío) es el padre de la fe de todos los cristianos, peor aún decir que un judío es el salvador del mundo y que su madre judía, es madre de Dios, madre de los cristianos, intercesora ante su hijo, asunta al cielo y reina universal. Esto llevó a los nazis a buscar una adaptación del cristianismo que pudiera ser temporalmente tolerable para su ideología, por eso Hitler usó el término «cristianismo positivo» en el artículo 24 de la Plataforma del Partido Nazi en 1920, afirmando que:

El ideólogo nazi Alfred Rosenberg jugó un papel importante en el desarrollo del cristianismo positivo para enfrentar al origen semítico del cristianismo tradicional. Rosenberg era neo-pagano y notoriamente anti-católico. Para él, el catolicismo y el judaísmo estaban fuertemente relacionados.[81]​ Siguiendo a los teóricos del movimiento racista völkisch, Rosenberg afirmaba que Jesús era un ario (específicamente un amorreo o hitita) y que el cristianismo original era una religión aria, pero que había sido corrompida y alterada (judaizada) por los seguidores de Pablo de Tarso y el catolicismo.[82]​ Enfatizaba que las enseñanzas antijudías de los marcionistas, maniqueistas y cátaros eran las verdaderas enseñanzas del Jesús original, ario, antijudío y sin la humildad que los católicos supuestamente le añadieron. Rosenberg escribió:


De acuerdo a los nazis existía un dualismo entre la raza aria nórdica divina (con su sangre, cultura y tierra) y la raza judía supuestamente maligna y opuesta a la raza aria.[84]​ Rosenberg escribió "el Mito del Siglo XX" (1930), donde como consecuencia de ese dualismo, describió a la Iglesia Católica como uno de los principales enemigos del nazismo[85]​ y proponía sustituir el cristianismo tradicional con el "mito de la sangre" neo-pagana.[86]​ El libro es antisemita radical y en consecuencia al cuestionar el origen semita del cristianismo se torna anticristiano en general y particularmente anticatólico, al considerar la universalidad del catolicismo y su «versión judaizada» del cristianismo como uno de los factores en la esclavitud espiritual de Alemania y de la contaminación semítica del mundo: 

En su ideología antisemita, los partidarios del cristianismo positivo afirmaban que las antiguas invasiones germánicas del imperio romano habían venido a «salvar» la civilización romana, que se había corrompido por la mezcla de razas y por el cristianismo «judaizado y cosmopolita». Pensaban que las persecuciones contra los protestantes en Francia y en otras áreas representaron la aniquilación de los últimos restos de la raza aria en esas zonas. Igualmente veían en las zonas del norte de Europa que abrazaron el protestantismo lo más cercano al ideal racial y espiritual ario, aunque no lo habían alcanzado al no haber roto totalmente el vínculo semítico. Rosenberg escribió: 

 Otro aspecto doctrinal del cristianismo positivo, consecuencia de la idea de superioridad aria, fue lograr la unidad nacional, para superar las diferencias confesionales, para eliminar el catolicismo y unir el protestantismo en una única iglesia nacional socialista cristiana que fue llamada Iglesia Evangélica Germánica.[89]​

Como consecuencia ocurrió una reacción del cristianismo, que provino especialmente de los católicos. El cardenal Michael von Faulhaber estaba consternado por el totalitarismo, el neopaganismo y el racismo del movimiento nazi y como arzobispo de Múnich y Freising, contribuyó al fracaso en 1923 del intento de golpe de estado de la cervecería de Múnich organizado por los nazis.[90]​

Hitler fue a la cárcel por el fallido intento golpista de Múnich y escogió a Rosenberg en 1924 para dirigir el movimiento nazi en su ausencia.[91]​ En prisión, Hitler escribió Mein Kampf (Mi lucha), libro en el que sostenía que la ética judeo-cristiana «afeminada» había debilitando a Europa y que Alemania necesitaba un hombre de hierro para su restauración y entonces construir un imperio.[92]​ Así para el nazismo el vínculo judeo-cristiano planteaba un dilema a ser superado y el catolicismo era el más importante desafío.

Durante los años de 1920 a 1937, los líderes católicos hicieron diversos ataques francos contra la ideología nazi y la principal oposición cristiana al nazismo y sus ideas de la superioridad de la sangre surgieron de la Iglesia católica.[93]​ Antes de la llegada de Hitler al poder, los obispos alemanes advirtieron los católicos contra el racismo nazi. Algunas diócesis prohibieron a los fieles la pertenencia al Partido Nazi y la prensa católica condenó el nazismo.[94]​

Este choque llevó a John Cornwell a escribir sobre el período nazi temprano:


En 1930 y 1931, diferentes conferencias de obispos católicos condenaron el nacionalsocialismo. Los obispos bávaros lo condenaron en cinco aspectos: colocar la raza sobre la religión; rechazar el antiguo testamento y por lo tanto los diez mandamientos; negar el primado del papa como autoridad externa a Alemania, querer una iglesia nacional alemana sin dogmas y usar en el artículo 24 del programa del partido la no oposición a los sentimientos morales de la raza germánica como criterio de moralidad cristiana.[96]​ Los obispos de Freising dijeron que el nazismo “adhiere a un programa religioso y cultural irreconciliable con la enseñanza católica” y que “el nacionalsocialismo contra nuestra esperanza adoptó los métodos de los bolcheviques, por lo tanto nosotros no podemos asumir la existencia de buena fe”.[97]​ Igual hicieron la Conferencia de Obispos de Colonia, los obispos de Paderborn y Friburgo y la conferencia de Fulda (agosto de 1931).[98]​

Con la hostilidad permanente hacia los nazis por parte de la prensa católica y el partido del Centro católico, pocos católicos votaron por los nazis en las elecciones de julio de 1932 que llevaron a la toma del poder por el partido nazi en Alemania. Las ciudades de mayoría católica como Colonia, Düsseldorf y Múnich y las zona rurales católicas fueron inmunes al nazismo y el nacionalsocialismo logró sus votos fuera de las áreas geográficas de mayor población católica como en las ciudades de Hanover, Wuppertal, Chemnitz y Königsberg (votos de 40 % o más por los nazis).[99]​[100]​

La sensación de que la concepción antisemita y racista de los nazis llegaba a la locura fue expresada por Konrad von Preysing obispo de Eichstät y uno de los mayores adversarios del nazismo, que al saber que Hitler había sido nombrado canciller dijo: «Hemos caído en las manos de los criminales y los locos».[101]​ Después del incendio del Reichstag o parlamento alemán el 27 de febrero de 1933, Hitler suspendió la mayoría de los derechos civiles (habeas corpus, libertad de expresión, de prensa, de asociación, a reuniones públicas y de la reserva de las comunicaciones), arrestó a los opositores e inició un proselitismo forzado con los paramilitares nazis para la elección parlamentaria del 5 de marzo de 1933[102]​ y el 23 de ese mes logró la aprobación de la ley habilitante (Ermächtigungsgesetz) que le daba poderes dictatoriales totales.

Durante el invierno y la primavera de 1933, Hitler ordenó la destitución de los funcionarios públicos católicos,[103]​ el líder de los trabajadores católicos, Adam Stegerwald, recibió una golpiza por parte de los camisas marrones pronazis, miles de miembros del partido católico estaban en campos de concentración para junio de 1933.[104]​ Bajo estas y otras fuertes medidas de presión por parte del gobierno nazi se acalló a los católicos y se firmó el concordato con la Iglesia Católica el 20 de julio de 1933[105]​ que entre otras imposiciones, forzó el reconocimiento de la disolución del único partido católico de Alemania, en efecto desde la ley habilitante, Alemania era para motivos prácticos un país con un único partido legal (el nazi).[106]​

El punto de inflexión en las relaciones entre el cristianismo institucional y el nazismo se produjo con la firma del concordato entre la Santa Sede y el Reich. Por un lado, se daba un supuesto compromiso del nazismo con la Iglesia católica para respetarla mientras sus actividades se limitasen estrictamente a lo religioso (abandonando la política, la educación y la prensa); por otro, la Alemania nazi buscaba conseguir con el mismo una importante legitimación internacional. Hitler tenía un «flagrante desprecio» por el Concordato, escribió Paul O'Shea y su firma era para él no más que un primer paso en la "supresión gradual de la Iglesia católica en Alemania”.[107]​

En enero de 1934, Hitler nombró a Alfred Rosenberg como líder cultural y educativo del Reich. El 7 de febrero, el Vaticano prohibió el libro de Rosenberg El mito del siglo XX dando como razones que: «El libro desdeña todos los dogmas de la Iglesia católica…» argumenta la necesidad de fundar una nueva religión o una iglesia germánica y el libro proclama el principio: «hoy está surgiendo una nueva fe, el mito de la sangre, la fe en defender con sangre el divino ser del hombre: esta fe encarna el absoluto conocimiento de que la sangre norteña representa ese misterio que ha remplazado y superado los viejos sacramentos».[108]​[109]​

En la llamada «noche de los cuchillos largos», del 30 de junio al 2 de julio de 1934, los nazis asesinaron a los líderes de la acción católica, de la asociación católica de jóvenes y del semanario católico de Múnich entre otros.[110]​ Esto tuvo un enorme efecto intimidatorio sobre la oposición política.

En julio de 1935 fue nombrado obispo de Berlín, Konrad von Preysing, uno de los mayores adversarios del nazismo. Hitler decía de él: «lo más sucio de la carroña son los que vienen vestidos con el manto de la humildad y el más sucio de estos es von Presying».[111]​ Von Preysing fue un decidido defensor de los judíos y algunos de sus más cercanos colaboradores en esta tarea murieron a manos de los nazis durante la segunda guerra mundial.[112]​[113]​   Von Preysing apoyó francamente a la resistencia clandestina alemana y el intento de golpe de estado contra Hitler de julio de 1944 que fracasó y terminó con la ejecución de 4980 de los implicados.[114]​

En septiembre de 1935 los nazis promulgan las leyes de Núremberg con medidas discriminatorias sin precedentes contra los judíos, retirándoles la ciudadanía, prohibiéndoles usar los símbolos patrios y casarse con personas no judías, entre otras medidas terribles. La población y los clérigos católicos se mostraron descontentos. Un reporte de la época trascribe: Aachen, septiembre de 1935. «Las nuevas leyes aprobadas en Núremberg no fueron recibidos con entusiasmo por el público... Como era de esperar conociendo la mentalidad de la población católica de la región, no hubo reacción de simpatía por parte de la iglesia. La única parte que fue bienvenida es que la legislación sobre la cuestión judía evitará las acciones ofensivas y la violenta propaganda antisemita. Sería deseable que a partir de ahora estas acciones antisemitas, a las que una gran parte de la población se opone, lleguen a su fin».[115]​

Con las nuevas leyes, el 15 de noviembre de 1935


En 1937, Hans Kerrl, el ministro nazi de Asuntos para la Iglesia, explicó que el "cristianismo positivo" no "depende del Credo de los Apóstoles", ni de "la fe en Cristo como el hijo de Dios", en los que el cristianismo se basó, sino más bien, siendo representado por el Partido nazi: "El Führer es el heraldo de una nueva revelación". Con lo cual reforzaba la idea de desligar el cristianismo de su origen judío.[117]​  

Sin embargo, tras cuatro años de acoso constante contra los católicos en forma de detenciones de curas y monjas, cierre de conventos, monasterios y escuelas parroquiales, el papa Pío XI publicó el 14 de marzo de 1937 la encíclica Mit brennender Sorge («Con intensa ansiedad»), en la que expresaba su queja por estos hechos y lo que de ruptura del concordato suponían, y alertaba contra la deificación de conceptos como la raza, la nación y el estado. Difundida clandestinamente en Alemania, se leyó en las iglesias de todo el país el 21 de marzo, domingo de Ramos.[118]​

En la encíclica, en cuya redacción participó el obispo de Berlín von Preysing, Pio XI compara a los líderes nazis con Judas (artículo 21): «Si el opresor ofrece el trato de apostasía que solo Judas puede, a costa de cualquier sacrificio mundano, respondan con Nuestro Señor: "Vete, Satanás, porque escrito está: Al Señor tu Dios adorarás, y a él solo servirás"».[119]​

En el artículo 23 condena la doctrina de la superioridad racial base del antisemitismo nazi: «"Revelación" en su sentido cristiano, significa la palabra de Dios dirigida al hombre. El uso de esta palabra para las "sugestiones" de raza y sangre, por las irradiaciones de la historia de un pueblo, es una mera equivocación. Monedas falsas de este tipo no son moneda cristiana» y en el artículo 29 describe como pecado las políticas nazis y alerta sobre el daño para las generaciones porvenir: «El abandono resultante de los principios eternos de una moral objetiva, que educa la conciencia y ennoblece cada parte y organización de vida, es un pecado contra el destino de una nación, un pecado cuyo fruto amargo envenenará las generaciones futuras».[119]​

En el artículo 30 de la encíclica Mit brennender Sorge refiriéndose al daño social y humano que las doctrinas nazis provocan y al derecho irrevocable de cada individuo independiente de su raza, dice: "Pero el antiguo paganismo reconoció que el axioma… "Nada puede ser útil, si no es a la vez moralmente bueno" (Cicerón, De Off. ii 30). Emancipado de esta regla oral, en el derecho internacional, el principio llevaría a un perpetuo estado de guerra entre las naciones; por ignorar en la vida nacional, por confusión del derecho y utilidad, el hecho básico de que el hombre como persona tiene derechos que recibe de Dios y que toda colectividad debe proteger contra la denegación, supresión o negligencia. Pasar por alto esta verdad es olvidar que el verdadero bien común, en última instancia, lleva a su medida la naturaleza del hombre, equilibrando los derechos personales y las obligaciones sociales, desde el propósito de la sociedad, establecida para el beneficio de la naturaleza humana. La sociedad, fue la intención del Creador para el pleno desarrollo de las posibilidades individuales y para los beneficios sociales, que por un proceso de dar y recibir, cada uno puede reclamar por su propio bien y el de los demás. Los valores más altos y más generales, que solo puede proporcionar colectivamente, también se derivan del Creador para el bien del hombre y para el pleno desarrollo, natural y sobrenatural y la realización de su perfección. Descuidar este orden es sacudir los pilares sobre los que descansa la sociedad y comprometer la paz social, la seguridad y la existencia."[119]​

Los nazis reaccionaron con furia, Hitler juró “venganza contra la Iglesia” por su apoyo al "sionismo" y tomaron severas represalias por esta encíclica.[120]​ Thomas Bokenkotter escribe: "los nazis estaban furiosos. Como venganza cerraron y sellaron todas las imprentas que imprimen la encíclica. Ellos tomaron numerosas medidas vengativas contra la Iglesia, incluyendo la organización de una larga serie de juicios de inmoralidad del clero católico”.[121]​ Por su parte Shirer informó que "durante los siguientes años, miles de sacerdotes católicos, monjas y líderes laicos fueron arrestados, muchos de ellos por cargos falsos de "inmoralidad" o "contrabando de moneda extranjera” ".[122]​

El 9 y 10 de noviembre de 1938 los nazis usando sus fuerzas paramilitares realizaron una serie de ataques masivos coordinados contra los judíos en Alemania y parte de Austria llamados en su conjunto «la noche de los cristales rotos», con el saqueo y destrucción de sus hogares, negocios, escuelas y el incendio de más de 1000 sinagogas. 91 judíos fueron asesinados y 30 000 arrestados y enviados a campos de concentración.[123]​ Esta acción fue condenada al día siguiente por Pio XI junto con los líderes de occidente.[124]​ Como respuesta a la crítica católica, Adolf Wagner, el líder nazi de Baviera, dijo ante 5000 nazis: «Cada expresión que el papa hace en Roma es una incitación de los judíos en todo el mundo para agitar contra Alemania».[124]​

El 21 de noviembre de 1938, el papa insistió en que «existe una sola raza humana», a lo que Robert Ley, ministro de trabajo nazi respondió al día siguiente: «No se tolerará la compasión para los judíos. Negamos la afirmación del papa de que no hay más que una sola raza humana. Los judíos son parásitos».[125]​ El Vaticano envió mensajes a los arzobispos del mundo para iniciar trámites para dar visas a los alemanes no arios para salir del país. Se estima que unos 200 000 judíos lograron usar estas visas para huir del tercer Reich.[126]​

Pero cuando se fue aproximando la guerra, las actitudes generales se fueron endureciendo, incluso entre el amplio sector de la población que mantenían cierta apatía al respecto. Además, la propia idiosincrasia del nazismo permitió la aparición de denuncias como forma de control social, de modo que vecinos y compañeros de trabajo de los judíos colaboraron activamente para construir un clima de represión y terror.

El primero de septiembre de 1939, Alemania nazi invadió a Polonia en conjunto con la Unión Soviética iniciando así la segunda guerra mundial. La población civil y el clero católico de Polonia fueron masacrados. A los judíos que vivían en Polonia se les trató de forma salvaje. En 20 de octubre de 1939 el sucesor de Pio XI, el papa Pio XII promulgó la encíclica Summi Pontificatus, sobre las limitaciones de la autoridad del estado, desaprobando la guerra, el racismo, el antisemitismo, la invasión a Polonia y la persecución a los católicos. Escribió acerca de la necesidad de traer de vuelta a la Iglesia los que habían acogido las ideas nazis y que estaban siguiendo «un falso estándar... engañados por el error, la pasión, la tentación y el prejuicio, [que] se han desviado lejos de la fe en el Dios verdadero». Escribió sobre «Los cristianos, por desgracia más de nombre que de hecho» que han mostrado «cobardía» de cara a la persecución y apoyó la resistencia.[127]​ También reiteró la condena al antisemitismo y la igualdad entre el judío y el no judío: «el hombre "no es ni gentil, ni Judío, circunciso ni incircunciso, bárbaro ni escita, siervo ni libre. Pero Cristo es todo y en todos"».[128]​

Desafortunadamente a pesar de las solicitudes papales al gobernante italiano, Benito Mussolini, para mantener la neutralidad, Italia entró en la guerra como aliada de Alemania el 10 de junio de 1940.[129]​ Esto complicó aún más la situación de los católicos.

En 1940, el ministro de Relaciones Exteriores nazi Joachim von Ribbentrop dirigió la única delegación nazi de alto nivel a la que se le permitió una audiencia con Pío XII y le preguntó por qué el papa se había puesto del lado de los Aliados. Pío XII respondió con una lista de las recientes atrocidades nazis y las persecuciones religiosas cometidas contra los cristianos y los Judíos, en Alemania y en Polonia, lo que llevó al New York Times de esa época a encabezar la noticia sobre la reunión "Derechos de los judíos defendidos" y a escribir de las «ardientes palabras que el papa pronunció a herr Ribbentrop sobre la persecución religiosa».[130]​

En Alemania durante la guerra, el clero católico, con el argumento de que los nazis aumentarían la agresión antisemita de forma paradójica, tendió a evitó dar condenas públicas sobre la situación de los judíos, quienes habían sido sacados de Alemania y trasferidos a los campos de concentración en la conquistada y devastada Polonia. Esta tendencia se vio reforzada cuando el beneficio de dar declaraciones contundentes nombrando directamente a los judíos quedó en entredicho en uno de los episodios más estremecedores de la persecución nazi contra los judíos. El hecho ocurrió en Holanda, país vecino de Alemania, invadido al inicio de la guerra por el ejército alemán por la venganza devastadora e inesperada de los nazis ante la reacción enérgica y pública de las iglesias contra el antisemitismo y la violencia contra los judíos. El arzobispo de Utrecht en Holanda, Johannes de Jong se opuso firmemente a los nazis que habían invadido su país y la iglesia católica en Holanda creó una importante red de apoyo a los judíos.[131]​

En 11 de julio de 1942, los obispos holandeses junto con las demás iglesias del país enviaron una carta de protesta al general nazi Friedrich Christiansen por el trato dado a los judíos y su texto fue leído en todas las iglesias católicas del país. En la carta decía: 

La respuesta de los nazis fue arreciar la persecución contra los judíos, revocar la excepción de persecución contra los judíos bautizados y la Gestapo asaltó las instituciones religiosas católicas capturando cerca de 300 católicos monjes, monjas o sacerdotes de raza judía y los enviaron a Auschwitz donde inmediatamente fueron ejecutados en cámaras de gas. Dentro de estas víctimas estaba Edith Stein posteriormente proclamada santa por la Iglesia católica.[133]​ El vocero de los obispos holandeses, Titus Brandsma, fue arrestado en enero de 1942, enviado al campo de concentración de Dachau sujeto a experimentos médicos y ejecutado con inyección letal el 26 de julio de 1942.[134]​ La situación de los judíos en Holanda empeoró sensiblemente y esto influyó en la jerarquía católica y en las demás Iglesias cristianas para seguir restringiendo las declaraciones públicas mencionado a los judíos por temor a un efecto contraproducente como el del caso neerlandés. Consideraron que la ayuda debería ser preferiblemente discreta.[135]​

En conclusión,


En este contexto fue en el que surgió el Partido Nacional Socialista Obrero Alemán (NSDAP), el partido nazi, fundado en Múnich en 1919, cuyo programa oficial de 1920 proponía la unión de todos los alemanes dentro de una Gran Alemania y que solo las personas de sangre o raza alemana pudiesen ser nacionales (ciudadanas del Estado) y directores de medios de comunicación. Explícitamente, además, el NSDAP propugnaba un cristianismo constructivo y luchaba contra el espíritu judeomaterialista en el interior y el exterior del país.[137]​

La primera declaración política conocida de su principal líder, el ex cabo del ejército alemán Adolf Hitler, expuesta en una carta del 16 de septiembre de 1919, incidía sobre la cuestión judía partiendo de la base de que la comunidad judía era un grupo estrictamente racial y no religioso. Además,


Con sus fundamentos nacionalistas y antisemitas, el partido nazi se fue desarrollando poco a poco sobre la base de una intensa y llamativa actividad casi diaria de sus militantes. Entre 1919 y 1924 su zona de acción se reducía a Baviera, donde captó a una heterogénea masa de alemanes compuesta de antiguos soldados, de anticomunistas y antisemitas y, en general, de desclasados atraídos por la idea de una revolución nacional. Sus ideas antisemitas eran expuestas con frecuencia en diversos discursos pronunciados tanto por Hitler como por otros nazis, como Alfred Rosenberg, Julius Streicher o Hermann Esser, e insistían en la necesidad de tomar medidas contundentes contra los judíos de forma que su influencia sobre la sociedad alemana se eliminase por completo.

En noviembre de 1923, el NSDAP intentó hacerse con el poder para marchar, a continuación, sobre Berlín con el objeto de derrocar a la República de Weimar. El conocido como putsch de Múnich fracasó con la simple intervención de la policía, y Hitler fue detenido.

Sin embargo, el juicio subsiguiente se convirtió en una plataforma publicitaria para Hitler y su partido, y durante los nueve meses que pasó en la cárcel en 1924 tuvo tiempo para escribir su autobiografía política, titulada Mein Kampf (Mi Lucha), que terminaría por convertirse en el libro de cabecera del movimiento nazi y en un texto esencial del antisemitismo, que el autor, según su propia confesión, había aprendido de personajes como el compositor Richard Wagner, Karl Lueger, alcalde de Viena, y el nacionalista extremista Georg von Schönerer.

Wagner, a quien musicalmente admiraba Hitler por encima de cualquier otro músico, había expuesto en numerosas ocasiones auténticas diatribas contra el papel corruptor de los judíos en el arte en general, a quienes consideraba la conciencia maligna de nuestra civilización moderna o el versátil genio corruptor de la humanidad.[139]​

De Lueger tomaría la inspiración para utilizar el antisemitismo como un instrumento de movilización de masas, en tanto podía materializar los resentimientos del ciudadano común (el judío como asesino de Cristo, el judío como usurero enriquecido mientras los demás se arruinan...).

Y en cuanto a von Schönerer, Hitler había asumido íntegramente sus postulados radicales


Además de estas influencias, determinadas experiencias personales del propio Hitler relatadas en Mi lucha, le llevaron a convertirse en un antisemita fríamente racional, comprendiendo, además, la naturaleza judaica de la socialdemocracia internacionalista austríaca.[141]​

Como consecuencia de lo anterior


Hasta 1924, la demagogia global antisemita era el tema principal en casi todos los discursos de Hitler y se dirigía, especialmente, contra los judíos por su supuesto papel como financieros, capitalistas, responsables del mercado negro y aprovechados. Sin embargo, el impacto de la guerra civil rusa modificó esta línea discursiva hacia la identificación de los judíos con el bolchevismo y hacia un explícito antimarxismo (que Hitler igualaba a la lucha contra los judíos).[143]​

Así, pues,


También en Mi lucha (1925-1926) habla de lo oportuno que hubiese sido gasear de doce mil a quince mil judíos o "hebreos corruptores" durante la Primera Guerra Mundial, convencido como estaba, al igual que otros muchos exsoldados, de que Alemania había sufrido en esa guerra la traición de pacifistas y marxistas, todos ellos incitados por los judíos. La fijación de esta culpa haría que a principios de 1939 le expresase al Ministro de Asuntos Exteriores checo su pretensión de destruir a los judíos como castigo por lo que habían hecho el 9 de noviembre de 1918 (fecha de la rendición de Alemania y de la consecuente instauración de la República de Weimar).[145]​ A través de su identificación del judío con el marxismo y el bolchevismo, también responsabilizaba a los judíos de lo que denominaba genocidio judeobolchevique durante la Revolución rusa.

Haciendo uso de un lenguaje no solo extremo, sino también proto-genocida,[146]​ era característico asimismo de los discursos de Hitler, cuando tocaba la cuestión judía, la deshumanización constante a la que sometía a los judíos


Todo este antisemitismo tuvo, además, diversas publicaciones como herramientas para llegar al gran público. Destacó entre ellas Der Stürmer, donde se acusaba habitualmente a los judíos de violar a jóvenes alemanas y explotarlas como prostitutas, de raptar a niños y luego asesinarlos ritualmente, y de pretender empozoñar la sangre alemana a través de las relaciones sexuales para destruir la familia y el Volk (pueblo) alemanes.

También entre 1926 y 1928 Hitler se fue interesando cada vez más por la cuestión del territorio, cuya escasez por parte de Alemania se habría de solventar sustentándose en su creencia en el darwinismo social y en su teoría de la historia racial, por lo cual el más débil debía caer en beneficio del más fuerte. Así las cosas,


Con todo, solo una minoría del partido nazi consideraba el antisemitismo como la cuestión principal, siendo un tema menos atractivo a la hora de conseguir seguidores como lo podían ser el anticomunismo, el nacionalismo o el desempleo. Aun así, constituyó un elemento clave en el reclutamiento entre los jóvenes, hasta el punto de convertirse en el trampolín para que los nazis pudiesen llegar a dominar las universidades alemanas ya hacia 1930, y fue relativamente fácil propagarlo entre las clases médicas y profesorales, donde se fomentó la competitividad con los numerosos judíos presentes en ellas.

El movimiento hitleriano fue un fenómeno minúsculo y marginal políticamente hablando hasta la elección del Reichstag en mayo de 1928. Sin embargo, el nazismo se fue extendiendo en las zonas rurales y la clase media urbana ya a finales de la década, justo en plena crisis económica, permitiendo que en las elecciones de septiembre de 1930 el partido se convirtiese en la segunda fuerza política de Alemania. Dos años después, sería la primera. Durante esos años, el mensaje nazi se centró más en la necesidad de un nacionalismo integral antes que en insistir en el antisemitismo, habida cuenta de que Hitler había percibido que no era el elemento más efectivo para captar votos por no ser una preocupación de primer orden entre el electorado.


En este contexto, en 1931 el jefe de las SS Heinrich Himmler y Richard Darré fundaron la «Oficina General de la Raza y la Repoblación» (conocida por sus siglas RuSHA, de Rasse-und Siedlungshauptamt) y en 1932 un grupo de nazis fundó el «Movimiento de la Fe» de los alemanes cristianos, para radicalizar los ideales antisemitas, anticatólicos y antimarxistas en el nacionalismo alemán.

Entre 1933 y 1939 se aprobaron en Alemania más de 1400 leyes contra los judíos.[151]​

Tras abrirse en marzo de 1933 el campo de concentración de Dachau, adonde se enviarían, como en los otros 50 que se crearían durante el año (hasta 1945, los nazis construirían más de 1000 campos), a los miles de sospechosos enemigos del régimen,[152]​ la primera gran actuación del gobierno nazi contra el, aproximadamente, medio millón de judíos alemanes (menos del uno por ciento de la población)[153]​ fue la declaración oficial para el 1 de abril de 1933 de un boicot económico contra las tiendas y negocios judíos:


La reacción de la población alemana fue desigual, pero la impresión causada en los judíos fue demoledora.

El 4 de abril, el periódico Jüdische Rundschau incitó a los judíos alemanes a portar la estrella amarilla identificativa, como una forma de reivindicar con orgullo su identidad judía.

Pocos días después, el 7 de abril, se aprobó la «Ley para la Renovación de la Función Pública Profesional», que, en virtud de su párrafo tres o, como fue conocido después, su «Párrafo ario», desplazó al retiro a todos los funcionarios de origen no ario (exceptuando a los veteranos de guerra), esto es, cualquier persona que tuviera un padre o abuelo judío. Siguieron diversas leyes que excluyeron del ejercicio profesional a multitud de abogados, jueces, fiscales, notarios y médicos judíos (estos, desde el 3 de marzo de 1936 perdieron el derecho de ejercer en hospitales públicos), y diversas medidas contra intelectuales judíos (universitarios, artistas, escritores, etc.), muchos de los cuales (en 1933, unos 2000), entre ellos Albert Einstein, emigraron. También hubo otro tipo de leyes encaminadas a entorpecer la vida social de los judíos: una ley que prohibía la preparación ritual judía de la carne; una ley que, pretendiendo reducir la masificación en las escuelas y universidades, limitaba la admisión de nuevos alumnos judíos, dejando obligatoriamente su número global por debajo del 5 %;[154]​ una ley que prohibía a los médicos judíos trabajar en hospitales y clínicos públicos; una ley que impedía a los judíos optar a licencias para farmacias, etc. En cuanto a la ley que excluyó a los judíos de las asociaciones deportivas, impidió finalmente la participación de la casi totalidad de los atletas judíos alemanes en los Juegos Olímpicos de 1936, con la excepción de Helena Mayer, que vivía en California y que estaba categorizada como Mischlinge (además de ser, físicamente, alta y rubia).

La iglesia luterana se opuso a las sanciones de empleo y económicas contra los judíos.[155]​

Obviamente, este tipo de leyes implicaba algún tipo de mecanismo para certificar el carácter ario, o no, de la población. A tal efecto, se desarrolló una red de oficinas de investigación y de gestión del proceso. Por lo demás, alrededor de 37 000 judíos emigraron de Alemania en 1933.[156]​

A finales de abril, Hitler se reunió con los representantes de la iglesia católica en Alemania y les explicó que sus acciones contra los judíos remedaban las realizadas por el catolicismo a lo largo de la historia y que con ellas se hacía un gran servicio al cristianismo.[157]​

Durante la noche del 10 de mayo se produjo una quema pública de más de 20 000 libros, muchos de ellos de autores judíos, en las plazas de ciudades de todo el país, lo que pretendía simbolizar el fin de la influencia intelectual del judaísmo en Alemania.

En septiembre, Goebbels, a través de la Cámara de Cultura del Reich, inició un proceso de depuración en el ámbito artístico y cultural, negando la posibilidad de la actividad profesional a los judíos en la prensa, el teatro, el cine y la música. Hacia finales del mes, a través de otro decreto, se excluyó también a los judíos de la profesión de granjero.

Como consecuencia de todo lo anterior, y con el respaldo del Acuerdo Haavara, el primer año de Hitler en el poder provocó la marcha de unos 40 000 judíos de Alemania, casi el 10 % de los que había; tras seis años de gobierno nazi, a finales de 1938 se habían marchado del país 200 000 judíos.

Paralelamente, durante 1933 se crearon más asociaciones antisemitas y anticomunistas como la Asociación General de Sociedades Alemanas Anticomunistas y el Movimiento de Creyentes Cristianos Alemanes.

En julio, después de que el Partido Nazi se convirtiese en el único partido legal de Alemania, se despojó de la ciudadanía a los judíos del este que vivían en el país y se aprobó la ley para la Prevención de Descendencia con Enfermedades Hereditarias, que estipulaba la esterilización, por un lado, para aquellas personas que pudiesen transmitir a su descendencia algún tipo de defecto (en consecuencia, antes de 1937 200 000 personas fueron esterilizadas), y, por otro, la eutanasia para los «defectuosos» y las «bocas inútiles»,[158]​ a los que se representaba en ocasiones como bajo el lema de «idiotas» o de «vida sin esperanza».[159]​ En junio de 1935 se modificaría la ley para incluir la obligatoriedad del aborto en el caso de fetos «incapacitados» de hasta seis meses.

El 17 de septiembre se creó la organización nacional judía Reichsvertretung der Deutschen Juden («Representación en el Reich de los judíos alemanes»), con el objeto de aglutinar a los judíos alemanes y hacer, en la medida de lo posible, frente común para defender sus intereses.

En octubre, una ley de Edición obligó a todo judío vinculado al periodismo a dimitir, en virtud de la necesidad de un periodismo racialmente puro.

Aunque la experiencia del boicot de principios de 1933, no muy seguido por la sociedad alemana, llevó a que la legislación incidiese en minar a los pequeños comerciantes y profesionales judíos, por provocar menos perjuicios a la economía en general, en 1935 la cuarta parte de las empresas judías se habían ya "arinizado". Y a partir de junio de 1938, cuando la economía estaba recuperada, se inició el expolio y la expropiación de las propiedades judías, lo cual implicó la emigración de unos 120 000 judíos.

El 1 de enero de 1934 se eliminaron oficialmente las fiestas judías del calendario alemán.

El 24 de marzo, el mismo mes en que se produjo un violentísimo pogromo en Gunzenhausen, se retiró oficialmente la ciudadanía a los miembros de la comunidad judía.

Paralelamente a las decisiones gubernamentales al respecto, se fueron incrementando las llamadas Einzelaktionen o acciones individuales contra los judíos por parte de elementos de las SA, actos violentos y sádicos contra ellos. Especialmente virulentos fueron los altercados producidos en el centro de Múnich el 18 y el 25 de mayo de 1935, culminación de una larga campaña incitada por el gauleiter Adolf Wagner, ministro del Interior de Baviera. Con todo, el rechazo de la población obligó a señalar como culpables a unos supuestos «grupos terroristas».[160]​

En mayo, Rudolph Hess creó la «Oficina de política racial del Partido Nacional-Socialista Alemán de Trabajadores».

En 1934 se publicó también el libro de Ernst Bergmann titulado Veinticinco puntos de la religión alemana, en el que se afirma que Jesús no era judío sino un guerrero nórdico asesinado por los judíos; por otro lado, se fundó el «Instituto de Biología Hereditaria e Investigación sobre las Razas» en la Universidad de Frankfurt del Meno.

A finales de 1934, unos 50 000 judíos emigraron de Alemania.[161]​

Tras prohibir en abril que los judíos se pudiesen exhibir con la bandera de Alemania y expulsarlos en mayo del ejército, la mala imagen exterior que generaba todo este tipo de acciones, y la convicción de que se promovían desde el gobierno actos de vandalismo contra los judíos, se resolvió con la promulgación en septiembre de 1935 de las leyes raciales de Núremberg,[162]​ con las que se intentó contentar tanto a la burocracia del partido nazi como a sus elementos más radicales.

Como comentario a las mismas, Hitler utilizó por primera vez expresiones tajantes respecto al futuro que les podría esperar a los judíos, si las leyes no llegasen a ser suficientes para controlarlos; en concreto, habló de la posibilidad de dejar el problema en manos del Partido Nacionalsocialista para que le buscase una solución definitiva (zur endgültigen Lösung).

Las leyes de Núremberg y la celebración de los Juegos Olímpicos en Berlín hicieron posible un periodo de tranquilidad física para los judíos, que duraría hasta 1938; en agosto de 1935, Hitler y Hess habían llegado incluso a prohibir las acciones individuales contras los judíos.[164]​ Sin embargo, los proyectos de recrudecimiento de la actitud de los nazis respecto de los judíos siguieron adelante.

En esta línea, se emitió ese mismo mes de septiembre la primera orden de aplicación de la ley de Ciudadanía del Reich por la que ningún judío podía ser ya ciudadano del mismo, lo que implicaba que no podían votar sobre asuntos políticos, ni ejercer cargos públicos. Además, se distinguía a los judíos completos (lo que tenían tres abuelos judíos, como mínimo) de los parciales (dos abuelos judíos y que no fuese practicante ni tuviese cónyuge judío), esto es, la categoría del mestizo o Mischlinge, categoría en la que entraron entre 250 000 y 500 000 ciudadanos. A su vez, el mestizo podía ser de primer y segundo grado (un abuelo judío y ser practicante o tener cónyuge judío).[165]​

El 7 de septiembre de 1936 se creó un impuesto del 25 % sobre todos los bienes judíos de Alemania.

En 1937 Hitler declaró que durante los dos o tres años siguientes la cuestión judía se habría de arreglar de un modo u otro, y a finales de ese año, con la consecución del pleno empleo en Alemania, la intención de expropiar y eliminar a los judíos de la economía alemana se hizo clara. Consecuentemente,


El 28 de marzo de 1938, todas las organizaciones judías perdieron definitivamente su estatus oficial y un mes después, el 21 de abril, un decreto excluyó a los judíos de la economía nacional, estipulando además la toma de sus bienes. Desde ese momento, los judíos hubieron de registrar todas sus propiedades y bienes por valor superior a los 5000 marcos, con el objeto final de poder ser subastadas entre los no judíos.[167]​

El 9 de junio la principal sinagoga de Múnich fue incendiada por los nazis y el 10 de agosto la de Núremberg. El 15 de junio todos los judíos con alguna condena previa, independientemente de su gravedad, fueron detenidos. El 25 se restringió la atención de los médicos judíos a pacientes judíos y un mes después se les cancelaron sus licencias. Al tiempo, se dio orden de que todos los judíos de Alemania solicitasen tarjetas especiales de identificación y que, como segundo nombre, los hombres judíos tomasen el genérico de Israel y las mujeres el de Sara (un año después, en agosto de 1939, se emitió por parte de las autoridades nazis un listado de nombres permitidos para los niños judíos). El 6 de julio se ordenó que desapareciese antes de fin de año cualquier negocio judío. El 20 de septiembre todas las radios de propiedad judía fueron confiscadas y desde el 27 de septiembre, se les prohibió a los judíos ejercer la abogacía y desde el 5 de octubre todos sus pasaportes fueron marcados con una gran «J» roja.

El 28 de octubre Alemania expulsó a los 17 000 judíos con ciudadanía polaca, que quedaron abandonados en la frontera, en Zbaszyn, al no aceptarlos Polonia. El 15 de noviembre todos los alumnos judíos fueron expulsados de las escuelas alemanas.

En marzo de 1938 Alemania se anexionó Austria, y con ellos incorporó a su población a los cerca de 200 000 judíos austriacos. Como herramienta para alcanzar la aspiración nazi de liberar a Alemania de la población judía, Viena se convirtió en el primer lugar en el que se pondría en práctica la que sería, a partir de entonces, una constante política nazi: la deportación de la comunidad judía de su territorio. Previamente, hubo


El antisemitismo ya presente en Austria sirvió para desbordar las medidas antijudías, hasta el punto de que llegaron a servir de modelo para las tomadas en la propia Alemania. Una campaña de detenciones provocó el traslado de unos mil judíos a campos de concentración, entre ellos el de Mauthausen.

Entre las medidas y acciones tomadas contra los judíos de Austria más destacables, estuvieron que se les privó de la ciudadanía, que sus organizaciones y congregaciones perdieron toda financiación gubernamental y que, además, en los primeros meses de anexión, los bancos vieneses de propiedad judía pasaron a manos de los nazis.

La consecuencia inmediata de ello fue una oleada de emigración judía tanto de Alemania como de Austria (el 4 de junio, por ejemplo, emigró de Viena Sigmund Freud, con 82 años). La Conferencia internacional de Evian, promovida por Estados Unidos en Francia con el objeto de tratar el tema de los refugiados judíos, se resolvió con pretextos generales por parte de todos los países presentes, los que terminaron por no acoger a un número sustancial de judíos que solicitaban refugio.

Al frente de la gestión del procedimiento industrial para la emigración forzada de los judíos de Viena, se situó desde la misma primavera de 1938 a Adolf Eichmann. En seis meses, expulsó a cerca de 45 000 judíos y antes de mayo de 1939 más del 50 % de la población judía (unos 100 000) se había ido de Austria.[169]​ En octubre de 1939, invadida ya Polonia, se inició la deportación de los judíos austriacos hacia campos de trabajos forzados en ese país. En febrero de 1941 se inició la deportación a guetos en Polonia ocupada, como los de Kielce y Lublin, adonde se fueron enviando unos 1000 judíos de Viena cada semana.[170]​

La primera deportación en masa se produjo en octubre de 1938, cuando 16 000 judíos de origen polaco fueron expulsados de Alemania, siendo abandonados en la frontera con Polonia, que les negó la entrada. El hijo de uno de ellos, Herschel Grynszpan, que residía en París, reaccionó asesinando al tercer secretario de la embajada alemana en París. La propaganda nazi calificó la acción como declaración de guerra y como un acto más de la conspiración judeomasónica mundial. Así, un día después de la muerte del diplomático, el 10 de noviembre, tuvo lugar la Noche de los cristales rotos (Kristallnacht), pogromo instigado por el ministro de propaganda Joseph Goebbels, pero con la expresa aprobación de Hitler, que constituyó la exhibición pública de antisemitismo más violenta en Alemania desde la época de las cruzadas y marca el inicio del Holocausto.[171]​


Otra consecuencia directa del pogromo fue que Reinhard Heydrich asumió la coordinación centralizada de la Cuestión Judía.

Las agresiones no solo fueron realizadas por los fanáticos ideológicos del partido nazi, sino también por alemanes corrientes. No hubo esta vez protestas públicas significativas por parte de las iglesias.

Dos días después, el gobierno alemán reforzó las consecuencias del pogromo imponiendo una multa de mil millones de marcos alemanes a la comunidad judía alemana por lo que se calificó como una "actitud hostil" hacia el Reich y su pueblo. Durante la reunión en la que se decidió la medida, se sugirieron diversas medidas discriminatorias muchas de las cuales serían aprobadas por Hitler el mes siguiente. Mediante ellas se puso prácticamente fin a la actividad empresarial de los judíos, a su libertad de movimiento y a sus relaciones con el resto de la población alemana.

Además, 30 000 judíos fueron detenidos y llevados a campos de concentración, donde llegaron a morir unos 10 000.[173]​

La radicalización de la actitud hacia los judíos fue reflejada por un artículo del 24 de noviembre de 1938 en el periódico de las SS, Das Schwarze Korps, en el que se afirmaba que


Dos meses después, en el plazo de una semana, haría dos declaraciones explícitas sobre sus intenciones respecto de los judíos: por un lado, el 21 de enero de 1939, en palabras dirigidas al ministro de Asuntos Exteriores checoslovaco, Hitler indicó que los judíos serían destruidos y que su provocación del 9 de noviembre de 1918 no les habría de salir gratis, sino que sería vengada;[175]​ por otro lado, el 30 de enero, pronunció un discurso en el Parlamento alemán que habría de gravitar sobre todas las decisiones que en adelante se tomarían sobre la cuestión judía. Alardeando de una aptitud profética, afirmó:


Hitler recordaría su profecía dos veces en 1942 y tres en 1943, aunque asociándola a una fecha equivocada, el 1 de septiembre de 1939, como forma de vincular el inicio de la guerra a los judíos.

Previamente, en el contexto de sus iniciativas diplomáticas para conseguir que la comunidad internacional se hiciese cargo de la población judía alemana, Hitler había declarado dos meses antes al ministro de Defensa de Sudáfrica, Oswald Pirow, que ya había adoptado una decisión irrevocable sobre ellos y que un día habrían de desaparecer de Europa.[177]​

Como consecuencia de esta situación, y tras el establecimiento, además, en enero de la Oficina de Emigración judía dirigida por el jefe de la Gestapo Heinrich Müller, la emigración judía aumentó considerablemente, tanto la legal como la ilegal, la cual llevó hacia Palestina a unos 27 000 judíos antes del fin de 1940.

A partir del 21 de febrero de 1939, los judíos de Alemania se vieron obligados a entregar a las autoridades todo el oro y la plata que tuviesen en posesión.
El 15 de marzo de 1939 Alemania inició la ocupación de Checoslovaquia; las SS hubieron de ocuparse de unos 120 000 judíos. En menos de seis meses, más de 30 000 emigraron y 19 000 dejaron Europa, tras haber sido hacinados en Praga. Al final, del total restante solo sobrevivirían 10 000.[179]​

El 30 de abril de 1939 se promulgó una ley que prohibía a judíos y no judíos compartir el mismo bloque de pisos; como consecuencia de ello, se crearon casas judías y guetos en las grandes ciudades que hicieron aumentar el aislamiento social de la población judía. La obligatoriedad de portar la insignia amarilla que entró en vigor en 1939, asentó definitivamente ese aislamiento y condujo a la segregación de los judíos de un modo drástico.[180]​

En 1939, 78 000 judíos abandonaron Alemania y se confiscó por todo el país objetos de valor pertenecientes a los judíos. Finalmente, el 12 y 13 de febrero de 1940 comenzaron las deportaciones de los judíos de Alemania, especialmente desde Pomerania. Los pocos que fueron quedando vieron como se deterioraba completamente su vida civil en el país: prohibición de la emigración (octubre de 1941), disolución de la Liga Cultural Judía (septiembre de 1941), leyes que prácticamente condenaban a la pena de muerte por cualquier infracción (diciembre de 1941) y marcado con una estrella de papel blanco de todo hogar judío (marzo de 1942).[181]​

Tras las distintas anexiones e invasiones de países y otros territorios durante la guerra, el principal objetivo de Hitler fue el genocidio racial de los mismos, en tanto que habían pasado a formar parte de la Gran Alemania. Como corolario de este objetivo y de las consecuencias naturales de una guerra, dos fueron los grandes problemas con lo que se encontró el nazismo: el reasentamiento de los deportados y de los prisioneros de guerra, y la manutención de los mismos. Y aunque, en parte, la política de exterminio fue una salida a ambos problemas, ya en septiembre de 1939 había constancia de las ideas de Hitler sobre la administración de Polonia:


La conquista de Polonia, en este sentido, provocó una transformación en el tratamiento de la Cuestión Judía. De repente, Alemania se había encontrado con tres millones de judíos más que gestionar. El trato que se le dio, como judíos del este que eran y por tanto especialmente despreciados y deshumanizados,[184]​ fue especialmente bárbaro, bastante más allá del trato dado a los judíos de Alemania y Austria. En parte, ello fue debido a la mayor libertad, dado que quedaba lejos la opinión pública alemana y las restricciones legales correspondientes, que se dio al partido y a la policía para tomar iniciativas individuales autónomas.[185]​ La invasión de Yugoslavia y Grecia en abril de 1941 terminó por ser, también, un desastre para los miles de judíos que allí vivían.

Esto quiere decir que durante el periodo 1939-1941, los nazis no llegaron a elaborar una política clara y coherente sobre qué hacer con los judíos, los polacos y el medio millón de germanos de pura cepa que fueron «repatriados» a territorios anexionados por Alemania. Solo cuando el Warthegau empezó a colapsarse con los judíos llegados de Alemania y cuando la invasión de la URSS multiplicó el problema del movimiento y la manutención de personas, se empezó a pensar más seriamente en la elaboración de planes más o menos precisos para solucionar tales problemas.

En este sentido,


Al poco de ser nombrado ministro de los Territorios Ocupados del Este, en noviembre de 1941, Rosemberg explicitó estas ideas declarando que esos territorios estaban llamados a ser el lugar de resolución de la cuestión judía; en su opinión, tal cuestión solo podía resolverse mediante la erradicación biológica de todos los judíos de Europa, expulsándolos al otro lado de los Urales o erradicándolos de alguna otra manera.[187]​

Los planes nazis para la reformulación racial de Europa encontraron en los 3 300 000 judíos de Polonia un campo de experimentación para el genocidio en masa. Para el nazismo, los polacos eran seres inferiores y los judíos polacos lo eran aún más.[188]​ La mayoría de los judíos europeos vivía en Polonia y Europa Oriental. El Gobierno General (Polonia ocupada) fue el país donde se establecieron los principales campos de exterminio: Auschwitz-Birkenau, Chelmno, Belzec, Treblinka, Sobibór y Majdanek. A Auschwitz-Birkenau los primeros prisioneros no judíos llegaron el 14 de junio de 1940.

Por lo demás, en el área ocupada por el Ejército Rojo desde el 17 de septiembre, la suerte de los judíos (tanto los residentes como los refugiados ante el avance alemán) fue también mala: 100 000 de ellos murieron en los procesos de deportación a Siberia y muchos se hubieron de resignar a regresar a la zona ocupada por los alemanes, debido a la situación insostenible con los soviéticos.[189]​

Al tiempo que entre el 1 de septiembre y el 25 de octubre de 1939 se desarrollaba en todo el país la Operación Tannenberg, ejecutada por las SS «Einsatzgruppen», cuyo resultado fue el asesinato de cerca de 10 000 intelectuales y miembros de la nobleza y el clero polacos con el objeto de eliminar la resistencia local,[190]​ desde principios de la invasión las acciones contra los judíos se fueron sucediendo. Por ejemplo, el 8 de septiembre, el grupo operativo especial del Servicio de Seguridad de las SS, que repetiría acciones similares en otras zonas, asesinó en Bedzin a un grupo de niños judíos e incendió la sinagoga y casas vecinas, al tiempo que disparaba indiscriminadamente con los judíos con los que se encontraba por la calle. El resultado fue de 500 judíos muertos.[191]​

El 11 de septiembre, en una reunión en Cracovia, se le comunicó a Udo von Woyrsch, jefe del mencionado Grupo Operativo, una orden de Himmler por la cual instaba a tomar las medidas más duras posibles contra los judíos para que, movidos por el terror, se desplazasen hacia el este abandonando la zona controlada por los alemanes.[192]​ El 21 de septiembre, Reinhard Heydrich, jefe de las SS, ordenó a los Einsatztruppen que, en colaboración con las autoridades civiles y militares, iniciasen una serie de concentraciones de los judíos en guetos y deportaciones masivas de ciudadanos hacia el Este (desde el 29 de noviembre, la pena de muerte era el castigo para quien no se presentase a la deportación). Toda comunidad judía por debajo de los 500 individuos quedó disuelta y enviada a guetos y campos de concentración.

A tal efecto, se estableció un departamento de las SS, el Rasse-und Siedlunghauptamt («Oficina Principal -o central- para la Raza y el Reasentamiento»), para gestionar la germanización de la zona conquistada y el bienestar de los colonos allí enviados. Sus miembros contaban con cuatro semanas de adiestramiento para la evaluación racial y biológica, que se basaba en veintiún criterios físicos (quince eran fisonómicos). La clasificación obtenida utilizaba descripciones como


El 7 de noviembre comenzaron las deportaciones de judíos del oeste de Polonia, procedimiento reforzado por la orden de Heydrich dada el 12 de ese mismo mes para que todos los judíos del Warthegau fuesen trasladados para dejar sitio a los asentamientos de alemanes de raza aria. Entre el 26 de octubre y principios de febrero de 1940 los nazis deportaron y reasentaron a unos 78 000 judíos en una reserva situada en el sudeste de Polonia, en la región Lublin-Nisko. Los testimonios de los alemanes de clase media que se encargaron de desalojar y preparar las granjas polacas para los colonos, subrayan la idea de que para ellos el sufrimiento de polacos y judíos era «o bien invisible o bien aceptable, cuando no justificado».[194]​

El 11 de noviembre, en las afueras de Ostrow Mazowiecka, se produjo uno de los primeros asesinatos masivos de judíos polacos por parte de los nazis. Acusados por la población local de haber incendiado parte de la ciudad, la policía alemana mató entre 162 y 500 judíos.[195]​ El 12 de diciembre se instauró en el este de la Gran Alemania un periodo obligatorio de trabajos forzados para los judíos de entre 14 y 60 años. Unos meses después, en mayo de 1940, se lanzó una nueva ofensiva contra los intelectuales polacos denominada «Operación de Pacificación Extraordinaria».

En resumen, con la invasión de Polonia en septiembre de 1939, los nazis se encontraron con que el imperio alemán había incorporado a su población a dos millones de judíos polacos (posteriormente, un millón de ellos, de la Polonia oriental, como resultado del pacto nazi-soviético pasarían a formar parte de la Rusia comunista). Esta situación, que se repitió en menor medida con cada conquista alemana (120 000 judíos más de los territorios checos, por ejemplo, etc.), puso a los nazis ante el problema de cómo lograr territorios judenrein, esto es, «libres de judíos».

Entre septiembre y diciembre se establecieron las divisiones administrativas para Europa del este, de donde se intentaría expulsar a la fuerza a los judíos: la Gran Danzig (norte de Polonia), Prusia Occidental (norte de Europa en el Báltico), la Gran Prusia Oriental (norte de Europa en el Báltico) y la zona del oeste de Polonia anexionada, que los nazis denominaron «Warthegau» («Reichsgau Wartheland»), cuya capital era Posen (Poznan), y que desempeñaría un papel crucial en la génesis de la Solución Final. Se creó también el conocido como Gobierno General en Polonia, zona administrativa que no quedó incorporada a la Gran Alemania y donde los nazis situarían sus campos de concentración y exterminio; fue nombrado jefe Hans Frank y Cracovia, importante comunidad judía, sería designada como su capital. De acuerdo con las indicaciones expresas de Hitler,[196]​ el Gobierno General se constituyó como un territorio autónomo con respecto del Reich, y en él, lugar de destino del resto de judíos de Polonia, se puso en práctica una lucha étnica sin límites.

En líneas generales,


Las figuras más importantes en la administración del territorio fueron Arthur Greiser, gobernador del Reich y gauleiter del partido nazi, y Wilhelm Koppe, jefe de las SS y la policía de la región, que sería, probablemente, quien tomara la iniciativa de empezar el genocidio en la zona.[198]​

En septiembre de 1939 se expusieron, en distintas directrices del Jefe de la Policía de Seguridad Reinhard Heydrich, las líneas definitorias de lo que entonces se entendía como objetivo final en relación a los judíos: concentrarlos en las grandes ciudades polacas para a continuación deportarlos hacia una reserva al este del Vístula.[199]​ A estas alturas, estaba ya claro que


El 21 de septiembre se emitió un decreto administrativo en el que Heydrich, distinguiendo entre un objetivo final a largo plazo y unas medidas preliminares a corto plazo, ordenaba, en el ámbito de estas últimas, que los judíos de Polonia fuesen concentrados en las ciudades más grandes, cerca de los cruces de líneas de tren con vistas a la deportación. Una semana después, Himmler especificó en otra orden que todos los judíos del oeste de Polonia (el Warthegau) debían ser deportados al centro del país con el fin de dejar sitio a los alemanes que iban a ocupar la zona.

En la primera quincena de diciembre, se llevó a cabo la primera deportación: se detuvo en Posen a 88 000 polacos y judíos y se les trasladó hasta el Gobierno General (los hombres aptos y sanos eran, sin embargo, enviados a Alemania a trabajar forzosamente). Por otro lado, estas deportaciones de cientos de miles de judíos supuso que desde finales de 1939, se empezasen a establecer guetos por toda Polonia; el primero, erigido en diciembre, estuvo en la ciudad de Lodz (en la que entre el 15 y 17 de noviembre los nazis habían destruido todas las sinagogas), precisamente en el Warthegau.[201]​ Simultáneamente, se establecieron trabajos forzados para todos los judíos de la Polonia anexionada; los no aptos para el trabajo (niños, mujeres y enfermos) fueron confinados en guetos.

Sin embargo, avanzado 1940, Hans Frank recibió una petición, a la que se negó, para albergar en la zona de su jurisdicción a un cuarto de millón de judíos provenientes del gueto de Lodz, de los que quería librarse el Warthegau. A su vez, Göring tuvo que prohibir en marzo del mismo año que los judíos de las zonas orientales del Reich fuesen deportados sin más a la Polonia anexionada. La imposibilidad de hacerse cargo de ellos, tanto en uno como en otro caso, hizo pensar ya que la solución al problema judío tendría que ser otra. En este sentido, en junio, Heydrich informó a Ribbentrop de que la solución para los tres millones y cuarto de judíos bajo dominio de Alemania habría de ser «territorial».[202]​

El 25 de mayo de 1940, Himmler le había presentado a Hitler un memorándum secreto titulado «Reflexiones sobre el trato a los pueblos de raza extranjera del Este» en el que hablaba del «pueblo subhumano del Este» y de la necesidad de utilizarlos como mano de obra. Explícitamente, rechazaba el exterminio físico (por antialemán y bolchevique), prefiriendo el reasentamiento y división de grupos sobre la base de un criterio racial. Respecto de los judíos, expresaba su esperanza de que estos desapareciesen de Europa a través de una gran emigración de los mismos a África o alguna otra colonia.[203]​

Entre las soluciones alternativas, que les fueron presentadas a los representantes judíos tras la crisis en el proceso de deportaciones materializada en una reunión celebrada el 31 de julio de 1940 en Cracovia donde, ante la evidencia expuesta por parte de Greiser de la masificación en el Warthegau (en el gueto de Litzmannstadt-Lodz había ya 250000 judíos), se sugirió la posibilidad de la emigración a Palestina o la de enviarlos a Madagascar. Esta última idea estaba en el aire desde que en 1937 el gobierno polaco había tratado de llegar a un acuerdo con franceses y británicos para enviar allí a un millón de judíos polacos. Tras la capitulación de Francia, Franz Rademacher, responsable alemán de asuntos judíos en la Cancillería, elaboró un memorándum que preveía la deportación de cuatro millones de judíos de Europa a la isla. La idea quedó descartada a principios de 1942.[202]​

Entre mayo y diciembre de 1940, miles de judíos de Polonia fueron enviados a la nueva frontera soviética para construir fortificaciones.

Los alemanes comenzaron a levantar guetos nada más invadir Polonia. Su finalidad inicial era la de concentrar transitoriamente a los judíos, antes de o bien deportarlos hacia el este, o bien recluirlos en campos de concentración de trabajos forzados o de exterminio de la misma Polonia. Sin embargo, dadas las terribles condiciones en que se desarrollaba la vida en ellos (hacinamiento extremo, carencia de servicios sanitarios adecuados y proliferación de enfermedades), supusieron también una muerte lenta para muchos de sus habitantes.

Los principales guetos en la Polonia ocupada, en funcionamiento entre 1939 y 1941, fueron los de Varsovia, Minsk, Lodz, Radom, Piotrkow, Lublin, Kielce, Czestochowa, Bedzin (con 27 000 judíos, un 45 % de la población, tras diversas muertes y deportaciones, el gueto fue liquidado en agosto de 1943), Sosnowiec, Tarnow y Cracovia (donde había 60 000 judíos, el 25 % de la población total).[204]​

El primer gueto se abrió el 8 de octubre de 1939 en Piotrków Trybunalski, distrito de Lodz, y el 19 de octubre se creó el de Lublin. En 1940 se crearon el de Lodz, el 18 de febrero, aislado del resto de la población el 30 de abril, y el Bedzin, el 1 de julio.

El mayor de todos los guetos en la Polonia ocupada por Alemania fue el de Varsovia, con una superpoblación que llegó a alcanzar el medio millón de judíos en un espacio de 3,3 kilómetros cuadrados. En octubre de 1940 se terminó de recluir en él a todos los judíos de la ciudad y fue aislado del resto de la misma en noviembre. En abril de 1941 llegaron más de 40 000 judíos deportados desde Alemania y Bélgica. Tanto en Varsovia como en Lodz, cerca de una cuarta parte de los judíos murieron a causa de las enfermedades, el hambre y la crueldad a la que eran sometidos.

En la primavera de 1941 se establecieron los guetos de Cracovia, Lublin (ambos en marzo) y el de Kielce. En abril se establecieron dos guetos separados en Radom y uno en Czestochowa.

Simultáneamente, desde noviembre de 1939, se ordenó la constitución de consejos judíos en las comunidades judías para encargarse de la aplicación precisa e inmediata de las directrices emitidas por las autoridades alemanas. Entre sus funciones, estuvo la gestión de la concentración de los judíos de las zonas rurales en ciudades con enlaces ferroviarios o cerca de vías férreas, pero sobre todo la de actuar de enlace entre la población y las autoridades nazis.

A finales de 1940, ya existía la percepción de que los guetos no tenían visos de disolverse a través de las deportaciones, aún a pesar de que las condiciones de vida en su interior estaban empeorando dramáticamente. En marzo de 1941, el que había sido responsable de la acción de eutanasia que entre 1939 y 1941 había asesinado a 70 000 enfermos mentales en Alemania, Victor Brack, propuso aplicar métodos de esterilización a entre 3000 y 4000 judíos por día.[205]​

Cuando en el otoño de 1941 empezaron a llegar al Warthegau los primeros judíos deportados de Alemania, las posibles salidas empezaron a buscarse de forma clara en otros métodos. De hecho,


Con todo, hubo numerosas dudas acerca de cómo tratar exactamente a los judíos por parte de quienes se tenían que enfrentar a la gestión directa de las deportaciones y a la aplicación de determinadas medidas. En concreto, hubo dudas acerca de cómo tratar a los judíos alemanes, como lo demostró, por ejemplo, la indecisión a la hora de gestionar el futuro de los judíos que había en Minsk, sobre muchos de los cuales se tenían dudas acerca de su grado de ascendencia aria y que, por tanto, podrían ser susceptibles de un trato radicalmente diferente del dedicado a los judíos rusos. Simultáneamente, había dudas también en relación a la aplicación de la estrella amarilla como distintivo de los judíos del Reich (Alemania, Austria y el protectorado checo). En todo caso, la estrella (llamada por los nazis Pour le Sémite, en relación irónica con la condecoración Pour le Mérite) se convirtió en un distintivo obligatorio (bajo pena de muerte) para todos los judíos de una edad superior a los diez años en todos los territorios ocupados. En Polonia, la estrella, de color azul sobre fondo blanco, fue obligada para los judíos que apareciesen en público desde el 23 de noviembre de 1939.

En general, la cautelas del nazismo venían provocadas por las dificultades de ocultar a la opinión pública alemana determinadas acciones y decisiones aplicadas sobre esos ciudadanos alemanes; así las cosas, el Reich no fue declarado judenrein hasta junio de 1943.

Estos meses del otoño de 1941 serían decisivos en el diseño y plasmación del Holocausto, por cuanto las autoridades regionales nazis, a falta de directrices claras desde Berlín, se vieron obligadas a recurrir a una toma de decisiones por propia iniciativa para liberar sus áreas de judíos. Fue en esas fechas cuando se iniciaron algunos programas locales de exterminio, aún con el conocimiento de Berlín.[207]​

Hacia noviembre de 1941, se planteó también si los judíos del Este deberían ser respetados en el caso de que fuesen útiles como trabajadores para la industria armamentística. A la pregunta de Heinrich Lohse (comisario del Reich para Ostland) si debían ser liquidados sin tener en cuenta consideraciones prácticas, el Ministerio del Reich para los Territorios Ocupados respondió que las consideraciones de tipo económico no debían tenerse en cuenta. Por lo demás, remitió la resolución de otras dudas al jefe superior de las SS y la policía.[208]​

Entre las primeras soluciones locales estuvo el fusilamiento de judíos a su llegada al Báltico desde Alemania. Asimismo, en noviembre, bajo órdenes del jefe de la policía de las SS de la zona, se comenzó a construir un pequeño campo de exterminio en Belzec (municipio de Lublin) con el fin de matar a los judíos no aptos para el trabajo.[207]​

Después, tras el comienzo de la invasión de la Unión Soviética, antes del final de 1941, dos millones de prisioneros soviéticos habían muerto ya de hambre en campos de concentración alemanes.

La política de deportaciones con el objeto de dar cabida a alemanes, que en la primavera de 1940 había llevado al Gobierno General a 128.011 judíos,[209]​ terminó provocando numerosas quejas tanto por parte de aquellos que echaban en falta la mano de obra para la guerra que esos judíos representaban, Göring, como por quienes como Frank veían como el territorio bajo su gestión se empezaba a ver congestionado por esos deportados.

Ion Antonescu, dictador rumano que se había hecho con el poder en septiembre de 1940, recibió en junio de 1941 una serie de directrices de parte de Hitler con las que se le instruía acerca de cómo tratar a los judíos en su territorio (aun cuando Antonescu ya había aprobado previamente numerosas leyes antijudías): reclusión en guetos a los que vivían en ciudades y exterminio inmediato de los que fuesen localizados en el campo. Desde ese mismo mes, se empezaron a desarrollar masacres en distintas localidad (Iaşi: probablemente, 10 000 víctimas; Besarabia y Bucovina: miles de judíos fusilados y muchos encerrados en guetos; Odesa: cientos fusilados o ahorcados, 19 000 ametrallados y luego quemados; Bogdanovka: 5000 judíos quemados vivos y 43 000 fusilados; Domanovka: 18 000 fusilados; etc.).[210]​


En Yugoslavia, parte de Serbia quedó bajo ocupación alemana. La lucha contra la resistencia incluía en esta a los judíos y comunistas. A los judíos, además de asesinárseles, se les impuso en algunos lugar el uso de la estrella judía, se le excluyó de diversas ocupaciones y se les expropió parte de sus propiedades sin compensación por ello.


 En Bulgaria, país inicialmente aliado al Eje, se promulgaron leyes antisemitas incluyendo la implementación de trabajos forzados (1940-41), pero los trenes nunca partieron. La deportación de 1943 fue cancelada debido a la resistencia, que contó con el apoyo de la Iglesia Cristiana Ortodoxa de Bulgaria y de la opinión pública.[213]​ En 1944, cuando Bulgaria rompió con Alemania y se unió a la coalición antihitleriana, a los judíos búlgaros se les restituyeron todos los derechos de los que habían sido privados. El periodista búlgaro Samuel Francés expresó que:


Antes de la Segunda Guerra Mundial vivían en Bulgaria unos 48 000 judíos; al terminar la guerra, su número llegaba casi a 50 000.[215]​

La presencia del antisemitismo en Europa, reforzada por la actuación de los nazis, se hizo patente incluso en zonas donde no había un dominio directo de estos. Así, por ejemplo, en la Francia de Vichy, donde el triunfo de Pétain y de los nacionalistas de extrema derecha hicieron que en el régimen hubiese un núcleo antisemita que sería responsable de distintas medidas discriminatorias: prohibición de poseer o dirigir empresas, expulsión de profesores universitarios, internamiento de los inmigrantes judíos... Más tarde, en octubre y noviembre de 1941, se empezarían los preparativos para la deportación de los judíos extranjeros, y, finalmente, tras la ocupación de la zona en noviembre de 1942 por parte de los alemanes, la deportación de los judíos franceses. En total, fueron asesinados 80 000 de los 350 000 que había.[216]​

El punto de inflexión respecto de la política antijudía del nazismo se produjo a partir de la invasión de la Unión Soviética, iniciada el 22 de junio de 1941; nada más entrar en el país y en los territorios controlados por este, el ejército alemán y los cuatro grupos operativos del Servicio de la Seguridad de las SS, con sus unidases operativas, y la ayuda de varios batallones policiales, comenzaron a matar a los civiles resistentes, a cuadros y judíos del Partido Comunista (a los que se responsabilizaba de alentar a los partisanos), y a todos los prisioneros de guerra judíos.[217]​ Cumplían, así, una serie de directivas criminales que ordenaban el asesinato de los comisarios políticos del ejército soviético («Orden de los Comisarios» de 6 de junio de 1941) y las indicaciones recibidas en las sesiones informativas dadas por Heydrich, a los líderes del Einsatzgruppen y a sus subunidades, los Einsatzkommandos, acerca de su función en la retaguardia, reuniones donde se hizo explícita la orden de Hitler de eliminar a los judíos rusos.[218]​

Esos


En otro orden de cosas, la decisión de Stalin de deportar a centenares de miles de alemanes del Volga, como consecuencia de la entrada de Alemania en Ucrania, convenció a Hitler a hacer uso también de la deportación hacia los territorios del Este de los judíos de Europa central. Una idea a la que era en principio reacio, mientras hubiese guerra,[220]​ y que, una vez aceptada, hizo abandonar definitivamente la posibilidad de enviarlos a Madagascar, plan además inviable en aquellos momentos en que Gran Bretaña mantenía su dominio marítimo.

Esas deportaciones plantearon el problema de dónde colocar a los judíos y qué hacer con ellos. En aquellos momentos ya era un problema para el ejército alemán el retener y alimentar a los tres millones de prisioneros de guerra soviéticos. Consecuentemente, se decidió no usar los campos de prisioneros y optar por los guetos masificados que ya había en Minsk, Riga y, sobre todo, Lódz.

En septiembre, Himmler (que desde julio tenía un control absoluto sobre las medidas de seguridad que hubiesen de adoptarse en los territorios conquistados en el este, incluida la eliminación de cualquier amenaza de subversión) avisó a Arthur Greiser de que unos 70 000 judíos (alemanes y checos) iban a ser enviados al gueto de Lodz; 20 000 fueron enviados al mes siguiente.

La imposibilidad material de sostener los guetos tan llenos de gente incentivó la maquinación del asesinato en masa como una respuesta nazi ante esa situación. El hambre y el frío mataron a muchos judíos, y desde enero de 1942 empezó a usarse el gaseamiento en Chelmno.[221]​

Respecto de la actividad criminal en durante la invasión, fueron especialmente relevantes las matanzas perpetradas por las brigadas de las SS y la Einsatzgruppen. Tras una orden de Himmler de principios de agosto («todos los hombres judíos deben ser fusilados. Empujen a las mujeres judías a los pantanos»), una brigada fusiló a más de 25 000 judíos en menos de un mes en la zona de los pantanos de Pripet. Más adelante, las órdenes de fusilamiento se extendieron a niños y mujeres. Así, por ejemplo, la unidad comandada por Friedrich Jeckeln, que operaba en la zona de Kiev, llegó a fusilar hasta octubre a más de 100 000 judíos entre hombres, mujeres y niños.[222]​

El eufemismo con el que los nazis identificaron en sus documentos y declaraciones sus planes genocidas respecto de la población judía europea fue «Solución final a la cuestión judía» (Endlösung der Judenfrage, en alemán). El primer uso del término se dio en una circular de Adolf Eichmann, de 20 de mayo de 1941, en la que aludía a esa solución como una futura vía en el tratamiento de los judíos europeos, tras comunicar que Göring prohibía la emigración de judíos de Francia y Bélgica.

Con la aprobación y sanción por parte de Hitler de las distintas fases de intensificación, la Solución Final, como proceso, arrancó en la primavera de 1941 con la planificación de la «Operación Barbarroja» y la propaganda para persuadir al pueblo alemán acerca de la conspiración judeo-anglosajona (Estados Unidos ya incluidos) contra Alemania;[223]​ se amplió durante el verano con el paso a un genocidio a gran escala en la Unión Soviética recién invadida (radicalizado en otoño por la deportación masiva hacia el este ordenada por Hitler de los judíos del Reich, Bohemia y Moravia) y se encaminó hacia su pleno desarrollo entre diciembre (una vez declarada la guerra a Estados Unidos) y la primavera de 1942, cuando surgió definitivamente un programa coordinado de exterminio que se materializaría en la matanza perpetrada en los distintos campos.[224]​

El 16 de julio de 1941, el jefe del Servicio de Seguridad (SD) en Posen, Rolf-Heinz Höppner envió a Adolf Eichmann, de la Oficina Principal de Seguridad del Reich en Berlín, un informe titulado Solución a la cuestión judía, en el que recogía las conclusiones de diversas discusiones al respecto entre distintos organismos del Reich. La idea principal que se exponía en el informe era la de concentrar a todos los judíos del Warthegau en un campo para 300 000 personas situado cerca del centro de la producción de carbón, para que los judíos aptos para el trabajo pudiesen ser explotados. Además, se señalaba, en relación con los judíos que no pudiesen trabajar y con aquellos a los que no fuese posible alimentar, que habría que considerar seriamente si la solución más humana no sería terminar con ellos mediante algún tipo de preparado de efecto rápido. Por lo demás, se sugería la esterilización de todas las judías para solventar el problema judío en esa misma generación. Así, pues, el informe destacaba la idea de genocidio en una fase embrionaria.[225]​

Posteriormente, el 31 de julio, Göring firmó un documento, que se supone redactado a partir de un borrador de Eichmann, en el que se instaba a Heydrich para que se encargara de llevar a cabo los preparativos necesarios para «la solución completa de la Cuestión Judía dentro de la esfera de influencia alemana en Europa», probablemente con el sentido de buscar todavía una solución territorial del tipo de intentar un traslado de los judíos alemanes y de otros lugares de Europa a una reserva situada más allá de los Urales.[226]​

Esa solución territorial dependía, por un lado, de una victoria rápida de Alemania sobre la Unión Soviética y, por otro, de un cambio en los planes de Hitler, que todavía tenía en mente usar a los judíos alemanes como rehenes y que no quería que fuesen deportados al Este. Sin embargo, en septiembre las ideas empezaron a cambiar, cuando, probablemente, Rosemberg convenció a Hitler de utilizar la deportación de judíos como forma de represalia por las deportaciones de alemanes del Volga a Siberia por parte de los soviéticos.[227]​ Hitler ordenó en septiembre, cuando los Einsatzgruppen habían emprendido el genocidio total en la Unión Soviética, la deportación inmediata de los judíos de Alemania, Austria y Checoslovaquia.


En el otoño de 1941, Heinrich Himmler, encargado principal de llevar a cabo el plan que conducía a exterminar a las tres cuartas partes de todos los judíos europeos, dio la orden al General de las SS Odilo Globocnik (jefe de las SS para el distrito de Lublin) de aplicar un plan para matar sistemáticamente a los judíos residentes en el Gobierno General.[229]​ “Aktion Reinhard” fue el nombre en clave dado a la operación por Heydrich (que había sido el encargado de preparar la "Solución final" y que fue asesinado por partisanos checos en mayo de 1942).[229]​

A finales de 1940, la Alemania nazi había asesinado ya a unos 100 000 judíos en toda Europa. En Rumania, por ejemplo, uno de los países más antisemitas antes de la guerra, fue eliminada la mitad de su población judía tras el estallido de esta: más de 350 000 judíos fueron asesinados por parte de los Einsatzgruppen y de las propias tropas nacionales rumanas.[230]​

A partir de 1941, cuando el asesinato en masa de judíos se convirtió en política de Estado, la cifra aumentó exponencialmente; solo ese año, murió 1 000 000.[231]​ En julio de 1941, el Reino Unido, a través del desciframiento de códigos, era ya conocedor de las masacres de judíos soviéticos.[232]​

A lo largo de ese año, fue la política genocida efectuada contra los judíos rusos la que, habiéndose iniciado la invasión de la Unión Soviética, monopolizó la atención de los jerarcas nazis, política que se encuentra en el origen mismo del Holocausto.

Tras una primera instrucción de Heydrich el 17 de junio a los comandantes de los Einsatzgruppen acerca de la puesta en práctica de la Solución final,[233]​ en una orden del 2 de julio de 1941, él mismo realizó una serie de indicaciones genéricas a los jefes superiores de las SS y la policía en el este sobre la necesidad de matar judíos, saboteadores, subversivos y funcionarios del Komintern (además de instigar a las poblaciones locales para desencadenar pogromos contra los judíos). La limitación del alcance de la orden parece ser una estratagema con el fin de justificar de alguna manera los fusilamientos en masa que la Wehrmacht y otras autoridades estaban ya practicando.


Es probable, por tanto, que en esas reuniones informativas hubiese habido ya indicaciones indirectas de aniquilar a los judíos, de forma que pudiesen ser comprendidas de distintas maneras.

Un mes después, en una conferencia de planificación, Hitler afirmó que había que aniquilar a cualquiera que se interpusiese en el camino de Alemania.[235]​

Un mensaje del 1 de agosto de Heinrich Müller, jefe de la Gestapo, indicaba que había que presentar informes continuos a Hitler acerca de los trabajos de los Einsatzgruppen en el Este.[236]​ También, a mediados del verano determinados elementos radicales del nazismo habían convencido a Goebbels de la necesidad de eliminar a los judíos de la retaguardia, de las ciudades alemanas; el primer paso fue marcarlos con una estrella amarilla, algo que Hitler aceptó a mediados de agosto.[237]​

Todo lo anterior, y muy especialmente la actividad desarrollada por los Einsatzgruppen, se corresponde con testimonios y pruebas documentales que apuntan a que el mandato de Hitler acerca de asesinar a determinado tipo de judíos rusos (sobre todo, dependiendo de su edad y sexo) fue transmitido a los Einsatzkommandos en el mes de agosto. El asesinato generalizado, que culminaría con los fusilamientos masivos de finales de septiembre en Babi-Yar (33.771 hombres, mujeres y niños), no habría sido ordenado explícitamente por Hitler, sino que este habría dado su respaldo a una sugerencia de, probablemente, Himmler, a partir de las impresiones transmitidas por los comandantes locales que tenían a su cargo los fusilamientos.[238]​

Por lo demás, se ha demostrado que la Wehrmacht colaboró con los Einsatzgruppen implicándose directamente en el asesinato de casi dos tercios de los prisioneros de guerra soviéticos, muchos de los cuales serían los primeros en probar las cámaras de gas de Auschwitz, y que aproximadamente 1 300 000 judíos (una cuarta parte de todos aquellos que murieron en el Holocausto) además fueron asesinados por ella.[239]​

A mediados de agosto, con la invasión de la Unión Soviética ya en marcha, Hitler no solo insistía en la relación entre una nueva guerra mundial y la aniquilación de los judíos, sino que aceptó la deportación hacia el este de los judíos que aún quedaban en Alemania. La situación de estos, como se refleja en los testimonios de Victor Klemperer, se había ido deteriorando con celeridad, hasta el punto de que una ley de diciembre de 1941 imponía la pena de muerte como castigo para prácticamente cualquier infracción cometida por un judío.[240]​ A los no deportados (por ejemplo, aquellos que formaban parte de matrimonios mixtos), se les sometía a trabajos forzados.

En octubre, Heydrich precisó todavía más que la deportación tenía que afectar a todos los judíos de los territorios ocupados por Alemania.

Simultáneamente, las declaraciones genocidas por parte de los jerarcas del nazismo eran frecuentes: por ejemplo, en noviembre, Alfred Rosenberg afirmaba que el objetivo de los asesinatos en masa que ya se estaban produciendo era el «exterminio biológico de toda la judería de Europa» y en diciembre Goebbels recordaba que la compasión o el arrepentimiento respecto de los judíos estaban fuera de lugar y que la guerra, desecadenada por ellos, los había sumido en «un proceso gradual de aniquilación».[241]​

Así, pues, a finales de 1941 se estaba aplicando un programa de exterminio, en el que intervenían tanto las autoridades militares alemanas, como la policía, las SS, las milicias locales y los administradores civiles de los distintos territorios. Sin embargo, estaba también claro que la intensidad reclamada por Himmler no se podía alcanzar a base, sobre todo, de los fusilamientos en masa. Por otro lado,


La alternativa puesta en práctica de inmediato fue el gaseamiento, que se había estado aplicando hasta agosto de 1941 en la operación de eutanasia T-4. El doctor August Becker, que se describía como «especialista en los procesos de gaseamiento utilizados en el exterminio de los enfermos mentales», junto con otro personal de dicha operación, fue trasladado por Himmler a la Oficina Central de Seguridad del Reich en Berlín. Por su parte, Albert Widmann, el inventor de la cámara de gas estándar que había sido empleada en el programa de eutanasia, estuvo colaborando en el este para asesinar a enfermos mentales bombeando monóxido de carbono al interior de habitaciones; como resultado de su presencia, Arthur Nebe, jefe del grupo operativo B de la zona de Minsk y Mogilev, ideó el uso de una camioneta herméticamente cerrada en la que se introducía los gases de su tubo de escape, mecanismo de asesinato aprobado por Heydrich.[243]​

Himmler aprobó en octubre la construcción en Belzec de un campo que sirviese de base para las camionetas de gas; también en Chelmno se estableció otro centro similar, de donde salían las tres camionetas que se utilizaban para asesinar a los judíos (y gitanos, también) transportados desde el gueto de Lódź, con el objeto de ir dejando sitio, como en otros guetos, para los judíos que iban llegando desde todas partes de Europa. Estas camionetas podían matar a 50 personas a la vez durante el trayecto de 16 km entre el gueto y el campo, donde eran enterradas en zanjas. Por este procedimiento, en Chelmno fueron asesinadas 360 000 personas.[244]​ A finales de 1941 los cuatro grupos operativos estaban empleando un total de unas 30 camionetas. También en Serbia se hizo uso de una camioneta de gas; a principios de mayo de 1942, más de 7500 judíos habían muerto en ella.

En diciembre de ese mismo año, los dos millones y medio de judíos del Gobierno General eran ya una preocupación real para los dirigentes nazis. Y, en este sentido, alguno de ellos, como Hans Frank ya hablaba de la «necesidad de tomar medidas que de algún modo conduzcan a lograr su aniquilación en sintonía con otras medidas» que habrían de tomarse desde el Reich.[245]​

El 20 de enero de 1942 se celebró la «Conferencia de Wannsee». Convocada por Heydrich, reunió a varios altos funcionarios de los ministerios con responsabilidad en el asunto judío, y a representantes de las SS y del Partido Nazi, implicados también en el mismo. El objetivo era establecer una directriz clara en cuanto a quién tenía que asumir el control sobre la cuestión judía en todos los territorios ocupados. En una de sus alocuciones, Heydrich remitió a un encargo de Göring de julio de 1941 por el que le encomendaba a él (a las SS y, por tanto, haciendo de Himmler el responsable superior) «tomar las medidas necesarias para la solución final de la cuestión judía en Europa», solución que habría de ponerse en práctica tras la deportación al este de los judíos. Durante las reuniones, fueron continuas las referencias al exterminio por medio del trabajo y, según algún testimonio posterior, se hizo referencia también al asesinato con camionetas de gas. Con todo, en las actas finales de la conferencia se utiliza un plural impreciso, «varios tipos posibles de solución», para aludir a la futura forma de resolver el asunto judío.[246]​

La primera consecuencia de lo hablado en Wannsee fue la reestructuración de todos los campos de concentración existentes: desde febrero de 1942, se convirtieron, de forma sistemática, en una fuente primordial de mano de obra para las industrias de guerra alemanas. Sin embargo, gestionados por las SS, el aumento de la aportación de los prisioneros se hizo por la vía de la violencia y el terror: con el objetivo siempre en mente de la reorganización racial del continente, el exterminio por el trabajo implicó que solo la productividad del trabajador podría salvar a este, eventualmente, de la muerte. En este sentido, aquellos que no eran aptos para el trabajo, fueron asesinados por millones.[247]​

En esta línea, el 14 de febrero Hitler le diría a Goebbels que estaba decidido a «limpiar Europa de judíos sin remordimientos» y que era necesario acelerar el proceso «con una frialdad implacable» para prestar un gran servicio «a una raza humana a la que la judería ha estado atormentando durante milenios». Poco más de un mes después, el mismo Goebbels aludía en su diario al proceso por el cual los guetos del Gobierno General estaban siendo liberados de judíos, para dejar sitio a los expulsados del Reich; insistía en que el contexto era el de «una lucha a vida o muerte entre la raza aria y el bacilo judío», e indicaba la singularidad del nazismo por su capacidad «para emprender una solución final de la cuestión», «una solución radical» de la que Hitler era su «pionero» y «portavoz persistente».[248]​

Durante esos primeros meses de 1942 quien estuvo supervisando las matanzas de judíos fue Himmler, que se reunía con frecuencia con Hitler de forma confidencial, y del que decía haber recibido directamente el encargo. Estuvo en Cracovia a mediados de marzo, cuando el uso de gas venenoso ya se había empezado a utilizar para asesinar judíos. En abril, ordenó en Varsovia el asesinato de los judíos de Europa occidental que habían llegado para entrar en el gueto de Lódz. En julio, apremió en el este el programa de matanzas. Mientras, intentaba acelerar el exterminio de los judíos que quedaban en el Gobierno General, que esperaba concluir a finales de año, y el de los judíos de Ucrania, que había comenzado en mayo.[249]​

La Conferencia de Wannsee supuso también que Adolf Eichmann, desde la Oficina Central de Seguridad del Reich, reiniciase en marzo los transportes en tren para deportar a los judíos que quedaban en Alemania, el Protectorado y la antigua Austria, hacia los guetos de Europa oriental. Esta decisión, junto con la situación ya insoportable para ellos, indujo al suicidio a numerosos judíos. Igualmente, el programa de deportaciones se amplió a otros lugares de Europa: Países Bajos, Bélgica y Francia, entre ellos.

El Holocausto está directamente asociado en la mentalidad popular a los llamados «campos de exterminio». Aunque no todos los judíos que murieron a causa de las políticas nazis lo hicieron en estos campos, lo cierto es que en ellos se pusieron en práctica de forma concentrada todos los sistemas y métodos (especialmente, el uso de cámaras de gas) que configuran la violencia extrema contra los judíos que desplegó el nazismo.[250]​

Unas semanas antes de que se celebrase la Conferencia de Wannsee, Himmler había encargado a Odilo Globocnik, jefe de la policía y las SS en Lublin, que organizara el exterminio de los judíos del Gobierno General. Con el nombre de «operación Reinhard», el objetivo del plan era liberar espacio en los guetos para dejar sitio a los judíos deportados del oeste. Para ello Globocnik se rodeó de varios de los participantes en la operación T-4, que quedaron empleados en los tres campos que se crearían dentro de la operación; se trataba de oficiales y suboficiales de las SS, ayudados por un personal básico compuesto de auxiliares ucranianos reclutados en campos para prisioneros de guerra.

Los campos se situaron al oeste del río Bug, con buenas conexiones por ferrocarril con otras zonas de Polonia y con los principales guetos. El primer campo, el de Belzec, se empezó a construir el 1 de noviembre de 1941, a partir de las instalaciones de un campo de trabajo. Su comandante era Christian Wirth, al que prestó ayuda uno de los especialistas en eutanasia. Contaba con cámaras de gas fabricadas con madera, aunque herméticamente cerradas; el gas se bombeaba al interior desde unos vehículos y no haciendo uso de botes de monóxido de carbono puro, como se había hecho en el plan de eutanasia, debido a la dificultad de hacerse con grandes cantidades de ellos. El campo empezó a funcionar en febrero de 1942. Se probó primero el gaseamiento de grupos pequeños de judíos, incluidos los que habían ayudado a construir el campo. A partir del 17 de marzo, se empezó el gaseamiento de los judíos deportados. En un mes, se asesinó a 75 000 judíos, 30 000 de los cuales provenían del gueto de Lublin (que contaba con 37 000 habitantes), siendo los demás de otras zonas del Gobierno General.

La disposición de algunos elementos del campo buscaba no levantar sospechas entre los judíos: se les decía que era un centro de tránsito, que iban a ser desinfectados antes de recibir ropa limpia y que sus objetos de valor les serían devueltos. Las cámaras de gas parecían habitaciones con duchas.

Entre junio y julio, las cámaras de madera fueron sustituidas por una construcción de hormigón con capacidad para seis cámaras de gas, que podían albergar al mismo tiempo a un total de 2000 personas. Hasta 600 000 judíos, tanto de la Polonia ocupada como de otros lugares de Centroeuropa, fueron allí asesinados antes de finales de año.

El segundo campo de exterminio que formaba parte de la operación Reinhard se empezó a construir en marzo de 1942 cerca de Sobibor, también sobre la base de un campo de trabajo, en este caso para mujeres judías. En mayo se finalizó: contaba con las áreas administrativas y de recepción al lado del correspondiente ramal ferroviario, y sus cámaras de gas (con capacidad para 100 personas cada una de ellas) estaban en un edificio de ladrillo fuera de la vista de quienes llegaban al apeadero, a unos 150 metros de distancia a través de una vereda conocida como «el tubo». El gas se tomaba desde un motor y detrás del edificio había fosas para los cadáveres, a las que se podía acceder también por vía férrea, dado que muchos de los que llegaban en tren lo hacían ya muertos. En sus tres primeros meses de funcionamiento, murieron en el campo hasta 100 000 judíos, de Lublin, Austria, Bohemia y Moravia, y del Antiguo Reich.

Durante el calor del verano, los cuerpos sepultados empezaron a generar problemas de salubridad. Se tomó entonces la decisión de incinerarlos, haciendo para uso de un grupo especial de judíos, el llamado Sonderkommando, que fue asesinado después.

A principios de 1943, Himmler visitó el campo y pudo observar un gaseamiento; posteriormente, concedería ascensos a varios oficiales de las SS y la policía y a otros responsables del campo. Respecto del cierre del campo, ordenó la eliminación de todas las huellas y su transformación en almacén de la munición capturada al ejército soviético. Durante este proceso, en octubre de 1943, hubo una rebelión de los trabajadores judíos que terminó con la fuga de varios de ellos, que contactaron con grupos de partisanos. El desmantelamiento final del campo se produjo en diciembre. Casi 250000 judíos murieron en Sobibor.

El tercer campo estuvo en Treblinka. Construido al lado de una vieja cantera, sus orígenes estaban en un campo de trabajo abierto en la primavera de 1941, con el objeto de conseguir materiales para las fortificaciones de la frontera germano-soviética de Polonia. Un año después, en junio de 1942, se empezó a reconvertir en campo de exterminio por parte de las SS, siguiendo las indicaciones del constructor de Sobibor, Richard Thomalla. Contó con tres cámaras de gas, que estaban situadas en una edificación oculta en la zona más elevada del campo, a la que se llegaba desde una estación por una vereda, llamada por las SS «el camino hacia el cielo». Los gases provenían de motores diésel. En la parte de atrás, había un grupo de zanjas para sepultar los cadáveres.

Los gaseamientos se iniciaron el 23 de julio. Una media de 5000 judíos llegaron al día a Treblinka durante las primeras semanas; sin embargo, desde agosto el número aumentó considerablemente, de forma que a finales de mes ya habían sido gaseados un total de 312 000 judíos. Por otro lado, miles de judíos murieron durante los transportes en tren, sin ventilación, sin agua y sin servicios sanitarios, y con un tiempo caluroso. Además, y según el testimonio de un superviviente, Oskar Berger, que llegó al campo el 22 de agosto,


En los casos de grandes cantidades de judíos llegados, muchos eran fusilados en la zona de recepción; en ocasiones, los trenes debían esperar llenos durante días, hasta que los judíos podían ser llevados a las cámaras de gas, que o bien no daban a basto, o bien se habían estropeado. En el mismo sentido, la excavación de zanjas no podía seguir el ritmo de los asesinatos, y los cuerpos quedaban habitualmente sin enterrar.

Ese mismo mes de agosto, se nombró a Christian Wirth inspector general de los tres campos para que se encargase de racionalizar las operaciones de matanza. Wirth entregó, a su vez, a Franz Stangl, comandante ya en Sobibor, el mando de Treblinka en septiembre. La apariencia externa del campo mejoró, pero las escenas de sadismo y crueldad continuaron.

La incineraciones de cuerpos comenzaron en diciembre, en Chelmno y Belzec, y en abril de 1943 en Treblinka. A finales de julio, con la orden de Himmler de cerrar los campos dado que la inmensa mayoría de los judíos de los guetos habían sido asesinados, ya se habían desenterrado e incinerado unos 700 000 cuerpos sepultados en fosas comunes.[262]​

Los cálculos modernos cifran el número de asesinados en los tres campos de la operación Reinhard en 1 700 000.

La creación de campos de concentración por parte de los alemanes desde septiembre de 1939 fue algo habitual en los territorios ocupados. Uno de ellos, cerca de la localidad de Oswiecim, en alemán «Auschwitz», se creó en abril de 1940 con el objetivo de albergar presos políticos polacos. En mayo, se nombró a Rudolf Höss comandante del mismo, quien colocaría a la entrada un cartel con el lema Arbeit macht frei, «el trabajo libera». Tras un periodo en que el campo constituía un lugar para reclutar trabajadores, finalmente se convirtió en un centro permanente para presos políticos polacos. Posteriormente, a partir de septiembre de 1941, tras el inicio de la operación Barbarroja, se le fueron añadiendo nuevos campos asociados, como por ejemplo el situado en Birkenau, para prisioneros de guerra soviéticos, que terminaría siendo el más grande, pues era también un campo de concentración y de trabajo industrial.[263]​

Probablemente a principios de 1942, Himmler anunció a Höss que el campo debía convertirse en un centro adicional a los de la operación Reinhard, que no eran suficientes para completar la «solución final»; Auschwitz estaba bien comunicado y, al tiempo, suficientemente alejado de núcleos de población importantes. Según indicaciones posteriores de Eichmann, la función de Auschwitz habría de ser la de matar a los judíos del resto de Europa. Los primeros deportados, llegados en marzo, provenían de Eslovaquia y Francia.

Los métodos empleados fueron desde el principio diferentes a los de otros campos. En julio de 1941 se había descubierto casualmente (durante una desinfección de ropa) que el pesticida químico conocido como Zyklon-B había sido capaz de matar rápidamente a un gato. En septiembre se probó con 600 prisioneros de guerra soviéticos (clasificados como «fanáticos comunistas») y 250 enfermos del campo, que fueron gaseados en un sótano del campo. El mismo mes, otros 900 prisioneros soviéticos fueron gaseados en el depósito de cadáveres. Tras una visita de Einchmann, se decidió usar el gas de forma sistemática.

Dado que los gritos de los gaseados podían ser oídos por el personal del campo, se decidió realizar las matanzas en Auschwitz-Birkenau. Allí se construyeron dos cámaras de gas (para 800 y 1200 personas) que empezaron a funcionar el 20 de marzo de 1942. A ellas se enviaba directamente a los menores de 16 años, las madres con hijos, los enfermos, los ancianos y los físícamente débiles. El resto pasaba al campo, tatuados todos con un número de serie en el brazo izquierdo y registrados. Posteriormente, selecciones periódicas iban decidiendo la muerte de los que ya no estaban en condiciones de trabajar.

El exterminio sistemático de judíos (franceses, eslovacos, polacos, belgas y holandeses) dio comienzo en mayo. Paralelamente, en julio de 1942, Himmler ordenó que el reasentamiento de toda la población judía se completase antes del final del año. En su visita de ese mismo mes a Auschwitz, ordenó que se intensificasen las matanzas y apremió a Höss en lo relativo a las construcción del campo en Birkenau. Por su parte, en su discurso anual ante los antiguos combatientes nazi, Hitler afirmó en noviembre que la guerra habría de finalizar con el «exterminio» de los judíos, una palabra que se le escaparía a Goebbels en un discurso radiado de febrero de 1943; en mayo de 1943, y en palabras dirigidas al mismo Goebbels, Hitler estaba ya justificando el exterminio de los judíos como una condición necesaria para que el dominio alemán se extendiese al mundo entero. Por lo demás, la propaganda antisemita se recrudeció por esas fechas.[264]​

Desde julio, empezaron a llegar judíos de Alemania; después, desde casi todos los países europeos: Rumanía, Croacia, Finlandia, Noruega, Bulgaria, Italia, Hungría, Serbia, Dinamarca, Grecia y el sur de Francia. En este contexto, entre 1942 y 1943 se ampliaron y perfeccionaron las instalaciones para gasear en Auschwitz. En junio de 1943 había una cámara más y se había ampliado a 4 el número de crematorios, conforme a los planes de la empresa Hoch-und Tiefbau AG Kattowitz; los hornos crematorios y las instalaciones de gaseamiento habían sido fabricados por la empresa J. A. Topf & Söhne de Erfurt.

El procedimiento de gaseamiento era el siguiente:


Cuando los crematorios se vieron desbordados por el número de cadáveres acumulados, la instalación se resintió y hubo que retomar la práctica anterior de quemarlos sobre parrillas extendidas sobre zanjas.

En Auschwitz murieron entre 1 100 000 y 1 500 000 personas. Alrededor del 90 % (cerca del millón) eran judías, lo que supone entre una quinta y una cuarta parte de los judíos que murieron durante la guerra. Por lo menos, la mitad de los muertos lo fueron por desnutrición, enfermedades, agotamiento e hipotermia.

Auschwitz recibía a muchos de sus judíos desde el campo de Theresienstadt. Construido desde noviembre de 1941 al norte de Terezin (en alemán, Theresienstadt), en Checoslovaquia, este campo constituía la prisión central de la Gestapo en el Protectorado. Recibió a 10 000 judíos checos durante los primeros días del año siguiente y se trataba inicialmente de un centro de reagrupamiento organizado como un gueto. De las 140 000 personas que llegaron deportadas al campo, al final de la guerra solo seguían con vida menos de 17 000.

Otro campo, el de Majdanek, se construyó en la parte oriental de Lublin; desde julio de 1942 se construyeron en él hasta siete cámaras de gas. Al final, habrían muerto en él unas 180 000 personas; 120 000 de ellas, judíos.

En octubre de 1942, Heinrich Himmler determinó que todos los judíos debían ser trasladados a Auschwitz o Majdanek. Ejecuciones masivas tuvieron lugar entre el 8 de mayo y el 29 de julio de 1944. Rudolf Höss, por orden de Heinrich Himmler, debía gasear a más de 400 000 judíos húngaros en Auschwitz. En determinados días fueron asesinadas cerca de 24 000 personas, muchas de las cuales fueron quemadas en hogueras al aire libre por la escasa capacidad de los crematorios.[263]​


Rudolf Höss cuenta en sus memorias que en el verano de 1941 fue recibido personalmente por Himmler y este le dijo: 

 Al finalizar la cita, le exigió guardar silencio, incluso ante sus superiores.[267]​

Pero Eichmann confesó en 1961, durante su proceso en Jerusalén, que durante esta conferencia "se estudiaron con rigor los [más efectivos] métodos para exterminar a todo el pueblo judío que vivía en Europa".[267]​

En su totalidad, la "solución final" incluyó el exterminio de numerosos judíos europeo por gaseamiento, fusilamiento y otras medidas de asesinato en masa. En la Europa ocupada por los nazis, vivían aproximadamente unos ocho millones de judíos.[268]​ Algo más de seis millones de judíos murieron, o sea, dos tercios de todos los judíos que vivían en Europa en 1939.[229]​[269]​[268]​

Junto con los judíos, otros grupos humanos como gitanos, soviéticos (especialmente, los prisioneros de guerra), comunistas, Testigos de Jehová, polacos étnicos, pueblos eslavos, discapacitados,  hombres homosexuales y disidentes políticos y religiosos, fueron también objeto de persecución y asesinato durante el nazismo.[270]​

Según el criterio más o menos restringido que se adopte para definir el Holocausto, la cifra de víctimas varía. Algunos historiadores lo circunscriben al genocidio de judíos a manos del Tercer Reich (algo más de 6 millones de víctimas).[268]​[269]​[271]​[272]​ Otros estudiosos consideran que debe aplicarse asimismo a las víctimas polacas y a otros pueblos eslavos y gitanos. Un tercer grupo amplía el término para que abarque igualmente a los homosexuales, los disminuidos físicos y mentales y los Testigos de Jehová, de modo que se estiman en 11 o 12 millones las víctimas del Holocausto, de las cuales más de la mitad eran judíos.[273]​

Se calcula que murieron víctimas de este exterminio algo más de seis millones de judíos,[268]​[269]​ aparte de unos 800 000 gitanos, cuatro millones de prisioneros de guerra soviéticos o víctimas de la ocupación (fueron también objeto de exterminio sistemático), polacos e individuos calificados de asociales de varias nacionalidades (presos políticos, homosexuales, discapacitados físicos o psíquicos, delincuentes comunes, etc.). Las aproximaciones oficiales son las siguientes:

De acuerdo con lo anterior, ni el decreto que impuso a los judíos la estrella amarilla, ni las primeras deportaciones efectuadas en el otoño de 1941 (e invierno de 1941-1942), fueron hechos respondidos por la población alemana de manera significativa. Sin embargo, fue la respuesta popular contraria la que consiguió parar la retirada de crucifijos en Baviera y evitar la gasificación de miles de enfermos mentales.[276]​

En cuanto a la reacción ante el genocidio,


En cuanto a las gasificaciones, fueron llevadas mucho más en secreto y tuvieron poco eco dentro de Alemania.

Con todo, mucha gente de Alemania consideró en su momento que los bombardeos aliados sobre sus poblaciones eran una venganza y un desquite por el trato dado a los judíos.

Los rumores en Alemania sobre el destino de los judíos fueron generalizados y contenían datos suficientes como para entender que en el este se estaba produciendo un asesinato masivo de judíos. Incluso, el conocimiento de las gasificaciones y del exterminio en los campos fue relativamente limitado.[278]​

Y aunque la planificación y ejecución de la "solución final" se llevó con un grado muy elevado de secretismo, lo que probablemente demuestra que los jerarcas nazis eran conscientes de que no podían contar para ello con el respaldo popular, la misma no


La exacta valoración de la actitud de los alemanes ante el destino de los judíos ha provocado divergencias entre algunos historiadores. Ian Kershaw ha insistido en el concepto de «indiferencia moral», que se reflejó en el hecho de que los alemanes apartaron la vista deliberadamente eximiéndose de cualquier responsabilidad personal. La razón principal habría sido que la población aceptó con naturalidad el derecho del Estado a decidir sobre la Cuestión Judía, un asunto que para ellos tendría poca relevancia personal.[280]​ Por su parte,


Sin embargo, debe profundizarse todavía el estudio sobre la actitud de la población europea y alemana en particular. Muchos partidarios de los nazis se enriquecieron por la persecución a los judíos, recibiendo beneficios, bienes y propiedades. (puede consultarse Los verdugos voluntarios de Hitler).

Respecto de los propios judíos, fueron varios los impedimentos con los que se encontraron para planificar o idear una resistencia ante las acciones genocidas de los nazis: en primer lugar, su subestimación del peligro que éstos suponían cuando Hitler llegó al poder, es decir, no reaccionaron a tiempo ante la propagación del terror;[282]​ en segundo lugar, el nazismo se esforzó constantemente en alentar falsas expectativas, ilusionando muchas veces a sus víctimas con la idea de que la sumisión y el trabajo podía ser causa de su salvación; en tercer lugar, que la idea misma del exterminio total resultaba más bien producto de una imaginación enferma que de un plan con alguna posibilidad de hacerse realidad; en cuarto lugar, que la aplicación sistemática de castigos terribles e indiscriminados por parte de los alemanes ante cualquier amago de rebelión ejercía un serio efecto de intimidación; en quinto lugar, que el ambiente antisemita y colaboracionista de muchos de los países europeos (sobre todo de Europa oriental) durante la guerra, hacían muy dificultosa una escapatoria a través de ellos para cualquier judío; y, en sexto lugar, que el grado de agotamiento físico y psicologíco de los judíos, en guetos, campos, etc., era de tal envergadura, que dificultaba enormemente cualquier expectativa que fuese más allá de garantizar la supervivencia del día a día.

Con todo, y a pesar de esta situación de enorme desventaja en la que se encontraron, hubo diversos casos de resistencia.

Durante los años previos a la guerra, hubo judíos que intentaron organizar grupos para hacer frente a la catarata legislativa antijudía. Destacó entre ellos el conocido como «Grupo Baum», liderado por Herbert Baum, que durante 1937 se reunió semanalmente en Berlín y que llegó a realizar sabotajes contra el nazismo.

También a través de la Conferencia de Evian, en julio de 1938, hubo un intento por dar una solución a la población judía migrante de Alemania y Austria. 

Ya en plena época de exterminio, el joven líder de la resistencia judía Abba Kovner lanzó en la noche del 31 de diciembre de 1941 un manifiesto en el que proclamaba que Hitler planificaba la destrucción de todos los judíos de Europa; se trata de la primera llamada pública a la resistencia.[283]​ Desde el día siguiente, quedó organizada la resistencia en el interior del gueto de Vilna, que sería el primero en sublevarse. En este sentido, los judíos se sublevaron en unos veinte guetos de Europa oriental, primero el de Vilna en Lituania en enero de 1942, y luego en guetos como los de Varsovia (entre el 19 de abril y el 15 de mayo de 1943) y Bialystok y, más tarde, en diversos campos de exterminio.


De Vilna lograron escapar algunos combatientes judíos en el verano de 1943, tras lo cual formaron unidades partisanas para ayudar a la liberación de la ciudad.

La revuelta más conocida fue la sublevación del gueto de Varsovia, que duró casi un mes, entre el 19 de abril y el 15 de mayo de 1943 y que estuvo protagonizada por la Organización Judía Combatiente, compuesta por unos 600 miembros y dirigida por Mordechai Anielewicz, de 24 años de edad, y la Organización Militar Nacional, con 400 miembros. El gueto fue finalmente arrasado por las fuerzas alemanas, muriendo unos 15 000 judíos y siendo enviados posteriormente a campos de exterminio más de 50 000.

Asimismo, se produjeron diversas revueltas de prisioneros en los campos de exterminio, incluidos los de Auschwitz (donde se voló un horno crematorio) y Treblinka, donde en agosto de 1943 tuvo lugar una importante sublevación. El 14 de octubre se rebelaron los prisioneros de Sobibor, y dos días más tarde hubo de cerrarse el campo, tras conseguir escapar al menos un centenar de ellos.

Más allá de los campos y los guetos, muchos judíos se alistaron en los grupos de partisanos que lucharon contra los nazis en los bosques de Ucrania y Polonia, en los montes Cárpatos, en Bielorrusia y en Lituania. Especialmente conocida fue la Brigada Judía liderada por Abba Kovner y que actuó en los bosques cercanos a Vilna. Hubo también grupos resistentes en Bialystok, Kovno y Minsk.

En Europa occidental y meridional, participaron en grupos de resistencia en casi todos los países, llegando a constituir en algún momento el 15% de los resistentes en Francia.

En Alemania, a pesar de las extraordinarias limitaciones, probablemente unos dos o tres mil judíos se involucraron activamente en el movimiento antinazi clandestino alemán.

En conclusión,


La eliminación física de los judíos se realizó de forma sistemática, meticulosa y efectiva conforme a una estrategia bien elaborada que se ha llegado a calificar de «industrial». De hecho, ningún otro genocidio en la historia se ha llevado a cabo mediante medios mecánicos en instalaciones especialmente construidas, como las cámaras de gas que funcionaron en Auschwitz o Treblinka. Con todo, y a pesar de las declaraciones de los nazis en el sentido de que veían a sus víctimas más como cargas o piezas que como seres humanos, existió una clara implicación emocional en muchos de ellos ante la matanza ininterrumpida de civiles indefensos que realizaron.

Por otro lado, la creencia generalizada en ellos era que estaban cumpliendo órdenes de Hitler, y que el objetivo era acabar con los enemigos no solo presentes, sino futuros, de la raza alemana. En este sentido, la característica de los dirigentes nazis era que compartían un antisemitismo exacerbado, no menor que el de sus subordinados. Así,


Ello no quiere decir necesariamente que el Holocausto tuviera un plan definido desde el principio: precisamente este es uno de los puntos que divide a los estudiosos, entre intencionalistas y funcionalistas:

En términos generales, la estructura del Holocausto fue la siguiente:

Dos elementos distinguen al Holocausto de otros casos de genocidio o asesinatos masivos:

El primer elemento es la ideología nazi, la cual es fervientemente nacionalista, aunque de corte político centralizado con un componente mítico añadido, que divide al mundo en cuatro categorías:

El discurso y la estructura ideológica nazi están cargados de significación religiosa y mitológica.

El segundo elemento es la sistematización de los procesos de asesinatos masivos, los cuales comenzaron con la concentración de la población judía en guetos y posteriormente en campos de concentración y culminó con la implantación de la llamada «solución final al problema judío», que consistió en el asesinato masivo y sistemático de la mayor parte de la población judía europea.

El principal elemento de dicha «solución» fueron los campos de exterminio, los cuales funcionaban como auténticas fábricas de muerte, cuya materia prima era la población a ser exterminada.

Durante el Holocausto, unos seis millones de judíos (alrededor de dos tercios de la población judía mundial de la época) fueron exterminados. En algunos casos desaparecieron comunidades enteras, entre ellas la floreciente comunidad judía de Polonia (de más de tres millones de miembros) y la comunidad sefardí de Salónica (en Grecia).

El número exacto de personas asesinadas durante el régimen nazi no se ha podido determinar, aunque se consideran fiables las siguientes cifras:

En total las víctimas suman una cifra de 15 510 000 a 22 450 000 (quince a veinte millones de personas, aproximadamente).[cita requerida]

El Holocausto dio el empuje final a la creación del Estado de Israel, ubicado sobre parte del territorio del Mandato Británico de Palestina, que acogió a los judíos supervivientes del exterminio.

Algunos sectores minoritarios sostienen que la «solución final» no suponía el exterminio de los judíos, sino que era un plan que pretendía deportar a los judíos de Alemania y de los países ocupados y aliados de Alemania,[287]​ y que a largo plazo suponía la creación de un Estado sionista en la isla de Madagascar (Plan Madagascar), territorio en dominio de Francia y poco poblado en esos momentos[288]​

La idea de que para los nazis la «solución final» no significaba el asesinato sistemático de los judíos,[289]​ sino su desplazamiento hacia el este de Europa, se basa en la reinterpretación de documentos tales como la carta del 31 de julio de 1941 donde Hermann Göring escribió a Reinhard Heydrich lo siguiente:

Martin Luther, funcionario de la cancillería nazi y participante en la conferencia de Wannsee, escribía en un memorándum el 21 de agosto de 1942:

Sin embargo, de acuerdo a la versión mayoritaria con respecto al Holocausto, los términos "evacuación", "desplazamiento", "emigración", "reinstalación", etc. eran palabras clave para ocultar la masacre.[292]​

Éstas y otras razones son esgrimidas por los negacionistas del holocausto, quienes niegan la existencia de Holocausto, llegando a afirmar que se trata de un medio propagandístico del sionismo y de una supuesta conspiración judía.[293]​

Basándose en supuestas investigaciones posteriores a la guerra, afirman que la cifra de judíos muertos en los campos de concentración nazi no es tan elevada,[294]​ y que todo sería un complot para evitar a toda costa el resurgimiento nacionalsocialista.[cita requerida]

La persecución y el genocidio se llevó a cabo por etapas. Las leyes de Núremberg fueron promulgadas años antes del estallido de la Segunda Guerra Mundial. Los campos de concentración nazis fueron creados como lugares donde los reclusos eran utilizados como mano de obra esclava hasta que morían por agotamiento o enfermedad. Allí donde la Alemania Nazi conquistaba nuevos territorios al este de Europa, escuadrones especializados llamados Einsatzgruppen asesinaban judíos y oponentes políticos en fusilamientos masivos. Los judíos y los gitanos fueron encerrados en guetos antes de ser transportados por centenas o millares en trenes de carga hacia campos de exterminio donde, si sobrevivían al viaje, la mayoría de ellos era asesinada en cámaras de gas. Todo el aparato burocrático alemán estuvo involucrado en la logística del asesinato masivo, convirtiendo al país en lo que un académico ha llamado "un Estado genocida".[295]​

A continuación se muestra una lista de los campos de concentración nazis. Estos campos fueron establecidos dentro de Alemania poco después de la ascensión al poder del partido nazi en 1933. Posteriormente se crearían otros campos en aquellos países anexionados o invadidos por Alemania antes y durante el transcurso de la Segunda Guerra Mundial, como Países Bajos y Polonia. Mientras que algunos campos tuvieron una existencia más bien efímera, otros permanecieron en activo hasta la definitiva derrota alemana en la guerra.

Los campos de exterminio se encuentran marcados en color rosa, mientras que los mayores campos de otros tipos están señalados con color azul.

En Buchenwald:

En Auschwitz:

En Mauthausen-Gusen:

La escritura es un sistema de representación gráfica de un idioma, por medio de signos trazados o grabados sobre un soporte. En tal sentido, la escritura es un modo gráfico específicamente  humano de conservar y transmitir información.

Como medio de representación, la escritura es una codificación sistemática mediante signos gráficos que permite registrar con gran precisión el lenguaje hablado por medio de signos visuales regularmente dispuestos; obvia excepción a esta regla es la bastante moderna escritura Braille cuyos signos son táctiles. La escritura se diferencia de los pictogramas en que estos no suelen tener una estructura secuencial lineal evidente.

Existen dos principios generales en la escritura, por un lado la representación mediante logogramas que pueden representar conceptos o campos semánticos, y por otro lado la codificación mediante grafemas que representan sonidos o grupos de sonidos (pudiéndose distinguir entre sistemas puramente alfabéticos, abugidas, silábicos, o mixtos). Las escrituras egipcia y china combinan ambos tipos de principios (logogramas y grafemas), mientras que las escrituras en alfabeto latino son puramente grafémicas.

Las escrituras jeroglíficas son las más antiguas de las escrituras propiamente dichas (por ejemplo; la escritura cuneiforme fue primeramente jeroglífica hasta que a ciertos jeroglifos se les atribuyó un valor fonético) y se observan como una transición entre los pictogramas y los ideogramas. La escritura jeroglífica fue abandonada en el período helenizante de Egipto. En la actualidad la escritura china y japonesa conservan algunos logogramas combinados con signos cuya interpretación es puramente fonética. La mayor parte de las escrituras del mundo son puramente grafémicas, así las escrituras románicas (basadas en el alfabeto latino), arábigas (basadas en el alfabeto arábigo), cirílicas (basadas en el alfabeto griego), hebraicas (basadas en el alfabeto hebreo), helénicas (basadas en el alfabeto griego), indias (generalmente basadas en el devanāgarī), y en mucha menor medida las escrituras alfabéticas arameas, siríacas, armenias, etiópicas (abugidas basadas en el ghez o ge'ez), coreanas, georgianas, birmanas, coptas, tibetanas, etc. Los alfabetos glagolíticas y el alfabeto rúnico que precedió a la escritura gótica, así como la pahlavi y zend usadas en lenguas hoy desaparecidas.

Aunque de las escrituras alfabéticas quizás la primera haya sido la escritura protosinaítica, documentada entre los siglos XVIII y XVI a. C., la primera escritura alfabética stricto sensu parece haber sido la escritura fenicia. Esta —al igual que sus inmediatas derivadas— es del tipo abyad, es decir solo constaba de consonantes, el método de escritura abyad también se encuentra en otros casos, como en la escritura aramea, la escritura hebrea basada en el alefbet o álef-bet, y la escritura arábiga; si bien la escritura hebrea precisó el valor de los fonemas utilizando el sistema de puntos diacríticos llamado masorético, a través del cual diversos puntos en relación a cada grafema tienen la función de vocales, también algo semejante ocurre con el alifato o alfabeto árabe.

La escritura fenicia fue modificada y adaptada por los griegos; a los griegos se les atribuye la notación explícita de las vocales (concretamente usaron algunos signos consonánticos del fenicio sin equivalente en griego, como signos para notar las vocales). No obstante, conviene notar que antes del sistema clásico de alfabeto griego, en el espacio geográfico que luego sería helénico existieron las escrituras minoicas (tipo lineal A y lineal B), así como el uso de la escritura en bustrofedon (‘arado de buey’), pues se considera que la escritura etrusca e indudablemente la escritura latina (de la cual proviene el alfabeto más usado actualmente), son modificaciones de la escritura alfabética griega (lo mismo que ocurre con el cirílico y el glagolítico). En cuanto a las escrituras ibéricas antiguas, estas parecen haber recibido un fuerte influjo fenicio, hasta el momento de ser suplantadas por las letras latinas.

En los territorios controlados por los celtas, en la antigüedad existió un sistema de escritura muy singular llamado ogam, usado principalmente por los druidas.

Por su parte y en lo atinente a las runas de los antiguos germanos, en ellas se nota una copia modificada de las letras latinas a las cuales sin embargo se les asignaron diversos valores fonéticos e inclusos "valores mágicos".[a]​
Algo similar ocurrió a fines del siglo XVIII entre los cheroquis de Norteamérica, entre ellos, el jefe Sequoyah promovió el uso de un "alfabeto" (en realidad un silabario) inspirado por sus formas en el alfabeto latino usado por los colonos anglosajones, aunque con diferentes valores fonéticos.

En cuanto a las escrituras con un componente ideográfico, prácticamente se restringen en la actualidad a la escritura china basada en sinogramas; estos mismos signos, llamados hànzì (en chino simplificado, 汉字; en chino tradicional, 漢字; literalmente «carácter han» o «carácter chino»), también se usan en la escritura japonesa junto a dos silabarios, el hiragana y el katakana; los sinogramas también forman parte tradicionalmente de la escritura del coreano con el nombre de hanja (china escritura), aunque actualmente apenas se usan ya estando suplantados en Corea por el alfabeto hangul.

Como bien ha señalado Roland Barthes [1]​ la escritura ha significado una revolución en el lenguaje y en el psiquismo y, con ello, en la misma evolución humana, ya que es una "segunda memoria" para el ser humano —además de la biológica ubicada en el cerebro—. Esto es tan evidente que se distingue la prehistoria de la historia, porque en la primera se carecía de escritura y solo existía la tradición oral.

La lengua oral constituida por una "sustancia fónica" tiene en tal sustancia un soporte efímero y requiere que el emisor y el receptor coincidan en el tiempo (y antes de la invención de las telecomunicaciones, también era necesaria la coincidencia en el lugar). En cambio, con la lengua escrita siempre es posible establecer una comunicación con mensajes diferidos (la praxis escritural hace que el mensaje pueda ser realizado in absentia del receptor y conservado a través del tiempo).[2]​ Para Vygotski, el lenguaje escrito es el paso del lenguaje abstracto al lenguaje que utiliza la representación de las palabras (considerado como la traducción o codificación del lenguaje oral), esto contra el supuesto muy extendido en la cultura en general y hasta inicios del presente siglo particularmente  en la educación, de que el escrito es una mera traducción o codificación del lenguaje oral. Por el contrario, Vygotski sostiene que el lenguaje oral genera construcciones de un determinado tipo (dialógicas, etc.) en la conciencia, y que el lenguaje escrito las produce de otro tipo, de modo que los procesos psicológicos del lenguaje oral y del escrito son distintos, y que por ende, también son distintos los procesos psíquicos que se involucran en la educación con uno u otro tipo de lenguaje.[3]​

Actualmente, los semiólogos y los lingüistas consideran totalmente probado que la escritura es posterior al habla, aunque algunos semiólogos a fines de siglo XX llegaron a suponer que las escrituras son previas al lenguaje verbal articulado ya que existe un placer (por usar un término barthesiano) por parte del sujeto humano en dejar rastro de sí en diversos soportes (huellas de manos, muescas, rayas, representaciones más o menos figurativas tal cual se observa en el Magdaleniense), pero tales protoescrituras no parecen ser indicios de que los textos escritos se anticiparon al habla; en todo caso, con la escritura como «segunda memoria» el habla fue reforzada por los escritos; más aún, la escritura permite una reflexión adicional, y esto hace que el lenguaje escrito pueda tener una clara estrategia de la cual carece el lenguaje oral ágrafo.[4]​

[b]​ Como en latín dijo Cayo Tito: Verba volant scripta manent (las palabras se vuelan lo escrito se mantiene).

Fundamentalmente, la lengua gráfica o la lengua escrita ha de considerarse un fenómeno lingüístico inventado por la sociedad humana para reemplazar a la lengua oral o fónica, la escritura aparece necesariamente cuando la evolución socioeconómica de las poblaciones impulsa la creación de un código alternativo que sea eficaz en situaciones en las cuales la lengua fónica es insuficiente o directamente inútil.[2]​[c]​

Sin embargo el pasaje del lenguaje ágrafo al escritural tuvo una fase de transición: antes de la escritura propiamente dicha están los pictogramas y los grafismos, solo hace poco más de cinco milenios aparecen las primeras escrituras en Sumeria y en el antiguo Egipto.[5]​

Existen diversos hallazgos de representaciones gráficas previas a la escritura propiamente dicha, como los de las cuevas de Chauvet (1995), Cosquer (1994) o Lascaux (1940) en Francia, con imágenes que datan de 31 000, 24 000 y 15 000 años aproximadamente de antigüedad, respectivamente, o la cueva de Altamira (descubierta en 1868). El desarrollo de la escritura pudo tener motivaciones y funciones completamente diferentes de las que llevaron a crear otro tipo de representaciones gráficas.[cita requerida]. Investigaciones que vienen realizándose desde finales del pasado siglo, han permitido la compilación de un signario nuclear básico de unos ochenta y ocho signos lineales que fueron usados para grafiar o escribir secuencias ordenadas que combinan y articulan signos como en cualquier escritura de signos lineales y geométricos, lo que ha permitido el desarrollo de una hipótesis sobre el uso de una Escritura Lineal Paleolítica (ELPA) logofonográfica o glotográfica durante el Paleolítico Superior, al parecer ya desde tiempos del Auriñaciense, o con mayor probabilidad desde el Solutrense.[6]​

Se ha observado el uso de tales signos lineales de una posible escritura lineal paleolítica no solo en la zona astur-cántabro-aquitana o franco-cantábrica, sino también en cuevas del sur de la península, concretamente en las cuevas de la Pileta y Nerja en Málaga.[7]​

La invención de la escritura se dio en varios lugares del mundo de manera independiente. Las primeras técnicas de escritura se remontan al cuarto milenio a. C. Surgió en Egipto, Mesopotamia y China. El sistema creado en Oriente Medio y Egipto se extendió rápidamente a las áreas culturales cercanas y es el origen de la mayoría de las escrituras del mundo. En América la escritura también apareció en Mesoamérica, teniendo como uno de sus primeros ejemplos conocidos los jeroglíficos de la escritura maya.

Se le atribuye a la escritura la historia siguiente: Las transacciones entre tierras alejadas y diferidas en el tiempo necesitaban plasmarse en contratos.[cita requerida]
Estos contratos se fundamentaban en unas bolas huecas de arcilla que contenían los datos, pequeñas formas de arcilla que simbolizaban los nombres de tres maneras diferentes: esferas, conos y cilindros a los que se añadían unas formas convencionales que designaban aquello que se contrataba.[cita requerida]
En caso de reclamación se rompía la bola seca, sobre la cual se había firmado con su sello para su control, y en la que se comparaba la cantidad y la entrega.[cita requerida]

Estas transacciones fueron puestas en forma de escuadra. Este era el medio para dibujar una cuña, un redondel y un cono, que representaban los datos y servían también para dibujar las formas convencionales. Finalmente se encontró la solución más simple: aplastar esta bola de arcilla y dibujar (escribir) en ambas caras el contenido del contrato: qué, cuánto y cuándo utilizando, siempre, esta pequeña caña.[cita requerida]

En Egipto se han encontrado placas de marfil y hueso probablemente indicativas del contenido o del origen de mercancías con una antigüedad de unos 5400 años.[5]​

La escritura evolucionó desde sistemas de representación meramente mnemotécnicos o contables (como está testimoniado en Mesopotamia), que inicialmente representaban objetos en forma de pictogramas, hasta sistemas más abstractos que acabaron representando sonidos o logogramas abstractos. Obviamente en ese sentido toda la escritura es dependiente de las lenguas naturales, tal como señaló el propio Aristóteles para quien la escritura está subordinada a la lengua hablada:


Esto es, para la tradición aristotélica, la escritura es un conjunto de símbolos de otros símbolos. Para esta tradición lo escrito no representa directamente a los conceptos sino a las palabras fónicas con las cuales se denominan a los conceptos. Tal tradición aristotélica ha implicado un fonocentrismo que inhibió muchas veces el estudio lingüístico de la escritura y puso el acento en la fonología, esto fue criticado particularmente por Jacques Derrida a fines del siglo XX, este pensador ha considerado de especial importancia a las escrituras.[9]​ 

La escritura ha evolucionado a través del tiempo. Fundamentalmente ha usado dos principios:

Tanto los sistemas jeroglíficos sumerios y egipcios, como en la escritura china se encuentran conjuntamente signos que siguen el principio ideográfico junto a signos que siguen el principio fonético.

No existe ningún sistema de escritura pleno que sea puramente ideográfico. El idioma chino es citado como ejemplo de escritura puramente ideográfica, pero eso no es exacto, puesto que un buen número de los signos son "complementos fonéticos" que tienen que ver más con el sonido de la palabra que con una representación pictográfica del referente. Algo similar sucede en la escritura jeroglífica egipcia, donde muchas palabras se escriben mediante signos monolíteros, bilíteros o trilíteros junto a un complemento semántico. Los "signos n-líteros" siguen el principio fonético, mientras que los complementos semánticos siguen el principio ideográfico, al menos parcialmente.

En la escritura se observa la complementariedad de dos códigos, el de la lengua hablada y el de la lengua escrita; ambos códigos conforman una estructura semiótica en la cual se vinculan dos universos de discurso: la estructura precisa de la lengua hablada consta de significados y de sus expresiones fónicas, los significantes; la lengua escrita, al ser complementaria de la oral, cuenta también con significados, siendo sus significantes de tipo gráfico. Se constata que ambos códigos (el oral o fónico por una parte y el escrito por la otra) poseen un mismo universo de contenido: el universo de contenido de la lengua gráfica es el mismo que el de la lengua hablada correspondiente.[2]​

Por otra parte, los sistemas formales como la notación matemática más abstracta son sistemas derivados de la escritura (inicialmente la notación matemática consistió en abreviaciones de expresiones habladas), sin embargo, en su uso moderno la notación matemática avanzada permite expresar nociones que en lenguaje hablado frecuentemente son más complicadas de expresar, por lo que en cierto modo los sistemas gráficos formales han dejado de estar subordinados a la lengua hablada, por más que las nociones expresadas en ellos se pueden traducir de forma aproximada a palabras en forma informal.

Otro asunto relacionado con la relación entre lengua hablada y escrita, es que ningún sistema ortográfico es igual de expresivo que la lengua hablada. Las lenguas naturales, pueden expresar silencios, pausas, y entonaciones que solo se pueden representar muy imperfectamente en la escritura. Por otra parte, las variantes habladas pueden reflejar diferencias sociales y dialectales muy sutiles, y reconocibles por los hablantes, que no son sencillas de representar en un sistema de escritura práctico.

Un sistema de escritura permite la escritura de una lengua. Si se refiere a una lengua hablada, como es lo normal y corriente, se habla entonces de "escritura glotográfica" (pero puede tratarse también de una lengua no hablada, en este caso se hablaría de "escritura semasiográfica")[d]​ Las escrituras glotográficas ordinarias pueden estar divididas en dos grandes grupos:

Un mismo sistema puede servir para muchas lenguas y una misma lengua puede estar representada por diferentes sistemas. Los grafemas fundamentales de una escritura pueden completarse con la utilización de diacríticos, de ligaduras y de grafemas modificados.

Desde la psicología, Gordon Wells (1987) explora el concepto de lo escrito e identifica cuatro niveles de uso, que no se deben considerar exactamente funciones en el sentido lingüístico: ejecutivo, funcional, instrumental y epistémico.[e]​

La taxonomía (clasificación científica) de funciones lingüísticas de M. A. K. Halliday (1973) distingue dos categorías en el nivel epistémico: el uso heurístico y el imaginativo.
Florian Coulmas (1989, págs. 13-14) se refiere a esta última función como estética, además de incluir otra con la denominación de control social.

Después de estas consideraciones, podemos distinguir y clasificar los siguientes tipos de funciones:

La primera distinción será entre usos individuales (intrapersonales) o sociales (interpersonales):

La energía nuclear o atómica es la que se libera espontánea o artificialmente en las reacciones nucleares. Sin embargo, este término engloba otro significado que es el aprovechamiento de dicha energía para otros fines, tales como la obtención de energía eléctrica, térmica y mecánica a partir de reacciones atómicas.[1]​ De esta manera, es común referirse a la energía nuclear no solo como el resultado de una reacción, sino como un concepto más amplio que incluye los conocimientos y técnicas que permiten la utilización de esta energía por parte del ser humano.

Estas reacciones se dan en los núcleos atómicos de algunos isótopos de ciertos elementos químicos (radioisótopos), siendo la más conocida la fisión del uranio-235 (235U), con la que funcionan los reactores nucleares, y la más habitual en la naturaleza, en el interior de las estrellas, la fusión del par deuterio-tritio (2H-3H). Sin embargo, para producir este tipo de energía aprovechando reacciones nucleares pueden ser utilizados muchos otros isótopos de varios elementos químicos, como el torio-232, el plutonio-239, el estroncio-90 o el polonio-210 (232Th, 239Pu, 90Sr, 210Po; respectivamente).

Existen varias disciplinas y/o técnicas que usan de base la energía atómica y van desde la generación de energía eléctrica en las centrales nucleares hasta las técnicas de análisis de datación arqueológica (arqueometría nuclear), la medicina nuclear usada en los hospitales, etc. 

Los sistemas más investigados y trabajados para la obtención de energía aprovechable a partir de la energía nuclear de forma masiva son la fisión nuclear y la fusión nuclear. La energía nuclear puede transformarse de forma descontrolada, dando lugar al armamento nuclear; o controlada en reactores nucleares en los que se produce energía eléctrica, mecánica o térmica. Tanto los materiales usados como el diseño de las instalaciones son completamente diferentes en cada caso.

Otra técnica, empleada principalmente en pilas de mucha duración para sistemas que requieren poco consumo eléctrico, es la utilización de generadores termoeléctricos de radioisótopos (GTR, o RTG en inglés), en los que se aprovechan los distintos modos de desintegración para generar electricidad en sistemas de termopares a partir del calor transferido por una fuente radiactiva.

La energía desprendida en esos procesos nucleares suele aparecer en forma de partículas subatómicas en movimiento. Esas partículas, al frenarse en la materia que las rodea, producen energía térmica. Esta energía térmica se transforma en energía mecánica utilizando motores de combustión externa, como las turbinas de vapor. Dicha energía mecánica puede ser empleada en el transporte, como por ejemplo en los buques nucleares.

La principal característica de este tipo de energía es la alta calidad de la energía que puede producirse por unidad de masa de material utilizado en comparación con cualquier otro tipo de energía conocida por el ser humano, pero sorprende la poca eficiencia del proceso, ya que se desaprovecha entre un 86 % y 92 % de la energía que se libera.[2]​

En las reacciones nucleares se suele liberar una grandísima cantidad de energía debido en parte a que la masa de partículas involucradas en este proceso se transforma directamente en energía. Lo anterior se suele explicar basándose en la relación masa-energía propuesta por el físico Albert Einstein.

En 1896 Henri Becquerel descubrió que algunos elementos químicos emitían radiaciones.[3]​ Tanto él como Marie Curie y otros estudiaron sus propiedades, descubriendo que estas radiaciones eran diferentes de los ya conocidos rayos X y que poseían propiedades distintas, denominando a los tres tipos que consiguieron descubrir: alfa, beta y gamma.

Pronto se vio que todas ellas provenían del núcleo atómico que describió Ernest Rutherford en 1911.

Con el descubrimiento del neutrino, partícula descrita teóricamente en 1930 por Wolfgang Pauli pero no detectada hasta 1956 por Clyde Cowan y sus colaboradores, se pudo explicar la radiación beta.

En 1932 James Chadwick descubrió la existencia del neutrón que Pauli había predicho en 1930, e inmediatamente después Enrico Fermi descubrió que ciertas radiaciones emitidas en fenómenos no muy comunes de desintegración eran en realidad estos neutrones.

Durante los años 1930, Enrico Fermi y sus colaboradores bombardearon con neutrones más de 60 elementos, entre ellos 235Uranio, produciendo las primeras fisiones nucleares artificiales. En 1938, en Alemania, Lise Meitner, Otto Hahn y Fritz Strassmann verificaron los experimentos de Fermi y en 1939 demostraron que parte de los productos que aparecían al llevar a cabo estos experimentos con uranio eran núcleos de bario. Muy pronto llegaron a la conclusión de que eran resultado de la división de los núcleos del uranio. Se había llevado a cabo el descubrimiento de la fisión.

En Francia, Joliot Curie descubrió que además del bario, se emitían neutrones secundarios en esa reacción, haciendo factible la reacción en cadena.

También en 1932 Mark Oliphant teorizó sobre la fusión de núcleos ligeros (de hidrógeno), describiendo poco después Hans Bethe el funcionamiento de las estrellas, basándose en este mecanismo.

En física nuclear, la fisión es una reacción nuclear, lo que significa que tiene lugar en el núcleo atómico. La fisión ocurre cuando un núcleo pesado se divide en dos o más núcleos pequeños, además de algunos subproductos como neutrones libres, fotones (generalmente rayos gamma) y otros fragmentos del núcleo como partículas alfa (núcleos de helio) y beta (electrones y positrones de alta energía).

Durante la Segunda Guerra Mundial, el Departamento de Desarrollo de Armamento de la Alemania nazi desarrolló un proyecto de energía nuclear (Proyecto Uranio) con vistas a la producción de un artefacto explosivo nuclear. Albert Einstein, en 1939, firmó una carta al presidente estadunidense Franklin Delano Roosevelt, escrita por Leó Szilárd, en la que se prevenía sobre este hecho.[4]​

El 2 de diciembre de 1942, como parte del proyecto Manhattan dirigido por J. Robert Oppenheimer, se construyó el primer reactor del mundo hecho por el ser humano (existió un reactor natural en Oklo): el Chicago Pile-1 (CP-1).

Como parte del mismo programa militar, se construyó un reactor mucho mayor en Hanford, destinado a la producción de plutonio, y al mismo tiempo, un proyecto de enriquecimiento de uranio en cascada. El 16 de julio de 1945 fue probada la primera bomba nuclear (nombre en clave Trinity) en el desierto de Alamogordo. En esta prueba se llevó a cabo una explosión equivalente a 19 000 000 kg de TNT (19 kilotones), una potencia jamás observada anteriormente en ningún otro explosivo. Ambos proyectos desarrollados finalizaron con la construcción de dos bombas, una de uranio enriquecido y una de plutonio (Little Boy y Fat Man) que fueron lanzadas sobre las ciudades japonesas de Hiroshima (6 de agosto de 1945) y Nagasaki (9 de agosto de 1945) respectivamente. El 15 de agosto de 1945 acabó la segunda guerra mundial en el Pacífico con la rendición de Japón. Por su parte el programa de armamento nuclear alemán (liderado este por Werner Heisenberg), no alcanzó su meta antes de la rendición de Alemania el 8 de mayo de 1945.

Posteriormente se llevaron a cabo programas nucleares en la Unión Soviética (primera prueba de una bomba de fisión el 29 de agosto de 1949), Francia y Gran Bretaña, comenzando la carrera armamentística en ambos bloques creados tras la guerra, alcanzando límites de potencia destructiva nunca antes sospechada por el ser humano (cada bando podía derrotar y destruir varias veces a todos sus enemigos).

Ya en la década de 1940, el almirante Hyman Rickover propuso la construcción de reactores de fisión no encaminados esta vez a la fabricación de material para bombas, sino a la generación de electricidad. Se pensó, acertadamente, que estos reactores podrían constituir un gran sustituto del diésel en los submarinos. Se construyó el primer reactor de prueba en 1953, botando el primer submarino nuclear (el USS Nautilus (SSN-571)) el 17 de enero de 1955 a las 11:00. El Departamento de Defensa estadounidense propuso el diseño y construcción de un reactor nuclear utilizable para la generación eléctrica y propulsión en los submarinos a dos empresas distintas norteamericanas: General Electric y Westinghouse. Estas empresas desarrollaron los reactores de agua ligera tipo BWR y PWR respectivamente.

Estos reactores se han utilizado para la propulsión de buques, tanto de uso militar (submarinos, cruceros, portaaviones,...) como civil (rompehielos y cargueros), donde presentan unas características de potencia, reducción del tamaño de los motores, reducción de las necesidades de almacenamiento de combustible y autonomía no superadas por ninguna otra técnica existente.

Los mismos diseños de reactores de fisión se trasladaron a diseños comerciales para la generación de electricidad. Los únicos cambios producidos en el diseño con el transcurso del tiempo fueron un aumento de las medidas de seguridad, una mayor eficiencia termodinámica, un aumento de potencia y el uso de las nuevas tecnologías que fueron apareciendo.

Entre 1950 y 1960 Canadá desarrolló un nuevo tipo, basado en el PWR, que utilizaba agua pesada como moderador y uranio natural como combustible, en lugar del uranio enriquecido utilizado por los diseños de agua ligera. Otros diseños de reactores para su uso comercial utilizaron carbono (Magnox, AGR, RBMK o PBR entre otros) o sales fundidas (litio o berilio entre otros) como moderador. Este último tipo de reactor fue parte del diseño del primer avión bombardero (1954) con propulsión nuclear (el US Aircraft Reactor Experiment o ARE). Este diseño se abandonó tras el desarrollo de los misiles balísticos intercontinentales (ICBM). 

Otros países (Francia, Italia, entre otros) desarrollaron sus propios diseños de reactores nucleares comerciales para la generación de energía eléctrica.

En 1946 se construyó el primer reactor de neutrones rápidos (Clementine) en Los Álamos, con plutonio como combustible y mercurio como refrigerante. En 1951 se construyó el EBR-I, el primer reactor rápido con el que se consiguió generar electricidad. En 1996, el Superfénix o SPX, fue el reactor rápido de mayor potencia construido hasta el momento (1200 MWe). En este tipo de reactores se pueden utilizar como combustible los radioisótopos del plutonio, el torio y el uranio que no son fisibles con neutrones térmicos (lentos).

En la década de los 50 Ernest Lawrence propuso la posibilidad de utilizar reactores nucleares con geometrías inferiores a la criticidad (reactores subcríticos cuyo combustible podría ser el torio), en los que la reacción sería soportada por un aporte externo de neutrones. En 1993 Carlo Rubbia propone utilizar una instalación de espalación en la que un acelerador de protones produjera los neutrones necesarios para mantener la instalación. A este tipo de sistemas se les conoce como Sistemas asistidos por aceleradores (en inglés Accelerator driven systems, ADS sus siglas en inglés), y se prevé que la primera planta de este tipo (MYRRHA) comience su funcionamiento el 2033 en el centro de Mol (Bélgica).[5]​[6]​

En física nuclear, la fusión nuclear es el proceso por el cual varios núcleos atómicos de carga similar se unen y forman un núcleo más pesado. Simultáneamente se libera o absorbe una cantidad enorme de energía, que permite a la materia entrar en un estado plasmático.
La fusión de dos núcleos de menor masa que el hierro (en este elemento y en el níquel ocurre la mayor energía de enlace nuclear por nucleón) libera energía en general. Por el contrario, la fusión de núcleos más pesados que el hierro absorbe energía. En el proceso inverso, la fisión nuclear, estos fenómenos suceden en sentidos opuestos.
Hasta el principio del siglo XX no se entendía la forma en que se generaba energía en el interior de las estrellas necesaria para contrarrestar el colapso gravitatorio de estas. No existía reacción química con la potencia suficiente y la fisión tampoco era capaz. En 1938 Hans Bethe logró explicarlo mediante reacciones de fusión, con el ciclo CNO, para estrellas muy pesadas. Posteriormente se descubrió el ciclo protón-protón para estrellas de menor masa, como el Sol.

En los años 1940, como parte del proyecto Manhattan, se estudió la posibilidad del uso de la fusión en la bomba nuclear. En 1942 se investigó la posibilidad del uso de una reacción de fisión como método de ignición para la principal reacción de fusión, sabiendo que podría resultar en una potencia miles de veces superior. Sin embargo, tras finalizar la Segunda Guerra Mundial, el desarrollo de una bomba de estas características no fue considerado primordial hasta la explosión de la primera bomba atómica rusa en 1949, RDS-1 o Joe-1. Este evento provocó que en 1950 el presidente estadounidense Harry S. Truman anunciara el comienzo de un proyecto que desarrollara la bomba de hidrógeno. El 1 de noviembre de 1952 se probó la primera bomba nuclear (nombre en clave Mike, parte de la Operación Ivy o Hiedra), con una potencia equivalente a 10 400 000 000 de kg de TNT (10,4 megatones). El 12 de agosto de 1953 la Unión Soviética realiza su primera prueba con un artefacto termonuclear (su potencia alcanzó algunos centenares de kilotones).

Las condiciones necesarias para alcanzar la ignición de un reactor de fusión controlado, sin embargo, no fueron derivadas hasta 1955 por John D. Lawson.[7]​ Los criterios de Lawson definieron las condiciones mínimas necesarias de tiempo, densidad y temperatura que debía alcanzar el combustible nuclear (núcleos de hidrógeno) para que la reacción de fusión se mantuviera. Sin embargo, ya en 1946 se patentó el primer diseño de reactor termonuclear.[8]​ En 1951 comenzó el programa de fusión de Estados Unidos, sobre la base del stellarator. En el mismo año comenzó en la Unión Soviética el desarrollo del primer Tokamak, dando lugar a sus primeros experimentos en 1956. Este último diseño logró en 1968 la primera reacción termonuclear cuasi-estacionaria jamás conseguida, demostrándose que era el diseño más eficiente conseguido hasta la época. ITER, el diseño internacional que tiene fecha de comienzo de sus operaciones en el año 2016 y que intentará resolver los problemas existentes para conseguir un reactor de fusión de confinamiento magnético, utiliza este diseño.

En 1962 se propuso otra técnica para alcanzar la fusión basada en el uso de láseres para conseguir una implosión en pequeñas cápsulas llenas de combustible nuclear (de nuevo núcleos de hidrógeno). Sin embargo hasta la década de los 70 no se desarrollaron láseres suficientemente potentes. Sus inconvenientes prácticos hicieron de esta una opción secundaria para alcanzar el objetivo de un reactor de fusión. Sin embargo, debido a los tratados internacionales que prohibían la realización de ensayos nucleares en la atmósfera, esta opción (básicamente microexplosiones termonucleares) se convirtió en un excelente laboratorio de ensayos para los militares, con lo que consiguió financiación para su continuación. Así, se han construido el National Ignition Facility (NIF, con inicio de sus pruebas programadas para 2010) estadounidense y el Láser Mégajoule francés (LMJ), que persiguen el mismo objetivo de conseguir un dispositivo que consiga mantener la reacción de fusión a partir de este diseño. Ninguno de los proyectos de investigación actualmente en marcha predicen una ganancia de energía significativa, por lo que está previsto un proyecto posterior que pudiera dar lugar a los primeros reactores de fusión comerciales (DEMO con confinamiento magnético e HiPER con confinamiento inercial).

Con la invención de la pila química por Volta en 1800 se dio lugar a una forma compacta y portátil de generación de energía. A partir de entonces fue incesante la búsqueda de sistemas que fueran aún menores y que tuvieran una mayor capacidad y duración. Este tipo de pilas, con pocas variaciones, han sido suficientes para muchas aplicaciones diarias hasta nuestros tiempos. Sin embargo, en el siglo XX surgieron nuevas necesidades, a causa principalmente de los programas espaciales. Se precisaban entonces sistemas que tuvieran una duración elevada para consumos eléctricos moderados y un mantenimiento nulo. Surgieron varias soluciones (como los paneles solares o las células de combustible), pero según se incrementaban las necesidades energéticas y aparecían nuevos problemas (las placas solares son inútiles en ausencia de luz solar), se comenzó a estudiar la posibilidad de utilizar la energía nuclear en estos programas.

A mediados de los años 1950 comenzaron en Estados Unidos las primeras investigaciones encaminadas a estudiar las aplicaciones nucleares en el espacio. De estas surgieron los primeros prototipos de los generadores termoeléctricos de radioisótopos (RTG). Estos dispositivos mostraron ser una alternativa sumamente interesante tanto en las aplicaciones espaciales como en aplicaciones terrestres específicas. En estos artefactos se aprovechan las desintegraciones alfa y beta, convirtiendo toda o gran parte de la energía cinética de las partículas emitidas por el núcleo en calor. Este calor es después transformado en electricidad aprovechando el efecto Seebeck mediante unos termopares, consiguiendo eficiencias aceptables (entre un 5 y un 40 % es lo habitual). Los radioisótopos habitualmente utilizados son 210Po, 244Cm, 238Pu, 241Am, entre otros 30 que se consideraron útiles. Estos dispositivos consiguen capacidades de almacenamiento de energía 4 órdenes de magnitud superiores (10 000 veces mayor) a las baterías convencionales.

En 1959 se mostró al público el primer generador atómico.[9]​ En 1961 se lanzó al espacio el primer RTG, a bordo del SNAP 3. Esta batería nuclear, que alimentaba a un satélite de la armada norteamericana con una potencia de 2,7 W, mantuvo su funcionamiento ininterrumpido durante 15 años.

Estos sistemas se han utilizado y se siguen usando en programas espaciales muy conocidos (Pioneer, Voyager, Galileo, Apolo y Ulises entre otros). Así por ejemplo en 1972 y 1973 se lanzaron los Pioneer 10 y 11, convirtiéndose el primero de ellos en el primer objeto humano de la historia que abandonaba el sistema solar. Ambos satélites continuaron funcionando hasta 17 años después de sus lanzamientos.

La misión Ulises (misión conjunta ESA-NASA) se envió en 1990 para estudiar el Sol, siendo la primera vez que un satélite cruzaba ambos polos solares. Para poder hacerlo hubo que enviar el satélite en una órbita alrededor de Júpiter. Debido a la duración del RTG que mantiene su funcionamiento se prolongó la misión de modo que se pudiera volver a realizar otro viaje alrededor del Sol. Aunque pareciera extraño que este satélite no usara paneles solares en lugar de un RTG, puede entenderse al comparar sus pesos (un panel de 544 kg generaba la misma potencia que un RTG de 56). En aquellos años no existía un cohete que pudiera enviar a su órbita al satélite con ese peso extra.

Estas baterías no solo proporcionan electricidad, sino que en algunos casos, el propio calor generado se utiliza para evitar la congelación de los satélites en viajes en los que el calor del Sol no es suficiente, por ejemplo en viajes fuera del sistema solar o en misiones a los polos de la Luna.

En 1965 se instaló el primer RTG terrestre para el faro de la isla deshabitada Fairway Rock, permaneciendo en funcionamiento hasta 1995, momento en el que se desmanteló. Otros muchos faros situados en zonas inaccesibles cercanas a los polos (sobre todo en la Unión Soviética), utilizaron estos sistemas. Se sabe que la Unión Soviética fabricó más de 1000 unidades para estos usos.

Una aplicación que se dio a estos sistemas fue su uso como marcapasos.[10]​ Hasta los 70 se usaba para estas aplicaciones baterías de mercurio-zinc, que tenían una duración de unos 3 años. En esta década se introdujeron las baterías nucleares para aumentar la longevidad de estos artefactos, posibilitando que un paciente joven tuviera implantado solo uno de estos artefactos para toda su vida. En los años 1960, la empresa Medtronic contactó con Alcatel para diseñar una batería nuclear, implantando el primer marcapasos alimentado con un RTG en un paciente en 1970 en París. Varios fabricantes construyeron sus propios diseños, pero a mediados de esta década fueron desplazados por las nuevas baterías de litio, que poseían vidas de unos 10 años (considerado suficiente por los médicos aunque debiera sustituirse varias veces hasta la muerte del paciente). A mediados de los años 1980 se detuvo el uso de estos implantes, aunque aún existen personas que siguen portando este tipo de dispositivos.

Sir James Chadwick descubrió el neutrón en 1932, año que puede considerarse como el inicio de la física nuclear moderna.[11]​

El modelo de átomo propuesto por Niels Bohr consiste en un núcleo central compuesto por partículas que concentran la mayoría de la masa del átomo (neutrones y protones), rodeado por varias capas de partículas cargadas casi sin masa (electrones). Mientras que el tamaño del átomo resulta ser del orden del angstrom (10-10 m), el núcleo puede medirse en fermis (10-15 m), o sea, el núcleo es 100 000 veces menor que el átomo.

Todos los átomos neutros (sin carga eléctrica) poseen el mismo número de electrones que de protones. Un elemento químico se puede identificar de forma inequívoca por el número de protones que posee su núcleo; este número se llama número atómico (Z). El número de neutrones (N) sin embargo puede variar para un mismo elemento. Para valores bajos de Z ese número tiende a ser muy parecido al de protones, pero al aumentar Z se necesitan más neutrones para mantener la estabilidad del núcleo. A los átomos a los que solo les distingue el número de neutrones en su núcleo (en definitiva, su masa), se les llama isótopos de un mismo elemento. La masa atómica de un isótopo viene dada por 



A
=
Z
+
N


{\displaystyle A=Z+N}

 u, el número de protones más el de neutrones (nucleones) que posee en su núcleo.

Para denominar un isótopo suele utilizarse la letra que indica el elemento químico, con un superíndice que es la masa atómica y un subíndice que es el número atómico (p. ej. el isótopo 238 del uranio se escribiría como 






92


238



U


{\displaystyle _{92}^{238}\!U}

).

Los neutrones y protones que forman los núcleos tienen una masa aproximada de 1 u, estando el protón cargado eléctricamente con carga positiva +1, mientras que el neutrón no posee carga eléctrica. Teniendo en cuenta únicamente la existencia de las fuerzas electromagnética y gravitatoria, el núcleo sería inestable (ya que las partículas de igual carga se repelerían deshaciendo el núcleo), haciendo imposible la existencia de la materia. Por este motivo (ya que es obvio que la materia existe) fue necesario añadir a los modelos una tercera fuerza: la fuerza fuerte (hoy en día fuerza nuclear fuerte residual). Esta fuerza debía tener como características, entre otras, que era muy intensa, atractiva a distancias muy cortas (solo en el interior de los núcleos), siendo repulsiva a distancias más cortas (del tamaño de un nucleón), que era central en cierto rango de distancias, que dependía del espín y que no dependía del tipo de nucleón (neutrones o protones) sobre el que actuaba. En 1935, Hideki Yukawa dio una primera solución a esta nueva fuerza estableciendo la hipótesis de la existencia de una nueva partícula: el mesón. El más ligero de los mesones, el pion, es el responsable de la mayor parte del potencial entre nucleones de largo alcance (1 rfm). El potencial de Yukawa (potencial OPEP) que describe adecuadamente la interacción para dos partículas de espines 




s

1




{\displaystyle s_{1}}

 y 




s

2




{\displaystyle s_{2}}

 respectivamente, se puede escribir como:

Otros experimentos que se realizaron sobre los núcleos indicaron que su forma debía de ser aproximadamente esférica de radio 



R
=
1
,
5
⋅

A

1

/

3




{\displaystyle R=1,5\cdot A^{1/3}}

 fm, siendo A la masa atómica, es decir, la suma de neutrones y protones. Esto exige además que la densidad de los núcleos sea la misma (



V
α

R

3


α
A


{\displaystyle V\alpha R^{3}\alpha A}

, es decir el volumen es proporcional a A. Como la densidad se halla dividiendo la masa por el volumen 



ρ
=


A
V


=
c
t
e


{\displaystyle \rho ={\frac {A}{V}}=cte}

 ). Esta característica llevó a la equiparación de los núcleos con un líquido, y por tanto al modelo de la gota líquida, fundamental en la comprensión de la fisión de los núcleos.

La masa de un núcleo, sin embargo, no resulta exactamente de la suma de sus nucleones. Tal y como demostró Albert Einstein, la energía que mantiene unidos a esos nucleones es la diferencia entre la masa del núcleo y la de sus elementos, y viene dada por la ecuación 



E
=
m
⋅

c

2




{\displaystyle E=m\cdot c^{2}}

. Así, pesando los distintos átomos por una parte, y sus componentes por otra, puede determinarse la energía media por nucleón que mantiene unidos a los diferentes núcleos.

En la gráfica puede contemplarse como los núcleos muy ligeros poseen menos energía de ligadura que los que son un poco más pesados (la parte izquierda de la gráfica). Esta característica es la base de la liberación de la energía en la fusión. Y, al contrario, en la parte de la derecha se ve que los elementos muy pesados tienen menor energía de ligadura que los que son algo más ligeros. Esta es la base de la emisión de energía por fisión. Como se ve, es mucho mayor la diferencia en la parte de la izquierda (fusión) que en la de la derecha (fisión).

Fermi, tras el descubrimiento del neutrón, realizó una serie de experimentos en los que bombardeaba distintos núcleos con estas nuevas partículas. En estos experimentos observó que cuando utilizaba neutrones de energías bajas, en ocasiones el neutrón era absorbido emitiéndose fotones. 

Para averiguar el comportamiento de esta reacción repitió el experimento sistemáticamente en todos los elementos de la tabla periódica. Así descubrió nuevos elementos radiactivos, pero al llegar al uranio obtuvo resultados distintos. Lise Meitner, Otto Hahn y Fritz Strassmann consiguieron explicar el nuevo fenómeno al suponer que el núcleo de uranio al capturar el neutrón se escindía en dos partes de masas aproximadamente iguales. De hecho detectaron bario, de masa aproximadamente la mitad que la del uranio. Posteriormente se averiguó que esa escisión (o fisión) no se daba en todos los isótopos del uranio, sino solo en el 235U. Y más tarde aún, se supo que esa escisión podía dar lugar a muchísimos elementos distintos, cuya distribución de aparición es muy típica (similar a la doble joroba de un camello).

En la fisión de un núcleo de uranio, no solo aparecen dos núcleos más ligeros resultado de la división del de uranio, sino que además se emiten 2 o 3 (en promedio 2,5 en el caso del 235U) neutrones a una alta velocidad (energía). Como el uranio es un núcleo pesado no se cumple la relación N=Z (igual número de protones que de neutrones) que sí se cumple para los elementos más ligeros, por lo que los productos de la fisión poseen un exceso de neutrones. Este exceso de neutrones hace inestables (radiactivos) a esos productos de fisión, que alcanzan la estabilidad al desintegrarse los neutrones excedentes por desintegración beta generalmente. La fisión del 235U puede producirse en más de 40 formas diferentes, originándose por tanto más de 80 productos de fisión distintos, que a su vez se desintegran formando cadenas de desintegración, por lo que finalmente aparecen cerca de 200 elementos a partir de la fisión del uranio.

La energía desprendida en la fisión de cada núcleo de 235U es en promedio de 200  MeV. Los minerales explotados para la extracción del uranio suelen poseer contenidos de alrededor de 1 gramo de uranio por kg de mineral (la pechblenda por ejemplo). Como el contenido de 235U en el uranio natural es de un 0,7 %, se obtiene que por cada kg de mineral extraído tendríamos 



1
,
8
⋅

10

19




{\displaystyle 1,8\cdot 10^{19}}

 átomos de 235U. Si fisionamos todos esos átomos (1 gramo de uranio) obtendríamos teóricamente una energía liberada de 



3
,
6
⋅

10

27


e
V
=
5
,
8
⋅

10

8


J


{\displaystyle 3,6\cdot 10^{27}eV=5,8\cdot 10^{8}J}

 por gramo. En comparación, por la combustión de 1 kg de carbón de la mejor calidad (antracita) se obtiene una energía de unos 



4
⋅

10

7


J


{\displaystyle 4\cdot 10^{7}J}

, es decir, se necesitan más de 10 toneladas de antracita (el tipo de carbón con mayor poder calorífico) para obtener la misma energía contenida en 1 kg de uranio natural.

La aparición de los 2,5 neutrones por cada fisión posibilita la idea de llevar a cabo una reacción en cadena, si se logra hacer que de esos 2,5 al menos un neutrón consiga fisionar un nuevo núcleo de uranio. La idea de la reacción en cadena es común en otros procesos químicos. Los neutrones emitidos por la fisión no son útiles inmediatamente si lo que se quiere es controlar la reacción, sino que hay que frenarlos (moderarlos) hasta una velocidad adecuada. Esto se consigue rodeando los átomos por otro elemento con un Z pequeño, como por ejemplo hidrógeno, carbono o litio, material denominado moderador.

Otros átomos que pueden fisionar con neutrones lentos son el 233U o el 239Pu. Sin embargo también es posible la fisión con neutrones rápidos (de energías altas), como por ejemplo el 238U (140 veces más abundante que el 235U) o el 232Th (400 veces más abundante que el 235U).

La teoría elemental de la fisión la proporcionaron Bohr y Wheeler, utilizando un modelo según el cual los núcleos de los átomos se comportan como gotas líquidas.

La fisión se puede conseguir también mediante partículas alfa, protones o deuterones.

Así como la fisión es un fenómeno que aparece en la corteza terrestre de forma natural (si bien con una frecuencia pequeña), la fusión es absolutamente artificial en nuestro entorno (aunque es común el núcleo de las estrellas). Sin embargo, esta energía posee ventajas con respecto a la fisión. Por un lado el combustible es abundante y fácil de conseguir, y por otro, sus productos son elementos estables, ligeros y no radiactivos.

En la fusión, al contrario que en la fisión donde se dividen los núcleos, la reacción consiste en la unión de dos o más núcleos ligeros. Esta unión da lugar a un núcleo más pesado que los usados inicialmente y a neutrones. La fusión se consiguió antes incluso de comprender completamente las condiciones que se necesitaban en el desarrollo de armas, limitándose a conseguir condiciones extremas de presión y temperatura usando una bomba de fisión como elemento iniciador (Proceso Teller-Ulam). Pero no es hasta que Lawson define unos criterios de tiempo, densidad y temperatura mínimos[7]​ cuando se comienza a comprender el funcionamiento de la fusión.

Aunque en las estrellas la fusión se da entre una variedad de elementos químicos, el elemento con el que es más sencillo alcanzarla es el hidrógeno. El hidrógeno posee tres isótopos: el hidrógeno común (







1


1



H


{\displaystyle {}_{1}^{1}\!H}

), el deuterio (







1


2



H


{\displaystyle {}_{1}^{2}\!H}

) y el tritio (







1


3



H


{\displaystyle {}_{1}^{3}\!H}

). Esto es así porque la fusión requiere que se venza la repulsión electrostática que experimentan los núcleos al unirse, por lo que a menor carga eléctrica, menor será esta. Además, a mayor cantidad de neutrones, más pesado será el núcleo resultante (más arriba estaremos en la gráfica de las energías de ligadura), con lo que mayor será la energía liberada en la reacción.

Una reacción particularmente interesante es la fusión de deuterio y tritio:

En esta reacción se liberan 17,6 MeV por fusión, más que en el resto de combinaciones con isótopos de hidrógeno. Además esta reacción proporciona un neutrón muy energético que puede aprovecharse para generar combustible adicional para reacciones posteriores de fusión, utilizando litio, por ejemplo. La energía liberada por gramo con esta reacción es casi mil veces mayor que la lograda en la fisión de un gramo de uranio natural (unas siete veces superior si fuera un gramo de 235U puro).

Para vencer la repulsión electrostática, es necesario que los núcleos a fusionar alcancen una energía cinética de aproximadamente 10 keV. Esta energía se obtiene mediante un intenso calentamiento (igual que en las estrellas, donde se alcanzan temperaturas de 108 K), que implica un movimiento de los átomos igual de intenso. Además de esa velocidad para vencer la repulsión electrostática, la probabilidad de que se produzca la fusión debe ser elevada para que la reacción suceda. Esto implica que se deben poseer suficientes átomos con energía suficiente durante un tiempo mínimo. El criterio de Lawson define que el producto entre la densidad de núcleos con esa energía por el tiempo durante el que deben permanecer en ese estado debe ser 



n
⋅
τ
=

10

14


s
⋅
n
u
c
l
e
o
s
⋅
c

m

−
3




{\displaystyle n\cdot \tau =10^{14}s\cdot nucleos\cdot cm^{-3}}

.

Los dos métodos en desarrollo para aprovechar de forma útil (no bélica) la energía desprendida en esta reacción son el confinamiento magnético y el confinamiento inercial (con fotones que provienen de láser o partículas que provienen de aceleradores).

Esta reacción es una forma de fisión espontánea, en la que un núcleo pesado emite una partícula alfa (α) con una energía típica de unos 5 MeV. Una partícula α es un núcleo de helio, constituido por dos protones y dos neutrones. En su emisión el núcleo cambia, por lo que el elemento químico que sufre este tipo de desintegración muta en otro distinto. Una reacción natural típica es la siguiente:

En la que un átomo de 238U se transforma en otro de 234Th.

Fue en 1928 cuando George Gamow dio una explicación teórica a la emisión de estas partículas. Para ello supuso que la partícula alfa convivía en el interior del núcleo con el resto de los nucleones, de una forma casi independiente. Por efecto túnel en algunas ocasiones esas partículas superan el pozo de potencial que crea el núcleo, separándose de él a una velocidad de un 5 % la velocidad de la luz.

Existen dos modos de desintegración beta. En el tipo β− la fuerza débil convierte un neutrón (n0) en un protón (p+) y al mismo tiempo emite un electrón (e−) y un antineutrino (







ν
¯




e




{\displaystyle {\bar {\nu }}_{e}}

):

En el tipo β+ un protón se transforma en un neutrón emitiendo un positrón (e+) y un neutrino (




ν

e




{\displaystyle \nu _{e}}

):

Sin embargo, este último modo no se presenta de forma aislada, sino que necesita un aporte de energía.

La desintegración beta hace cambiar al elemento químico que la sufre. Por ejemplo, en la desintegración β− el elemento se transforma en otro con un protón (y un electrón) más. Así en la desintegración del 137Cs por β−;

En 1934, Enrico Fermi consiguió crear un modelo de esta desintegración que respondía correctamente a su fenomenología.

Un arma es todo instrumento, medio o máquina que se destina a atacar o a defenderse.[12]​ Según tal definición, existen dos categorías de armas nucleares:

Existen dos formas básicas de utilizar la energía nuclear desprendida por reacciones en cadena descontroladas de forma explosiva: la fisión y la fusión.

El 16 de julio de 1945 se produjo la primera explosión de una bomba de fisión creada por el ser humano: La Prueba Trinity.

Existen dos tipos básicos de bombas de fisión: utilizando uranio altamente enriquecido (enriquecimiento superior al 90 % en 235U) o utilizando plutonio. Ambos tipos se fundamentan en una reacción de fisión en cadena descontrolada y solo se han empleado en un ataque real en Hiroshima y Nagasaki, al final de la Segunda Guerra Mundial.

Para que este tipo de bombas funcionen es necesario utilizar una cantidad del elemento utilizado superior a la Masa crítica. Suponiendo una riqueza en el elemento del 100 %, eso suponen 52 kg de 235U o 10 kg de 239Pu. Para su funcionamiento se crean 2 o más partes subcríticas que se unen mediante un explosivo químico convencional de forma que se supere la masa crítica.

Los dos problemas básicos que se debieron resolver para crear este tipo de bombas fueron:

El rango de potencia de estas bombas se sitúa entre aproximadamente el equivalente a una tonelada de TNT hasta los 500.000 kilotones.

Tras el primer ensayo exitoso de una bomba de fisión por la Unión Soviética en 1949 se desarrolló una segunda generación de bombas nucleares que utilizaban la fusión. Se la llamó bomba termonuclear, bomba H o bomba de hidrógeno. Este tipo de bomba no se ha utilizado nunca contra ningún objetivo real. El llamado diseño Teller-Ullam (o secreto de la bomba H) separa ambas explosiones en dos fases.

Este tipo de bombas pueden ser miles de veces más potentes que las de fisión. En teoría no existe un límite a la potencia de estas bombas, siendo la de mayor potencia explotada la bomba del Zar, de una potencia superior a los 50 megatones. 

Las bombas de hidrógeno utilizan una bomba primaria de fisión que genera las condiciones de presión y temperatura necesarias para comenzar la reacción de fusión de núcleos de hidrógeno. Los únicos productos radiactivos que generan estas bombas son los producidos en la explosión primaria de fisión, por lo que a veces se le ha llamado bomba nuclear limpia. El extremo de esta característica son las llamadas bombas de neutrones o bomba N, que minimizan la bomba de fisión primaria, logrando un mínimo de productos de fisión. Estas bombas además se diseñaron de tal modo que la mayor cantidad de energía liberada sea en forma de neutrones, con lo que su potencia explosiva es la décima parte que una bomba de fisión. Fueron concebidas como armas anti-tanque, ya que al penetrar los neutrones en el interior de los mismos, matan a sus ocupantes por las radiaciones.

Durante la segunda guerra mundial se comprobó que el submarino podía ser un arma decisiva, pero poseía un grave problema: su necesidad de emerger tras cortos períodos para obtener aire para la combustión del diésel en que se basaban sus motores (la invención del snorkel mejoró algo el problema, pero no lo solucionó). El Almirante Hyman G. Rickover fue el primero que pensó que la energía nuclear podría ayudar con este problema.

Los desarrollos de los reactores nucleares permitieron un nuevo tipo de motor con ventajas fundamentales:

Estas ventajas condujeron a buques que alcanzan velocidades de más de 25 nudos, que pueden permanecer semanas en inmersión profunda y que además pueden almacenar enormes cantidades de munición (nuclear o convencional) en sus bodegas. De hecho las armadas de Estados Unidos, Francia y el Reino Unido solo poseen submarinos que utilizan este sistema de propulsión.

En los submarinos se han utilizado reactores de agua a presión, de agua en ebullición o de sales fundidas. Para conseguir reducir el peso del combustible en estos reactores se usa uranio con altos grados de enriquecimiento (del 30 al 40 % en los rusos o del 96 % en los estadounidenses). Estos reactores presentan la ventaja de que no es necesario (aunque sí es posible) convertir el vapor generado por el calor en electricidad, sino que puede utilizarse de forma directa sobre una turbina que proporciona el movimiento a las hélices que impulsan el buque, mejorando notablemente el rendimiento.

Se han construido una gran variedad de buques militares que usan motores nucleares y que, en algunos casos, portan a su vez misiles de medio o largo alcance con cabezas nucleares:

Estados Unidos, Gran Bretaña, Rusia, China y Francia poseen buques de propulsión nuclear.

Tanto Estados Unidos como la Unión Soviética se plantearon la creación de una flota de bombarderos de propulsión nuclear. De este modo se pretendía mantenerlos cargados con cabezas nucleares y volando de forma permanente cerca de los objetivos prefijados. Con el desarrollo del Misil balístico intercontinental (ICBM) a finales de los años 1950, más rápidos y baratos, sin necesidad de pilotos y prácticamente invulnerables, se abandonaron todos los proyectos.

Los proyectos experimentales fueron:

La energía atómica se utiliza desde los años 1950 como sistema para dar empuje (propulsar) distintos sistemas, desde los submarinos (el primero que utilizó la energía nuclear), hasta naves espaciales.

Tras el desarrollo de los buques de propulsión nuclear de uso militar se hizo pronto patente que existían ciertas situaciones en las que sus características podían ser trasladadas a la navegación civil.

Se han construido cargueros y rompehielos que usan reactores nucleares como propulsión.

El primer buque nuclear de carga y pasajeros fue el NS Savannah, botado en 1962. Solo se construyeron otros tres buques de carga y pasajeros: El Mutsu japonés, el Otto Hahn alemán y el Sevmorput ruso. El Sevmorput (acrónimo de 'Severnii Morskoi Put'), botado en 1988 y dotado con un reactor nuclear tipo KLT-40 de 135 MW, sigue en activo hoy en día transitando la ruta del mar del norte.

Aunque existen varias opciones que pueden utilizar la energía nuclear para propulsar cohetes espaciales, solo algunas han alcanzado niveles de diseño avanzados.

El cohete termonuclear, por ejemplo, utiliza hidrógeno recalentado en un reactor nuclear de alta temperatura, consiguiendo empujes al menos dos veces superiores a los cohetes químicos. Este tipo de cohetes se probaron por primera vez en 1959 (el Kiwi 1), dentro del Proyecto NERVA, cancelado en 1972. En 1990 se relanzó el proyecto bajo las siglas SNTP (Space Nuclear Thermal Propulsion)[13]​ dentro del proyecto para un viaje tripulado a Marte en 2019. En 2003 comenzó con el nombre de Proyecto Prometeo. Otra de las posibilidades contempladas es el uso de un reactor nuclear que alimente a un propulsor iónico (Nuclear Electric Xenon Ion System o NEXIS).

El Proyecto Orión[14]​ fue un proyecto ideado por Stanisław Ulam en 1947, que comenzó en 1958 en la empresa General Atomics. Su propósito era la realización de viajes interplanetarios de forma barata a una velocidad de un 10 % de c. Para ello utilizaba un método que se denominó propulsión nuclear pulsada (External Pulsed Plasma Propulsion es su denominación oficial en inglés). El proyecto fue abandonado en 1963, pero el mismo diseño se ha utilizado como base en el Proyecto Daedalus[15]​ británico con motor de fusión, el Proyecto Longshot[16]​ americano con motor de fisión acoplado a un motor de fusión inercial o el Proyecto Medusa.

También se ha propuesto el uso de RTG como fuente para un cohete de radioisótopos.[17]​

La única propuesta conocida es el diseño conceptual lanzado por Ford en 1958: el Ford Nucleon.[18]​ Nunca fue construido un modelo operacional. En su diseño se proponía el uso de un pequeño reactor de fisión que podía proporcionar una autonomía de más de 8000 km. Un prototipo del coche se mantiene en el museo Henry Ford.

Una opción, incluida en las alternativas al petróleo, es el uso del hidrógeno en células de combustible como combustible para vehículos de hidrógeno. Se está investigando en este caso el uso de la energía nuclear para la generación del hidrógeno necesario mediante reacciones termoquímicas o de electrólisis con vapor a alta temperatura.[19]​[20]​

Probablemente, la aplicación práctica más conocida de la energía nuclear es la generación de energía eléctrica para su uso civil, en particular mediante la fisión de uranio enriquecido. Para ello se utilizan reactores en los que se hace fisionar o fusionar un combustible. El funcionamiento básico de este tipo de instalaciones industriales es similar a cualquier otra central térmica, sin embargo poseen características especiales con respecto a las que usan combustibles fósiles:

Tras su uso exclusivamente militar, se comenzó a plantear la aplicación del conocimiento adquirido a la vida civil. El 20 de diciembre de 1951 fue el primer día que se consiguió generar electricidad con un reactor nuclear (en el reactor estadounidense EBR-I, con una potencia de unos 100 kW), pero no fue hasta 1954 cuando se conectó a la red eléctrica una central nuclear (fue la central nuclear soviética Obninsk, generando 5 MW con solo un 17 % de rendimiento térmico). El primer reactor de fisión comercial fue el Calder Hall en Sellafield, que se conectó a la red eléctrica en 1956. El 25 de marzo de 1957 se creó la Comunidad Europea de la Energía Atómica (EURATOM), el mismo día que se creó la Comunidad Económica Europea, entre Bélgica, Francia, Alemania, Italia, Luxemburgo y los Países Bajos. Ese mismo año se creó el Organismo Internacional de Energía Atómica (OIEA). Ambos organismos con la misión, entre otras, de impulsar el uso pacífico de la energía nuclear.

Su desarrollo en todo el mundo experimentó a partir de ese momento un gran crecimiento, de forma muy particular en Francia y Japón, donde la crisis del petróleo de 1973 influyó definitivamente, ya que su dependencia del petróleo para la generación eléctrica era muy marcada (39 y 73 % respectivamente en aquellos años, en 2008 generan un 78 y un 30 % respectivamente mediante reactores de fisión).[cita requerida] En 1979 el accidente de Three Mile Island provocó un aumento muy considerable en las medidas de control y de seguridad en las centrales, sin embargo no se detuvo el aumento de capacidad instalada. Pero en 1986 el accidente de Chernóbil, en un reactor RBMK de diseño soviético que no cumplía los requisitos de seguridad que se exigían en Occidente, cortó drásticamente ese crecimiento.

En octubre de 2007 existían 439 centrales nucleares en todo el mundo que generaron 2,7 millones de MWh en 2006. La potencia instalada en 2007 fue de 370 721 MWe. En marzo de 2008 había 35 centrales en construcción, planes para construir 91 centrales nuevas (99 095 MWe) y otras 228 propuestas (198 995MWe).[25]​ Aunque solo 30 países en el mundo poseen centrales nucleares, aproximadamente el 15 % de la energía eléctrica generada en el mundo se produce a partir de energía nuclear.[26]​

La mayoría de los reactores son de los llamados de agua ligera (LWR por su sigla en inglés), que utilizan como moderador agua intensamente purificada. En estos reactores el combustible utilizado es uranio enriquecido ligeramente (entre el 3 y el 5 %).

Más tarde se planteó añadir el plutonio fisible generado (







94


239


P
u


{\displaystyle {}_{94}^{239}Pu}

) como combustible extra en estos reactores de fisión, aumentando de una forma importante la eficiencia del combustible nuclear y reduciendo así uno de los problemas del combustible gastado. Esta posibilidad incluso llevó al uso del plutonio procedente del armamento nuclear desmantelado en las principales potencias mundiales. Así se desarrolló el combustible MOX, en el que se añade un porcentaje (entre un 3 y un 10 % en masa) de este plutonio a uranio empobrecido. Este combustible se usa actualmente como un porcentaje del combustible convencional (de uranio enriquecido). También se ha ensayado en algunos reactores un combustible mezcla de torio y plutonio, que genera una menor cantidad de elementos transuránicos. 

Otros reactores utilizan agua pesada como moderador. En estos reactores se puede utilizar uranio natural, es decir, sin enriquecer y además se produce una cantidad bastante elevada de tritio por activación neutrónica. Este tritio se prevé que pueda aprovecharse en futuras plantas de fusión. 

Otros proyectos de fisión, que no han superado hoy en día la fase de experimentación, se encaminan al diseño de reactores en los que pueda generarse electricidad a partir de otros isótopos, principalmente el 







90


232


T
h


{\displaystyle {}_{90}^{232}Th}

 y el 







92


238


U


{\displaystyle {}_{92}^{238}U}

.

La diferencia básica entre los distintos diseños de reactores nucleares de fisión es el combustible que utilizan. Esto influye en el tipo de moderador y refrigerante usados. De entre todas las posibles combinaciones entre tipo de combustible, moderador y refrigerante, solo algunas son viables técnicamente (unas 100 contando las opciones de neutrones rápidos). Pero solo unas cuantas se han utilizado hasta el momento en reactores de uso comercial para la generación de electricidad (ver tabla).

El único isótopo natural que es fisionable con neutrones térmicos es el 







92


235


U


{\displaystyle {}_{92}^{235}U}

, que se encuentra en una proporción de un 0.7 % en peso en el uranio natural. El resto es 







92


238


U


{\displaystyle {}_{92}^{238}U}

, considerado fértil, ya que, aunque puede fisionar con neutrones rápidos, por activación con neutrones se convierte en 







94


239


P
u


{\displaystyle {}_{94}^{239}Pu}

, que sí es físil mediante neutrones térmicos.

Los reactores de fisión comerciales, tanto de primera como de segunda o tercera generación, utilizan uranio con grados de enriquecimiento distinto, desde uranio natural hasta uranio ligeramente enriquecido (por debajo del 6 %). Además, en aquellos en los que se usa uranio enriquecido, la configuración del núcleo del reactor utiliza diferentes grados de enriquecimiento, con uranio más enriquecido en el centro y menos hacia el exterior. Esta configuración consigue dos fines: por una parte disminuir los neutrones de fuga por reflexión, y por otra parte aumentar la cantidad de 







94


239


P
u


{\displaystyle {}_{94}^{239}Pu}

 consumible. En los reactores comerciales se hacen fisionar esos átomos fisibles con neutrones térmicos hasta el máximo posible (al grado de quemado del combustible se le denomina burnup), ya que se obtienen mayores beneficios cuanto más provecho se saca del combustible.

Otro isótopo considerado fértil con neutrones térmicos es el torio (elemento natural, compuesto en su mayoría por el isótopo 







90


232


T
h


{\displaystyle {}_{90}^{232}Th}

), que por activación produce 







92


233


U


{\displaystyle {}_{92}^{233}U}

, físil con neutrones térmicos y rápidos (es regla general que aquellos elementos con número atómico A impar sean fisibles, y con A par fértiles).

Esos tres isótopos son los que producen fisiones exoergicas, es decir, generan más energía que la necesaria para producirlas, con neutrones térmicos. Los demás elementos (con z<92) solo fisionan con neutrones rápidos. Así el 







92


238


U


{\displaystyle {}_{92}^{238}U}

 por ejemplo puede fisionarse con neutrones de energías superiores a 1,1 MeV.

Aunque hay varias formas de clasificar los distintos reactores nucleares, la más utilizada, y con la que se denominan los distintos tipos de reactores de fisión es por la combinación moderador/refrigerante utilizado. Estas son las denominaciones de los reactores comerciales de neutrones térmicos utilizados en la actualidad (de segunda generación), junto a su número en el mundo (entre paréntesis)[28]​ y sus características principales:

Los diseños de reactores que utilizan neutrones rápidos, y por tanto pueden utilizar como combustible 







92


238


U


{\displaystyle {}_{92}^{238}U}

, 







94


239


P
u


{\displaystyle {}_{94}^{239}Pu}

 o 







90


232


T
h


{\displaystyle {}_{90}^{232}Th}

 entre otros, no necesitan moderador para funcionar. Por ese motivo es difícil utilizar los mismos materiales que se usan en los térmicos como refrigerantes, ya que en muchas ocasiones también actúan como moderador. Todos los reactores de este tipo hasta el momento han utilizado como refrigerante metales líquidos (mercurio, plutonio, yoduro potásico, plomo, bismuto, sodio...). Cuando estos reactores además consiguen producir más cantidad de material físil que el que consumen se les denomina reactores reproductores rápidos. En la actualidad existen cuatro FBR, tres en parada fría y solo uno en operación comercial.[28]​

Los diseños de reactores que aprovechan las lecciones aprendidas en el medio siglo transcurrido (aproximadamente una docena de diseños distintos) se denominan de tercera generación o reactores avanzados. Solo se han puesto en marcha algunos en Japón y se están construyendo algunos otros. En general son evoluciones de los reactores de segunda generación (como el BWR avanzado o ABWR o el PWR avanzado: el EPR o el AP1000), aunque existen algunos diseños completamente nuevos (como el PBMR que utiliza helio como refrigerante y combustible TRISO que contiene el moderador de grafito en su composición).

Los reactores de cuarta generación no saldrán del papel al menos hasta 2020, y en general son diseños que buscan, además de niveles de seguridad superiores a las plantas de fisión de las generaciones anteriores, que los únicos residuos de alta actividad tengan vidas muy cortas, quemando los actínidos de vida larga. A este grupo pertenecen por ejemplo los reactores asistidos por acelerador (ADS). En general estos reactores se basarán en neutrones rápidos.

Existen algunos otros diseños, basados fundamentalmente en los descritos, para generar energía en lugares remotos, como el reactor flotante ruso KLT-40S o el microrreactor nuclear de 200 kW de Toshiba.[30]​

Como cualquier actividad humana, una central nuclear de fisión conlleva riesgos y beneficios. Los riesgos deben preverse y analizarse para poder ser mitigados. A todos aquellos sistemas diseñados para eliminar o al menos minimizar esos riesgos se les llama sistemas de protección y control. En una central nuclear de uso civil se utiliza una aproximación llamada defensa en profundidad. Esta aproximación sigue un diseño de múltiples barreras para alcanzar ese propósito. Una primera aproximación a las distintas barreras utilizadas (cada una de ellas múltiple), de fuera adentro podría ser:

Además debe estar previsto qué hacer en caso de que todos o varios de esos niveles fallaran por cualquier circunstancia. Todos, los trabajadores u otras personas que vivan en las cercanías, deben poseer la información y formación necesaria. Deben existir planes de emergencia que estén plenamente operativos. Para ello es necesario que sean periódicamente probados mediante simulacros. Cada central nuclear posee dos planes de emergencia: uno interior y uno exterior, comprendiendo el plan de emergencia exterior, entre otras medidas, planes de evacuación de la población cercana por si todo lo demás fallara.

Aunque los niveles de seguridad de los reactores de tercera generación han aumentado considerablemente con respecto a las generaciones anteriores, no es esperable que varíe la estrategia de defensa en profundidad. Por su parte, los diseños de los futuros reactores de cuarta generación se están centrando en que todas las barreras de seguridad sean infalibles, basándose tanto como sea posible en sistemas pasivos y minimizando los activos. Del mismo modo, probablemente la estrategia seguida será la de defensa en profundidad.

Cuando una parte de cualquiera de esos niveles, compuestos a su vez por múltiples sistemas y barreras, falla (por defecto de fabricación, desgaste, o cualquier otro motivo), se produce un aviso a los controladores que a su vez se lo comunican a los inspectores residentes en la central nuclear. Si los inspectores consideran que el fallo puede comprometer el nivel de seguridad en cuestión elevan el aviso al organismo regulador (en España el CSN). A estos avisos se les denomina sucesos notificables.[37]​[38]​ En algunos casos, cuando el fallo puede hacer que algún parámetro de funcionamiento de la central supere las Especificaciones Técnicas de Funcionamiento (ETF) definidas en el diseño de la central (con unos márgenes de seguridad), se produce un paro automático de la reacción en cadena llamado SCRAM. En otros casos la reparación de esa parte en cuestión (una válvula, un aspersor, una compuerta,...) puede llevarse a cabo sin detener el funcionamiento de la central.

Si cualquiera de las barreras falla aumenta la probabilidad de que suceda un accidente. Si varias barreras fallan en cualquiera de los niveles, puede finalmente producirse la ruptura de ese nivel. Si varios de los niveles fallan puede producirse un accidente, que puede alcanzar diferentes grados de gravedad. Esos grados de gravedad se organizaron en la Escala Internacional de Accidentes Nucleares (INES) por el OIEA y la AEN, iniciándose la escala en el 0 (sin significación para la seguridad) y acabando en el 7 (accidente grave). El incidente (denominados así cuando se encuentran en grado 3 o inferiores) más grave ocurrido en España fue el de Vandellós I en 1989, catalogado a posteriori (no existía ese año la escala en España) como de grado 3 (incidente importante).[39]​

La ruptura de varias de estas barreras (no existía independencia con el gobierno, el diseño del reactor era de reactividad positiva, la planta no poseía edificio de contención, no existían planes de emergencia, etc.) causó el accidente nuclear más grave ocurrido: el accidente de Chernóbil, de nivel 7 en la Escala Internacional de Accidentes Nucleares (INES).

Al igual que la fisión, tras su uso exclusivamente militar, se propuso el uso de esta energía en aplicaciones civiles. En particular, los grandes proyectos de investigación se han encaminado hacia el desarrollo de reactores de fusión para la producción de electricidad. El primer diseño de reactor nuclear se patentó en 1946,[8]​ aunque hasta 1955 no se definieron las condiciones mínimas que debía alcanzar el combustible (isótopos ligeros, habitualmente de hidrógeno), denominadas criterios de Lawson, para conseguir una reacción de fusión continuada. Esas condiciones se alcanzaron por primera vez de forma cuasiestacionaria el año 1968.

La fusión se plantea como una opción más eficiente (en términos de energía producida por masa de combustible utilizada) segura y limpia que la fisión, útil para el largo plazo.[40]​ Sin embargo faltan aún años para poder ser utilizada de forma comercial (la fusión no será comercial al menos hasta el año 2050).[41]​ La principal dificultad encontrada, entre otras muchas de diseño y materiales, consiste en la forma de confinar la materia en estado de plasma hasta alcanzar las condiciones impuestas por los criterios de Lawson, ya que no hay materiales capaces de soportar las temperaturas impuestas.

Se han diseñado dos alternativas para alcanzar los criterios de Lawson, que son el confinamiento magnético y el confinamiento inercial.

Aunque ya se llevan a cabo reacciones de fusión de forma controlada en los distintos laboratorios, en estos momentos los proyectos se encuentran en el estudio de viabilidad técnica en centrales de producción eléctrica como el ITER o el NIF. El proyecto ITER, en el que participan entre otros Japón y la Unión Europea, pretende construir una central experimental de fusión y comprobar su viabilidad técnica. El proyecto NIF, en una fase más avanzada que ITER, pretende lo mismo en Estados Unidos usando el confinamiento inercial.

Una vez demostrada la viabilidad de conseguir un reactor de fusión que sea capaz de funcionar de forma continuada durante largos períodos, se construirán prototipos encaminados a la demostración de su viabilidad económica.[42]​

Existen dos grandes grupos, separados por el método empleado para alcanzar las condiciones de tiempo, densidad y temperatura necesarias para que pueda alcanzarse la fusión controlada de forma continua:

En el primer caso, en un recipiente donde se ha practicado un vacío elevado, se eleva la temperatura de una mezcla de deuterio-tritio mediante campos electromagnéticos hasta convertirla en plasma.

También mediante campos electromagnéticos se confina el plasma en una región lo más pequeña y alejada de las paredes del recipiente que sea posible, aumentando de forma continua la densidad y la temperatura.

A este tipo de fusión corresponden los diseños del Tokamak, como el futuro ITER, o del Stellarator, como el TJ-II español.

En el segundo caso se hace incidir un haz de fotones o de partículas cargadas (electrones o protones) muy energético e intenso sobre un blanco compuesto por el combustible (deuterio-tritio actualmente). Ese haz puede estar enfocado de forma directa sobre el blanco, o bien de forma indirecta sobre un dispositivo denominado holraum construido con un material de alto Z que genera a su vez un intensísimo campo de rayos X que está enfocado sobre el blanco.

Hasta la década de 1970 no se desarrollaron láseres con las potencias necesarias para conseguir iniciar la reacción.

En la actualidad se investiga en varios centros, pero a nivel nacional. Esto se debe a que el mecanismo empleado produce microexplosiones termonucleares, de forma que tanto el software empleado en cálculos y simulaciones termohidráulicas, como los resultados obtenidos, pueden emplearse directamente en el armamento termonuclear. Por este motivo las instalaciones construidas hasta el momento, además de buscar la aplicación civil mediante generación de electricidad, poseen una importante componente militar ya que permiten, tras la prohibición de ensayos nucleares en superficie, realizar pruebas a escala diminuta (para los parámetros del armamento nuclear).

Aunque existen múltiples diseños tanto con el uso de láseres como de aceleradores de partículas, los proyectos más importantes hasta el momento en el mundo son el NIF de Estados Unidos y el LMJ francés, ambos diseños empleando láseres.

Aunque la misma filosofía empleada en la fisión puede emplearse en los reactores de fusión, se ha planteado esta como una opción no contaminante e intrínsecamente segura. Desde el punto de vista de la seguridad, ya que los reactores diseñados necesitan un aporte exterior de energía y de combustible, si existiera un accidente que produjese el fallo de la máquina la reacción se detendría, con lo que se hace imposible una reacción en cadena descontrolada.

El residuo principal de la reacción de fusión deuterio-tritio sería el helio, que es un gas noble y por tanto no interacciona con nada, incluido el organismo humano. Sin embargo las reacciones nucleares de fusión desprenden neutrones altamente energéticos. Esto implica la producción de materiales radiactivos por activación neutrónica. Además, en un ciclo deuterio-tritio, una parte del propio combustible es también radiactivo (el tritio). Para minimizar los efectos, por tanto:

Para ello se está investigando en el uso de materiales de baja activación, utilizando aleaciones que no son comunes en otras aplicaciones. Este aspecto podría disminuir la cantidad de residuos radiactivos generados, pero además en caso de accidente donde parte de los materiales se fundieran por las altas temperaturas, el inventario radiactivo emitido también sería menor. Además, la estrategia de diseño se centra en conseguir que todos los radioisótopos generados sean de semiperiodo corto (menor de 10 años). Si no se consiguiera, las estrategias a seguir serían idénticas a las estudiadas en el caso de los reactores de fisión.

Hasta los años 1990 no se había planteado realmente este problema, por lo que los materiales válidos para la fusión se pensaba que eran los aceros austeníticos (SS316L y SS316-modTi) y ferríticos/martensíticos (HT-9 y DIN 1.1494/MANET). Las investigaciones se habían centrado en la gestión de residuos, dejando de lado el estudio de los posibles accidentes. A partir de los 90 se plantea que debían contemplarse varios problemas en la optimización de los materiales de baja activación, subrayándose principalmente el aspecto de la seguridad frente accidentes además del clásico de la gestión de los residuos. A partir de los aceros convencionales propuestos para fusión se propusieron versiones de baja activación, resultado de la sustitución de elementos que daban lugar a una radiactividad alta por otros metalúrgicamente equivalentes y de baja actividad inducida.

Las soluciones que se adopten en la fusión inercial o en la magnética en principio no tendrán que ser iguales. Así se han desarrollado aleaciones de vanadio, titanio y cromo que presentan mejores comportamientos en la fusión inercial que en la magnética. Se sabe que los materiales cerámicos tienen mejor comportamiento que los aceros en ambos tipos de fusión.

Un método ampliamente utilizado en aquellas aplicaciones en las que se requiere un aporte eléctrico de baja corriente, con una larga duración, es el uso de unidades de calor mediante radioisótopos acoplados a una serie de termopares que proporcionan una corriente eléctrica, los llamados generadores termoeléctricos de radioisótopos.

En este caso se aprovecha la radiactividad emitida por los núcleos de algunos isótopos. Los isótopos considerados más interesantes para este tipo de aplicación son aquellos que emiten partículas alfa (como por ejemplo el 241Am o el 210Po), ya que se reaprovechan más eficientemente las radiaciones emitidas, y es más sencillo su manejo. Sin embargo también se han utilizado emisores beta, como el 90Sr.

Estos generadores suelen poseer duraciones de varias décadas, y son extremadamente útiles en aplicaciones en las que otras soluciones no sirven. Por ejemplo, en zonas donde es difícil el mantenimiento o sustitución de las baterías y además no existe suficiente luz solar o viento. Se han utilizado en faros cercanos al polo norte en la antigua Unión Soviética y se utilizan frecuentemente en sondas espaciales. Una de sus aplicaciones más curiosas puede ser su uso en marcapasos.

En algunas sondas espaciales que deben permanecer a muy baja temperatura se utiliza simplemente su capacidad de generar calor, obviando la posibilidad de generación eléctrica.

El 15 de octubre de 1997 se lanzó la misión Cassini-Huygens con destino a Saturno y Titán, en la que se ensambló uno de estos dispositivos.[43]​

En estos dispositivos la seguridad se basa en dos sistemas principalmente:

En el caso de los GTR situados en zonas de alta inaccesibilidad, como los utilizados en faros instalados cerca de los polos, se suponía que la propia inaccesibilidad de las zonas aseguraba su integridad. Esto sin embargo no ha impedido que sucedieran varios accidentes.

En el caso de los utilizados en satélites espaciales, la seguridad de los materiales radiactivos se asegura al mantener una vigilancia continua en las instalaciones, tanto en la construcción como en el montaje de los satélites. Una vez lanzados al espacio, evidentemente se hace imposible su mal uso. Sin embargo, en algunas ocasiones se han usado GTR en satélites en órbita alrededor de la Tierra. Cuando esa órbita se hace inestable es posible que el satélite caiga de nuevo, fundiéndose en su mayor parte en la reentrada. Este, junto a un posible accidente en el lanzamiento son los principales problemas de seguridad en este caso. En total se han producido 6 accidentes conocidos de este tipo (el último en 1996 en una sonda rusa). Para evitar la dispersión del material radiactivo que contienen se fabrican en materiales cerámicos (insolubles y resistentes al calor), rodeado de una capa de iridio, otra de bloques de grafito de alta resistencia y un gel que le da resistencia ante una posible reentrada en la atmósfera.

Para los GTR utilizados como marcapasos el principal problema se encuentra en la pérdida de información acerca de los pacientes en los que se han utilizado, imposibilitando así su debido seguimiento. Por este motivo, existe la posibilidad de que el paciente, tras su fallecimiento, fuera incinerado, incinerando con ello el propio dispositivo y su material radiactivo.

Las fuentes radiactivas de los GTR sobre los que se ha perdido el control (principalmente tras la caída de la URSS) son el principal motivo de preocupación por su posible uso en atentados terroristas (como parte de una bomba sucia), y por este motivo se realizan grandes esfuerzos a nivel internacional por recuperarlas y ponerlas bajo control de nuevo.

En general, cualquier aplicación industrial genera residuos. Todas las formas de generación de energía nuclear también los generan. Tanto los reactores nucleares de fisión o fusión (cuando entren en funcionamiento) como los GTR generan residuos convencionales que son trasladados a vertederos o instalaciones de reciclaje, residuos tóxicos convencionales (pilas, líquido refrigerante de los transformadores, etc.) y residuos radiactivos. El tratamiento de todos ellos, con excepción hecha de los residuos radiactivos, es idéntico al que se da a los residuos del mismo tipo generado en otros lugares (instalaciones industriales, ciudades,...).

Es diferente el tratamiento que se emplea en los residuos radiactivos. Para ellos se desarrolló una regulación específica, gestionándose de formas diferentes en función del tipo de radiactividad que emiten y del semiperiodo que poseen. Esta regulación engloba todos los residuos radiactivos, ya procedan de instalaciones de generación de electricidad, de instalaciones industriales o de centros médicos.

Se han desarrollado diferentes estrategias para tratar los distintos residuos que proceden de las instalaciones o dispositivos generadores de energía nuclear:

Para gestionar los residuos radiactivos suele existir en cada país un organismo creado exclusivamente para ello. En España se creó la Empresa Nacional de Residuos Radiactivos, que gestiona los residuos radiactivos de todo tipo generados tanto en las centrales nucleares como en el resto de instalaciones nucleares o radiactivas.

La regulación nuclear puede separarse en cuatro grandes grupos:

Las bases científicas de toda la regulación internacional existente se fundan en estudios propios y recopilaciones llevadas a cabo por la CIPR,[47]​ UNSCEAR[48]​ o el NAS/BEIR americano.[49]​ Además de estos, existen una serie de agencias de investigación y desarrollo en seguridad, como pueden ser la AEN[50]​ o el EPRI.[51]​ A partir de todas ellas, existen dos organismos internacionales que desarrollan las bases para la legislación: el OIEA (a nivel internacional)[52]​ y EURATOM (en Europa).[53]​

También existen algunos organismos nacionales, que emiten documentación dedicada a cada uno de los campos, que sirven de guía a otros países. Así ocurre por ejemplo con la NCRP, la NRC o la EPA estadounidenses, la HPA inglesa (antiguamente NRPB) o el CEA francés.

Además de estas regulaciones específicas, existen otras leyes y acuerdos que tienen en mayor o menor medida relación con la energía nuclear. Así por ejemplo las leyes de calidad del agua o la convención OSPAR. Aunque en el Protocolo de Kioto, que trata sobre las industrias que emiten gases de efecto invernadero, no se menciona la energía nuclear, sí aparece en otros documentos referentes al calentamiento global antropogénico. Así, en los acuerdos de Bonn de 2001,[54]​ se establecieron los mecanismos de compraventa de emisiones de gases de efecto invernadero y los mecanismos de intercambio de tecnologías, excluyendo ambos explícitamente a la energía nuclear. De este modo, no se pueden reducir las cuotas de emisión de los países altamente industrializados mediante la venta de tecnología nuclear a países menos desarrollados, ni se pueden vender las cuotas de emisiones a países que funden sus bajas emisiones en la energía nuclear. El IPCC, sin embargo, sí recomienda en su cuarto informe el uso de la energía nuclear como una de las únicas formas (junto a las energías renovables y la eficiencia energética) de reducir la emisión de gases de efecto invernadero.[55]​

Las centrales nucleares generan aproximadamente un tercio de la energía eléctrica que se produce en la Unión Europea, evitando así la emisión a la atmósfera de 700 millones de toneladas de dióxido de carbono por año [1][cita requerida] y del resto de emisiones contaminantes asociadas con el uso de combustibles fósiles.

Por otra parte, la aplicación de la tecnología nuclear a la medicina ha tenido importantes aportes: emisiones de radiación para diagnóstico, como los rayos X, y para tratamiento del cáncer como la radioterapia; radiofármacos, que principalmente consiste en la introducción de sustancias al cuerpo, que pueden ser monitoreadas desde el exterior. En la alimentación ha permitido, por medio de las radiaciones ionizantes, la conservación de alimentos. También se ha logrado un aumento en la recolección de alimentos, ya que se ha combatido plagas, que creaban pérdidas en las cosechas.[56]​

En la agricultura, se pueden mencionar las técnicas radioisotópicas y de radiaciones, las cuales son usadas para crear productos con modificación genética, como dar mayor color a alguna fruta o aumentar su tamaño.[56]​

Algunas de estas desventajas son poco probables.

Otro inconveniente de la energía atómica es el coste de construcción y mantenimiento de las centrales nucleares, siendo éste muy elevado. Los últimos proyectos que se han llevado a cabo, como la central de Olkiluoto 3 en Finlandia, la central de Hinkley Point C en el Reino Unido, la central de Flamanville-3 en Francia y los reactores Vogtle 3 y 4 en los Estados Unidos han costado entre 5,3 y más de 10 millones de Euros por MW instalado. Estos costes resultan mucho mayores que los de las instalación de energías renovables, ya que a finales del 2019 una instalación fotovoltaica tenía un coste de entre 600.000 y 700.000 € por MW y una instalación eólica alrededor de 1 millón de € por MW.[57]​
Sin embargo, la cantidad de energía que producen en su vida útil las centrales nucleares  compensa los gastos de sus construcciones y mantenimientos.

La pizza es una preparación culinaria que consiste en un pan plano, habitualmente de forma circular, elaborado con harina de trigo, levadura, agua y sal (a veces aceite de oliva) que tradicionalmente se cubre con salsa de tomate y mozzarella y se hornea a alta temperatura en un horno de leña.[2]​[3]​[4]​ El lugar donde se venden pizzas se denomina «pizzería» y al obrador, «pizzero» (pizzaiolo en italiano). Aunque se considera que su origen está en la gastronomía italiana,[3]​ particularmente la napolitana,[2]​ su consumo está extendido a casi todos los países del mundo con diversas variantes locales, que incorporan distintos ingredientes para cubrir la masa.[1]​ Junto con la hamburguesa, la pizza está considerada la comida más difundida del mundo,[5]​[6]​ como consecuencia de la diáspora italiana que se estableció en América a lo largo del siglo xx principalmente en Nueva York,[3]​ Buenos Aires o Chicago.[7]​[8]​

Las técnicas e ingredientes para elaborar pizza se han diversificado enormemente. Entre los ingredientes habituales se incluyen aceitunas, albahaca, anchoas, cebolla, champiñones, jamón, orégano, distintos tipos de queso, salami, salsas varias y tomate, entre otros. Asimismo, hay pizzas con forma cuadrada, con masas finas o gruesas, con harinas que no son de trigo,[4]​ sin salsa de tomate o sin queso. Aunque típicamente se considera una preparación artesanal, existen pizzas preparadas de forma total o parcialmente industrializada («prepizza» o take and bake) que se venden en comercios.[9]​

Desde 1989, tiene lugar anualmente en Italia el Campeonato Mundial de la Pizza que realiza varias competencias y premia a quienes obtengan los tres primeros lugares en cada una de ellas.[10]​[11]​ En la edición de 2018, participaron 773 pizzaioli (pizzeros) provenientes de 44 países.[12]​ En 2010, la Unión Europea reconoció la pizza napolitana como Especialidad tradicional garantizada (ETG)[13]​ y, en 2017, la UNESCO reconoció el arte de los pizzaioli napolitanos como Patrimonio Cultural Inmaterial de la Humanidad.[14]​

Los antecedentes de la pizza se encuentran en el empleo del pan de trigo en las antiguas culturas de Egipto, Persia, Grecia y Roma. En la época de Darío I el Grande (521-500 a. C.), los soldados persas comían un pan plano con queso fundido y dátiles en la parte superior.[15]​ En la Antigua Roma, los soldados consumían un pan plano con aceite de oliva y hierbas, similar a la focaccia.[16]​ Panes planos con agregados similares se encuentran en diversas culturas del Mediterráneo.[17]​[18]​

La pizza en su versiones más tradicionales, la marinara y la cubierta de salsa de tomate y mozzarella (napolitana), procede de la ciudad italiana de Nápoles.[17]​ Las primeras referencias a hornos para pizza datan de finales del siglo xvii.[19]​ La pizzería 'Ntuono, trasladada en 1738 a la zona de Port'Alba, se encontraba en actividad desde 1732.[19]​ Hacia mediados del siglo xviii, en Nápoles había más de ochenta pizzerías.[19]​ En la primera mitad del siglo xix se consideraba una «comida plebeya»,[19]​ de napolitanos pobres.[20]​ Ya en la década de 1830, se encuentran menciones a este plato, por ejemplo en el libro Napoli, contorni e dintorni (1830), escrito por un tal Riccio.[21]​[22]​ Poco después, en 1843, Alejandro Dumas publicó Le corricolo, una crónica de viaje en la que registra las impresiones de su visita al Reino de Nápoles en 1835.[23]​ En ella dedica varios párrafos a la pizza, a la que señala como la única comida de los napolitanos pobres (lazzaroni) en invierno, y describe varios tipos, la mayoría in bianco (sin tomate), aunque menciona también las que llevan ese fruto:

Nápoles se había diferenciado por haber incorporado, en la segunda mitad del siglo xvii, el tomate proveniente de América a la alimentación, mientras que en otros países europeos se creía que era venenoso o causa de enfermedades.[25]​ Las semillas de tomate provenientes de Perú en la década de 1770 originaron una variedad conocida como tomate San Marzano, cuya baja acidez lo hizo óptimo para la preparación de salsa.[26]​ La combinación de pan, salsa de tomate y queso dio a origen a un alimento caliente, apetecible y barato para los habitantes humildes de la ciudad. La vinculación con la pasta y la pizza en el siglo xix hizo que el tomate se convirtiera en un ingrediente dominante de la gastronomía italiana.[27]​[28]​

Pese a que la pizza hecha con pan, queso, tomate y albahaca data al menos de comienzos del siglo xix, un relato tradicional, que se ha probado históricamente falso,[29]​ sostiene que en junio de 1889, en ocasión de la visita a Nápoles de la reina de Italia, Margarita de Saboya, un cocinero de la pizzería Brandi llamado Raffaele Esposito quiso homenajearla con una pizza que llevara los colores de la bandera italiana (blanco, rojo y verde), utilizando como ingredientes mozzarella, tomate y albahaca, y la llamó por ello pizza margarita.[30]​[31]​ Gentilcore evalúa que la tradición de la pizza margarita combina varios movimientos históricos: por un lado un proceso de difusión nacional de una clásica comida napolitana, bajo el influjo de la unificación italiana que incorporó el reino del sur de la península, hasta entonces bajo el poder de los Borbones, el populismo de la nueva monarquía saboyana y el triunfo de la cocina local sobre la cocina francesa.[19]​

La diáspora italiana iniciada a mediados del siglo xix, difundió el gusto por la pizza a varios países de América y, finalmente, a todo el mundo.[17]​

Los ingredientes de la tradicional pizza margarita son:

Hay también diferentes tipos de pizzas que se realizan con agregados muy variados, como rodajas de tomate, diversos tipos de queso, jamón, anchoas, morrones, palmitos, salami, ananá, huevo, champiñones, verduras o cebolla, entre otros.

Una parte crucial de la elaboración de la pizza es la preparación de la masa y su leudado.[33]​ La harina debe mezclarse manualmente con el agua, la sal y la levadura hasta formar una masa homogénea, elástica y suave.[33]​ A continuación, la masa debe dejarse en reposo, generalmente en forma de bollo, para que leude y luego se separa para formar cada una de las bases, generalmente redondas.[33]​ La tradicional pizza napolitana utiliza harina doble cero (que tiene mayor cantidad de gluten), una gran cantidad de agua (un litro cada 1,7 kilos de harina) y se deja leudar dos días para hacerla más digestible, ya que evita que el proceso de fermentación finalice en el estómago.[26]​ Luego se forma un disco relativamente delgado, generalmente del tamaño de un plato grande o algo mayor. Antes de ingresarla al horno, se pinta la superficie con salsa de tomate y se cubre con queso cortado en pedazos y distribuido para que se derrita uniformemente, sin desbordar.[33]​ Lo mismo se hace en caso de que sean otros los ingredientes agregados.

Para la preparación de la pizza, es esencial la temperatura del horno que, para la Associazione Verace Pizza Napoletana, tiene que alcanzar 420 grados centígrados. Esa temperatura permite que la cocción sea homogénea y rápida, de modo tal que la masa mantenga una textura adecuada y los ingredientes no se quemen. A esa temperatura la pizza debe cocinarse unos dos minutos. Todo este proceso fue corroborado de manera científica, a excepción de la temperatura, que determinaron en 325-330 grados centígrados con un máximo de 390 cuando son muchos los comensales.[34]​

Para trocear la pizza en porciones se puede utilizar un cortapizzas, un disco afilado sujeto a un mango y suspendido por el centro de forma que puede girar cortando por presión sin arrastrar los ingredientes ni desgarrar la masa. 

La forma tradicional de servir las pizzas redondas es en porciones triangulares, generalmente de un sexto o un octavo del tamaño de la pizza completa. Las pizzas rectangulares, típicas de la pizza al taglio ('pizza al corte') suelen cortarse en cuadrados o rectángulos.

En Italia, la variante regional con más tradición es la pizza napolitana, que es, además, el único tipo de pizza italiana reconocido a nivel nacional y europeo. La masa contiene agua, harina, levadura y sal, nunca aceite, y el horno donde se cuece debe ser a leña. Su proceso e ingredientes están definidos en la norma UNI 10791: 98, elaborada por la Associazione Verace Pizza Napoletana junto con la especificación ETG.[35]​ Esta asociación, fundada en 1984, se dedica a promover el conocimiento de la pizza napolitana artesanal. Solo dos variantes están reconocidas: marinara y margarita.

En Sicilia, las pizzas se caracterizan por ser rectangulares y de masa gruesa. Es muy popular la pizza de Palermo (sfincione palermitano), que contiene tomate, anchoas, caciocavallo, cebolla, pan rallado y orégano. En Catania se consume la scacciata y en la provincia de Siracusa el pizzolo, que puede ser dulce o salado. 

En el Piamonte, particularmente en Turín, se puede encontrar la pizza al tegamino o al padellino. Se cocina en la sartén, por lo que la base queda ligeramente frita y es bastante gruesa y blanda.[36]​[37]​ En Marcas, la pizza marchigiana se puede encontrar en cuatro variantes: bianca con romero, bianca con cebolla, rossa simple y rossa con mozzarella, donde por bianca y rossa se entiende, respectivamente, con y sin tomate. Estas derivan de la más antigua crescia y tienen en común que se usa manteca de cerdo.[38]​[39]​ Otras se hacen también con aceite de oliva como el cacciannanze de Áscoli.[40]​

En Vico Equense, pequeña villa en la península sorrentina, se ha popularizado la pizza a metro, que es rectangular, contiene menos levadura y es más esponjosa y espesa debido a una cocción más suave y prolongada. Su nombre se debe a que se vende por metro.[41]​ En Milán, la más típica es la pizza al trancio, que es alta y blanda, ligeramente crujiente en la base y cubierta de abundante mozzarella. En Roma es muy popular la pizza al taglio y en Liguria la sardenara. En la región francesa de Provenza, es típica la pissaladière.[17]​

Otras pizzas clásicas italianas son:

La pizza argentina se consume en todo el país, principalmente en Buenos Aires, ciudad que tiene una gran proporción de habitantes descendientes de italianos y cuenta con la mayor cantidad de pizzerías por habitante del mundo.[7]​ Se caracteriza por la "media masa" (masa base de mayor grosor) y la abundancia de queso mozzarella de vaca, "hasta los bordes, para que se dore y se gratine",[42]​ así como por el acompañamiento con fainá y vino moscato.[43]​[44]​ Entre las pizzerías históricas argentinas se encuentra Banchero, fundada en 1932 pero originada mucho antes, en la panadería instalada por el genovés Agustín Banchero en 1893, donde su hijo Juan inventó la fugazza y la fugazzetta, y se generó la costumbre de acompañarla con fainá.[45]​[42]​ También se destaca la pizzería Güerrín, cuya pizza está considerada entre las mejores del mundo, con una producción diaria promedio de mil unidades, que alcanza 1500 los fines de semana.[8]​[46]​ En Buenos Aires es muy popular también una variante conocida como «pizza de cancha», muy similar a la pizza marinera, hecha con masa cubierta de salsa de tomate, sin queso y fuertemente condimentada, que fue creada por los vendedores ambulantes a la salida de los partidos de fútbol.[47]​

Las pizzas estadounidenses, conocidas internacionalmente como pizza americana,[nota 1]​ nacen a partir de la inmigración italiana a lo largo del siglo xx y han evolucionado hasta tener ciertos rasgos propios. Por lo general, la masa es más gruesa que la original napolitana y suelen tener más ingredientes, además de ser más ricas en queso y grasas. También suelen sustituir el aceite de oliva de la masa por mantequilla. 

La pizza estilo Nueva York se caracteriza por un gran tamaño y una masa muy fina. Su enorme tamaño se debe a que originalmente se vendía en porciones para llevar. Es la pizza por antonomasia de Nueva York y toda su área metropolitana, así como de muchos otros lugares de Estados Unidos. La variante más famosa es la cheesepizza, básicamente salsa de tomate y queso.[48]​

La pizza estilo Chicago se hornea en una sartén para que tenga un borde grueso y alto, lo que permite agregar mucha más cantidad de tomate y queso. Por eso se conoce más comúnmente como deep-dish pizza. La corteza queda crujiente y los ingredientes se desmoronan al cortar la pizza. Se suele comer con cuchillo y tenedor.[49]​

La pizza estilo California tiene la masa fina como la del estilo neoyorquino, pero lleva ingredientes típicos de la cocina californiana: barbacoa, piña, gambas, pato rostido, pollo al curry, brotes de helecho y otras hierbas.[50]​ Su invención se atribuye al chef Ed LaDou, aunque fue popularizada por Wolfgang Puck.[51]​

La pizza hawaiana contiene una base de tomate, queso, jamón y piña, generalmente enlatada, y a pesar de su nombre no procede de Hawái. Se cree que es un invento de Sam Panopoulos, un chef griego afincado en Canadá que se atribuyó la creación.[52]​ Otra teoría dice que es un invento alemán.[53]​ Se ha difundido en varios países y es una de las variantes de la pizza que genera mayor aceptación y rechazo a la vez.[54]​

La pizza enrollada (pizza arrotolata), también conocida como pizza Stromboli, es una pizza que ha sido enrollada. Fue creada por Nazzareno Romano, italiano residente en Filadelfia,[55]​ y se considera un clásico de la gastronomía italoestadounidense. Aunque es similar al calzone, a diferencia de este es de forma cilíndrica y no contiene salsa de tomate.[56]​ El nombre de Stromboli le sería dado posteriormente en honor a la película homónima.[56]​

Otras variantes regionales son la estilo Detroit, la estilo San Luis, la estilo Quad City, la estilo New Haven (o apizza), la estilo Nueva Jersey (o Trenton) y la beach pizza.

En Brasil se consume una variedad denominada pizza dolce, cubierta con dulce de leche, chocolate o frutas y comida como postre.[57]​[58]​ Las variantes que se denominan 'a la mexicana' o pizza mexicana incluyen ingredientes típicos de la cocina mexicana. En Escocia, la deep-fried pizza se vende como comida callejera en porciones fritas en el momento, a veces servidas con sal y vinagre o con salsa gravy espesa.[59]​

Aunque no es originaria de Irán, la pizza iraní o persa (پیتزای ایرانی piatzaa ayrana) se ha convertido en un plato muy popular, y las pizzerías se pueden encontrar en todas las ciudades y pueblos grandes. Se distingue por una base gruesa que es crujiente en el exterior y esponjosa en el interior. Sus ingredientes son bastante similares a los de la italiana, aunque está más especiada.[60]​

La democracia (del latín tardío democratĭa, y este del griego δημοκρατία dēmokratía)[1]​ es una manera de organización social que atribuye la titularidad del poder al conjunto de la ciudadanía. En sentido estricto, la democracia es una forma de organización del Estado en la cual las decisiones colectivas son adoptadas por el pueblo mediante mecanismos de participación directa o indirecta que confieren legitimidad a sus representantes. En sentido amplio, democracia es una forma de convivencia social en la que los miembros son libres e iguales y las relaciones sociales se establecen conforme a mecanismos contractuales. 

La democracia se puede definir a partir de la clasificación de las formas de gobierno realizada por Platón, primero, y Aristóteles, después, en tres tipos básicos: monarquía (gobierno de uno), aristocracia (gobierno «de los mejores» para Platón, «de los menos», para Aristóteles), democracia (gobierno «de la multitud» para Platón y «de los más», para Aristóteles).[2]​ 

Hay democracia indirecta o representativa cuando la decisión es adoptada por personas reconocidas por el pueblo como sus representantes. 

Hay democracia participativa cuando se aplica un modelo político que facilita a los ciudadanos su capacidad de asociarse y organizarse de tal modo que puedan ejercer una influencia directa en las decisiones públicas o cuando se facilita a la ciudadanía amplios mecanismos plebiscitarios consultivos.

Finalmente hay democracia directa cuando la decisión es adoptada directamente por los miembros del pueblo, mediante plebiscitos y referéndums vinculantes, elecciones primarias, facilitación de la iniciativa legislativa popular y votación popular de leyes, concepto que incluye la democracia líquida. Estas tres formas no son excluyentes y suelen integrarse como mecanismos complementarios en algunos sistemas políticos, aunque siempre suele haber un mayor peso de una de las tres formas en un sistema político concreto.

No debe confundirse República con Democracia, pues aluden a principios distintos, la república es el gobierno de la ley mientras que democracia significa el gobierno de la gente.

El término democracia proviene del griego antiguo (δημοκρατία) y fue acuñado en Atenas en el siglo V a. C. a partir de los vocablos δῆμος (dḗmos, que puede traducirse como «pueblo») y -κρατία -kratía, de la raíz de κράτος (krátos, que puede traducirse como «fuerza», «dominio» o «poder» ).[3]​

Sin embargo, la significación etimológica del término posiblemente sea mucho más compleja. El término «demos» parece haber sido un neologismo derivado de la fusión de las palabras demiurgos (demiurgi) y geomoros (geomori).[4]​ El historiador Plutarco señalaba que los geomoros y demiurgos, eran junto a los eupátridas, las tres clases en las que Teseo dividió a la población libre del Ática (adicionalmente la población estaba integrada también por los metecos, esclavos y las mujeres). Los eupátridas eran los nobles, los demiurgos eran los artesanos, y los geomoros eran los campesinos. Estos dos últimos grupos, «en creciente oposición a la nobleza, formaron el demos».[5]​ Textualmente entonces, «democracia» significaría, siempre según Plutarco, el «gobierno de los artesanos y campesinos», excluyendo del mismo expresamente a los esclavos y a los nobles.

Algunos pensadores consideran a la democracia ateniense como el primer ejemplo de un sistema democrático. Otros pensadores han criticado esta conclusión, argumentando por un lado que tanto en la organización tribal como en antiguas civilizaciones en todo el mundo existen ejemplos de sistemas políticos democráticos,[6]​ y por otro lado que solo una pequeña minoría del 10% de la población tenía derecho a participar de la llamada democracia ateniense, quedando automáticamente excluidos la mayoría de trabajadores, campesinos, esclavos y las mujeres.

De todas formas, el significado del término ha cambiado varias veces con el tiempo, y la definición moderna ha evolucionado mucho, sobre todo desde finales del siglo XVIII, con la sucesiva introducción de sistemas democráticos en muchas naciones y sobre todo a partir del reconocimiento del sufragio universal y del voto femenino en el siglo XX. Hoy en día, las democracias existentes son bastante distintas al sistema de gobierno ateniense del que heredan su nombre.

La democracia aparece por primera vez en muchas de las civilizaciones antiguas que organizaron sus instituciones sobre la base de los sistemas comunitarios e igualitarios tribales (democracia tribal).

Entre los casos mejor conocidos se encuentran la relativamente breve experiencia de algunas ciudades estado de la Antigua Grecia, en especial Atenas alrededor del año 500 a. C. Las pequeñas dimensiones y la escasa población de las polis (o ciudades griegas) explican la posibilidad de que apareciera una asamblea del pueblo, de la que solo podían formar parte los varones libres, excluyendo así al 75% de la población integrada por esclavos, mujeres y extranjeros. La asamblea fue el símbolo de la democracia ateniense. En la democracia griega no existía la representación, los cargos de gobierno eran ocupados alternativamente por todos los ciudadanos y la soberanía de la asamblea era absoluta. Todas estas restricciones y la reducida población de Atenas (unos 300.000 habitantes) permitieron minimizar las obvias dificultades logísticas de esta forma de gobierno.

En la América del siglo XII se formó la Liga Democrática y Constitucional de Haudenosaunee, integrada por las naciones Séneca, Cayuga, Oneida, Onondaga y Mohicanos, donde se consagraron los principios de limitación y división del poder, así como de igualdad democrática de hombres y mujeres. La democracia de Haudenosaunee ha sido considerada por varios pensadores como el antecedente más directo de la democracia moderna.[7]​

Durante la Edad Media europea se utilizó el término de «democracias urbanas» para designar a las ciudades comerciales, sobre todo en Italia y Flandes, pero en realidad eran gobernadas por un régimen aristocrático. También existieron algunas democracias llamadas campesinas, como la de Islandia, cuyo primer Parlamento se reunió en 930 y la de los cantones suizos en el siglo XIII. A fines del siglo XII se organizaron sobre principios democráticos las Cortes del Reino de León (1188), inicialmente llamado «ayuntamiento», debido a que reunía representantes de todos los estamentos sociales. En escritores como Guillermo de Ockham, Marsilio de Padua y Altusio aparecen concepciones sobre la soberanía del pueblo, que fueron consideradas como revolucionarias y que más tarde serían recogidas por autores como Hobbes, Locke y Rousseau. En Europa este República de las Dos Naciones con sistema político de la mancomunidad, llamado Democracia de los Nobles o Libertad dorada, se caracterizaba por la limitación del poder del monarca por las leyes y la cámara legislativa (Sejm) controlada por la Nobleza de Polonia (Szlachta). Este sistema fue el precursor de los conceptos modernos de democracia,[8]​Monarquía constitucional,[9]​[10]​[11]​ y federación.[12]​

En Europa el protestantismo fomentó la reacción democrática al rechazar la autoridad del papa, aunque por otra parte, hizo más fuerte el poder temporal de los príncipes. Desde el lado católico, la Escuela de Salamanca atacó la idea del poder de los reyes por designio divino, defendiendo que el pueblo era el receptor de la soberanía. A su vez, el pueblo podía retener la soberanía para sí (siendo la democracia la forma natural de gobierno) o bien cederla voluntariamente para dejarse gobernar por una monarquía. En 1653 se publicó en Inglaterra el Instrument of Government, donde se consagró la idea de la limitación del poder político mediante el establecimiento de garantías frente al posible abuso del poder real. A partir de 1688 la democracia triunfante en Inglaterra se basó en el principio de libertad de discusión, ejercida sobre todo en el Parlamento.

En América la revolución de los comuneros de Paraguay de 1735 sostuvo el principio democrático elaborado por José de Antequera y Castro: la voluntad del común es superior a la del propio rey. Por su parte, en Brasil, los afroamericanos que lograban huir de la esclavitud a la que habían sido reducidos por los portugueses, se organizaron en repúblicas democráticas llamadas quilombos, como el Quilombo de los Palmares o el Quilombo de Macaco.

La Independencia de Estados Unidos en 1776 estableció un nuevo ideal para las instituciones políticas de base democráticas, expandido por la Revolución francesa de 1789 y la Guerra de Independencia Hispanoamericana (1809-1824), difundiendo las ideas liberales, los derechos humanos concretados en la Declaración de Derechos de Virginia y la Declaración de los Derechos del Hombre y del Ciudadano, el constitucionalismo y el derecho a la independencia, principios que constituyeron la base ideológica sobre la que se desarrolló toda la evolución política de los siglos XIX y XX. La suma de estas revoluciones se conoce como las Revoluciones burguesas.

Las constituciones de Estados Unidos de 1787 con las enmiendas de 1791, Venezuela de 1811, España de 1812, Francia de 1848, y Argentina de 1853 ya tienen algunas características democráticas, que registrarán complejos avances y retrocesos. La evolución democrática inglesa fue mucho más lenta y se manifestó en las sucesivas reformas electorales que tuvieron lugar a partir de 1832 y que culminaron en 1911 con la Parliament Act, que consagró la definitiva supremacía de la Cámara de los Comunes sobre la de los Lores.

En realidad recién puede hablarse de la aparición progresiva de países democráticos a partir del siglo XX, con la abolición de la esclavitud, la conquista del sufragio universal, el reconocimiento de la igualdad legal de las mujeres, el fin del colonialismo europeo, el reconocimiento de los derechos de los trabajadores y las garantías de no discriminación para las minorías raciales y étnicas.

Clásicamente la democracia ha sido dividida en dos grandes formas: directa y representativa.

La democracia representativa, también llamada indirecta, es aquella donde los ciudadanos ejercen el poder político a través de sus representantes, elegidos mediante el voto, en elecciones libres y periódicas.

Algunos autores también distinguen una tercera categoría intermedia, la democracia semidirecta, que suele acompañar, atenuando, a la democracia indirecta. En la democracia semidirecta el pueblo se expresa directamente en circunstancias particulares, básicamente a través de cuatro mecanismos:

Se trata de la democracia en estado puro, tal como la vivieron sus fundadores atenienses, se practica en Suiza. Las decisiones las toma el pueblo soberano en asamblea. No existen representantes del pueblo, sino, en todo caso, delegados que se hacen portavoces del pueblo, que únicamente emiten el mandato asambleario. Se trata del tipo de democracia preferido no solo por los demócratas de la Antigua Grecia, sino también para muchos pensadores modernos como Rousseau. 

La Democracia Líquida es una clase de democracia directa en la que cada ciudadano tiene la posibilidad de votar cada decisión del parlamento y realizar propuestas, pero puede ceder su voto a un representante para aquellas decisiones en las que prefiere no participar.

En la práctica, existen muchas variantes del concepto de democracia, algunas de ellas llevadas a la realidad y otras solo hipotéticas. En la actualidad los mecanismos de democracia más extendidos son los de la democracia representativa; de hecho, se trata del sistema de gobierno más utilizado en el mundo. Algunos países como Suiza o Estados Unidos cuentan con algunos mecanismos propios de la democracia directa. La democracia deliberativa es otro tipo de democracia que pone el énfasis en el proceso de deliberación o debate, y no tanto en las votaciones. El concepto de democracia participativa propone la creación de formas democráticas directas para atenuar el carácter puramente representativo (audiencias públicas, recursos administrativos, ombudsman). El concepto de democracia social propone el reconocimiento de las organizaciones de la sociedad civil como sujetos políticos (consejos económicos y sociales, diálogo social).[13]​

Estas diferenciaciones no se presentan en forma pura, sino que los sistemas democráticos suelen tener componentes de unas y otras formas de democracia.
Las democracias modernas tienden a establecer un complejo sistema de mecanismos de control de los cargos públicos. Una de las manifestaciones de estos controles horizontales es la figura del proceso de destitución o «juicio político», al que pueden ser sometidos tanto los presidentes como los jueces, por parte de los parlamentos, de acuerdo a ciertas constituciones, como la de Argentina, Brasil o Estados Unidos. Otras agencias más modernas orientadas al mismo fin son el defensor del pueblo u ombudsman, las sindicaturas de empresas públicas, los organismos de auditoría, las oficinas de ética pública, etc.[14]​

Finalmente, cabe señalar que existe una corriente crecientemente relevante en el mundo anglosajón que propugna combinaciones de las instituciones actuales con aplicaciones democráticas del sorteo. Entre los autores más relevantes de esta corriente puede citarse a John Burnheim, Ernest Callenbach, A. Barnett y Peter Carty, Barbara Goodwin o, en el ámbito francés, Yves Sintomer. Los autores consagrados que han dedicado más espacio a este tipo de propuestas son Robert A. Dahl y Benjamin Barber. En el mundo hispanohablante la recepción aún es muy reducida, si bien autores como Juan Ramón Capella han planteado la posibilidad de acudir al sorteo como herramienta democratizadora.[15]​

En la democracia moderna juega un rol decisivo la llamada regla de la mayoría, es decir el derecho de la mayoría a que se adopte su posición cuando existen diversas propuestas. Ello ha llevado a que sea un lugar común de la cultura popular asimilar democracia con decisión mayoritaria. Las elecciones son el instrumento en el que se aplica la regla de mayoría; haciendo así de la democracia el ejercicio más eficiente, eficaz y transparente, donde se aplica la igualdad y la oportunidad de justicia, práctica originada en los siglos XVIII y XIX; cuando la mujer se hace partícipe del derecho al voto. Además, la democracia contemporánea, no se mantiene paralela al régimen absolutista y al monopolio del poder.

Sin embargo muchos sistemas democráticos no utilizan la regla de la mayoría o la restringen mediante sistemas de elección rotativos, al azar, derecho a veto (mayorías especiales), etc.[16]​ De hecho, en determinadas circunstancias, la regla de la mayoría puede volverse antidemocrática cuando afecta derechos fundamentales de las minorías, de los individuos o vulnera los principios fundamentales de la vida del Estado, cuestiones que conoceremos como la esfera de lo indecidible.[17]​

Las democracias reales suelen ser complejos mecanismos articulados, con múltiples reglas de participación en los procesos de deliberación,
toma de decisiones, en los que el poder se divide constitucionalmente o estatutariamente, en múltiples funciones y ámbitos territoriales, y se establecen variedad de sistemas de control, contrapesos y limitaciones, que llevan a la conformación de distintos tipos de mayorías, a la preservación de ámbitos básicos para las minorías y a garantizar los derechos humanos de los individuos y grupos sociales. Existe también una diferencia fundamental entre el concepto de democracia y democratización. El concepto de democracia esta conectado a la capacidad de la clase política de responder a las necesidades de la población. En cambio, el concepto de democratización tiene que ver con la capacidad de una sociedad de adaptarse al proceso de homogeneización cultural, legal y política que tuvo lugar luego del fin de la Guerra Fría.[18]​ 

Por esto es que debemos analizar cuales son los principios esenciales de la democracia.

La democracia debe entenderse como un sistema político entre las diferentes posibilidades que han existido para configurar los Estados a lo largo de la historia. Esto es, la democracia es una de las formas políticas en las que puede organizarse la convivencia social,[19]​ pues así como una sociedad puede establecerse como una democracia, también puede hacerlo como una Aristocracia o en una Autocracia.
La democracia conlleva la posibilidad de que existan medios de participación por parte de la ciudadanía, de que existan diferencias entre los participantes de dicho proceso y de que se expresen opiniones contrapuestas.[20]​ De este modo, se afirma que la democracia repudia la posibilidad de que una sola persona se abrogue el poder por propio y exclusivo arbitrio,[21]​ abriéndose el escaño del poder a una pluralidad de personas así como a la crítica y oposición por parte de los propios miembros de la sociedad.

De lo expuesto, podemos inferir ciertos principios sin los cuales no es posible afirmar que existe una democracia, veamos.

La democracia reconoce la posibilidad de que cualquier persona pueda participar en el ejercicio del poder político dentro de un determinado Estado. Por esto, es necesario reconocer la existencia de igualdad entre los ciudadanos, ya que, sin ella, no existirían los medios necesarios para que la participación y la oposición se desarrollen libremente. A la luz de esto, se abre la puerta a dos paradigmas que condicionan el desarrollo de la democracia por lo que respecta a la igualdad:[22]​

De esto obtenemos los ideales de igualdad y de libertad, puesto que, por un lado, tenemos la posibilidad de que una sociedad sea plural y con diversas necesidades e ideales sobre lo que es lo justo y, por otro lado, se tiene que los miembros de la sociedad –aun cuando no tuvieren entre sí cuestiones en común– participen en la entidad política que ostenta el poder en igualdad de circunstancias.

Es ahí que se observa la esencia de la democracia:

Es evidente que, partiendo del supuesto de que todos los individuos que participamos en la toma de decisiones políticas somos iguales –por lo que respecta a nuestro Derecho a participar–, nace el concepto de la democracia. Esto es, de la afirmación de que cualquier ciudadano tiene posibilidad de participar en la entidad política que ostentará el poder, obtenemos que el principal rasgo de la democracia, mismo que consiste en que la voluntad política proviene de quienes se encuentran gobernados por el mismo.[23]​
Es esta la trascendencia del principio de igualdad, porque, sin ella, no será posible generar que los individuos se sientan con la responsabilidad de participar en la toma de decisiones dentro del ente político que ostenta el poder. De algún modo, sin la sensación de igualdad, los individuos no se sentirán miembros de una misma colectividad, por lo que su sensación de responsabilidad se disminuirá, afectándose la esencia del Estado democrático.

Debe garantizar dicha posibilidad de acceso; es decir, los individuos debemos gozar de una serie de condiciones que propicien nuestra participación en la entidad política que ostenta el poder, mismas que solo pueden desarrollarse cuando existen los precursores democráticos ya mencionados.

Se ha afirmado que la Democracia, para el efecto de garantizar las condiciones mínimas para la participación ciudadana, impone al poder público límites en su ejercicio, mismos que serán tendientes a salvaguardar los intereses y derechos de los individuos, y, además, determina las funciones del propio poder y así lo divide;[17]​ hecho ello, se crean instituciones como el Legislativo, Ejecutivo y Judicial, y se les asigna a cada rama una función específica del poder, así como competencias y supuestos para su ejercicio.
De algún modo, en un Estado democrático se busca el límite del poder como garantía para que los ciudadanos participen en la política nacional, límites que se pueden identificar como dos tipos:

De acuerdo a esto, la Constitución de un Estado democrático contará con límites del poder tanto público como privado frente a los individuos y ante las propias instituciones que conforman al Estado; de ese modo se evita, por un lado, que se prive a los individuos de las condiciones necesarias para que desarrollen su vida y estén en condiciones para participar en la entidad política nacional, mientras que, por otro lado, se impide que el poder se encuentre concentrado en una sola persona o institución como sucede en los Estados autocráticos.

Al limitarse el poder, se garantiza que no existirán abusos en el ejercicio del mismo. De acuerdo a esto, los individuos podrán gozar de condiciones propias para el libre ejercicio de sus derechos individuales. Además, también se impide que el poder político se concentre en una sola institución o persona, lo cual resultaría pernicioso al no tener esta sola persona una visión global de las necesidades sociales y, por otro lado, podría ejercer sin limitación alguna su poder, inclusive sobre cualquier derecho individual.

La Constitución de un Estado democrático reconoce la posibilidad de que la totalidad de los miembros de la sociedad participen en la decisión de cómo habrá de configurarse el nuevo ente político. Esto deriva por la injerencia de los factores reales del poder en la toma de decisiones en el origen de la vida del Estado.[24]​ De algún modo, las decisiones tomadas por los factores reales del poder al haber decidido el rumbo que el Estado emprendería son los principios que regirán su desarrollo sociopolítico.

A estas se le denominan las decisiones políticas fundamentales, pues la totalidad de los poderes fácticos que rigen en un determinado lugar y momento erigirán los principios superiores que caracterizarán al sistema político-jurídico de su comunidad.[25]​ Por ejemplo, en un dereterminado Estado democrático podrá decidirse que el desarrollo económico se centre en la creación de empresas productivas del Estado, mientras que en otro Estado podría optarse por un desarrollo liberal de tales cuestiones. Tales ideales serán conocidos como las decisiones políticas fundamentales y, como veremos, formarán parte de la esfera de lo indecidible.

Como se ha visto en otros apartados, una democracia se fundamenta en diversos principios, tales como el de la división del poder, la igualdad o el respeto a los derechos fundamentales. Así, estos mismos principios democráticos no pueden ser desconocidos por persona o institución alguna, incluyéndose a las mayorías.

Así es, hay ciertos principios del Estado Democrático que no pueden ser reducidos por la actuación de las propias instituciones que se han constituido a la luz de la Democracia y, además, tampoco pueden ser olvidados por las mayorías democráticas aun cuando estas lo hubieren determinado así mediante los procesos y mecanismos que se hubieren establecido en la Constitución. De acuerdo a este postulado es que se constituye una "esfera de lo indecidible",[17]​ mismas que contienen decisiones políticas y jurídicas fundamentales que no pueden ser objeto de limitación alguna por parte de una mayoría.

A razón de esto, es que es factible hacer una diferenciación entre la democracia formal y la material. Por un lado, se puede considerar que una decisión democrática tomada por una mayoría es formalmente válida si la misma es tomada conforme al procedimiento que un Estado democrático estableció en su Constitución; pero, por otro lado, ello no es suficiente para considerar que dicha decisión también es materialmente válida, pues esto depende de que su contenido sea acorde con los principios fundamentales adoptados en la Constitución por la totalidad de los miembros de la sociedad.

Los actos de las mayorías, aun cuando hubieren sido creados conforme a la normatividad formal de la Democracia, pueden ser inválidos por transgredir aquello que hemos llamado la esfera de los indecidible: la Democracia sustancial también conocida como material. Las normas y actos de autoridad no deben ajustarse únicamente a los procedimientos democráticos, sino que también deben contener criterios mínimos a la luz de conceptos esenciales del Estado.[23]​

Este principio constitucional busca impedir el problema democrático conocido como "tiranía de las mayorías" y que más adelante es desarrollado.

Por último, se reconoce que un Estado democrático no puede subsistir si no existen herramientas que garanticen la regularidad de los actos de autoridad con la esencia del Estado.[26]​

De acuerdo a esto, el control de la constitucionalidad de los actos se torna en un eje de la eficacia constitucional, reforzando el carácter de obligatorio de la propia Constitución y las decisiones políticas fundamentales que fueron tomadas ahí y dotando de equilibrio a los derechos fundamentales y las estructuras institucionales determinadas por el acuerdo constitucional. Entonces, los medios de control de la constitucionalidad se identifican como los recursos jurídicos diseñados para verificar la correspondencia entre los actos emitidos por quienes detentan el poder y la Constitución, anulándolas cuando aquellas quebranten los principios constitucionales,[27]​ de esta forma también se desprende la naturaleza correctiva de los medios de control, por lo que destruyen actos ya emitidos.
Es con motivo de esta característica por virtud de la cual podemos afirmar que los derechos y principios contenidos en la Constitución -el cual resulta ser el pacto político por excelencia de una democracia- adquieren la naturaleza de norma jurídica, específicamente de una regla, que puede ser oponible frente a todos aquellos actos que la reten, adquiriendo firmeza inquebrantable al invalidar todos aquellos actos que transgredan su esencia.
Ante esto, se hacen exigibles los principios fundamentales adoptados en un Estado democrático.

No es factible considerar que todas las democracias son iguales. La creación de un Estado democrático deriva de la decisión del pueblo, por lo que la forma en que esta se regulará dependerá de los intereses de quienes resulten ser los factores reales del poder del momento y lugar en el que se ha decidido por el régimen democrático.[24]​ En razón de ello, hemos visto a lo largo de la historia política moderna la creación de diversas clases de modelos democráticos como los que a continuación se enuncian.

En muchos casos la palabra «democracia» se utiliza como sinónimo de democracia liberal. Suele entenderse por democracia liberal un tipo genérico de Estado surgido de la Independencia de Estados Unidos de 1776 y luego más o menos generalizado en las repúblicas y monarquías constitucionales que emergieron de los procesos de emancipación o revolucionarios contra las grandes monarquías absolutas y establecieron sistemas de gobierno en los que la población puede votar y ser votada, al mismo tiempo que el derecho de propiedad es preservado.[13]​[28]​

Así, aunque estrictamente el término «democracia» solo se refiere a un sistema de gobierno en que el pueblo ostenta la soberanía, el concepto de «democracia liberal» supone un sistema con las siguientes características:[cita requerida]

A partir de lo anterior algunos estudiosos[¿quién?] han sugerido la siguiente definición de democracia liberal: la regla de la mayoría con derechos para las minorías.[cita requerida]

Al respecto, este tipo de democracia tiene algunas particularidades que la distinguen de otras formas de democracia, entre ellas la libre confrontación de ideas. En palabras de Pío Moa:

La socialdemocracia es una versión de la democracia en la que se recurre a la regulación estatal y a la creación de programas y organizaciones patrocinados por el Estado, para atenuar o eliminar las desigualdades e injusticias sociales que, según consideran sus defensores, existirían en la economía libre y el capitalismo. La socialdemocracia se apoya básicamente en el sufragio universal, la noción de justicia social y un tipo de Estado denominado Estado de Bienestar.[30]​[31]​

La socialdemocracia surgió a finales del siglo XIX a partir del movimiento socialista, como una propuesta alternativa, pacífica y más moderada, a la forma revolucionaria de toma del poder y de imposición de una dictadura del proletariado, que sostenía una parte del movimiento socialista, dando origen a un debate alrededor de los términos de «reforma» y «revolución».[31]​

En general se ha presentado como ejemplo real de socialdemocracia al sistema de gobierno que predomina en los países escandinavos, el llamado modelo nórdico de bienestar.[32]​

El término «democracia» también se utiliza ampliamente no solo para designar una forma de organización política, sino una forma de convivencia y organización social, con relaciones más igualitarias entre sus miembros. En este sentido es habitual el uso del término «democratización», como por ejemplo la democratización de las relaciones familiares, de las relaciones laborales, de la empresa, de la universidad, de la escuela, de la cultura, etc., tales ejercicios están orientados básicamente al ámbito de la participación ciudadana, cuyos principales mecanismos utilizados para tales efectos son elecciones a través de voto popular, asambleas, propuestas de proyectos y todos aquellos en que se canaliza la voluntad de cambios o aprobaciones con participación directa de los distintos grupos sociales.[cita requerida]

Dos casos especiales para la idea de democracia son las monarquías constitucionales y las democracias populares que caracterizan al socialismo real.

La monarquía constitucional es una forma de gobierno que caracteriza a varios países de Europa (Gran Bretaña, España, Países Bajos, etc.), América (Canadá, Jamaica, etc.), y Asia (Japón, Malasia, etc.).

Las monarquías constitucionales varían bastante de país a país. En el Reino Unido las normas constitucionales actuales le conceden ciertos poderes formales al rey y los nobles (designación del primer ministro, designación de gobernantes en las dependencias de la Corona, veto suspensivo, tribunal de última instancia, etc.), además de los poderes informales derivados de sus posiciones.[33]​

Existe una tendencia general a la reducción progresiva del poder de los reyes y nobles en las monarquías constitucionales que se ha ido acentuando desde el siglo XX. Si bien, por tratarse de monarquías, en estos países existe una notable desigualdad ante la ley y de hecho de los reyes y demás nobles frente al resto de la población, la severa restricción de sus facultades de gobierno y judiciales ha llevado a que su participación en la mayoría de los actos de gobierno sea excepcional y sumamente controlada por otros poderes del Estado. Ello ha dado origen al expresivo dicho popular de que los reyes «reinan pero no gobiernan» para referirse a la débil influencia legal que los reyes y eventualmente los nobles tienen en los actos de gobierno cotidianos.

En el Reino de España el Rey promulga las leyes, convoca y disuelve las Cortes Generales, convoca referéndum, propone y cesa al Presidente, ejerce el derecho de gracia (indulto y conmutación de penas), declara la guerra, hace la paz, etc. En el ejercicio de todas sus funciones, el Rey actúa como mediador, árbitro o moderador, pero sin asumir la responsabilidad de sus actos que han de ser refrendados por el poder ejecutivo o legislativo,[34]​ lo que lo convierte en una figura representativa del estado pero sin poder político. El rey también goza de inviolabilidad y al igual que otros muchos jefes de estado republicanos, no puede ser juzgado por crimen alguno.[35]​

Los opositores a las monarquías constitucionales sostienen que no son democráticas, y que un sistema de gobierno en la que los ciudadanos no son todos iguales ante la ley, a la vez que no se puede elegir al jefe de Estado y otros funcionarios estatales, no puede denominarse democracia, si bien en España la monarquía no es constitucional sino parlamentaria.
Los defensores, en cambio, defienden que no tiene que ser democrático; se carga de ideologías. Es mejor que el jefe de Estado sea una persona imparcial, que alguien cargado de ideologías; y que, como su cargo es vitalicio, no va a cometer actos con fines electoralistas.[36]​

Modelo de representatividad basado en la experiencia de la Comuna de París y en la superación en el grado de representatividad de la Democracia liberal. Esta Democracia directa parte desde los puestos de trabajo cotidiano, donde se eligen representantes en cada fábrica, taller, granja u oficina, con mandato revocable en cualquier momento. Estos delegados se constituyen en una Asamblea local (soviets) y luego mandaban su representante a la Asamblea Nacional de Delegados del Pueblo.

Se le niega el voto y el poder político, al 10% de la población que abarca a empresarios, banqueros y terratenientes, que ya poseen el poder económico.[cita requerida] Por eso se dice que es Democracia obrera o dictadura del proletariado, ya que se aplica el poder político contra el poder económico instituido.

Este nuevo Estado debe ser instaurado por la insurrección de las masas, guiadas por un partido único o frente pluripartidista si fuese posible, con una línea partidaria que apunte a barrer con las instituciones del Estado burgués y la legalidad que asegura el poder económico de la minoría. La élite revolucionaria consciente tiene el objetivo de instruir a la sociedad en las formas de auto gobernarse, insta a elegir sus delegados en los puestos de trabajo, comités de fábricas, granjas y talleres, mediante el cual se aprenderá a administrar la economía, transformándose en una ciudadanía cotidiana y un poder permanente.

Se discute sobre la viabilidad de la eliminación de las condiciones de la existencia burguesa, supuesto para el paso de la sociedad enajenada a la comunista.[37]​ Esto significa que a medida que se avance en la socialización del poder político y del poder económico se producirá la «extinción del Estado» pasando a ser solo una estructura administrativa bajo control de todos los ciudadanos. Este «no Estado» es el considerado como la etapa final del socialismo: el comunismo.[38]​

Los países con sistemas políticos inspirados en el comunismo marxista conocidos como «socialismo real» como Cuba poseen sistemas de gobierno que suelen utilizar la denominación de «democracias populares». Las llamadas «democracias populares» se caracterizan por estar organizadas sobre la base de un sistema de partido político único o hegemónico, íntimamente vinculado al Estado, en el que según sus promotores puede participar toda la población y dentro del cual debe organizarse la representación de las diferentes posiciones políticas, o al menos de la mayor parte de las mismas permitidas por el Estado.[39]​ Por otra parte en las llamadas «democracias populares» actuales la libertad de expresión y de prensa están restringidas y controladas por el Estado.[13]​

Según sus defensores, la «democracia popular» es el único tipo de democracia en la cual se puede garantizar la igualdad económica, social y cultural de los ciudadanos, ya que los poderes económicos privados no puede influir en el sistema de representación.

Algunos marxistas opinan también que las actuales «democracias populares» no son verdaderas democracias socialistas y que constituyen una deformación de los principios originales del marxismo. En el caso concreto de China, sostienen que ha desarrollado una economía orientada al capitalismo, pero se vale de su título de “República Democrática Popular” para poder contar con mano de obra barata, mediante la explotación de los trabajadores chinos, hasta niveles de vida calificados como infrahumanos, tal como pasa en muchas democracias capitalistas.

Por derechos humanos y de los ciudadanos se entiende el conjunto de derechos civiles, políticos y sociales que están en la base de la democracia moderna. Estos alcanzan su plena afirmación en el siglo XX.

También se ha distinguido entre derechos humanos de primera (políticos y civiles), segunda (sociolaborales), tercera (socioambientales) y cuarta generación (participativos).

Guillermo O'Donnell ha puesto de manifiesto la importancia de los mecanismos de control o accountability horizontal, en las democracias modernas, a las que él prefiere denominar «poliarquías». El control horizontal, se diferencia del control vertical democrático que se realiza por medio de las elecciones periódicas, visualizado como una conformación del Estado, integrado por diversas agencias con poder para actuar contra las acciones u omisiones ilícitas realizadas por otros agentes del Estado.[40]​

Las democracias modernas tienden a establecer un complejo sistema de mecanismos de control de los cargos públicos. Una de las manifestaciones de estos contrales horizontales es la figura del proceso de destitución o «juicio político», al que pueden ser sometidos tanto los presidentes como los jueces, por parte de los parlamentos, de acuerdo a ciertas constituciones, como la de Argentina, Brasil o Estados Unidos. Otras agencias más modernas orientadas al mismo fin son el defensor del pueblo u ombudsman, las sindicaturas de empresas públicas, los organismos de auditoría, las oficinas de ética pública, etc.[14]​

En aquellos países que no tienen una fuerte tradición democrática, la introducción de elecciones libres por sí sola raramente ha sido suficiente para llevar a cabo con éxito una transición desde una dictadura a una democracia. Es necesario también que se produzca un cambio profundo en la cultura política, así como la formación gradual de las instituciones del gobierno democrático. Hay varios ejemplos de países que solo han sido capaces de mantener la democracia de forma muy limitada hasta que han tenido lugar cambios culturales profundos, en el sentido del respeto a la regla de la mayoría, indispensable para la supervivencia de una democracia.

Uno de los aspectos clave de la cultura democrática es el concepto de «oposición leal». Este es un cambio cultural especialmente difícil de conseguir en naciones en las que históricamente los cambios en el poder se han sucedido de forma violenta. El término se refiere a que los principales actores participantes en una democracia comparten un compromiso común con sus valores básicos, y que no recurrirán a la fuerza o a mecanismos de desestabilización económica o social, para obtener o recuperar el poder.

Esto no quiere decir que no existan disputas políticas, pero siempre respetando y reconociendo la legitimidad de todos los grupos políticos. Una sociedad democrática debe promover la tolerancia y el debate público civilizado. Durante las distintas elecciones o referéndum, los grupos que no han conseguido sus objetivos aceptan los resultados, porque se ajusten o no a sus deseos, expresan las preferencias de la ciudadanía.

Especialmente cuando los resultados de unas elecciones conllevan un cambio de gobierno, la transferencia de poder debe realizarse de la mejor forma posible, anteponiendo los intereses generales de la democracia a los propios del grupo perdedor. Esta lealtad se refiere al proceso democrático de cambio de gobierno, y no necesariamente a las políticas que ponga en práctica el nuevo gobierno.

El proceso de expansión mundial de las instituciones representativas entre mediados de los años setenta y el final del Siglo XX, conocido como Tercera Ola de Democratización según la denominación de Samuel Huntington (1991), produjo un número considerable de regímenes híbridos y democracias duraderas pero de calidad menos que óptima.[41]​ Este saldo no se ajustaba a las expectativas iniciales de muchos politólogos y puso en cuestión algunos de los supuestos de la transitología, el paradigma teórico que había predominado en los análisis de la ola democrática.[42]​ Uno de estos supuestos era que la viabilidad de la democracia no dependía de la existencia de pautas culturales específicas arraigadas en la sociedad, sino principalmente de la racionalidad de los actores políticos.[43]​

El problema de la calidad de las nuevas democracias generó un renovado interés por la cultura política, un enfoque que había surgido a principios de los años sesenta con los estudios pioneros de Gabriel Almond, Sidney Verba, Harry Eckstein y otros.[44]​ La difusión de encuestas transnacionales, como la World Values Survey, la European Social Survey y los Barómetros regionales, igual que los estudios de caso, han impulsado el progreso de esta corriente. La investigación empírica desarrollada a partir de la década de los ochenta, en la que sobresalen los trabajos de Ronald Inglehart, Robert D. Putnam y Christian Welzel, sugiere que un sistema definido de valores, creencias y hábitos parece ser esencial para la estabilidad, profundidad y efectividad de la democracia.[45]​

Este conjunto convergente de teorías, hipótesis y modelos subraya la influencia que ejercen en la calidad de las nuevas democracias rasgos culturales como los “valores de emancipación” o “autoexpresión”, el “capital social” o “comunidad cívica”, el apoyo de la población al sistema democrático y la confianza en las instituciones. Entre los elementos específicos de la cultura, tendrían un papel crítico el respeto por los otros, las aspiraciones de libertad, la igualdad de género, la confianza interpersonal, la participación política autónoma y la inserción en organizaciones voluntarias con objetivos que beneficien al conjunto de la sociedad.[46]​

Las diferencias y similitudes entre los conceptos de «democracia» y «república» dan lugar a varias confusiones habituales y diferencias de criterio entre los especialistas.

En general puede decirse que la república es un tipo de gobierno en el que se permite la participación de personas distintas en el ejercicio del poder político, lo cual evita que una misma persona ocupe un escaño en el poder. Por su lado, la democracia es un sistema en el que el poder político emana del pueblo y conlleva diversos principios tales como la división del poder, el control del poder y el trato igualitario entre los miembros de la sociedad.

Una república puede no ser democrática, cuando se encuentran excluidos amplios grupos de la población, como sucede con los sistemas electorales no basados en el sufragio universal, o en donde existen sistemas racistas en los que, si bien permiten la transición del poder político a distintas personas, desconocen principios como la igualdad, la participación y la posibilidad de manifestar la oposición por parte de cualquier persona de la sociedad.[20]​

Parece existir una relación entre democracia y pobreza, en el sentido de que aquellos países con mayores niveles de democracia poseen también un mayor PIB per cápita, un mayor índice de desarrollo humano y un menor índice de pobreza.

Sin embargo, existen discrepancias sobre hasta qué punto es la democracia la responsable de estos logros. Sin embargo, Burkhart y Lewis-Beck[47]​ (1994) utilizando series temporales y una metodología rigurosa han descubierto que:

La investigación posterior reveló cual es el proceso material por el que un mayor nivel de renta conduce a la democratización. Al parecer un mayor nivel de renta favorece la aparición de cambios estructurales en el modo de producción que a su vez favorecen la aparición de la democracia:

La afirmación de que el desarrollo económico conduce a la aparición de democracias ha merecido también algunas críticas,[48]​ que sostienen que se trataría de una relación espuria. Más que conducir directamente a la democracia, el desarrollo económico habría producido transformaciones en la estructura de clases de la sociedad capitalista, que posibilitaron una progresiva estabilización democrática en el mundo en los últimos 150 años, pero el desarrollo económico no condujo a la democracia en otros períodos previos de la historia. Asimismo, incluso en el siglo XX, algunas regiones como América Latina experimentaron retrocesos de la democracia en medio de procesos de modernización y expansión económica.[49]​[50]​[51]​

Un importante economista, Amartya Sen, ha señalado que ninguna democracia ha sufrido nunca una gran hambruna, incluidas democracias que no han sido muy prósperas históricamente, como India, que tuvo su última gran hambruna en 1943 (y que algunos relacionan con los efectos de la Primera Guerra Mundial), y que sin embargo tuvo muchas otras en el siglo XIX, todas bajo la dominación británica.[cita requerida]

El término democracia económica se utiliza en economía y sociología para designar a aquellas organizaciones o estructuras productivas cuya estructura decisional se basa en el voto unitario (una persona = un voto, o regla democrática), contrariamente a lo que se produce empresas privadas típicas de carácter capitalista, donde impera el voto plural ponderado por la participación en el capital (una acción = un voto).
El ejemplo típico de empresa democrática es la cooperativa, uno de cuyos principios cooperativos es precisamente el principio democrático de decisión. El ejemplo de democratización de la economía aplicado a mayor escala fueron los consejos de trabajadores y consumidores instituidos en la Unión Soviética.

Para BID, la democracia es un requisito esencial para que el Estado pueda:

La democracia es una forma de gobierno en el que la toma de decisiones queda legitimada por una base racional.[52]​ Una crítica común es la debilidad que muestra ante influencias desequilibradas en la toma de decisiones (conocidas como «democracias autoritarias», ya que autoridad es el poder legitimado) enmascaradas bajo esta legitimación, generando otras estructuras tales como:[53]​

Una de las críticas más comunes a la democracia es la que alega una ignorancia de la ciudadanía acerca de los aspectos políticos, económicos y sociales fundamentales en una sociedad, que la inhabilita para elegir entre las diversas propuestas. Este sistema fue denominado por Polibio como oclocracia.[54]​ Esta ignorancia haría que las decisiones tomadas por distintos sectores fueran erróneas en la mayoría de los casos, al no estar basadas en conocimientos técnicos.

El filósofo Sócrates creía que la democracia sin masas educadas (educadas en el sentido más amplio de ser conocedores y responsables) solo conduciría al populismo como criterio para convertirse en un líder elegido y no en una competencia. Esto conduciría finalmente a la desaparición de la nación. Esto fue citado por Platón en el libro X de La República, en la conversación de Sócrates con Adimanto. Sócrates era de la opinión de que el derecho al voto no debe ser un derecho indiscriminado, sino que debe otorgarse solo a las personas que pensaban lo suficiente sobre su elección.[55]​

Este argumento suele ser citado también por políticos para discutir los resultados de referendos y elecciones legítimas y también en contextos en los que se plantean reformas en busca de una profundización hacia formas de democracia más participativas o directas que la democracia representativa. Por otro lado, hay documentos (religiosos, filosóficos, teóricos, académicos) que mencionan a la clase política y gobernante como responsable de la ignorancia de la ciudadanía para lograr objetivos personales o elitistas. Para evitar esa circunstancia existen leyes que obligan a dedicar parte del patrimonio gubernamental a proporcionar información a la población mediante los boletines oficiales sobre las nuevas leyes o mediante la publicación de las sentencias sobre decisiones judiciales o mediante campañas a la población antes de celebrarse un referéndum, todas ellas grandes conquistas judiciales nobles que buscan mantener la paz social y económica, dejando un claro marco legal que defiende a todos los ciudadanos de la tiranía.

En algunos países se sabe que la ignorancia se traduce en las elecciones en abstención, en los países en que todos sus ciudadanos están obligados a votar la ignorancia puede afectar seriamente (o no) el resultado de las elecciones.

Varias tendencias de izquierda suelen pregonar por el abstencionismo electoral, ya que ven al sufragio como una «mentira» para el pueblo.

Aunque a efectos de cuantificar el grado de ignorancia popular a través de la abstención, se considera que la abstención recoge tanto los votos de quienes se dicen desconocedores de temas políticos (apolíticos) como de aquellos a quienes no les satisface el sistema en sí o ninguno de los candidatos o partidos que se presentan, por lo que muchas veces es difícil discernir la abstención por ignorancia y la abstención de protesta.

La regla de la mayoría en la que se basa la democracia puede producir un efecto negativo conocido como la tiranía de la mayoría, que no debe confundirse con la Oclocracia. Se refiere a la posibilidad de que en un sistema democrático una mayoría de personas pueden en teoría perjudicar o incluso oprimir a una minoría particular. Esto es negativo desde el punto de vista de la democracia, pues esta trata de que la ciudadanía como un todo tenga mayor poder.

He aquí algunos ejemplos reales en los cuales una mayoría actúa o actuó en el pasado de forma controvertida contra las preferencias de una minoría en relación a temas específicos:

Los defensores de la democracia exponen una serie de argumentos como defensa a todo esto. Uno de ellos es que la presencia de una constitución actúa de salvaguarda ante una posible tiranía de la mayoría. Generalmente, los cambios en estas constituciones requieren el acuerdo de una mayoría cualificada de representantes, o que el poder judicial avale dichos cambios, o incluso algunas veces un referéndum, o una combinación de estas medidas. También la separación de poderes en poder legislativo, poder ejecutivo y poder judicial hace más difícil que una mayoría poco unánime imponga su voluntad. Con todo esto, una mayoría todavía podría discriminar a una minoría, pero dicha minoría ya sería muy pequeña (aunque no por ello dicha discriminación deja de ser éticamente cuestionable).

Otro argumento es que una persona suele estar de acuerdo con la mayoría en algunos asuntos y en desacuerdo en otros. Y también las posturas de una persona pueden cambiar. Por tanto, los miembros de una mayoría pueden limitar la opresión hacia una minoría ya que ellos mismos en el futuro pueden ser parte de una minoría oprimida.

También hay quienes afirman que la democracia debe tratar asuntos objetivos, ya que esta clase de “opresión” es subjetiva pues está sujeta al sentir o pensar de unos cuantos y que por lo general no pasan de la trivialidad.

Un último argumento común es que, a pesar de los riesgos comentados, la regla de la mayoría es preferible a otros sistemas, y en cualquier caso la «tiranía de la mayoría» es una mejora sobre la "tiranía de una minoría". Los defensores de la democracia argumentan que la estadística empírica evidencia claramente que cuanto mayor es la democracia menor es el nivel de violencia interna. Esto ha sido formulado como «ley de Rummel», la cual sostiene que a menor nivel de democracia hay más probabilidades de que los gobernantes asesinen a sus propios ciudadanos

Una crítica hacia la democracia, derivada a su vez de un equívoco histórico [aclaración requerida], es la afirmación de que la democracia impulsó el ascenso de Adolf Hitler al poder al ser elegido democráticamente como presidente de la República de Weimar en 1933.

Los hechos históricos son que en 1932 Hitler perdió las elecciones presidenciales frente a Paul von Hindenburg, quien obtuvo un 53% frente al 36% de aquel. En las elecciones parlamentarias de julio del mismo año, el Partido Nazi de Hitler alcanza 230 escaños que lo volvieron el más numeroso. En ese momento el presidente Hindenburg le ofrece a Hitler la vicecancillería, pero este la rechaza; sin embargo los nazis concretan una alianza con las fuerzas de centro en el gobierno, a resultas de la cual, Hermann Goering, uno de los principales colaboradores de Hitler fue elegido presidente del parlamento (Reichstag).[cita requerida]

En noviembre de 1932 hubo nuevas elecciones parlamentarias en las que el Partido Nazi perdió dos millones de votos y el bloque se redujo a 196 escaños. La crisis electoral de la alianza de centro y derecha llevó a la renuncia del canciller Franz von Papen. Hindenburg piensa entonces en ofrecerle la cancillería a Hitler, pero ante la oposición del ejército nombra canciller al general Kurt von Schleicher. Este logra debilitar más aún a Hitler quien sufre una nueva derrota electoral en las elecciones regionales de Turingia. En esa situación las bancadas socialistas y comunistas quitan su apoyo a Schleicher, lo que lo obliga a renunciar en enero de 1933. Hindenburg nuevamente oscila entre von Papen y Hitler, decidiéndose por el primero. Pero no llega a asumir porque las SA (Sturmabteilung), la fuerza paramilitar del nazismo que dirigía Ernst Röhm, toman el control militar de Berlín. En esas condiciones Hindenburg nombró canciller a Hitler el 30 de enero de 1933. Hitler entonces disolvió el Parlamento y llamó elecciones para el 5 de marzo. En el ínterin, se produjo el incendio del Reichstag, lo cual aprovechó Hitler para anular las garantías constitucionales, impuso la pena de muerte para aplicar a aquellos que realizaran «alteraciones graves de la paz», y colocó a sus hombres en la conducción del ejército. En esas condiciones ya dictatoriales se realizaron las elecciones en las que obtuvo el 44% del Parlamento, número que tampoco le otorgó la mayoría. Para entonces la dictadura ya se había instalado definitivamente, y el Parlamento no volvió a tener influencia política.

Además, la constitución vigente en aquel contexto permitía el establecimiento de poderes dictatoriales y la suspensión de la mayoría de la propia constitución en caso de «emergencia», sin ningún tipo de votación, algo impensable en la mayoría de democracias modernas. De cualquier forma es importante señalar que las violaciones a los derechos humanos más grandes tuvieron lugar después de que Hitler aboliera por completo el sistema democrático.

Dentro de la concepción marxista bajo el materialismo histórico, el Estado es el órgano de la sociedad para el mantenimiento del orden social al servicio de la clase dominante.[56]​[57]​ La democracia burguesa se ejerce como una dictadura de la burguesía sobre el proletariado. La sociedad capitalista está fundada en la explotación humana, al robo del trabajo humano a través del concepto de «plus valor», legitimado en la propiedad privada. Luego el Estado burgués no puede ser defensor de los intereses generales, ya que estos se oponen a los de la propiedad privada. Por el contrario, la dictadura del proletariado, es la dictadura de la clase más numerosa que no busca sostener su situación de dominio sino hacer desaparecer los antagonismos de clase. Solo en la sociedad comunista, cuando se haya roto cuando hayan desaparecido los capitalistas y no haya clases sociales, solo entonces "desaparecerá el Estado y podrá hablarse de libertad".[58]​[59]​


Asimismo, Marx pensaba que el sufragio universal tendría como «resultado inevitable […] la supremacía política de la clase obrera»; sin embargo, también opinaba que el gobierno representativo, al brindar a los funcionarios amplias facultades, podría debilitar el potencial emancipador del voto. De esta manera, él proponía —para sancionar inmediatamente a los representantes— que las elecciones fuesen más frecuentes y con mandatos revocables en cualquier momento. De igual modo, Marx apoyaba el «mandato imperativo» en el cual la población tiene influencia directa sobre el proceso legislativo. Además, criticó el poder ejecutivo excesivo.[60]​

Por su parte, Mao Zedong planteó que durante la revolución china una democrática que él denominaba Nueva Democracia, la cual precede a una segunda etapa socialista.[61]​

Se denomina período helenístico, helenismo o periodo alejandrino (por Alejandro Magno) a una etapa histórica de la Antigüedad cuyos límites cronológicos vienen marcados por dos importantes acontecimientos políticos: la muerte de Alejandro Magno (323 a. C.) y el suicidio de la última soberana helenística, Cleopatra VII de Egipto, y su amante Marco Antonio, tras su derrota en la batalla de Accio (31 a. C.). Es la herencia de la cultura helénica de la Grecia clásica que recibe el mundo griego a través de la hegemonía y supremacía de Macedonia, primero con la persona de Alejandro Magno y después de su muerte con los diádocos (διάδοχοι) o sucesores, reyes que fundaron las tres grandes dinastías que predominarían en la época: Ptolemaica, Seléucida y Antigónida. Estos soberanos supieron conservar y alentar el espíritu griego, tanto en las artes como en las ciencias. Entre la gente culta y de aristocracia, «lo griego» era lo importante, y en este concepto educaban a sus hijos. El resto de la población de los reinos situados en Egipto y Asia no participaba del helenismo y continuaba sus costumbres, su lengua y sus religiones. Las ciudades-estado griegas (Atenas, Esparta y Tebas, entre otros) llegaron al declive y las sustituyeron en importancia las ciudades modernas de Alejandría, Pérgamo y Antioquía, cuyo urbanismo y construcción tenían nada que ver con las anteriores. En todas ellas se hablaba griego en su variante llamada koiné (κoινή), adjetivo griego significando «común». Vale decir, la lengua común o panhelénica, principal vehículo de cultura. Este se usaba mucho en aquel tiempo.

Es considerado un período de transición entre el declive de la época clásica griega y el alza del poder romano. Sin embargo, el esplendor de ciudades como Alejandría, Antioquía o Pérgamo, la importancia de los cambios económicos, el mestizaje cultural y el papel dominante del idioma griego y su difusión son factores que modificaron profundamente el Oriente Medio antiguo en esta etapa. Esta herencia cultural será asimilada por el mundo romano, surgiendo así con la fusión de estas dos culturas lo que se llama «cultura clásica», fundamento de la civilización occidental.

El término «helenístico» lo usó por primera vez el historiador alemán Johann Gustav Droysen en Geschichte des Hellenismus (1836 y 1843), a partir de un criterio lingüístico y cultural, es decir, la difusión de la cultura propia de las regiones en las que se hablaba el griego (ἑλληνίζειν – hellênizein), o directamente relacionadas con la Hélade a través del propio idioma, fenómeno alentado por las clases gobernantes de origen heleno de aquellos territorios que nunca tuvieron relación directa con Grecia, como pudo ser el caso de Egipto, Bactriana o los territorios del Imperio seléucida. Este proceso de helenización de los pueblos orientales, y la fusión o asimilación de rasgos culturales orientales y griegos, tuvo continuidad, como se ha mencionado, bajo el Imperio romano.

Los trabajos arqueológicos e históricos recientes conducen a la revalorización de este período y, en particular, a dos aspectos característicos de la época: la importancia de los grandes reinos dirigidos por las dinastías de origen griego o macedónico (Lágidas, Seléucidas, Antigónidas, Atálidas, etc.), unida al cometido determinante de decenas de ciudades cuya importancia fue mayor que la idea comúnmente aceptada durante mucho tiempo.

Después de las guerras de Peloponeso, las polis griegas siguieron luchando entre sí. Esta situación la aprovechó el Reino de Macedonia, situado en el norte de Grecia. Su rey Filipo II sometió a las ciudades griegas.

En el año 336 a. C., a los 20 años de edad, el hijo de Filipo II fue proclamado rey de Macedonia como Alejandro III, siendo reconocido como el gobernante de toda la Hélade tras su aplastante victoria sobre Tebas dos años más tarde. Durante su breve reinado, que duró apenas 13 años hasta el 323 a. C., realizó la conquista más rápida y espectacular de toda la Antigüedad. El pequeño reino balcánico, en alianza con algunas polis griegas, se convirtió inesperadamente en el imperio más grande de la época, tras sojuzgar al Imperio persa de Darío III. Este soberano aqueménida fue derrotado en cuatro años (334–330) tras tres batallas: en el río Gránico, en Issos y en la llanura de Gaugamela. Durante los cuatro  años siguientes (hasta el 327 a. C.) Alejandro se dedicó a la lenta y difícil conquista de las satrapías de Asia Central, además de asegurar, en el 325 a. C., la dominación macedónica en el valle del río Indo. En ese momento Alejandro, presionado por sus agotadas tropas, hubo de renunciar a proseguir con su epopeya, regresando a lo que se había convertido en el núcleo de su imperio, Mesopotamia. En ese momento sus dominios se extendían desde el Danubio al Indo y desde Egipto hasta el Sir Daria.

A fin de asegurar su poder en todo el territorio, trató de asociar la clase dirigente del antiguo Imperio aqueménida a la estructura administrativa de Macedonia. Intentó crear una monarquía que asumiera, a la vez, la herencia macedónica y griega y, por otro, la herencia persa y, en términos generales, la asiática. La muerte inesperada del rey, víctima probablemente de la malaria a la edad de 32 años, puso fin a esta tentativa original, que fue muy criticada por el entorno macedónico del soberano.


La prematura muerte de Alejandro supuso que sus herederos directos no tuviesen la edad necesaria como para afrontar la tarea de gobernar el imperio. De los dos hijos varones de Alejandro, Heracles (hijo de Barsine) tenía 4 años, mientras que Alejandro (hijo de la princesa bactriana Roxana) no había nacido aún en el momento de la muerte de su padre. De esta forma, los llamados diádocos, los generales y oficiales de Alejandro a lo largo de la campaña persa, fueron quienes lucharon por el control del imperio durante 40 años, hasta el año 281 a. C. Las sucesivas guerras en las que se enfrentaron Pérdicas, Ptolomeo, Casandro, Lisímaco, Antígono y Seleuco, por citar a los más relevantes, acabaron tanto con la cohesión del imperio (repartido finalmente entre los vencedores) como con los familiares de Alejandro: su madre Olimpia, su hermana Tesalónica, y sus dos hijos.
Las regiones de Grecia, Macedonia y Asia Menor fueron las que más profundamente se vieron afectadas por las incesantes campañas militares que enfrentaron a los diádocos, mientras que la parte oriental del imperio se separó rápidamente, creándose varios reinos griegos en Bactriana. Los generales prestaron poca atención a la pérdida de los territorios orientales, puesto que lo esencial para ellos era hacerse con el control total del imperio luchando contra sus rivales. La excepción fue Ptolomeo, uno de los compañeros de infancia de Alejandro, del que algunos autores aventuran que era un hijo ilegítimo de Filipo II. Con inteligencia se apoderó enseguida de Egipto y se apresuró a crear un estado duradero, renunciando a las ambiciones imperiales que consideraba poco realistas. Fue uno de los principales oponentes a la causa imperial, convirtiéndose de esta forma en uno de los fundadores del mundo helenístico.

Sin embargo, Antígono y su hijo Demetrio fueron quienes más lucharon por restablecer el Imperio macedónico, llegando a controlar Anatolia y el Levante mediterráneo antes de ser derrotados por una coalición del resto de los diádocos (excepto Ptolomeo) en la batalla de Ipso (301 a. C.). Muerto Antígono, Demetrio huyó a Europa, donde consiguió apoderarse temporalmente de Macedonia, antes de ser derrotado y terminar sus días miserablemente como prisionero de Seleuco. El hijo mayor de Ptolomeo I, Ptolomeo Cerauno, fue expulsado de Egipto por su padre, refugiándose en casa de su cuñado Lisímaco en Tracia, y apoderándose de su reino y de Macedonia, tras lo cual llegó a asesinar a Seleuco, que se enfrentaba a él. El Medio Oriente estaba, por tanto, dominado por las ambiciones de estos generales, que con presteza se coronaban reyes, apoyados por sus tropas, constituidas generalmente por mercenarios griegos y macedonios.

De esta forma, se estableció en el siglo III a. C. un precario equilibrio entre las tres dinastías descendientes de los diádocos, (los llamados epígonos —επιγονος—, 'los nacidos después' o 'sucesores') que se repartieron los territorios de forma poco homogénea y aun forzada. Macedonia y la Grecia continental fue gobernada por los descendientes de Antígono (los Antigónidas); Egipto, Chipre y Cilicia por los Lágidas; y Asia Menor, Siria, Mesopotamia y Persia occidental conformaron el poco homogéneo Imperio seléucida.

Al lado de las tres monarquías principales, coexistían otros reinos más pequeños, pero que desempeñaron un papel destacado, como el reino de Pérgamo, controlado por los Atálidas; el reino del Epiro, en la actual Albania; los reinos del Ponto y de Bitinia, en Anatolia; o el que fundó Hierón II en Siracusa, en la Magna Grecia. 

Es preciso añadir además las confederaciones de ciudades que se oponían a los intereses de otros reinos mayores, especialmente a Macedonia, como fueron la Liga Aquea y la Liga Etolia, que desempeñaron un importante papel en la zona egea hasta la conquista romana. Algunas de estas ciudades llegaron incluso a preservar completamente su independencia y a mantener relaciones en pie de igualdad con los reinos helenísticos, como es el caso de Rodas.

A finales del siglo II a. C., y tras 150 años de enfrentamientos y debilitamiento de todas las ciudades, Grecia cayó finalmente bajo la dominación romana. Fue a principios del siglo II a. C. cuando Roma intervino realmente en Oriente. En principio se enfrentó militarmente a los antigónidas, concretamente a Antíoco III Megas, el más importante de los soberanos helenísticos antes de Mitrídates y Cleopatra. La derrota de Antíoco fue decisiva en la pérdida de influencia política de los seléucidas en Asia Central, en Persia y, por último en Mesopotamia. Antíoco III fue el último rey seléucida que todavía poseía los medios para dirigir una expedición hasta los límites de la India. Durante el reinado de su hijo, los seléucidas no consiguieron dominar la insurrección de los Asmoneos en Palestina, que consiguieron instaurar un reino judío independiente. La irrupción de los partos aceleró la descomposición política y, a principios del siglo I a. C., los soberanos seléucidas ya sólo gobernaron en Siria.

Después de su victoria sobre los seléucidas, Roma promovió un lento y complejo proceso de desgaste sobre los reinos helenísticos, con la complicidad de varias ciudades griegas y del reino de Pérgamo, asegurándose tras dos siglos el completo dominio del Mediterráneo oriental. El acto final de esta conquista fue la lucha que enfrentó a Octaviano (César Augusto) contra Marco Antonio y su aliada, la última soberana de Egipto, Cleopatra VII. Tras ser derrotados en Accio, ambos se suicidaron ante la inminente victoria de Octaviano (30 a. C.).

No obstante, la penetración romana en el Oriente helenístico no se produjo sin resistencia, y los romanos precisaron no menos de tres guerras para doblegar al rey del Ponto, Mitrídates VI, en el siglo I a. C. El general Cneo Pompeyo Magno suprimió en el 63 a. C. el debilitado reino seléucida, reducido al territorio de Siria, reorganizando el Oriente, según el orden romano. El mundo helenístico se convirtió desde entonces en el campo de batalla donde se definieron las ambiciones de los diversos generales de la República romana, como sucedió en Farsalia, Filipos o Accio, donde se impuso finalmente Octaviano.

La monarquía helenística era personal, lo cual significaba que podía llegar a ser soberano cualquiera que, por medio de su conducta, sus méritos o sus acciones militares, pudiese aspirar al título de basileus. En consecuencia, la victoria militar era, la mayoría de las veces, el acto que legitimaba el acceso al trono, permitiendo así reinar sobre una provincia o un estado. Seleuco I utilizó la ocupación de Babilonia en 312 a. C. para legitimar su presencia en Mesopotamia, o su victoria en 281 a. C. sobre Lisímaco para justificar sus reivindicaciones sobre el Bósforo y Tracia. Asimismo, los reyes de Bitinia sacaron provecho de la victoria en 277 a. C. de Nicomedes I sobre los gálatas para afirmar sus pretensiones territoriales.

Esta monarquía personal no tenía reglas de sucesión precisas, por lo cual eran frecuentes querellas incesantes y asesinatos entre los muchos aspirantes. Tampoco existían leyes fundamentales ni textos que determinaran los poderes del soberano, sino que era el propio soberano quien determinaba el alcance de su poder. Este carácter absoluto y personal era, a la vez, la fuerza y la debilidad de estas monarquías helenísticas, en función de las características y la personalidad del soberano. Por tanto, fue necesario crear ideologías que justificaran la dominación de las dinastías de origen macedonio y de cultura griega sobre los pueblos totalmente ignorantes de esta civilización. Los lágidas pasaron, de este modo, a ser faraones ante los egipcios y tenían derecho a aliarse con el clero autóctono, otorgando espléndidas donaciones a los templos.

En cuanto a los pueblos de origen griego y macedónico que también gobernaban, los soberanos helenísticos debían mostrar la imagen de un rey justo, que asegurase la paz y el bienestar de sus pueblos, existiendo así la noción de evergetes, el rey como benefactor de sus súbditos. Una de las consecuencias, acaecida ya en el reinado de Alejandro Magno, fue la divinización del soberano, a quien rendían honores los súbditos y las ciudades autónomas o independientes que habían sido favorecidas por el rey, lo que permitió reforzar la cohesión de cada reino en torno a la dinastía reinante.

La fragilidad del poder de los soberanos helenísticos les obligaba a una incesante actividad. En primer lugar era necesario vencer militarmente a sus adversarios, por lo que el periodo se caracterizó por una serie de conflictos entre los propios soberanos helenísticos o contra otros adversarios exteriores, como los partos o la incipiente Roma. Los soberanos se veían obligados a viajar constantemente a fin de instalar guarniciones, a la vez que erigían ciudades que controlasen mejor las divisiones administrativas de sus reinos, siendo sin duda Antíoco III el monarca helenístico que más viajó entre Grecia, Siria, Egipto, Mesopotamia, Persia y las fronteras de India y Asia Menor, antes de morir cerca de la ciudad de Susa en 187 a. C. A fin de mantener sus armadas y financiar la construcción de las ciudades, fue indispensable que los soberanos desarrollaran una sólida administración y fiscalidad. Los reinos helenísticos se convirtieron así en gigantescas estructuras de explotación fiscal, erigiéndose en herederos directos del Imperio Aqueménida. Este trabajo agotador, al que se unían las incesantes quejas y recriminaciones (ya que el rey era también juez para sus súbditos) hicieron exclamar a Seleuco I:


Alrededor de estos soberanos gravitaba una corte en la que el cometido de los favoritos se volvió gradualmente preponderante. Por regla general, eran los griegos y los macedonios los que casi siempre ocuparon el título de amigos del rey (philoi). El deseo de Alejandro Magno de asociar las elites asiáticas al poder fue abandonado, por lo que esta dominación política greco-macedónica adquirió, en muchos aspectos, la apariencia de una dominación colonial. Para conseguir unos colaboradores fieles y eficaces, el rey tenía que enriquecerlos con donaciones y dominios pertenecientes al dominio real, lo cual no impidió que algunos favoritos mantuvieran una dudosa fidelidad, y en ocasiones, especialmente en caso de una minoría de edad real, ejercer efectivamente el poder. Son los casos de Hermias, del que Antíoco III no pudo deshacerse fácilmente, o Sosibio en Egipto, al que Polibio achacó una reputación siniestra.

Estos reyes disponían de un poder absoluto, pero estaban sometidos a múltiples obligaciones, como asegurar sus fronteras, vencer a sus enemigos y poner a prueba su naturaleza real por medio de su comportamiento, legitimando su función por la divinización de su persona. En la época clásica, el modelo de la monarquía, rechazada por los filósofos griegos, era asiático; en la época helenística era griego.

La monarquía helenística se apoyó en una aristocracia creada por el propio rey y desarrolló un carácter especialmente cosmopolita, muy lejos de la anterior nobleza solariega. En adelante el rey no sería elegido libremente por sus ciudadanos. Los reyes helenísticos y sus nobles fueron elegidos por el propio rey, pero para llevar a cabo con éxito y ante el pueblo tal sistema, insistieron en la idea de la divinidad, es decir, el rey tenía derecho a gobernar y a seleccionar la nobleza porque su poder lo había obtenido a través de su linaje divino y porque él mismo era en cierto modo un dios. El paso siguiente fue iniciar el culto al rey.

Este sistema de divinización fue más político que religioso y tenía sus antecedentes en el pensamiento griego anterior con ejemplos de veneración a héroes y otros personajes mortales que se convirtieron en deidades después de su muerte, como es el ejemplo de Asclepio y otras figuras menores que habían sido jefes militares o fundadores de ciudades. La deificación o apoteosis en vida de los reyes helenísticos nunca o casi nunca fue un asunto puramente religioso o espiritual; nadie fue a rezar o a pedir gracias especiales a ninguno de estos personajes. Sin embargo, fue necesario establecer el poder político en seres considerados por sus súbditos como dioses.

El culto al rey había empezado ya en la figura de Alejandro Magno que fue reconocido como un mortal realizador de grandes hazañas y descendiente de Heracles, confirmado en el oráculo de Siwa como hijo del propio Zeus-Amón. La deificación de Alejandro en vida le sirvió en muchas ocasiones como aprobación y reconocimiento legal de su poder real. El propio Alejandro se tomaba su deificación como algo muy serio. Después de su muerte muchas de las ciudades helenísticas siguieron este proceso, deificando a algunos de sus diádocos, como ocurrió con Demetrio Poliorcetes, Antígono II Gónatas, Lisímaco de Tracia, Casandro de Macedonia, Seleuco I Nicátor y Ptolomeo I.

Ptolomeo I nunca pidió honores divinos, pero su hijo Ptolomeo II organizó la ceremonia de la apoteosis para su padre y su madre Berenice, con el título de Dioses Salvadores (Sóter). Más tarde, hacia el año 270, Ptolomeo II y su esposa Arsínoe fueron deificados en vida con el título de Dioses hermanos (Filadelfo). Se sabe que se les rindió culto en el santuario de Alejandro Magno que aún existía, donde su diádoco Ptolomeo I había depositado el cuerpo (en la actualidad es un misterio el paradero de este santuario).

Los reyes y reinas sucesores de Ptolomeo II fueron deificados inmediatamente después de su ascenso al trono, con ceremonias de apoteosis en que podía verse la influencia de la religión y tradición egipcias. En el Egipto helenístico el culto al rey fue una fusión entre las tradiciones griegas para la deificación política y las tradiciones egipcias, con una gran carga religiosa.

Son unas jarras de cerámica vidriada, fabricadas en serie, que se utilizaban en las fiestas que se hacían para el culto de los reyes. Se levantaban altares provisionales donde se hacían las ofrendas. Las libaciones de vino se depositaban en estas jarras especiales que solían estar decoradas con el retrato de la reina que ocupaba el trono en ese momento. En el entorno artístico se llaman vasos de la reina porque siempre viene representada la reina, con una cornucopia en la mano izquierda y un plato de libaciones en la derecha, con un altar y un pilar sagrado. Los relieves descritos iban acompañados con inscripciones que servían para identificar a la reina representada. Algunas de estas jarras o vasos han aparecido en distintas tumbas. Estos ejemplares se pueden fechar desde Ptolomeo II hasta el año 116 a. C. El vestido de las reinas es fundamentalmente griego: llevan un quitón sin mangas y un himatión enrollado alrededor de la cintura y recogido sobre el brazo izquierdo.

A la muerte de Seleuco I su hijo Antíoco I Sóter preparó la ceremonia para su apoteosis. Más tarde se fundó un sacerdocio especializado para el culto del monarca vivo y de sus antepasados. Los reyes de Pérgamo dijeron ser descendientes del dios Dioniso. Estos reyes eran venerados en vida, pero solo después de su muerte recibían el título de theos. Antíoco III en el 193 a. C. creó una comunidad de sacerdotisas que serían las encargadas del culto a su esposa Laodice. Una de las normas dictadas por este rey para dichas sacerdotisas fue que en su indumentaria debían llevar una corona de oro decorada con retratos de la reina.

Aparentemente, algunas ciudades de la Grecia independiente, como Atenas y Corinto, conservaban su autonomía, sus instituciones y sus tradiciones. Los problemas sociales que iban surgiendo, más el empobrecimiento paulatino hicieron que esta Grecia clásica, no perteneciente a los estados helenísticos, fuera sufriendo una crisis tras otra hasta la intervención de Roma.

Las islas griegas mantuvieron una cierta prosperidad gracias a las importantes vías creadas para el intercambio entre Asia, Egipto y Occidente. Contaban, sin embargo, con la constante inseguridad provocada por los piratas de regiones como Iliria, Creta y Cilicia.

Las koiná (κoινά, plural de koinón, κoινόν) fueron los estados federales, también llamados ligas, formados por las ciudades más pequeñas. Estas confederaciones surgieron como una forma de protección y resistencia frente a los gobernantes de Macedonia, el poder hegemónico de este período, y al que sólo hacían frente estas ligas federales. Fueron dos las más influyentes durante el periodo helenístico, el Koinón Etolio (o Liga Etolia) y el Koinón Aqueo (o Liga Aquea).

Los sucesores de Alejandro tuvieron buen cuidado en seguir el espíritu que su gran general les había infundido: helenizar el Oriente y llevar hasta los confines conquistados la civilización griega a la que consideraban la mejor (si no la única) para el hombre. Durante la etapa del griego clásico los grandes centros urbanos fueron llamados polis (Atenas, Siracusa, Corinto), que eran verdaderos Estados independientes. Las nuevas ciudades del mundo helenístico contaban con una autonomía jurídica y financiera, estaban gobernadas por magistrados, pero ya no era el Estado independiente, sino que todas ellas dependían de un gobernador nombrado por el rey, llamado epistates. Por otra parte los reyes de los territorios helenísticos participaban personalmente con su fortuna en el embellecimiento y engrandecimiento de muchas de estas ciudades, siendo los principales mecenas de la construcción de edificios públicos o de la reconstrucción o restauración. Todas estas ciudades con su régimen de vida y su política reformada en gran medida favorecieron el auge económico y como consecuencia, el tesoro real.

Aunque en el fondo la política administrativa fue casi la misma en los reinos helenísticos, y el afán de conservar y extender la cultura griega era un lazo de unión, cada reino dotó a sus ciudades de un estilo propio y diferente. No siempre la fundación de estas ciudades partió de la nada. Dentro del concepto fundacional se puede incluir un simple cambio de nombre de una ciudad ya existente (con añadidos y mejoras) o la transformación de un pueblo pequeño indígena en una ciudad próspera.

El trazado de las ciudades era la consecuencia de un estudio bastante serio. Además de la belleza y el sentido práctico se tenían en cuenta muchos más detalles que se conocen en la actualidad gracias a las inscripciones de reglamentos municipales descubiertas en los yacimientos arqueológicos. Se daban normas para la anchura de las calles, para la distancia entre las viviendas, para la construcción de acueductos, recogida de basura, etc.

El primero de los reyes, Seleuco I Nicátor fundó 16 ciudades a las que dio el nombre de Antioquía en recuerdo de su padre llamado Antíoco. Y con otros nombres diversos llegó a fundar hasta 60. Su hijo, Antíoco I Sóter, siguió multiplicando la fundación de ciudades y más tarde, en época de Antíoco IV Epífanes, hubo otro gran impulso de construcción.

La fundación de una ciudad nueva, desde un punto de vista urbanístico, seguía las reglas difundidas por el filósofo y arquitecto griego Hipódamo de Mileto hacia el año 480 a. C. y que aconsejan un proyecto cuadrilátero con calles cortadas en ángulo, con zonas que puedan ocupar los servicios, los edificios oficiales, templos y con otras zonas dedicadas a vivienda. Las mejores ciudades seléucidas son las construidas en Siria y de todas ellas las más conocidas y estudiadas son Antioquía (en la orilla izquierda del río Orontes, navegable hasta el mar) y Apamea, situada más al norte de Antioquía.

En la antigua Mesopotamia surgieron zonas de gran actividad urbanística donde aparecieron Antioquía-Edesa, Antioquía-Nisibis, Dura Europos, Seleucia del Tigris y Babilonia.

Alejandría fue la ciudad capital de los ptolomeos y la que más importancia tuvo durante el periodo helenístico. Fundada por el propio Alejandro Magno fue durante muchos siglos la referencia a la grandiosidad y actividad económica así como el gran centro del estudio de las ciencias y de las artes.

Ptolomeo I Sóter fundó Náucratis y Ptolemaida, pero Alejandría siguió siendo la ciudad por excelencia.

La capital de los atálidas fue Pérgamo, una ciudad que quiso ser la Atenas de los tiempos clásicos. Tuvo una gran biblioteca y un museo de escultura donde se dice que nació la crítica de arte. Los arquitectos siguieron en Pérgamo las mismas normas de Hipódomo de Mileto, pero el enclave que ofrecían los terrenos hizo que los constructores se lucieran edificando una ciudad totalmente distinta, con la acrópolis en todo lo alto y el perímetro urbano dividido en tres terrazas, cada una con sus templos, que se unían entre sí por una original vía trazada en zigzag y con grandes escaleras.

Como en épocas anteriores, los edificios públicos fueron un capítulo importante en estas ciudades helenísticas, adaptándolos a la necesidad de los tiempos, pero siguiendo siempre el modelo griego que tanto admiraban.

Se prestó gran atención a este espacio público que en tiempos anteriores se había limitado a ser una simple plaza de mercado. Los pórticos vinieron a configurar este espacio, favoreciendo su aspecto, dándole nueva y mejor prestancia. El ágora se empezó a construir de acuerdo con un plan hipodámico (calles trazadas en ángulo recto), es decir, se acotó un espacio rectangular y porticado en varios de sus lados. Fueron ágoras diseñadas con amplitud, donde se reunía la actividad comercial que podía disfrutar de un espacio suficiente y cómodo. Cada ciudad tenía al menos una, según sus necesidades. En Delos se construyeron varias ágoras en las cercanías del puerto. En Atenas también se modificó este espacio y se embelleció con tres nuevos pórticos, uno de ellos ofrecido a Átalo II.

La construcción de pórticos fue una moda que se extendió de manera asombrosa por todas las ciudades. La sensación de magnitud y suntuosidad que ofrecían estas grandes obras hicieron que las ciudades que poseían un pórtico fueran las más bellas y armoniosas. Pero además se consideraban de gran utilidad dando cobijo en las horas de mucho sol o en los días de lluvia. Los pórticos monumentales de las ciudades importantes llamaron enseguida la atención de los romanos cuando tuvieron contacto con ellas en sus conquistas de Oriente. Muchos historiadores y críticos de arte, como José Pijoán, opinan que fue a la vista de estos pórticos cuando los romanos desarrollaron el gusto por el arte griego. Muchas veces se construía un pórtico por el capricho de embellecer un santuario, el rincón de una ciudad o por delimitar un ágora.

Los teatros también se multiplicaron. Se construyeron a la antigua usanza, generalmente adosados a la ladera de una colina o elevación del terreno. En esta época tuvieron una modificación que dio lugar al escenario permanente donde actuaban los actores. Anteriormente éstos se situaban sobre una plataforma que se colocaba en el momento de la actuación delante del proscenio. Uno de los teatros que más información puede dar al respecto es el de Priene del año 150 a. C.

Este fue el complejo arquitectónico más difundido en el mundo helenístico. No hubo ciudad o poblamiento por muy humilde que fuera que no tuviese construido su gimnasio. El gusto por los ejercicios físicos (heredado de los griegos) fue general en este periodo y fue parte de la educación de los jóvenes. Además, en el complejo gimnástico no solo se realizaban ejercicios físicos, sino que se daban enseñanzas diversas, conferencias, y se organizaban lo que hoy se llamaría «actos culturales». Los edificios solían estar rodeados de grandes jardines con bonitos y agradables paseos donde los discípulos escuchaban las charlas de sus maestros filósofos. Tampoco olvidaron el tema religioso, de manera que los gimnasios fueron protegidos y dedicados a un dios o en algunos casos a un héroe como Hermes o Heracles.

Estos centros fueron de una gran ayuda para la educación de los nativos, sobre todo en Asia. Acudían a ellos con gran entusiasmo y deseos de aprender. Llegaron a formar asociaciones que de manera general eran llamadas apo tou gymanasiou ('los que salen del gimnasio').

El mundo de los comerciantes y de los negocios también tuvo necesidad de enclaves especiales. Se construyeron edificios comparables con las cámaras de comercio y otros menos importantes, pero igualmente necesarios como almacenes y despachos. Las excavaciones de Delos han dado abundante información sobre estos edificios, en especial sobre el conjunto de los Posidoneístas de Bertos, actual Beirut, que poseían un importante complejo formado por una lujosa residencia llena de obras de arte, y sobre el otro conjunto de los Negotiatiores itálicos con un ágora particular, tiendas, despachos y demás dependencias. Los romanos lo imitarían en época imperial en Ostia con la Plaza de las corporaciones.

La religión consistía en una suerte de sincretismo entre el panteón clásico, los dioses locales y las deidades del antiguo Oriente. Entre las divinidades propias de este período destacan la diosa Tique (Τύχη) y el dios grecoegipcio Serapis (Σέραπις). Asimismo, cobraron gran relevancia los cultos de Isis, Dionisos y Cibeles.

La filosofía, que en épocas anteriores abarcaba todos los saberes, se desmembró paulatinamente de las ciencias empíricas y se quedó como ciencia del pensamiento cuya preocupación se inclinó más a los problemas individuales que a la propia naturaleza del mundo. En este período surgieron varias sectas y escuelas filosóficas de entre las que cabe mencionar:

La mayor parte de las escuelas del siglo IV subsistieron en época helenística. La escuela de Platón continuó la obra filosófica y la Academia sobrevivió hasta el siglo I a. C., recibiendo en distintas etapas distintos nombres.

Su característica es seguir siendo fiel al maestro Platón. Después de este filósofo los directores de la Academia fueron: su sobrino Espeusipo (407–339 a. C.) durante ocho años, su discípulo Jenócrates (c. 395–314 a. C.) que fue director hasta su muerte, Polemón (351–270 a. C.) que estuvo al frente desde el 314 hasta su muerte y el tebano Crates.

Se caracteriza por la introducción del escepticismo y sus directores fueron el escéptico Arcesilao de Pitane en Eolia (c. 315–240 a. C.) (fue maestro de Eratóstenes), Carnéades de Cirene (214–129 a. C.) que había estudiado en la propia Academia con Hegesino, Clitómaco de Cartago, filósofo cartaginés discípulo del anterior y Metrodoro de Estratonicea.

Sus filósofos se centran más en el eclecticismo, abandonando las teorías del escepticismo. Su director fue Filón de Larisa (150–83 a. C.) que departió sus enseñanzas en Roma y tuvo como discípulo a Cicerón sobre quien ejerció una gran influencia; su discípulo Antíoco de Ascalón fue su rival en la dirección de la Academia. Después tuvo lugar el neoplatonismo de Plotino cuyo máximo exponente fue Proclo.

La escuela de Aristóteles se vio engrandecida con el gran impulso que le dio el orador Arcesilao, fundador de la Academia Nueva. Su doctrina rechazaba el dogmatismo de los estoicos y trataba de demostrar que lo más importante era buscar y descubrir lo más verosímil o probable.

Teofrasto de Éreso (370–287 a. C.), alumno de Aristóteles y colaborador, fue también su sucesor en la escuela peripatética que experimentó un gran desarrollo a partir de su ingreso y colaboración.

El escepticismo se desarrolló en gran medida durante el periodo helenístico, aunque no hubo ninguna auténtica figura que lo representase, pero la escuela se mantuvo muy activa aun después de la conquista romana dándose el caso de que sus mejores representantes son de la época imperial: Enesidemo de Cnoso (en Creta), maestro en Alejandría y Sexto Empírico, perteneciente además a la escuela médica empírica.

Epicuro (341–270) compró en Atenas una casa con huerto o jardín que se convirtió en el lugar de encuentro de sus alumnos, que acabaron llamando al sitio «El Jardín». Uno de los fines que llevó a Epicuro a la utilización de esta sede nueva fue el de oponerse a la influencia de la Academia heredera de las enseñanzas de Platón. El epicureísmo intentaba dar solución al problema de la felicidad. Los epicúreos buscaban la paz consigo mismos para lo que elaboraron un método que pretendía combatir la tristeza, la angustia, el aburrimiento y las preocupaciones inútiles que llegaban a acongojar al ser humano.

Su creador fue Zenón de Citio (335–263), un semita comerciante que optó por dedicarse a la filosofía. Su doctrina se llamó también doctrina del pórtico, stoa en griego, de donde le viene el nombre de estoicismo. Se trataba del Pórtico de Poecile en Atenas, lugar donde se reunían sus discípulos. A su muerte la escuela fue dirigida por Cleantes de Aso (ciudad de la Tróade) y Crisipo de Solos quienes coordinaron y ordenaron sus teorías. Estos tres filósofos enseñaron lo que después se ha llamado antiguo estoicismo o estoicismo antiguo. En el siglo II se renovaron las teorías con el nombre de estoicismo medio siendo uno de sus mejores representantes Diógenes de Babilonia, nacido en Seleucia del Tigris, seguido por su discípulo Crates de Mallos y después Blosio de Cumas que fue maestro de Tiberio Graco. En la segunda mitad del siglo II a. C. destacan dos grandes pensadores y maestros del estoicismo medio: Panecio de Rodas (180–110 a. C.) y Posidonio de Apamea de Orontes (155–51 a. C.).

Las grandes ciudades se convirtieron, en este período, en los centros del saber, de las ciencias y del arte. A partir del siglo IV, la mayoría de los artistas fueron griegos de las colonias de Asia. Se dio un gran avance en el mundo de las ciencias, medicina, astronomía y matemáticas. Estas últimas fueron disciplinas estudiadas y enseñadas por grandes sabios como Euclides, Apolonio, Eratóstenes, Arquímedes, etc.

Nació la filología en todos los aspectos abarcables. Muchos bibliotecarios y hombres de letras dedicaron su vida y sus estudios a dar forma a las obras literarias, a la gramática, las palabras, la crítica literaria, clasificación de libros, etc.

En literatura, se siguieron los modelos clásicos. Son dignos de mención los nombres de Calímaco de Cirene y de su discípulo Apolonio de Rodas.

Con respecto a las artes plásticas, el período helenístico alcanzó una grandiosidad y una madurez que no tuvo nada que envidiar al período anterior. Célebres monumentos, entre los que se encuentran dos de las llamadas por los romanos «Siete Maravillas del Mundo», se construyeron en esta época: el Faro de Alejandría y el Coloso de Rodas. Asimismo cabe mencionar otras importantísimas obras como el Templo de Apolo, cerca de Mileto y el Altar de Zeus en Pérgamo.

Hubo también muchos y buenos pintores entre los que se destacó Apeles, el pintor de Alejandro Magno.

En el período comprendido entre el siglo II a. C. y el I a. C., salieron a la luz las esculturas más famosas:

Sin olvidar las de otros siglos como:

El ámbito de las joyas tuvo su estilo propio, aunque ligeramente influenciado por la etapa anterior. Se pusieron de moda los colgantes con formas de victorias aladas, palomas, ánforas y cupidos, utilizando para su elaboración las piedras de colores, sobre todo el granate. También se utilizaban otras gemas para hacer figuras en miniatura, como el topacio, ágata y amatista. El vidrio entró en los talleres de los artistas como sustituto de las piedras preciosas y con este material confeccionaban toda clase de objetos, sobre todo camafeos.

Durante el periodo helenístico las ciencias tal y como las entendemos hoy se independizaron de la filosofía, concepto este que en la antigüedad comprendía todo el saber. Se constituyeron en materias autónomas, siendo favorecidas para su desarrollo por el mecenazgo gracias al cual fueron creadas aulas de investigación y museos como el de Alejandría, que comprendía observatorios, jardines botánicos y zoológicos, salas de medicina y disección, etc. Contribuyó también a este desarrollo la ampliación del mundo conocido.

El estudio de las matemáticas, sobre todo en Alejandría tuvo una importancia enorme no solo por la materia en sí, sino como aplicación al conocimiento del Universo. En el museo de Alejandría estudiaron, investigaron y enseñaron grandes sabios como Euclides (que fue solicitado por Ptolomeo I Sóter), que supo organizar todas las investigaciones precedentes y añadir las suyas propias, aplicando un método sistemático a partir de principios básicos. Euclides sentó las bases del saber matemático a partir de las cuales evolucionó dicha materia a través de los siglos hasta llegar a la reciente invención de las nuevas matemáticas.

En geometría el gran maestro en Pérgamo y en Alejandría fue Apolonio de Perge. Ofreció la primera definición racional de las secciones cónicas. Arquímedes de Siracusa (287–212 a. C.) fue un gran matemático, interesado en el número π al que dio el valor de 3,1416. Se interesó también por la esfera, el cilindro y fundó la mecánica racional y la hidrostática. Estudió la mecánica práctica inventando máquinas de guerra, palancas y juguetes mecánicos. Su mejor invento práctico de uso inmediato fue el tornillo sin fin, utilizado en Egipto para las labores de irrigación. Sóstrato de Cnido, ingeniero y arqueólogo fue considerado como otro de los grandes sabios. Fue el constructor del faro de Alejandría.

El estudio de las matemáticas favoreció el conocimiento de la astronomía. Se despertó un nuevo interés científico por conocer la Tierra, su forma, su situación, su movimiento en el espacio. Eratóstenes de Cirene, bibliotecario de Alejandría creó la geografía matemática y fue capaz de medir la longitud del meridiano terrestre. Aristarco de Samos (310–230 a. C.) fue matemático y astrónomo y determinó las dimensiones del Sol y la Luna y sus respectivas distancias a la Tierra. Aseguró que el Sol estaba quieto y que era la Tierra quien se movía a su alrededor. Se le considera como el primer antecesor de Copérnico.

Hiparco de Nicea estaba dotado de un gran don de observación y desde su observatorio de Rodas pudo elaborar un gran mapa del cielo con más de 800 estrellas catalogadas y estudiadas por él. Gran conocedor de las teorías de los caldeos, comparó sus estudios con aquellos, descubriendo la precesión de los equinoccios. Hiparco sentó las bases de la trigonometría estableciendo la división del ángulo en 360 grados que dividió en minutos y segundos.

Posidonio de Apamea además de dedicarse a la filosofía fue un gran científico. Estudió el hasta entonces misterio de las mareas, explicando científicamente su existencia y su relación con la luna.

Algunas deficiencias

El sistema de notación de los números se hacía con la ayuda del alfabeto, así α era igual a 1, ι era igual a 10, ρ era igual a 100. Si escribían ρια, estaban escribiendo el número 111. Este sistema dificultaba mucho el manejo de las matemáticas. En el siglo III a. C. Diofanto aportó una notación algebraica que fue buena, pero que todavía resultó insuficiente. Otra deficiencia era la gran carencia de instrumentos de observación para las ciencias naturales. Pese a todo esto, la humanidad llegó hasta el Renacimiento utilizando y valiéndose de los grandes inventos y descubrimientos de los sabios helenísticos, sobre todo de los procedentes de Alejandría, Pérgamo y Rodas.

La figura del médico pasó a sustituir al mago o hechicero que se valía de los milagros. Fue un personaje respetado y estimado, fue considerado un gran sabio en quien se podía confiar no solo para ayuda física, sino también para ayuda psicológica. Los lugares helenísticos donde floreció principalmente la medicina fueron:

Herófilo de Calcedonia aprendió en Alejandría mucho sobre anatomía, practicando con la disección de cadáveres e incluso con la vivisección de seres humanos (criminales convictos). Descubrió el sistema nervioso y explicó su funcionamiento y explicó el de la médula espinal y del cerebro y estudió el ojo y el nervio óptico. Fue poniendo nombres de objetos que él creía parecidos en la forma a las partes de anatomía que iba estudiando y descubriendo. Este sabio fue un pionero de la anatomía humana. Sus estudios y descubrimientos fueron trasmitidos gracias a la labor de la escuela de medicina que fundó y que duró unos 200 años.

Erasístrato de Ceos (315–240 a. C.) trabajó e investigó en Alejandría siguiendo la labor de Herófilo. Fundó también una escuela de medicina. Se le considera el padre de la fisiología. Se dedicó sobre todo al estudio de la circulación de la sangre cuyos descubrimientos no fueron superados hasta la aparición de Miguel Servet o William Harvey.

A principios del siglo I a. C. tiene lugar la diáspora helenística, vale decir, la dispersión del pueblo judío a través del mundo alejandrino. A partir de entonces, gran parte de los judíos —especialmente los que vivían en Egipto, Cirenaica y Siria— comenzaron a usar el griego para entenderse entre ellos y también en las sinagogas. De este modo, comenzó a hacerse distinción entre los «judíos helenísticos» (o helenizados) y los «hebreos» (o judaizantes), que fueron aquellos que se opusieron y resistieron a la influencia griega. San Lucas escribió sobre este tópico en los Hechos de los Apóstoles 6:1 y 11:20. Es así como el término «helenístico» pasó a designar a grupos humanos que, aunque no tuvieran sangre griega, seguían y adoptaban la cultura y la lengua griegas.

En este período tuvo lugar también la traducción griega del Antiguo Testamento que se conoce con el nombre de Septuaginta o Biblia de los Setenta, ya que, según se cree, habría sido efectuada por un grupo de setenta y dos sabios alejandrinos.

De entre los judíos helenizados más destacados, puede mencionarse al filósofo Filón de Alejandría y al historiador Flavio Josefo.

Las guerras de los diádocos (herederos del imperio de Alejandro Magno), que duró aproximadamente 150 años, terminó debilitando a todas las polis griegas y extrahelenísticas. Roma apoyaba las causas de unas y otras, oficiando como mediador y aportando ejércitos al servicio de estas polis. Hasta que finalmente toma Atenas, Esparta y el reino de Macedonia, pasando a ser estas provincias romanas, a excepción de Alejandría, que fue ocupada finalmente en el año 30 a. C. Con la llegada de los romanos y su hegemonía sobre todos estos pueblos de la antigüedad, llegó a su fin, en teoría, el período helenístico; aunque lo cierto es que Roma, pasados algunos años y como consecuencia del contacto y conocimiento del arte griego extendido por todas sus colonias y provincias, tomó el relevo y puede decirse que fue la continuación de la cultura helenística, empezando por el propio idioma. La clase alta tenía a gala hablar griego y se educaba a los hijos en esta cultura. Los grandes políticos romanos, por mucho que tuvieran un cargo importante, serían siempre menospreciados por el resto si no eran capaces de entenderse en el idioma griego.



El liberalismo es una filosofía y movimiento político que defiende la libertad individual, la igualdad ante la ley y la limitación de los poderes del Estado. Como doctrina económica propugna la iniciativa privada y el libre mercado. Como actitud vital propone la tolerancia.[1]​

Representa corrientes muy heterogéneas, con muchas formas y tipos de liberalismo, pero en general defiende los derechos individuales —como el derecho de propiedad, la libertad de asociación, la libertad de religión o la libertad de expresión—; el libre mercado o capitalismo; la igualdad ante la ley de todo individuo sin distinción de sexo, raza, origen o condición social; y el Estado de derecho o imperio de la ley al que deben someterse los gobernantes.[2]​

El liberalismo contemporáneo surgió en la Ilustración y se popularizó rápidamente entre muchos filósofos y economistas europeos y más tarde en la sociedad en general, especialmente entre la burguesía. Los liberales buscaban eliminar la monarquía absoluta, los títulos nobiliarios, la confesionalidad del Estado y el derecho divino de los reyes y fundar un nuevo sistema político basado en la democracia representativa y el Estado de derecho. Los liberales acabaron con las políticas mercantilistas y las barreras al comercio, promoviendo el comercio libre y la libertad de mercado. Los líderes de la Revolución francesa y la Revolución americana se sirvieron de la filosofía liberal para defender la rebelión contra la monarquía absoluta. En el siglo XX, el fascismo y el comunismo fueron ideologías populares que se oponían abiertamente al liberalismo y lo opacaron durante el siglo, también surgieron otras ideologías que se plantearon como una vía intermedia entre el liberalismo y el estatismo.

Los liberales tienen varias ramificaciones. Las ideas del liberalismo clásico de los siglos XVII al XIX —el adjetivo «clásico» fue agregado a posteriori por teóricos políticos luego del declive a finales del siglo XIX de este liberalismo de libertades individuales y economía de libre mercado— fueron recuperadas y repensadas en el siglo XX por los libertarios,[3]​[4]​quienes están presentes principalmente en los Estados Unidos[4]​ y el resto de América.[5]​[6]​[7]​ En Europa, los llamados liberal-conservadores, que se llaman así por abogar por reformas más moderadas, suelen ser una de las ramas más notables.[8]​ También se ha señalado una divergencia entre las tradiciones liberales anglosajona y francesa; el liberalismo anglosajón pretende limitar el poder del Estado, mientras que el liberalismo francés pretende un Estado fuerte que garantice la igualdad ante la ley y la eliminación de los privilegios.[9]​

Se lo identifica como una doctrina que propone la libertad y la tolerancia en las relaciones humanas. Promueve las libertades civiles y económicas, oponiéndose al absolutismo y al conservadurismo. Constituye la corriente en la que se fundamentan tanto el Estado de derecho como la democracia representativa y la división de poderes.

Desde sus primeras formulaciones, el pensamiento político liberal se ha fundamentado sobre tres grandes ideas:[10]​

El liberalismo fue un movimiento de amplia proyección (económica, política y filosófica) que defendía como idea esencial el desarrollo de la libertad personal individual como forma de conseguir el progreso de la sociedad.

Aboga principalmente por:[cita requerida]

El liberalismo está inspirado en parte en la organización de un Estado de derecho con poderes limitados —que idealmente tendría que reducir las funciones del gobierno a seguridad, justicia y obras públicas— y sometido a una constitución, lo que permitió el surgimiento de la democracia liberal durante el siglo XVIII, todavía vigente hoy en muchas naciones actuales, especialmente en las de Occidente.

El liberalismo europeo del siglo XX ha hecho mucho hincapié en la libertad económica, abogando por la reducción de las regulaciones económicas públicas y la no intervención del Estado en la economía. Este aspecto del liberalismo ya estuvo presente en algunas corrientes liberales del siglo XIX opuestas al absolutismo y abogó por el fomento de la economía de mercado y el ascenso progresivo del capitalismo. Durante la segunda mitad del siglo XX, la mayor parte de las corrientes liberales europeas estuvieron asociadas a la comúnmente conocida como derecha política.

Debe tenerse en cuenta que el liberalismo es diverso y existen diferentes corrientes dentro de los movimientos políticos que se autocalifican como "liberales"

Sus características principales son :

El liberalismo normalmente incluye dos aspectos interrelacionados: el social y el económico. El liberalismo social es la aplicación de los principios liberales en la vida política de los individuos, como por ejemplo la no intromisión del Estado o de los colectivos en la conducta privada de los ciudadanos y en sus relaciones sociales, existiendo plena libertad de expresión y religiosa, así como los diferentes tipos de relaciones sociales consentidas ya sean de carácter amistoso, amoroso o sexual, así como en aspectos de moralidad.

Esta negativa permitiría (siempre y cuando sea sometida a aprobación por elección popular usando figuras como referendos o consultas públicas, ya que dentro del liberalismo siempre prevalece el Estado de derecho y este en un Estado democrático se lleva a su máxima expresión con la figura del sufragio) la libertad de paso, la no regulación del matrimonio por parte del Estado (es decir, este se reduciría a un contrato privado como otro cualquiera), la liberalización de la enseñanza, etc. Por supuesto, en el liberalismo hay multitud de corrientes que defienden con mayor o menor intensidad diferentes propuestas.

El liberalismo económico es la aplicación de los principios liberales en el desarrollo material de los individuos, como por ejemplo la no intromisión del Estado en las relaciones mercantiles entre los ciudadanos, impulsando la reducción de impuestos a su mínima expresión y reducción de la regulación sobre comercio, producción, etc. Según la doctrina liberal, la no intervención del Estado asegura la igualdad de condiciones de todos los individuos, lo que permite que se establezca un marco de competencia, sin restricciones ni manipulaciones de diversos tipos. Esto significa neutralizar cualquier tipo de beneficencia pública, como aranceles y subsidios, a favor de la ganancia de cada persona mediante el trabajo, favoreciendo la meritocracia y la producción.

Para John Locke la sociedad es una creación humana, es decir por consentimiento, debido a ello puede elegir a quien(es) gobierne(n). Sin embargo, como los miembros de la sociedad o dicho de otro modo, los miembros del cuerpo político decidieron a quien elegir, por cuanto tiempo y bajo qué condiciones, si quienes gobiernan contravienen los principios del gobierno y los derechos del pueblo, el poder debe regresar a sus manos originarias.

El pueblo no está obligado a obedecer cuando se infringen las normas "Locke se refiere en todo momento a la pérdida de autoridad, a la ilegalidad como condición de posibilidad de la disolución del gobierno, ante la cual se habilita la resistencia en forma legítima"[13]​ la pregunta es ¿podrá el pueblo sublevarse por cualquier cuestión que considere importante? La respuesta es NO, "Locke insiste en que el pueblo no se subleva por nimiedades, y es capaz de tolerar un gran número de injusticias. Sólo cuando las violaciones a la ley o a los fines de la sociedad se perpetúan en el tiempo los pueblos se resisten".[13]​

Otro pensador clásico liberal fue Immanuel Kant, quien también estudia la conformación de la sociedad, la libertad y la sujeción al gobierno. Para Kant la libertad está directamente relacionada con el derecho del individuo de obedecer solo aquellas leyes en las que vea reflejada su propia voluntad legisladora.[14]​ Hasta este punto parece estar de acuerdo con Locke, pero si bien el pueblo es una suma de voluntades que pactan para una mejor forma de vida, «las ideas de voluntad general y de contrato no implican, en este marco, el reconocimiento de derechos inalienables del pueblo, sino que son asumidas, en todo caso, como criterios que permiten al legislador dictar leyes tales que hubiesen podido ser aceptadas por la voluntad unida de todo un pueblo».[14]​

Si bien el pueblo tiene derechos, estos se pueden y deben enajenar en el momento que se conforma un gobierno, mismo que se vuelve su representante que puede diseñar y ejecutar leyes pensando en el bienestar del mismo. De ahí que «Para que una ley sea considerada legítima (y pueda reclamar el consentimiento de aquellos que se someten a ella), no es preciso que sea el pueblo reunido en asamblea quien dicte tal ley, ni tampoco es necesario que éste preste su consentimiento efectivo: si una ley es de tal índole que resulte imposible que todo un pueblo pueda otorgarle su aprobación, entonces no es legítima, pero con que sea solo posible que alguna vez el pueblo prestara su conformidad a dicha ley establecida, entonces ésta puede ser considerada justa».[14]​

Luego entonces, para poder contar con un gobierno justo quienes lo eligen, deben conocer las cualidades y capacidades de sus elegidos, porque de acuerdo a Kant, una vez electos, no hay marcha atrás. ¿Perdió algo el liberalismo? Así es, la posibilidad de desobediencia civil.

Ahora bien, ¿Es aplicable la desobediencia civil en tiempos contemporáneos? ¿Qué dicen los nuevos abanderados del liberalismo?

Actualmente, la sociedad se encuentra inmersa en la injusticia, la pobreza y la desigualdad; que se han extendido de una manera vertiginosa. De ahí que los estudiosos de las ciencias sociales retomen al liberalismo como salida o resolución de un problema que se está agravando. Ellos sostienen «que las situaciones de pobreza extrema y miseria existentes en los países del mundo subdesarrollado constituyen un problema de justicia económica global».[15]​ Una de las propuestas de John Rawls, máximo exponente del liberalismo actual, es la implementación de políticas de asistencia social, pero de ninguna manera cambiar el sistema económico.[15]​

Según Rawls, los problemas sociales actuales nada tienen que ver que las estructuras económicas internacionales, más bien son problemas locales, que los gobiernos internos han sido incapaces de resolver.[15]​

Contrario a la mayoría de los pensadores clásicos, que procuran explicar las condiciones sociopolíticas de su tiempo, pensadores contemporáneos como Rawls buscan justificar el sistema económico actual. Así pues, nos encontramos con dos posturas: una que defiende la posición del pueblo y otro que defiende la posición del gobierno. Uno de los desafíos conceptuales de más relevancia en la teoría liberal es la dicotomía entre libertad y justicia y la forma en la cual estas interactúan para conseguir el bien común.[16]​

Una división menos famosa pero más rigurosa es la que distingue entre el liberalismo predicado por Jeremías Bentham y Wilfredo Pareto propusieron otras dos concepciones para el cálculo de un óptimo de satisfacción social.

En el cálculo económico se diferencian varias corrientes del liberalismo. En la clásica y neoclásica se recurre con frecuencia a la teoría del homo œconomicus, un ser perfectamente racional con tendencia a maximizar su satisfacción. Para simular este ser ficticio se ideó el gráfico Edgeworth-Pareto, que permitía conocer la decisión que tomaría un individuo con un sistema de preferencias dado (representado en curvas de indiferencia) y unas condiciones de mercado dadas. Es decir, en un equilibrio determinado.

Sin embargo, existe una gran controversia cuando el modelo de satisfacción se ha de trasladar a una determinada sociedad. Cuando se tiene que elaborar un gráfico de satisfacción social, el modelo benthamiano y el paretiano chocan frontalmente.

Según Wilfredo Pareto, la satisfacción de que goza una persona es absolutamente incomparable con la de otra. Para él, la satisfacción es una magnitud ordinal y personal, lo que supone que no se puede cuantificar ni relacionar con la de otros. Por lo tanto, sólo se puede realizar una gráfica de satisfacción social con una distribución de la renta dada. No se podrían comparar de ninguna manera distribuciones diferentes. Por el contrario, en el modelo de Bentham los hombres son en esencia iguales, lo cual lleva a la comparabilidad de satisfacciones y a la elaboración de una única gráfica de satisfacción social.

En el modelo paretiano, una sociedad alcanzaba la máxima satisfacción posible cuando ya no se le podía dar nada a nadie sin quitarle algo a otro. Por lo tanto, no existía ninguna distribución óptima de la renta. Un óptimo de satisfacción de una distribución absolutamente desigual sería, a nivel social, tan válido como uno de la más absoluta igualdad (siempre que estos se encontrasen dentro del criterio de óptimo paretiano).

No obstante, para igualitaristas como Bentham no valía cualquier distribución de la renta. El que los humanos seamos en esencia iguales y la comparabilidad de las satisfacciones llevaba necesariamente a un óptimo más afinado que el paretiano. Este nuevo óptimo, que es necesariamente uno de los casos de óptimo paretiano, surge como conclusión lógica necesaria de la ley de los rendimientos decrecientes.

El liberalismo, en origen, defiende la libertad individual y económica, siendo reacio a un estado fuerte (antiestatismo) y a gravar con altos impuestos a los ciudadanos. Sin embargo, a partir de esta doctrina, han surgido numerosas variantes. A continuación, se presentan las principales manifestaciones de liberalismo contemporáneo, organizadas de menor a mayor regulación (desde aceptar cierto nivel de gobierno, hasta no aceptarlo en absoluto):

*Nota: se ha omitido en esta escala el neoliberalismo, puesto que su criterio distintivo no es ideológico, sino cronológico (aunque hay divergencia de opiniones, la acepción más generalizada es que es el mismo liberalismo tradicional, adaptado al tiempo actual).

Babilonia fue una antigua ciudad de la Baja Mesopotamia situada cerca de la actual ciudad de Hilla (Irak). Fue la capital del antiguo Reino babilónico, y por varios siglos fue considerada como un importante centro de comercio, arte y aprendizaje. 

Sus ruinas, parcialmente reconstruidas a finales del siglo xx, se encuentran en la provincia iraquí de Babil, en las proximidades de la ciudad de Hilla

Originalmente esta ciudad se llamaba Ka2.dingir, transcripción romanizada de la escritura cuneiforme del sumerio Ka-dingirra(k), ocasionalmente escrito también Ka2.dingir.ra y K2.dingir.ma, que significa 'Puerta de los dioses' o 'Puerta del dios'. Algunos autores han relacionado este topónimo con Bãbil, también escrito Ba(b)bal, formas preacadias. Posteriormente Ká.dingir fue traducido al acadio, resultando la forma Bãb-ilim, que evolucionó en sus dialectos asirio y babilonio, a partir de la dinastía casita, en una gran variedad de formas, como Bab-ilu y Bab-ilani, de las cuales deriva la forma en antiguo griego Βαβυλών (Bab-ilu-on o Bab-il-on, según textos), génesis de las actuales formas occidentales, como Babilonia (en español), Babylon (en inglés) y Babylone (en francés).[2]​

La ciudad más antigua fue destruida por Senaquerib, por lo que no conocemos su forma. Aunque es singularmente difícil estudiar la ciudad anterior a Hammurabi debido a que está por debajo del nivel freático del terreno,[7]​ parece que fue una ciudad de provincias sin originalidad arquitectónica ni urbanística.[8]​ Se sabe que ya en la primera dinastía babilónica se dotó a la ciudad de murallas.[9]​

En el siglo VII a. C., con una extensión cercana a las 850 hectáreas (unas 400 de ellas protegidas por el perímetro interior de murallas), Babilonia se convirtió en la mayor ciudad de Mesopotamia, superando a la capital asiria Nínive, que ocupaba unas 750 hectáreas.[10]​

La arqueología nos ha permitido conocer la ciudad tal y como fue diseñada en tiempos de Nabucodonosor II.[nota 1]​ Entonces fue reconstruida sobre su forma casi rectangular (unos 2400 × 1600 m[10]​)[11]​ posiblemente influenciada por la ortogonalidad con la que los sumerios habían construido sus canales para ampliar el territorio cultivable.[12]​ Fuertemente amurallada y dividida en dos zonas de área desigual por el río Éufrates, esta planimetría data del año 2000 a. C.[11]​ El recinto descrito, a su vez, Nabucodonosor lo protegió[13]​ con un segundo cinturón amurallado, mucho más amplio y de geometría tendente al triángulo,[10]​ a lo que Nabónido añadió una muralla interior entre la ciudad y el río, creando una nueva puerta para el paso por el puente al lado oeste de la ciudad;[13]​ esto había sido intentado ya por Nabopolasar y Nabucodonosor, quienes protegieron ese flanco con murallas menores.[14]​ En esencia, Babilonia era una sucesión de recintos amurallados, unos dentro de otros, regulados geométricamente mediante ángulos rectos y en los que incluso las viviendas reproducían en pequeño, con sus muros escalonados y sus patios interiores, las forma de los templos y los palacios.[11]​ Cabe decir que este afán constructivo, que permitió a Babilonia albergar unos 500 000 habitantes en el siglo VII a. C., se centró casi exclusivamente en la capital, mientras que el resto del imperio estaba sumido en un proceso de desurbanización.[15]​

La muralla del recinto interior consistía en un doble cinturón defensivo de 7 m de anchura, al que se le añadía un foso conectado con el río que la rodeaba. El espacio entre los dos muros, de unos 12 m, estaba rellenado con tierra en toda su altura. Había una torre cada más o menos 50 m, con lo que se calcula que hubo cerca de 350.[11]​ La defensa se completaba con rejas de hierro sumergidas en el Éufrates, allí donde terminaban las murallas, para evitar que el enemigo pudiera vadear el río por sus zonas menos profundas. Las murallas, en ese punto, se protegían del efecto del agua untándose con brea.[10]​

El río Éufrates tenía un papel importante en la defensa ya que, además de la utilización de sus aguas para crear los fosos, constituía una barrera natural para el recinto exterior, bordeando uno de sus lados.[10]​

La ciudad estaba cruzada por canales y una trama viaria en la que destacaba la avenida de las Procesiones, una vía paralela al río que dejaba entre este y ella misma los principales espacios militares, palaciegos y religiosos de la ciudad, incluyendo los jardines colgantes, el recinto dedicado a Marduk y el palacio real. Al norte, en torno a la Puerta de Ishtar, se levantaba un abigarrado complejo defensivo y palaciego que incluía la propia puerta, en realidad más una fortificación en sí que una simple puerta, los palacios del Norte y del Sur y las fortalezas del Este y del Oeste. Las demás calles del recinto interior tomaban direcciones paralelas u ortogonales al río, creando una malla regular extraña en la baja Mesopotamia. Varias de ellas recibían el nombre de divinidades, y algunas se acompañaban de leyendas o eslóganes.[10]​

Las viviendas eran típicamente mesopotámicas. De dos o tres plantas, estaban focalizadas hacia patios interiores, con pocas ventanas al exterior y cubiertas planas. Al igual que toda la ciudad el material más utilizado en su construcción era el adobe pintado para los muros, con pilares interiores de madera de palmera. Son excepcionales las grandes dimensiones de muchas de ellas, lo que da una idea de la prosperidad de la ciudad neobabilónica, aunque su arquitectura no tiene especial relevancia.[10]​[16]​

La ciudad interior estaba dividida en diez grandes manzanas, barrios o cuartos que recibían nombres propios. Cinco de ellos aparecen en las fuentes helenas: Shuanna, Eridu y Te.e (o Te.eki) al este del Éufrates y Tuba y Komar (o Kumar) al oeste. Los otros son Kadingira (o Kadingirra), Kuliab y la Ciudad Nueva al este, y Bãb-Lugalirra y Nu[...][nota 2]​ al oeste. El recinto exterior estaba dividido en distritos, de los cuales se han distinguido cuatro.[13]​[17]​ Había además suburbios situados dentro y fuera de la muralla exterior. La necrópolis se situaba al oeste de la ciudad, fuera de ella.[18]​

Las puertas del perímetro amurallado interior de la época de Nabucodonosor II, con una sola excepción, recibían nombres de dioses. Las entradas eran llamadas de Ishtar, de Marduk, de Shamash, de Adad, de Enlil, de Zababa, de Urash y real. En época de Nabónido, con la construcción de la muralla del río, se añadió la Puerta del Puente.[13]​

De acuerdo con los textos existieron 43 santuarios en Babilonia.[13]​ Nabopolasar tímidamente y Nabucodonosor II a gran escala emprendieron una ambiciosa reconstrucción y un embellecimiento de los templos, labor continuada en algunos edificios por Nabónido.[14]​ Los importantes fueron los siguientes:

En la antigua Mesopotamia había dos grandes poderes institucionales, la realeza y la religión, reflejados en sendos complejos edificatorios, el palacio y el templo. Mientras que el templo ocupaba el lugar más elevado, el palacio solía situarse cercano al centro de la ciudad, y funcionaba tanto de residencia como de salón del trono y administración. Normalmente se protegía fuertemente, haciéndose difícil la distinción entre palacio y fortaleza.[25]​

En las últimas décadas del siglo XX, desde 1978[34]​ hasta la guerra de Irak, el gobernante iraquí Saddam Hussein comenzó la reconstrucción de la antigua Babilonia sobre sus mismos cimientos. Las opiniones al respecto son encontradas. Hay quien dice que se trata de un trabajo meritorio, aunque no preciso, mientras otros hablan de circo turístico y parque temático.[35]​[nota 3]​ Independientemente de las opiniones, y con el paréntesis temporal de la guerra de Irak, Babilonia, parte de un plan gubernamental similar al de Nínive durante el mandato de Saddam Hussein, se convirtió en un centro turístico. La reconstrucción también logró que los iraquíes comenzaran a tener una idea de su inmenso patrimonio preislámico.[36]​

En general, suele basarse la reconstrucción de la ciudad en un deseo de Saddam de legitimar su gobierno en la historia del territorio ocupado por Irak, utilizando la arquitectura como medio de poder y propaganda, al igual que habían intentado otros antes que él en gobiernos tendentes a la autarquía.[37]​[38]​ En este sentido son especialmente conocidos los planos para Berlín mandados elaborar por Hitler y el Moscú de Stalin. Por ello no solo reconstruyó Babilonia, también hizo lo propio con otros asentamientos, tales como Nínive y Ur, con materiales más bien propios de la modernidad y destruyendo o modificando en el proceso las ruinas originales.[37]​

Babilonia fue recuperada como lugar no solo en su arquitectura. En 1987 se celebró allí el primer Festival Internacional de Babilonia, repetido en septiembre de 1988.[39]​

Hasta que alemanes comenzaron a excavar Babilonia en el inicio del siglo XX, ésta era una ciudad brumosa, casi mitológica, que en la cultura occidental servía de alegoría de la lujuria por influencia de algunos historiadores griegos y romanos, y aun de la maldad por influencia de la Biblia. En el Nuevo Testamento la huella de Babilonia es tan fuerte que su nombre se utiliza para nominar a cualquier ciudad grande y poderosa.[41]​

Babilonia es una leyenda que aún resuena en nuestro tiempo, a pesar de que en el año 539 a. C. ya había perdido su imperio, y que hace muchos siglos que fue abandonada. Largamente mencionada en el Libro de Isaías y en el Apocalipsis, Babilonia fue identificada como fuente de lascivia y soberbia, llegando a ser descrita como «La Gran Ramera».[42]​ No obstante, Babilonia brilló mucho tiempo por su alto nivel cultural, que se mantuvo vivo mientras fue parte de Asiria. El mito de su belleza y de su poder, labrado desde Hammurabi, llegó a oídos de Alejandro Magno, donde residió durante un tiempo y donde murió.

Históricamente el nombre de Babilonia ha servido de inspiración para múltiples escritos y también para otras ciudades y proyectos de ciudades, como la conocida Nueva Babilonia, y para la provincia iraquí de Babil. El mismo Miguel de Cervantes se refirió a Babilonia en el sentido bíblico de caos.[43]​

También algunos de sus edificios han sido mitificados por la religión, la literatura, la pintura y la historiografía occidentales; así, tanto la Torre de Babel como los Jardines Colgantes han sido objeto de innumerables conjeturas, y en menor medida el palacio Sur de Nabucodonosor II. El propio rey de la ciudad inspiró la ópera Nabucco de Giuseppe Verdi.

Existen distintas opiniones acerca de la fundación de Babilonia. Antiguamente se consideraba que la ciudad había sido fundada o reconstruida por Sargón de Acad. La fuente más antigua conocida que menciona la ciudad es datada en tiempos del Imperio acadio formado por Sargón de Acad en el siglo siglo xxiv a. C. La Crónica Weidner establece que fue el propio Sargón quien construyó Babilonia "frente a Agadé".[44]​ Otra crónica establece, en el mismo sentido, que "Sargón excavó el polvo del pozo de Babilonia, e hizo una contraparte de Babilonia cerca de Agadé" (ABC 20:18-19).[44]​ Más recientemente, sin embargo, se ha sostenido que dichas fuentes en realidad se refieren a Sargón II de Asiria (siglo VIII a. C.) y no a Sargón de Acad.[45]​ Esto coincidiría con el resurgimiento y embellecimiento de la ciudad durante el Imperio neobabilónico.

Algunos eruditos, incluyendo al lingüista Ignace Gelb, han sugerido que el nombre Babilon refleja el de una ciudad anterior. Según Ranajites Pallmin, esta ciudad estaba ubicada hacia el este.[46]​ Herzfeld ha escrito sobre Baver en Irán, cuya fundación se atribuye a Jamshid: el nombre Babil podría ser un eco de Baver. David Rohl sostiene que la Babilonia original debe ser identificada con Eridu. Según la Biblia, Babilonia fue fundada por Nemrod (Génesis, 10). 

Ya comenzada la segunda mitad del siglo XXI a. C., nómadas semitas procedentes del desierto de Arabia (amorreos y tidnum) expropiaron a Amar-Sin, rey de Sumer y Acad, parte de los territorios del centro de Mesopotamia (Acad), queriendo penetrar en Kish; pero fueron expulsados de esta última ciudad, quedando limitados a las orillas del Éufrates, es decir, a Babilonia. Por ser su única posesión importante por mucho tiempo, los martu, se encargaron de engrandecerla y embellecerla. En el año 2004 a. C. el imperio de Ur, conocido como el periodo Ur III, cayó ante una coalición de pueblos nómadas procedentes de los montes Zagros: elamitas, la ciudad de Isin, y los amorreos o martu. Estos últimos se asentaron en la Media y Baja Mesopotamia, apoderándose de las ciudades y fundando dinastías amorritas en ellas.

La Primera Dinastía Babilónica fue fundada hacia el año 1894 a. C. (cronología media) por el amorrita Sumu-Abum, quien estableció una pequeña ciudad-Estado independiente. Su sexto rey, Hammurabi (1792-1750 a. C.) engrandecería colosalmente sus fronteras y extendería sus dominios, dominando toda Mesopotamia —Isin, Larsa, Ur, Uruk, Nippur, Lagash, Eridu, Kish, Adab, Eshnunna, Akshak, Akkad, Shurupak, Bad-tibira, Sippar y Ngirsu—. Desde entonces adquirió gran relevancia como la verdadera metrópoli de región. Sin embargo, el imperio no duró mucho. Tras la muerte de Hammurabi, comenzaron varias revueltas y conflictos, especialmente de parte de los primeros caudillos casitas (tribu aria indoeuropea y nómade) y unos invasores del llamado "País del Mar".
El imperio no era sólido, y tras la muerte de Hammurabí tuvo que enfrentarse a distintos problemas: principalmente el nacionalismo sumerio al sur, el avance de los casitas al este y el poder creciente de los hurritas al norte, que en aquella época crearon un imperio llamado Mitanni.[47]​ Finalmente, en el 1595 a. C., sin poder resistir las presiones de los casitas del sur, Samsu-Ditana, último rey del Primer Imperio Babilónico, fue depuesto por el caudillo casita, Agum II. En el 1531 a. C. el rey hitita Mursili II saqueó la ciudad de Babilonia y sobre sus ruinas se estableció la dinastía Casita.[47]​

Tras la caída de los casitas hacia el 1155 a. C., Babilonia fue gobernada por la II Dinastía de Isin; era la primera vez que una dinastía nativa de Baja Mesopotamia tomaba el poder. Sin embargo, Babilonia permaneció débil y sujeta al dominio de Asiria. Sus ineficaces reyes fueron incapaces de evitar que nuevas oleadas de invasiones occidentales, incluidos los arameos en el siglo XI a.C. y caldeos en el siglo IX a.C., entraran y se apropiaron de áreas de Babilonia para ellos mismos. En el 713, tras varias décadas de inestabilidad, la ciudad fue conquistada por Imperio neoasirio.

El gobierno asirio de la ciudad de Babilonia estuvo marcado por las rebeliones de la nobleza local, fuertemente nacionalista. La ciudad de Babilonia era entonces una provincia menor, aunque culturalmente fuerte, que mantenía sus propios reyes dependientes de los monarcas asirios. El rey asirio Ashur-uballit II intentó mejorar las relaciones con la ciudad casando a una de sus hijas con su rey Karakhardash y colocando a su nieto como rey de los casitas. Sin embargo, el asesinato de este último marcó una represalia militar que encendió el nacionalismo babilonio, ya arraigado hasta la nueva independencia de la ciudad.

Con Salmanasar III (859-824 a.C.) Babilonia, como otras provincias, aprovechó las revueltas internas asirias para tratar de obtener la independencia. Para entonces ya estaban instalados en ella los caldeos, que habían llegado uno o dos siglos antes. Babilonia volvió a estar bien controlada por los asirios durante el reinado del usurpador asirio Tiglatpileser III (745-727 a. C.), que aprovechó el fin de la dinastía babilónica para hacer valer su poder allí con más fuerza. Volvieron a producirse nuevas rebeliones en Babilonia durante el reinado de Senaquerib, que se vio obligado a intervenir militarmente en la ciudad y a deportar a parte de su población. Más tarde, ante la continuidad de las rebeliones, destruye completamente la ciudad. Asarhaddon (681-669 a. C.), hijo de Senaquerib que accedió al trono tras una guerra civil, casó con una babilonia y fundó una doble monarquía en el imperio, una en Nínive, la capital de su padre, y otra en Babilonia. A su muerte reparte el imperio entre dos hijos, dándole a uno Asiria y a otro Babilonia, pero Asurbanipal, el heredero de Asiria, no tarda en volver a controlar Babilonia, esta vez mediante un pacto pacífico, firmado solamente después de una larga serie de incidentes bélicos que siguió a la división del imperio. Este pacto sólo duró unos pocos años; después, Babilonia y Asiria nuevamente en guerra, el rey babilonio acabó suicidándose en el año 648 a. C. y Asurbanipal conquistó la ciudad, ordenando asesinar a sus habitantes.[47]​

Fue bajo el gobierno del rey Nabucodonosor II (605-562 a. C.) cuando Babilonia llegó a ser una de las ciudades más espléndidas del mundo antiguo. Nabucodonosor ordenó la completa reconstrucción de las tierras imperiales, incluyendo la reconstrucción de los Jardines Colgantes de Babilonia (una de las siete maravillas del mundo antiguo), de los cuales se dice haber sido construidos para su nostálgica esposa Amytis. La existencia de los jardines es un tema de disputa: a pesar de que las excavaciones del arqueólogo alemán Robert Koldewey parecen confirmar su existencia, muchos historiadores están en desacuerdo sobre la localización, y algunos creen que pueden haber sido confundidos con los jardines de Nínive.

Babilonia perdió definitivamente su independencia en el siglo VI a. C., cuando fue conquistada por los persas.  Después de pasar varias vicisitudes, la ciudad cayó en el 539 a. C. bajo el mando de Ciro el Grande, rey de Persia. Bajo Ciro y su heredero, Darío I el Grande, Babilonia se convirtió en un centro de aprendizaje y avance científico. Los eruditos babilonios completaron mapas de constelaciones, y crearon los fundamentos de la astronomía y las matemáticas modernas. Sin embargo, bajo el reinado de Darío III Codomano, Babilonia empezó a estancarse progresivamente.

En el período helenístico —sometida a imperios extranjeros y caída en desgracia frente a ciudades como Persépolis— Alejandro Magno quiso convertirla en su capital.[48]​La ciudad helénica básicamente fue la misma que la neobabilónica y la aqueménida. Los edificios más altos seguían siendo el zigurat Etemenanki y el Palacio real del Sur. Ni siquiera variaron significativamente las viviendas. La única huella indudablemente griega en la urbe fue la construcción de un teatro en la zona interior oriental. Alejandro Magno intentó una restauración de la urbe que se vio truncada por su muerte y cuyo mayor efecto fue el derribo del zigurat para construir uno nuevo que nunca llegó a realizarse. A partir de entonces la decadencia de la urbe se aceleró hasta abandonarse.

Después de la conquista de los partos de Babilonia en 141 a. C. se convirtió en la capital del Imperio Parto y más tarde el Imperio Sasánida. Su población se había reducido considerablemente a 20,000 a 30,000 personas en el período parto.[49]​ Hasta cerca del año 500 fue un centro religioso de los Amoraim, sabios judíos que comentaron la Torá Oral tomando como base la Mishná.

A mediados del siglo VII, Mesopotamia fue invadida y colonizada por el Imperio musulmán en expansión, y siguió un período de islamización. Babilonia se disolvió como provincia y el cristianismo arameo y de la Iglesia del Este finalmente quedó marginado. Ibn Hawqal (siglo X) y el erudito árabe, al-Qazwini (siglo XIII), describen a Babilonia (Babil) como una pequeña aldea.[50]​  Este último describió un pozo conocido como el "calabozo de Daniel" que era visitado por cristianos y judíos durante las vacaciones. El santuario de la tumba de Amran ibn Ali fue visitado por musulmanes.  Monedas excavadas de los períodos parto, sasánida y árabe demuestran que Babilonia estuvo poblada continuamente.[49]​  En el siglo XVII, Pietro della Valle viajó a un pueblo de Babilonia.[51]​ El Capitán Robert Mignan exploró el sitio brevemente en 1827 y en 1829 completó un mapa de Babilonia que incluye la ubicación de varias aldeas.[52]​  La aldea de Qwaresh creció en respuesta a la necesidad de trabajadores durante las excavaciones de Robert Koldewey (1899-1917) y las generaciones posteriores también trabajaron en excavaciones arqueológicas.[53]​

El 14 de febrero de 1978, el gobierno baazista de Irak presidido por Sadam Husein inició el "Proyecto de restauración arqueológica de Babilonia" para reconstruir las características de la antigua ciudad sobre sus ruinas. En la década de 1980, eliminó por completo el pueblo de Qwaresh, desplazando a sus residentes.[54]​ Más tarde construyó un palacio moderno en esa zona llamada Saddam Hill sobre algunas de las antiguas ruinas, en el estilo piramidal de un zigurat. 

En 2003, durante la guerra de Irak, el ejército estadounidenses estableció un campamento militar en las ruinas de Babilonia.[55]​ Su presencia ocasionó numerosos destrozos; vertidos, paso de vehículos pesados y helicópteros, destrucción de la avenida de las Procesiones por el paso de tanques, extracción de ladrillos de la Puerta de Ishtar reconstruida por Saddam Husein, cementerio de vehículos, cava de zanjas en torno al Etemenanki, grafitis y contaminación de suelos, entre otros. Por otra parte, varios artefactos hallados en la ciudad y expuestos en los museos locales y el de Bagdad fueron sustraídos y vendidos en una página web de subastas.[56]​Estas acciones, además de estropear los monumentos, también han podido alterar áreas no exploradas.[56]​[57]​ 

El nuevo gobierno iraquí, con el asesoramiento y apoyo financiero de la organización privada de origen estadounidense, World Monuments Fund, desarrolló un plan integral de gestión del sitio y presentó su candidatura a Patrimonio de la Humanidad ante la UNESCO. Este programa contó con con apoyo del Departamento de Estado de los Estados Unidos.[58]​ La Junta Estatal de Antigüedades y Patrimonio de Irak es la principal autoridad responsable de la conservación del sitio arqueológico, asistida por la Policía de Antigüedades y Patrimonio, Sus oficinas están ubicadas dentro del perímetro de las antiguas murallas internas de la ciudad y varios miembros del personal y sus familias residen en viviendas subvencionadas en esta zona. Actualmente, miles de personas viven dentro del perímetro de las antiguas murallas exteriores de la ciudad y las comunidades dentro y alrededor se están desarrollando rápidamente a pesar de leyes que restringen construcciones.[59]​ [60]​[61]​

En 2017, cuando volvió a abrirse al público, más de 35.000 personas visitaron el sitio arqueológico en Babilonia y [62]​ el 5 de julio de 2019, este fue inscrito finalmente en la Lista de la UNESCO del Patrimonio de la Humanidad. [63]​

El conocimiento histórico de la topografía de Babilonia ha derivado de los escritores clásicos, las inscripciones de Nabucodonosor II y las excavaciones del Deutsche Orientgesellschaft (Sociedad Oriental Alemana), comenzadas en 1899. La topografía se corresponde prácticamente con la Babilonia del citado monarca, ya que la ciudad más antigua fue destruida por Senaquerib sin dejar apenas rastro. Las citadas excavaciones alemanas fueron las que verdaderamente comenzaron a descubrir la ciudad, si bien la británica Indian Company había dado pie a las excavaciones de Layard en 1850, Rawlinson en 1854 y Rassam en 1880; sin embargo, estas incursiones sirvieron más para expoliar objetos hoy expuestos en el Museo Británico que para descubrir y comprender verdaderamente la ciudad.[64]​ Los alemanes encontraron en Kasar, nombre de uno de los montículos que crecieron sobre las ruinas, los restos de los palacios y fortalezas y reales, y en Amram, otro de los montículos, los cimientos del zigurat.[65]​ Los equipos de las excavaciones de la Sociedad Oriental Alemana estaban formados íntegramente por arquitectos desde la expedición de Robert Koldewey, cuyo interés primordial era localizar edificios y levantar planos precisos, para lo cual habían desarrollado un método que permitía encontrar con mayor facilidad muros de barro. A pesar de que les debemos el conocimiento de la Babilonia de Nabucodonosor II, su interés por la arquitectura de la ciudad les hizo prestar poca atención a inscripciones, cerámicas, huesos y otro tipo de restos.[7]​

Para 1900, Robert Koldewey ya había encontrado la Puerta de Ishtar y despejado parte de la Avenida de las Procesiones e inspeccionado someramente toda el área, que incluía, además de los montículos de Amram y Kasar (o Kasr), los de Babil, Merkez y Homera, en total unos 18 km².[7]​ Los tres últimos se corresponden con las zonas del palacio de Verano,[14]​ el área residencial de grandes casas en torno al templo de Ishtar de Agadé, y la zona del anfiteatro griego.[30]​ Centrado en los dos primeros y con visos a trasladar tablillas y otros restos a Alemania, Koldewey solicitó la construcción de una vía férrea y contrató entre 200 y 250 hombres. Ayudado por otros arquitectos, comenzó un estudio sistemático del yacimiento que le llevó a diseccionar las distintas capas históricas hasta la del segundo milenio antes de Cristo; aunque su intención era continuar, no fue posible debido al nivel freático del lugar.[7]​ En 1924, Koldewey revisó los informes de las excavaciones, recientemente publicados. Murió en 1925.[7]​

Otras excavaciones importantes, aunque no tan reveladoras, fueron las también alemanas de 1970 y las iraquíes de 1978-1989, realizadas por la Organización Estatal de Antigüedades y Patrimonio de Irak, cuyo mayor interés era la reconstrucción de diversos monumentos arquitectónicos.[7]​

Además de las propias ruinas y de las reconstrucciones efectuadas en el siglo XX, hay una serie de restos arqueológicos que fueron trasladados de su lugar de origen y se encuentran actualmente expuestos en diferentes lugares del mundo.

Antes de las excavaciones modernas en Mesopotamia, la apariencia de Babilonia era un misterio representado de diversas maneras por artistas occidentales como un híbrido entre el antiguo Egipto, el griego clásico y la cultura otomana contemporánea. [67]​

Debido a la importancia histórica de Babilonia, así como a las referencias a ella en la Biblia, la palabra "Babilonia" en varios idiomas ha adquirido un significado genérico de una ciudad grande, bulliciosa y diversa.

Según la Biblia, Nemrod fundó Babel (Babilonia). [68]​  Luego, la Biblia relata la construcción de la Torre de Babel por personas que hablaban un solo idioma.  Para evitar el desarrollo de la edificación, Dios hizo que los constructores comenzasen a hablar diferentes idiomas y se dispersaran por toda la Tierra. 

El libro de Jeremías dice en cuanto a Babilonia, "nunca nadie volverá a habitarla", el lugar quedará "completamente deshabitado" y que será "tierra por la que nadie pasará y en la que ningún ser humano vivirá".  [69]​

La biodiversidad o diversidad biológica es, según el Convenio Internacional sobre la Diversidad Biológica, el término por el que se hace referencia a la amplia variedad de seres vivos sobre la Tierra y lo que sucede con los patrones naturales que la conforman, resultado de miles de millones de años de evolución según procesos naturales y también de la influencia creciente de las actividades del ser humano. La biodiversidad comprende igualmente la variedad de ecosistemas y las diferencias genéticas dentro de cada especie (diversidad genética) que permiten la combinación de múltiples formas de vida, y cuyas mutuas interacciones con el resto del entorno fundamentan el sustento de la vida sobre el mundo.

El término «biodiversidad» es un calco del inglés «biodiversity». Este término, a su vez, es la contracción de la expresión «biological diversity» que se utilizó por primera vez en octubre de 1986 como título de una conferencia sobre el tema, el National Forum on BioDiversity, convocada por Walter G. Rosen, a quien se le atribuye la idea de la palabra.[1]​

La Cumbre de la Tierra celebrada por la Organización de las Naciones Unidas en Río de Janeiro en 1992 reconoció la necesidad mundial de conciliar la preservación futura de la biodiversidad con el progreso humano según criterios de sostenibilidad o sustentabilidad  promulgados en el Convenio internacional sobre la Diversidad Biológica que fue aprobado en Nairobi el 22 de mayo de 1994, fecha posteriormente declarada por la Asamblea General de la ONU como Día Internacional de la Biodiversidad. Con esta misma intención, el año 2010 fue declarado Año Internacional de la Diversidad Biológica por la 61.ª sesión de la Asamblea General de las Naciones Unidas en 2006, coincidiendo con la fecha del Objetivo Biodiversidad 2010.[2]​

En el año 2007, la Asamblea de la Organización de las Naciones Unidas declaró el 22 de mayo como Día Internacional de la Diversidad Biológica.[3]​

Según la RAE, el término biodiversidad define la “Variedad de especies animales y vegetales en su medio ambiente”[4]​

Sin embargo el concepto, por su carácter intuitivo, ha presentado ciertas dificultades para su definición precisa, tal como señaló Fermín Martín Piera[5]​ al argumentar que el abuso en su empleo podría «vaciarlo de contenido», ya que en sus palabras: «suele acontecer en la historia del pensamiento que los nuevos paradigmas conviven durante un tiempo con las viejas ideas», considerando junto a otros autores que el concepto de biodiversidad fue ya apuntado por la propia teoría de la evolución.

A principios del siglo XX, los ecólogos Jaccard y Gleason propusieron en distintas publicaciones los primeros índices estadísticos destinados a comparar la diversidad interna de los ecosistemas. A mediados del siglo XX, el interés científico creciente permitió el desarrollo del concepto para describir la complejidad y organización, hasta que en 1980, Thomas Lovejoy propuso la expresión diversidad biológica.[6]​

Si en el campo de la biología, la biodiversidad se refiere al número de poblaciones de organismos y especies distintas, para los ecólogos el concepto incluye la diversidad de interacciones durables entre las especies y su ambiente inmediato o biotopo, el ecosistema en que los organismos viven. En cada ecosistema, los organismos vivientes son parte de un todo actuando recíprocamente entre sí, pero también con el aire, el agua, y el suelo que los rodean.

Se distinguen habitualmente tres niveles en la biodiversidad:[7]​

Hay que incluir también la diversidad interna de los ecosistemas, a la que se refiere tradicionalmente la expresión diversidad ecológica.

La biodiversidad que hoy se encuentra en la Tierra es el resultado de cuatro mil millones de años de evolución.[8]​

Aunque el origen de la vida no se ha podido datar con precisión, las evidencias sugieren que pudo haber surgido hace 3800[9]​[10]​[11]​  a 3235 millones de años.[12]​ Algunas investigaciones más recientes han abierto la posibilidad de que haya comenzado inclusive hace 4100 millones de años[13]​ aunque no son aún concluyentes. Hasta hace aproximadamente 600 millones de años, toda la vida consistía en bacterias y microorganismos.[9]​

La historia de la diversidad biológica durante el Fanerozoico —últimos 540 millones de años— comienza con el rápido crecimiento durante la explosión cámbrica, periodo durante el que aparecieron por primera vez los filos de organismos multicelulares.[14]​ Durante los siguientes 400 millones de años la biodiversidad global mostró un relativo avance, pero estuvo marcada por eventos puntuales de extinciones masivas.[15]​

La biodiversidad aparente que muestran los registros fósiles sugiere que unos pocos millones de años recientes incluyen el período con mayor biodiversidad de la historia de la Tierra. Sin embargo, no todos los científicos sostienen este punto de vista, ya que no es fácil determinar si el abundante registro fósil se debe a una explosión de la biodiversidad, o —simplemente— a la mejor disponibilidad y conservación de los estratos geológicos más recientes.[cita requerida]

Algunos, como Alroy y otros[16]​ piensan que mejorando la toma de muestras, la biodiversidad moderna no difiere demasiado de la de 300 millones de años atrás. Las estimaciones sobre las especies macroscópicas actuales varían de 2 a 100 millones, con un valor lógico estimable en 10 millones de especies, aproximadamente.[14]​

La mayoría de los biólogos coinciden sin embargo en que el período desde la aparición del hombre forma parte de una nueva extinción masiva, el evento de extinción holocénico, causado especialmente por el impacto que los humanos tienen en el desarrollo del ecosistema. Se calcula que las especies extinguidas por acción de la actividad humana es todavía menor que las observadas durante las extinciones masivas de las eras geológicas anteriores.[cita requerida] Sin embargo, muchos opinan que la tasa actual de extinción es suficiente para crear una gran extinción masiva en el término de menos de 100 años.[cita requerida] Los que están en desacuerdo con esta hipótesis sostienen que la tasa actual de extinción puede mantenerse por varios miles de años antes que la pérdida de biodiversidad supere el 20 % observado en las extinciones masivas del pasado.[cita requerida]

Se descubren regularmente nuevas especies —un promedio de tres aves por año—[cita requerida] y muchas ya descubiertas no han sido aún clasificadas: se estima que el 40 % de los peces de agua dulce de Sudamérica permanecen sin clasificación.[cita requerida]

El valor esencial y fundamental de la biodiversidad reside en que es resultado de un proceso histórico natural de gran antigüedad. Por esta sola razón, la diversidad biológica tiene el inalienable derecho de continuar su existencia. El hombre y su cultura, como producto y parte de esta diversidad, debe velar por protegerla y respetarla.

Además la biodiversidad es garante de bienestar y equilibrio en la biosfera. Los elementos diversos que componen la biodiversidad conforman verdaderas unidades funcionales, que aportan y aseguran muchos de los “servicios” básicos para nuestra supervivencia.

Finalmente desde nuestra condición humana, la diversidad también representa un capital natural.[17]​ El uso y beneficio de la biodiversidad ha contribuido de muchas maneras al desarrollo de la cultura humana, y representa una fuente potencial para subvenir a necesidades futuras.

Considerando la diversidad biológica desde el punto de vista de sus usos presentes y potenciales y de sus beneficios, es posible agrupar los argumentos en tres categorías principales.

Hace referencia al papel de la diversidad biológica desde el punto de vista sistémico y funcional (ecosistemas). Al ser indispensables a nuestra propia supervivencia, muchas de estas funciones suelen ser llamadas “servicios”.

Los elementos que constituyen la diversidad biológica de un área son los reguladores naturales de los flujos de energía y de materia. Cumplen una función importante en la regulación y estabilización de las tierras y zonas litorales. Por ejemplo, en las laderas montañosas, la diversidad de especies en la capa vegetal conforma verdaderos tejidos que protegen las capas inertes subyacentes de la acción mecánica de los elementos como el viento y las aguas de escorrentía. La biodiversidad juega un papel determinante en procesos atmosféricos y climáticos. Muchos intercambios y efectos de las masas continentales y los océanos con la atmósfera son producto de los elementos vivos (efecto albedo, evapotranspiración, ciclo del carbono, etc.).

La diversidad biótica de un sistema natural es uno de los factores determinantes en los procesos de recuperación y reconversión de desechos y nutrientes. Además algunos ecosistemas presentan organismos o comunidades capaces de degradar toxinas, o de fijar y estabilizar compuestos peligrosos de manera natural.

Aún con el desarrollo de la agricultura y la domesticación de animales, la diversidad biológica es indispensable para mantener un buen funcionamiento de los agroecosistemas, ya que garantiza la fertilidad de la tierra, la polinización natural de varias especies domésticas, el control eficaz de plagas, etc. La regulación trofodinámica de las poblaciones biológicas solo es posible respetando las delicadas redes que se establecen en la naturaleza. El desequilibrio en estas relaciones ya ha demostrado tener consecuencias negativas importantes.[18]​ Esto es aún más evidente con los recursos marinos, donde la mayoría de las fuentes alimenticias consumidas en el mundo son capturadas directamente en el medio. La respuesta a las perturbaciones (naturales o antrópicas) tiene lugar a nivel sistémico, mediante vías de respuesta que tienden a volver a la situación de equilibrio inicial. Sin embargo, las actividades humanas han aumentado dramáticamente en cuanto a la intensidad, afectando irremediablemente la diversidad biológica de algunos ecosistemas y vulnerando en muchos casos esta capacidad de respuesta con resultados catastróficos.

La investigación sugiere que un ecosistema más diverso puede resistir mejor a la tensión medioambiental y por consiguiente es más productivo. Es probable que la pérdida de una especie disminuya la habilidad del sistema para mantenerse o recuperarse de daños o perturbaciones. Al igual que una especie con alta diversidad genética, un ecosistema de alta biodiversidad puede tener más oportunidades de adaptarse al cambio medioambiental. En otras palabras: cuantas más especies comprende un ecosistema, más probable es que el ecosistema sea estable y resistente. Los mecanismos que están debajo de estos efectos son complejos y acaloradamente disputados. Sin embargo, en años recientes ha quedado claro que realmente hay efectos ecológicos de la biodiversidad.

Una elevada disponibilidad de recursos en el ambiente favorece una mayor biomasa, pero también la dominancia ecológica y frecuentemente ecosistemas relativamente pobres en nutrientes presentan una mayor diversidad, algo que es cierto sistemáticamente en los ecosistemas acuáticos. Una mayor biodiversidad permite a un ecosistema resistir mejor a grandes cambios ambientales, haciéndolo menos vulnerable, más resiliente por cuanto el estado del sistema depende de las interrelaciones entre especies y la desaparición de cualquiera de ellas es menos crucial para la estabilidad del conjunto que en ecosistemas menos diversos y más marcados por la dominancia.

Para todos los humanos, la biodiversidad es el primer recurso para la vida diaria. Un aspecto importante es la diversidad de la cosecha, que también se denomina agrobiodiversidad.

La mayoría de las personas ven la biodiversidad como un depósito de recursos útil para la fabricación de alimentos, productos farmacéuticos y cosméticos. Este concepto sobre los recursos biológicos explica la mayoría de los temores de desaparición de los recursos. Sin embargo, también es el origen de nuevos conflictos sobre la división y apropiación de recursos naturales.

Algunos de los artículos económicos importantes que la biodiversidad proporciona a la humanidad son:

Los ecólogos y activistas ecológicos fueron los primeros en insistir en el aspecto económico de la protección de la diversidad biológica.

La estimación del valor de la biodiversidad es una condición previa necesaria a cualquier discusión en la distribución de sus riquezas. Este valor puede ser discriminado entre valor de uso (directo como el turismo o indirecto como la polinización) y valor intrínseco.

Si los recursos biológicos representan un interés ecológico para la comunidad, su valor económico también es creciente. Se desarrollan nuevos productos debido a las biotecnologías y los nuevos mercados. Para la sociedad, la biodiversidad es también un campo de actividad y ganancia. Exige un arreglo de dirección apropiado para determinar cómo estos recursos serán usados.

Todavía tiene que evaluarse la importancia económica actual y futura de la mayoría de las especies. Sin embargo, debemos ser conscientes de que aún nos falta mucho para saber calcular con precisión, no solamente lo económico sino, mucho más importante, el valor que cada especie tiene para su ecosistema.

Se considera generalmente que la expansión demográfica y económica de la especie humana está poniendo en marcha una extinción masiva, de dimensiones incomparablemente mayores que las de cualquier extinción anterior. Las causas concretas están en la desaparición indiscriminada de ecosistemas, por la tala de bosques, la degradación de los suelos, la contaminación ambiental, la caza y la pesca excesivas, etc. La comunidad científica juzga, en general, que tal extinción representa una amenaza para la capacidad de la biosfera para sustentar la vida humana a través de diversos servicios naturales y recursos renovables.

Por ello la comprensión de la biodiversidad cultural en su relación con los ecosistemas es clave, siempre que no se disocien los recursos naturales de su contexto cultural, histórico y geográfico.

La biodiversidad es importante ya que cada especie puede dar una pista a los científicos sobre la evolución de la vida. Además, la biodiversidad ayuda a la ciencia a entender cómo funciona el proceso vital y el papel que cada especie tiene en los ecosistemas.

La diversidad es una propiedad fenomenológica que pretende expresar la variedad de elementos distintos. Como cualidad fundamental de nuestra percepción, sentimos la necesidad de cuantificarla. El desarrollo de una medida que permita expresar de manera clara y comparable la diversidad biológica presenta dificultades y limitaciones. No se trata simplemente de medir una variación de uno o varios elementos comunes, sino de cuantificar y ponderar cuantos elementos o grupos de elementos diferentes existen. Las medidas de diversidad existentes pues, no son más que modelos cuantitativos o semicuantitativos de una realidad cualitativa con límites muy claros en cuanto a sus aplicaciones y alcances. El desarrollo de un concepto matemático lógico y coherente para la modelación de la diversidad biológica a nivel específico y genético ha sido bastante explorado y presenta un cuerpo sintético y robusto. La modelación de la diversidad a nivel de ecosistemas es más reciente, y se ha visto beneficiada por los adelantos tecnológicos (como los SIG).[22]​ Las medidas de diversidad más sencillas consisten en índices matemáticos que expresan la cantidad de información y el grado de organización de la misma. Básicamente las expresiones métricas de diversidad tienen en cuenta tres aspectos:

Cada uno de estos índices de la diversidad es unidimensional y de lectura limitada. Las comparaciones y valoraciones de la diversidad biológica son forzosamente incompletas en estos términos. Se usan por su carácter práctico y sintético, pero insuficiente frente a modelos analíticos alternativos multiescalares y multidimensionales que responden mejor a las necesidades específicas de conservación y manejo. Así, la modelación bidimensional (riqueza y abundancia relativa) puede considerarse como el estándar “clásico” de medida y expresión de la diversidad. De acuerdo a la escala espacial en la que se mide la diversidad biológica, se habla de diversidad alpha (diversidad puntual, representada por α), beta (diversidad entre hábitats, representada por β) y gamma (diversidad a escala regional, representada por γ). Estos términos fueron acuñados por Robert Whittaker en 1960 y gozan en general de una gran aceptación.

La biodiversidad no es estática: es un sistema en evolución constante, tanto en cada especie, así como en cada organismo individual. Una especie actual puede haberse iniciado hace uno a cuatro millones de años, y el 99 % de las especies que alguna vez han existido en la Tierra se han extinguido.

La biodiversidad no se distribuye uniformemente en la tierra. Es más rica en los trópicos, y conforme uno se acerca a las regiones polares se encuentran poblaciones más grandes y menos especies. La flora y fauna varían, dependiendo del clima, altitud, suelo y la presencia de otras especies.

La distribución de la diversidad biológica actual es el resultado de los procesos evolutivos, biogeográficos y ecológicos a lo largo del tiempo desde la aparición de la vida en la tierra. Su existencia, conservación y evolución depende de los factores ambientales que la hacen posible. Cada especie presenta requerimientos ambientales específicos sin los cuales no le es posible sobrevivir. Aunque los cambios orográficos y oceanográficos, altitudinales y latitudinales permiten definir unidades de paisaje con bastante aproximación, la componente específica de las especies presentes es la que finalmente permite identificar áreas relativamente homogéneas en cuanto a las características que presenta u ofrece para las poblaciones biológicas.

Estas unidades de biosfera, pueden ser identificadas como unidades de biodiversidad según diferentes criterios de valoración: por ejemplo, el número de endemismos, riqueza específica, ecosistémica o filogenética. Aunque es común argumentar que tal o cual país presenta determinados índices de biodiversidad, las unidades espaciales de la diversidad biológica son por definición independientes de los límites o barreras geopolíticas.

Dos de las unidades espaciales vigentes de la biosfera, donde el factor de la biodiversidad precede en importancia, son las ecorregiones de Global 200[23]​ identificadas por la WWF y los puntos calientes de biodiversidad o hotspots de la Unión Internacional para la Conservación de la Naturaleza.[24]​

Global 200 identifica las ecorregiones más importantes del planeta, tanto marinas como continentales —cuerpos de agua dulce y terrestres— de acuerdo a la riqueza específica, el número de endemismos y los estados de conservación.[25]​

El término «punto caliente de biodiversidad» fue acuñado por Norman Myers en 1998 e identifica regiones biogeográficas terrestres importantes según el número de endemismos y el grado de amenaza sobre la biodiversidad.[26]​ En su última revisión Conservation International propone 34 hotspots.

Durante el siglo XX se ha venido observando la erosión cada vez más acelerada de la biodiversidad. Las estimaciones sobre las proporciones de la extinción son variadas, entre muy pocas y hasta 201 especies extintas por día, pero todos los científicos reconocen que la proporción de pérdida de especies es mayor que en cualquier época de la historia humana.

En el reino vegetal se estima que se encuentran amenazadas aproximadamente un 12,5 % de las especies actualmente conocidas. Todos están de acuerdo en que las pérdidas se deben a la actividad humana, incluyendo la destrucción directa de plantas y su hábitat.

Existe también una creciente preocupación por la introducción humana de especies exóticas en hábitats determinados, alterando la cadena trófica.[27]​

Algunos ejemplos de actividades de desarrollo que pueden tener las más significativas consecuencias negativas para la diversidad biológica son:

A los anteriores puede añadirse con sentido la biodiversidad cultural. Los trabajos sobre biodiversidad biológica están incorporando el estudio, el fomento y la protección de la biodiversidad cultural, además de la biodiversidad específica, de ecosistemas y de la genética.

Eugenio Reyes Naranjo[32]​ define la Biodiversidad Cultural como diversidad de saberes que los seres humanos han desarrollado a través de la historia en su relación con la biodiversidad.

Esto incluye creencias, mitos, sueños leyendas, lenguaje, conocimientos científicos, actitudes psicológicas en el sentido más amplio posible, manejos, aprovechamientos, disfrute y compresión de entorno natural.

Se trata de comprender la evolución biológica teniendo en cuenta todos los aspectos de la intervención humana.

El Volcán del Ruiz , Nevado del ruiz , también conocido como Mesa de Herveo,[4]​ y en la época precolombina como Cumanday, Tabuchía y Tama,[1]​ se conoce como Volcán del Ruiz quizás en honor a Alfonso Ruiz de Sahajosa, miembro del cabildo y persona notable de Ibagué, [5]​ es el más septentrional de los volcanes activos del cinturón volcánico de los Andes, ubicado en el límite entre los departamentos de Tolima y Caldas, en Colombia. Es un estratovolcán compuesto por muchas capas de lava que se alternan con ceniza volcánica endurecida y otros piroclastos. Ha estado activo durante cerca de dos millones de años, desde el Pleistoceno temprano o el Plioceno tardío, con tres periodos eruptivos importantes. La formación del cono volcánico formado durante el curso del período eruptivo actual comenzó hace 150 mil años.

En general, sus erupciones son de tipo pliniano, dando origen a rápidas tortugas de gas caliente y roca denominadas flujos piroclásticos. Estas erupciones masivas a menudo generan lahares (flujos de lodo y escombros), que suponen una amenaza para la vida humana y el medio ambiente. El 13 de noviembre de 1985 una pequeña erupción desencadenó un enorme lahar que enterró la cabecera urbana de Armero en lo que se conoció como la tragedia de Armero, en la que según se calcula, ocurrieron aproximadamente 25.000 muertes,[6]​ por lo que se le considera como la segunda erupción volcánica más devastadora del siglo XX, tras la erupción del Monte Pelée de 1902.[7]​ Otros incidentes similares ocurrieron en 1595 y 1845, pero fueron menos mortíferos.

El volcán forma parte del Parque nacional natural Los Nevados e incluye otras cumbres nevadas como las de los Nevados del Tolima, Santa Isabel, El Cisne y Quindío, las cuales están cubiertas por glaciares que han ido retirándose de manera significativa desde 1985 a causa del calentamiento global. El parque es un popular destino turístico e incluye varios refugios para los turistas; las laderas del volcán son utilizadas para deportes de invierno, y la Laguna del Otún, para la pesca de trucha.[8]​ Así  mismo, en la región se encuentran algunos balnearios con aguas termales operados comercialmente.[8]​ Entre 1868 y 1869, los geólogos alemanes Reiss y Stübel fueron los primeros en intentar escalar el Ruiz en una expedición documentada, y en 1936, Cunet y Gansser fueron los primeros en hacerlo con éxito, lo que repitieron en 1939.[3]​

El volcán se encuentra a 220 km al occidente de Bogotá y hace parte de la Cordillera de los Andes, específicamente del macizo volcánico Ruiz–Tolima (o Cordillera Central), del que también forman parte los volcanes Nevado del Tolima, de Santa Isabel, del Quindío y el Cerro Machín.[9]​[10]​ El macizo está ubicado en la intersección de cuatro fallas, algunas de las cuales aún se encuentran activas.[11]​

Hace parte del cinturón de Fuego del Pacífico y es el más septentrional del cinturón volcánico de los Andes, que incluye 75 de los 204 volcanes sudamericanos formados durante el Holoceno.[12]​ Este cinturón es el resultado de la subducción con dirección al oriente de la placa de Nazca por debajo de la placa Sudamericana.[13]​ Como en el caso de otros volcanes en zonas de subducción, el Nevado del Ruiz puede dar origen a erupciones plinianas explosivas asociadas a flujos piroclásticos que pueden fundir glaciares aledaños a la cumbre, produciendo lahares.[14]​

Al igual que muchos otros volcanes andinos, el Nevado del Ruiz es un estratovolcán, es decir, un volcán cónico y de gran altura, compuesto por múltiples capas de lava endurecida, piroclastos alternantes y cenizas volcánicas.[15]​ Sus lavas son de composición andesítica–dacítica;[16]​ El cono volcánico moderno comprende cuatro domos de lava, todos ellos construidos dentro de la caldera del ancestral Ruiz: Alto de la Laguna, la Olleta, Alto la Piraña, y Alto de Santano.[17]​ Antiguamente se consideraba el Nevado el Cisne como parte de los domos de lava del Ruiz, sin embargo, en la época reciente se ha demostrado que este forma un pequeño complejo volcánico aparte con el morro negro.[18]​ Cubre un área de más de 200 km², abarcando 65 km de este a oeste.
[19]​ La extensa cumbre incluye el cráter Arenas, con 1 km de diámetro y 240 m de profundidad.[16]​

La cima del volcán tiene laderas con inclinaciones de los 20 a los 30 grados. A alturas más bajas las laderas son menos pronunciadas, con inclinaciones cercanas a los 10 grados. A partir de ahí, los piedemontes se extienden casi hasta el la rivera del Magdalena al oriente, y la del Cauca al occidente.[20]​ En los dos principales lados de la cima, los acantilados de los glaciares muestran los lugares en donde se han producido corrimientos de tierra; asimismo, en algunas ocasiones se ha fundido el hielo de los glaciares, generando lahares devastadores, incluyendo la erupción más mortal del continente en 1985.[14]​[16]​[21]​ En el lado suroccidental del volcán se encuentra el cono piroclástico La Olleta, que no está activo actualmente, pero ha erupcionado varias veces en el pasado.[16]​

El Nevado del Ruiz está cubierto por glaciares, que se formaron hace varios miles de años y generalmente han ido retirándose desde el Último Máximo Glacial. Desde hace 28 000 a 21 000 años atrás, los glaciares cubrían unos 1500 km² del macizo Ruiz–Tolima. Tan tarde como hace 12 000 años, cuando se retiraban las capas de hielo de la última glaciación, cubrían aún 800 km², y durante la Pequeña Edad de Hielo, la capa de hielo cubría aproximadamente 100 km².[22]​

A partir de entonces, los glaciares han ido retrocediendo aún más debido al calentamiento de la atmósfera.[11]​ Para 1959, el área del glaciar se reducía a únicamente 34 km²;[23]​ y desde la erupción de 1985, que destruyó cerca del 10 % del área helada de la cumbre, ésta se ha reducido a la mitad, de 17 a 21 km² justo después de la erupción, a cerca de 10 km² en 2003. En 1985, los glaciares alcanzaban alturas tan bajas como los 4500 metros, pero ahora solo llegan hasta alturas entre los 4800 y los 4900 metros.[11]​

La capa de hielo tiene un grueso promedio de aproximadamente 50 m; es más gruesa en algunas zonas de la meseta de la cumbre y bajo el glaciar Nereidas en la ladera suroeste, donde alcanza profundidades de 190 m. Los glaciares en las laderas del norte, y en menor medida los de las laderas orientales, perdieron la mayor parte de su hielo en la erupción de 1985,[24]​ y por ello sólo llegan a 30 m de profundidad.[25]​ El grueso hielo que cubre la meseta de la cumbre puede estar ocultando una caldera; cinco cúpulas alrededor de dicha meseta han aparecido conforme el hielo se ha ido retirando.[25]​

El agua de deshielo es drenada en su mayor parte a los ríos Cauca, al occidente, y Magdalena, al oriente.[20]​ La escorrentía proveniente de los glaciares del Ruiz y de las cumbres nevadas aledañas es la encargada del abastecimiento de agua potable de 40 poblaciones cercanas, por lo que tanto los científicos colombianos como los funcionarios del gobierno están preocupados por el suministro de agua a las ciudades si los glaciares se fundieran por completo.[26]​

En general, el Nevado del Ruiz está pobremente forestado principalmente debido a su elevación, y su cubierta de árboles disminuye conforme aumenta la altitud. En las menores alturas, están presentes bosques templados bien desarrollados (20–35 m); por encima de estos, pero por debajo de la línea de los árboles, ciertos sectores de la superficie cuentan con bosques enanos (3–8 m). Por encima de dicha línea, en el páramo, la vegetación está dominada por Espeletia.[8]​ La vegetación de la región está formada por diferentes familias de plantas leñosas, incluyendo Rubiaceae, Fabaceae, Melastomataceae, Lauraceae y Moraceae. Algunas herbáceas, de familias como Polypodiaceae, Araceae, Poaceae, Asteraceae, Piperaceae y Orchidaceae también están presentes en la región.[8]​

Entre los animales que habitan el volcán se encuentran el tapir andino y el oso de anteojos, considerados como amenazados.[8]​ Asimismo, en los alrededores del volcán se encuentran especies como B. ferrugineifrons, O. Stubelii y O. percrassa. Además, el Nevado del Ruiz es hábitat del cóndor de los Andes y de 27 especies endémicas de Colombia, con 14 de ellas confinadas a la región alrededor del volcán. 15 especies de aves de la región también son consideradas como amenazadas.[8]​

Sus primeras erupciones tuvieron lugar hace 1,8 millones de años, a principios del Pleistoceno,[11]​ a partir del cual se han identificado tres periodos primarios de erupción: el ancestral, el antiguo y el actual. Durante el primero de ellos, entre un millón y dos millones de años atrás, se creó un complejo de grandes estratovolcanes,[10]​ que luego colapsaron parcialmente hace un millón y 800.000 años, formando calderas de entre 5 y 10 km de ancho. Durante el periodo antiguo, que duró desde hace 800.000 a 200.000 años, se desarrolló un nuevo complejo de grandes estratovolcanes, incluyendo lo que para aquella época eran el Ruiz, el Tolima, el Quindío, y el Santa Isabel. Una vez más, se formaron calderas explosivas en sus cumbres, entre hace 200.000 y 150.000 de años.[10]​

El periodo presente inició hace aproximadamente 150 000 años, y supuso el desarrollo del actual edificio volcánico a través del emplazamiento de domos de lava hechos a base de andesita y dacita dentro de las viejas calderas.[11]​ Durante los últimos 11 000 años, el Nevado del Ruiz ha pasado por al menos 12 etapas de erupción, las cuales han incluido múltiples corrimientos de tierra, flujos piroclásticos y lahares, conduciendo a la destrucción parcial de los domos de la cima.[10]​[11]​ Durante los últimos miles de años, la mayor parte de las erupciones de los volcanes del macizo Ruiz–Tolima han sido pequeñas, y los flujos piroclásticos depositados han sido menos voluminosos que los del Pleistoceno.[10]​ Dado que las erupciones más antiguas del volcán no han sido registradas, los vulcanólogos han usado la técnica de tefrocronología para datarlas.[27]​

Durante la historia registrada del volcán, las erupciones han consistido principalmente de una chimenea central en la caldera, seguida de una erupción explosiva, y luego, de lahares. La más antigua de las erupciones identificadas en el Holoceno ocurrió cerca del 6660 a. C., y se produjeron nuevas erupciones en 1245 a. C.±150 años (usando la datación por radiocarbono), cerca del 850 a. C., en el 200 a. C.±100 años, así como en el 350 d. C.±300 años, el 675 d. C.±50 años, en 1350, 1570, 1595, 1623, 1805, 1826, 1829, 1831, 1845, 1916, diciembre de 1984 a marzo de 1985, septiembre de 1985 a julio de 1991, y posiblemente en 1541, 1687,[28]​ 1828, 1833 y abril de 1994 y la última el día 30 de junio de 2012 que fue de gases y ceniza únicamente.[nota 1]​[27]​[29]​ Muchas de esas erupciones incluyeron una erupción de chimenea central, una de chimeneas a los costados, y una explosión freática.[27]​ El Ruiz es considerado como el segundo volcán más activo de Colombia, tras el Galeras, en Nariño.[21]​

En la mañana del 12 de marzo de 1595, el volcán entró en erupción. Este episodio consistió en tres erupciones plinianas que llegaron a oírse a más de 100 km de la cima, y fue expulsada una gran cantidad de ceniza, lo que oscureció el área circundante. Durante las erupciones, el volcán también expulsó lapillus, una forma de tefra, y bombas volcánicas. En total, la erupción produjo 0,16 km³ de tefra.[16]​ La erupción estuvo precedida por un gran terremoto, tres días antes;[30]​ y la erupción precursora causó lahares, que viajaron por los valles de los ríos Gualí y Lagunillas, obstruyendo el flujo de agua, matando los peces y destruyendo la vegetación. 636 personas murieron a causa del lahar.[31]​

La de 1595 fue la última gran erupción antes de 1985; y fueron similares en muchos aspectos, incluyendo la composición química del material erupcionado.

En la mañana del 19 de febrero de 1845, un terremoto de gran magnitud dio como resultado un flujo de lodo,[32]​ que corrió por el valle del Lagunillas por aproximadamente 70 km,[14]​ extendiéndose y vertiéndose al exterior del cauce del río y matando gran parte de la población local.[32]​ Tras superar un abanico aluvial, el flujo de lodo se dividió en dos ramas: la más grande se unió al Lagunillas y siguió hasta confluir con el Magdalena, mientras que la más pequeña fue desviada por las colinas frente al cañón del Lagunillas, para luego fluir al oriente junto al río Sabandija y finalmente reincorporarse al flujo principal en la desembocadura del río. Se estima que unas mil personas fallecieron a causa de lo sucedido.[32]​

Desde principios de noviembre de 1984, los geólogos notaron un incremento en el nivel de la actividad sísmica cerca del Nevado del Ruiz;[33]​ así como otros indicios de la erupción que se aproximaba, tales como el aumento de la actividad de las fumarolas, el depósito de azufre en la cumbre del volcán, y pequeñas erupciones freáticas. Al final, el magma caliente entró en contacto con el agua, resultando en explosiones debidas a la casi instantánea evaporación del agua. El más notable de esos eventos fue la expulsión de ceniza el 11 de septiembre de 1985.[33]​ La actividad del volcán se redujo en octubre de 1985, siendo la elevación del magma en el nuevo edificio volcánico, antes de septiembre de 1985, la explicación más probable de los acontecimientos.[33]​

Una misión vulcanológica italiana analizó muestras de gas provenientes de fumarolas a lo largo del suelo del cráter Arenas, obteniendo anhídrido carbónico (CO2) y anhídrido sulfuroso (SO2), lo que indicaba una liberación directa de magma a la superficie del medio ambiente. De acuerdo a las consideraciones del informe de la misión, publicado el 22 de octubre de 1985, el riesgo de lahares era muy alto. Además, en el informe se recomendaron varias técnicas de preparación sencillas para las autoridades locales.[34]​

En noviembre de 1985, la actividad volcánica se incrementó una vez más, conforme el magma se acercaba a la superficie.[33]​ El volcán empezó a lanzar grandes cantidades crecientes de gases ricos en azufre, principalmente anhídrido sulfuroso; el contenido de agua de las fumarolas se redujo, y las fuentes de agua cercanas se enriquecieron en magnesio, calcio y potasio lixiviados del magma.[33]​ Las temperaturas del equilibrio termodinámico (energía térmica estacionaria), correspondiente a la composición química de los gases descargados, fue de 200 °C a 600 °C. La extensa desgasificación del magma causó un aumento de la presión dentro del volcán, justo en el espacio situado encima del magma, lo que finalmente dio origen a una erupción explosiva.[35]​

El Nevado del Ruiz hizo erupción a las 9:09 p. m. del 13 de noviembre de 1985,[36]​ expulsando tefra dacítica a más de 30 km en la atmósfera.[33]​ La masa total del material erupcionado, incluyendo magma, fue de 35 millones de toneladas,[33]​ únicamente el 3 % de la cantidad que expulsó el St. Helens en 1980.[37]​ La erupción alcanzó el nivel 3 en el índice de explosividad volcánica.[38]​ La masa de anhídrido sulfuroso expulsada fue de aproximadamente 700 000 toneladas, o cerca del 2 % de la masa del material sólido expulsado,[33]​ haciendo que la erupción fuera atípicamente rica en azufre.[39]​
La erupción produjo flujos piroclásticos que fundieron los glaciares y la nieve, generando cuatro lahares que corrieron por las vertientes del volcán;[40]​ también destruyeron un pequeño lago que podía ser observado en el cráter Arenas varios meses antes de la erupción.[33]​ Dado que el agua de los lagos volcánicos suele ser extremadamente salada y contener gases volcánicos disueltos, la composición ácida del lago, así como su calor, aceleró la fusión del hielo; este efecto fue confirmado por las grandes cantidades de sulfatos y cloruros encontrados en el lahar.[33]​

Los lahares, conformados por agua, hielo, material pirocástico incandescente piedra pómez, arena, lodo y otras rocas,[40]​ se mezclaron a medida que avanzaban cuesta abajo.[41]​ Continuaron su trayecto a una velocidad promedio de 60 km/h erosionando el suelo, arrastrando rocas y destruyendo la vegetación. Luego de descender kilómetros, los lahares se dirigieron a los seis ríos que drenan el volcán. Una vez en sus valles, los lahares crecieron a casi cuatro veces su tamaño original. En el Río Gualí, un lahar alcanzó un ancho máximo de 50 m.[40]​

Uno de los lahares virtualmente borró la pequeña área urbana de Armero, en Tolima, que se asentaba sobre el valle del Lagunilla. Únicamente sobrevivió la cuarta parte de sus 28 000 habitantes.[40]​ El segundo lahar, que descendió por el valle del Chinchiná, mató a cerca de 1800 personas y destruyó cerca de 400 casas en Chinchiná.[42]​ En total, más de 23 000 personas perdieron la vida y otras 5000 resultaron heridas,[40]​ y más de 5000 hogares quedaron destruidos.[40]​ La tragedia de Armero, fue el segundo desástre volcánico más mortífero de su siglo, siendo sobrepasado por la erupción del Monte Pelée en 1902.[43]​ y el cuarto en toda la historia conocida.[44]​ También es el lahar más mortífero del que se tiene conocimiento,[11]​ y el mayor desastre natural de Colombia.[45]​

La pérdida de tantas vidas se debió al hecho de que los científicos nunca precisaron cuándo ocurriría la erupción, y porque las autoridades gubernamentales, no tomarían medidas costosas en prevención, sin una clara advertencia de peligro.[46]​ Por otro lado, como la última erupción se había producido 140 años atrás, ya no existía en la memoria de los pobladores y para muchos fue difícil aceptar el peligro que representaba el volcán, que los habitantes conocían como el león dormido.[31]​ Los mapas de amenaza que mostraban al Municipio de Armero inundado por completo, fueron distribuidos un mes antes de la erupción, pero el Congreso de la República criticó a los científicos y a las agencias de defensa civil por su alarmismo. Las autoridades locales fallaron al alertar a la población sobre la seriedad de la situación, con el alcalde y el párroco de Armero tranquilizando a la población tras una erupción de cenizas en la tarde del 13 de noviembre y la subsecuente lluvia de cenizas en la noche.[47]​ Otro factor fue la tormenta de esa noche, que causó cortes de electricidad, dificultando las comunicaciones. A pesar de que los oficiales de defensa civil de cuatro pueblos cercanos intentaron advertir a Armero del lahar que se aproximaba y llegaría en una hora o menos, no lograron establecer contacto por radio.[48]​

Luego de la catástrofe, los científicos analizaron la información previa a la erupción y notaron que habían ocurrido varios sismos de periodo largo, que empezaban fuertes y se iban atenuando lentamente. El vulcanólogo Bernard Chouet se refirió diciendo que: "el volcán estaba gritando 'estoy a punto de estallar'", pero los científicos que estaban estudiando el volcán en el momento de la erupción no tenían la experiencia para leer estas señales.[49]​

El volcán continúa representando una seria amenaza para las poblaciones cercanas. El riesgo más probable es que erupciones de pequeño volumen puedan desestabilizar los glaciares y generar lahares.[10]​ Pese a la significativa reducción del tamaño de los glaciares, el volumen de hielo en las cumbres nevadas de la zona sigue siendo grande. Si únicamente se fundiera el 10 % del hielo, podría producir flujos de lodo de más de 200 millones de metros cúbicos, cantidad similar a la del flujo que enterró a Armero en 1985.[11]​ Como los lahares pueden viajar más de 100 km por los valles de los ríos en cuestión de unas pocas horas,[11]​ las estimaciones muestran que unos 500 000 habitantes de los valles de los ríos Chinchiná, Lagunilla, Azufrado y Gualí están en riesgo, y 100 000 de ellos son considerados como de alto riesgo.[50]​ A pesar de que las pequeñas erupciones son más probables, la historia eruptiva de dos millones de años del macizo Ruiz–Tolima incluye numerosas erupciones de magnitud significativa, por lo que no debe ser ignorada la amenaza de una erupción grande.[10]​ Una erupción de este tipo podría tener efectos más generalizados, incluyendo el posible cierre del aeropuerto de Bogotá debido a la caída de ceniza.[51]​

Como la tragedia de Armero se vio agravada por la falta de alertas tempranas,[46]​ uso imprudente de la tierra,[52]​ y la falta de preparación de las comunidades aledañas,[46]​ el gobierno colombiano dio inicio a un programa oficial denominado Oficina Nacional para la Atención de Desastre, en 1987, con el propósito de prevenir incidentes similares en el futuro. Las principales ciudades colombianas fueron orientadas para promover la planificación de la prevención con el fin de mitigar las consecuencias de los desastres naturales,[52]​ y se han llevado a cabo evacuaciones debido a amenazas volcánicas. Cerca de 2300 habitantes de las riberas de cinco ríos cercanos fueron evacuados cuando el volcán erupcionó nuevamente en 1989.[53]​ Cuando otro volcán colombiano, el Nevado del Huila, entró en erupción en abril de 2008, miles de personas fueron evacuadas, pues los vulcanólogos alertaron a la población afirmando que la erupción podría ser otro "Nevado del Ruiz";[54]​ De igual manera, la zona aledaña al Galeras, ha sido constantemente evacuada debido a su actividad.[55]​

En abril de 2012, el volcán aumentó su actividad sísmica,[56]​ poniendo al descubierto la escasa preparación de las poblaciones aledañas.[57]​

A principios de marzo del 2012 se da un pulso inicial de emisión de ceniza volcánica asociada al tremor volcánico. Según el Servicio Geológico Colombiano[2] la ceniza emitida tuvo un volumen de 1 340 000 metros cúbicos; hasta el día 8 de junio de 2012 sigue cayendo ceniza de manera irregular, motivo por el cual la aeronáutica civil,[58]​ cerró el aeropuerto de la ciudad de Manizales y restringió el tráfico aéreo en algunas zonas del país como medida de prevención.

El Servicio Geológico Colombiano realizó un análisis de las cenizas, encontrando lo siguiente: según el análisis preliminar de componentes y granulometría de las muestras de ceniza recolectadas de la emisión del día 29 de mayo de 2012, bajo la lupa binocular, se pudo observar que su composición es líticocristalina, con cristales de plagioclasa, anfíbol, cuarzo, biotita, piroxeno y magnetita; los líticos son volcánicos, además presenta fragmentos de pómez y vidrio. Después de realizar el tamizado de algunas muestras recolectadas se puede observar que los tamaños de grano oscilan entre ceniza muy gruesa (1 y 2 mm) y gruesa (0,5–1 mm) en los sectores más cercanos al cráter Arenas, mientras que en los municipios de Manizales, Chinchiná, Palestina y Villamaría el tamaño de grano varía entre ceniza media (0,25-0,5 mm) y ceniza extremadamente fina (<0,0625 mm); y en las partes más lejanas el tamaño de grano es menor a 0,125 mm.



Alan Mathison Turing (Paddington, Londres; 23 de junio de 1912-Wilmslow, Cheshire; 7 de junio de 1954) fue un matemático, lógico, informático teórico, criptógrafo, filósofo y biólogo teórico británico.[1]​[2]​[3]​[4]​[5]​

Es considerado uno de los padres de la ciencia de la computación y precursor de la informática moderna. Proporcionó una influyente formalización de los conceptos de algoritmo y computación: la máquina de Turing. Formuló su propia versión que hoy es ampliamente aceptada como la tesis de Church-Turing (1936).

Durante la segunda guerra mundial, trabajó en descifrar los códigos nazis, particularmente los de la máquina Enigma, y durante un tiempo fue el director de la sección Naval Enigma de Bletchley Park. Se ha estimado que su trabajo acortó la duración de esa guerra entre dos y cuatro años.[6]​ Tras la guerra, diseñó uno de los primeros computadores electrónicos programables digitales en el Laboratorio Nacional de Física del Reino Unido y poco tiempo después construyó otra de las primeras máquinas en la Universidad de Mánchester.

En el campo de la inteligencia artificial, es conocido sobre todo por la concepción de la prueba de Turing (1950), un criterio según el cual puede juzgarse la inteligencia de una máquina si sus respuestas en la prueba son indistinguibles de las de un ser humano.

La carrera de Turing terminó súbitamente tras ser procesado por homosexualidad en 1952. Dos años después de su condena, murió —según la versión oficial por suicidio; sin embargo, su muerte ha dado lugar a otras hipótesis, incluida la del envenenamiento accidental —. El 24 de diciembre de 2013, la reina Isabel II del Reino Unido promulgó el edicto por el que se exoneró oficialmente al matemático, quedando anulados todos los cargos en su contra.[7]​

Turing nació en Maida Vale, Londres,[8]​ aunque fue concebido en Chatrapur (India británica). Su padre Julius Mathison Turing (1873-1944) era miembro del cuerpo de funcionarios británicos en la India, donde estaba de permiso en su puesto en el Indian Civil Service (ICS) en la entonces Presidencia de Madrás y actualmente estado de Odisha. Julius y su esposa Ethel querían que su hijo Alan naciera en el Reino Unido y regresaron a Paddington, donde finalmente nació.

Durante su infancia, sus padres viajaban entre Hastings, Reino Unido, y el Raj Británico debido a que la Administración Colonial en la que trabajaba su padre seguía en activo, por lo que pasó algunos años viviendo con su hermano en la casa de un matrimonio retirado del ejército. Desde muy pequeño Turing mostró un gran interés por la lectura (se cuenta que aprendió a leer por sí solo en tres semanas), por los números y los rompecabezas. Sus padres lo inscribieron en el colegio St. Michael cuando tenía seis años; su profesora se percató enseguida de la genialidad de Turing. Sus ansias de conocimiento y experimentación llegaban hasta tal punto que a los ocho años, atraído por la química, diseñó un pequeño laboratorio en su casa. Su carrera escolar estuvo marcada, por un lado, por sus aptitudes y su facilidad por las matemáticas y, por el otro, por su carácter inconformista que le llevaba a seguir sus propias ideas y apartarse del rígido (e ilógico, según su parecer) sistema educativo.

Como curiosidad, cabe decir que Turing recorrió alrededor de 90 km en bicicleta durante la huelga general de 1926 para poder ir a la escuela,[9]​ dato que nos hace entender cómo, más adelante, además de científico, fue un atleta notable de rango casi olímpico. Con poco más de quince años, entró en contacto con el trabajo de Albert Einstein y, además de entender sus bases, comprendió sus críticas a las leyes de Newton a partir de un texto en el que no se explicitaba tal cometido.

Entre enero de 1922 y 1926, Turing estudió en la preparatoria Hazelhurst, una escuela independiente en el pueblo de Frant en Sussex (hoy East Oriental).[10]​

En 1926, con trece años, ingresó en el internado de Sherborne en Dorset. Su primer día de clase coincidió con una huelga general en Inglaterra, pero su determinación por asistir a clase era tan grande que recorrió en solitario, con su bicicleta, los más de 96 km que separaban Southampton de su escuela, pasando la noche en una posada. Tal hazaña fue recogida en la prensa local.[11]​

La inclinación natural de Turing hacia la matemática y la ciencia no le atrajo el respeto de sus profesores de Sherborne, cuyo concepto de educación hacía mayor énfasis en los clásicos. En la escuela de Sherbone, ganó la mayor parte de los premios matemáticos que se otorgaban y, además, realizaba experimentos químicos por su cuenta aunque la opinión del profesorado respecto a la independencia y ambición de Turing no era demasiado favorable. A pesar de ello, Turing continuó mostrando una singular habilidad para los estudios que realmente le gustaban, y llegó a resolver problemas muy avanzados para su edad (16 años) sin ni siquiera haber estudiado cálculo elemental.[12]​

Christopher Morcom estudiaba junto con Turing en la escuela de Sherborne y ambos compartían la pasión por la ciencia. Durante las clases de matemática o física, se intercambiaban notas de comentarios sobre rompecabezas. Christopher invitó a Alan a conocer a su madre, una artista. Alan se enamoró de él. Fue su primer amor y la primera persona que creyó en sus ideas y con quien podía continuar desarrollándolas.[13]​[14]​[15]​  El 13 de febrero de 1930,[16]​ solo unas pocas semanas después de su última temporada en Sherborne, Christopher Morcom falleció debido a complicaciones de la tuberculosis bovina, contraída tras beber leche de alguna vaca infectada.  Al recordarlo Turing afirmaba: «Mis recuerdos más vívidos de Chris son casi siempre de las cosas tan amables que me decía».[15]​

Desde entonces la fe religiosa de Turing se hizo pedazos, y se volvió ateo. También se obsesionó por entender la naturaleza de la consciencia, su estructura y orígenes. Adoptó la convicción de que todos los fenómenos, incluyendo el funcionamiento del cerebro humano, son materialistas.[17]​ Sin embargo, siguió creyendo en la supervivencia del espíritu después de la muerte.

Debido a su falta de voluntad para esforzarse con la misma intensidad en el estudio de los clásicos que en el de la ciencia y la matemática, Turing suspendió sus exámenes finales varias veces y tuvo que ingresar en la escuela universitaria que eligió en segundo lugar, King's College, Universidad de Cambridge, en vez de en la que era su primera elección, Trinity. Tras su graduación, se trasladó a la Universidad estadounidense de Princeton, donde trabajó con el lógico Alonzo Church. Recibió las enseñanzas de Godfrey Harold Hardy, un respetado matemático que ocupó la cátedra Sadleirian en Cambridge, y que posteriormente, fue responsable de un centro de estudios e investigaciones matemáticas entre 1931 y 1934. En 1935 Turing fue nombrado profesor del King's College.

El Entscheidungsproblem, que se traduce como «problema de decisión», fue un reto en lógica simbólica para encontrar un algoritmo general que decidiera si una fórmula de cálculo de primer orden es un teorema. El problema fue planteado inicialmente por Leibniz en el siglo XVII luego de construir su máquina mecánica de cálculo. David Hilbert formalizó el problema en el VII Congreso Internacional de Matemáticas (Bolonia, 1928), planteando la búsqueda de un procedimiento algorítimico válido para solucionar las posibles cuestiones matemáticas, a través de tres preguntas:

Si bien Hilbert suponía que la respuesta a las preguntas era afirmativa, Kurt Gödel, mediante los teoremas de Incompletitud demostró que las dos primeras preguntas no podrían serlo ya que, tal como afirma Gödel:

«En cualquier formalización consistente de las matemáticas que sea lo bastante fuerte para definir el concepto de los números naturales, se puede construir una afirmación que ni se puede demostrar ni se puede refutar dentro de ese sistema», mientras que el primero afirma: «Ningún sistema consistente se puede usar para demostrarse a sí mismo».[18]​

Sin embargo, no podían resolver la última pregunta. La dificultad estaba en la ausencia de significado de lo que se entiende por un «procedimiento mecánico». En 1936, Alan Turing en su trabajo Acerca de los números computables,  introduce el concepto de la máquina de Turing y, junto a  Alonzo Church demostraron ambos que es imposible escribir tal algoritmo. Como consecuencia, es también imposible decidir con un algoritmo general si ciertas frases concretas de la aritmética son ciertas o falsas.

La tesis de Church-Turing formula hipotéticamente la equivalencia entre los conceptos de función computable y máquina de Turing, que expresado en lenguaje corriente vendría a ser: «Todo algoritmo es equivalente a una máquina de Turing». No es en sí un teorema matemático: es una afirmación formalmente indemostrable, una hipótesis que, no obstante, tiene una aceptación prácticamente universal.

La tesis Church-Turing postula que cualquier modelo computacional existente tiene las mismas capacidades algorítmicas, o un subconjunto, de las que tiene una máquina de Turing.

En su estudio Los números computables, con una aplicación al Entscheidungsproblem (publicado el 28 de mayo de 1936), Turing reformuló los resultados obtenidos por Kurt Gödel en 1931 sobre los límites de la demostrabilidad y la computación, sustituyendo al lenguaje formal universal descrito por Gödel por lo que hoy se conoce como máquina de Turing, unos dispositivos formales y simples.[19]​

Turing demostró que dicha máquina era capaz de resolver cualquier problema matemático que pudiera representarse mediante un algoritmo. Las máquinas de Turing siguen siendo el objeto central de estudio en la teoría de la computación. Llegó a probar que no había ninguna solución para el problema de decisión, Entscheidungsproblem, demostrando primero que el problema de la parada para las máquinas de Turing es irresoluble: no es posible decidir algorítmicamente si una máquina de Turing dada llegará a pararse o no. Aunque su demostración se publicó después de la demostración equivalente de Alonzo Church respecto a su cálculo lambda, el estudio de Turing es mucho más accesible e intuitivo.[20]​ También fue pionero con su concepto de «máquina universal (de Turing)», con la tesis de que dicha máquina podría realizar las mismas tareas que cualquier otro tipo de máquina. Su estudio también introduce el concepto de números definibles.[18]​

La mayor parte de 1937 y 1938 la pasó en la Universidad de Princeton, estudiando bajo la dirección de Alonzo Church. Entre 1938 y 1939 volvió a Inglaterra y estudió filosofía de las matemáticas. En 1938 obtuvo el Doctorado en Princeton; en su discurso introdujo el concepto de hipercomputación, en el que ampliaba las máquinas de Turing con las llamadas máquinas oracle, las cuales permitían el estudio de los problemas para los que no existe una solución algorítmica.[21]​

Tras su regreso a Cambridge en 1939, asistió a las conferencias de Ludwig Wittgenstein sobre las bases de las matemáticas. Ambos discutieron y mantuvieron un vehemente desencuentro, ya que Turing defendía el formalismo matemático y Wittgenstein criticaba que la matemática estaba sobrevalorada y no descubría ninguna verdad absoluta.[22]​

Un día después de la declaración de guerra de Gran Bretaña, en septiembre de 1939, Turing fue convocado a Bletchley Park, donde se encontraba la Escuela Gubernamental de Código y Cifrado (GC&CS). Las nueve mil personas que trabajaban allí se dedicaron a intentar interpretar las comunicaciones alemanas cifradas en código morse.

El cifrado lo hacían a través de una máquina de sistema rotatorio llamada Enigma (máquina). Enigma había sido inventada en 1918 por Arthur Scherbius. Era similar a una máquina de escribir, en la cual cada vez que una letra era pulsada, era sustituida por otra mediante el uso de tres rotores internos (las máquinas militares llegaron a usar cinco), cuyo resultado era más de diez mil billones de configuraciones distintas. Debido al carácter portátil de la máquina, los operadores podían estar ubicados en los puestos de mando, interior de los tanques, submarinos, en bombardeos, etc. Independientemente de su locación, los operadores, llevaban las instrucciones de cómo debían colocarse los rotores, y las posiciones cambiaban cada pocos días.[23]​ 

El equipo liderado por Turing, a través de ecuaciones y cálculos, encontraron pautas en los mensajes con lo que pudieron detectar una pequeña parte de su funcionamiento. Sin embargo, todavía no podían descifrarlos. Fue entonces, cuando Turing se preguntó:  

¿Y si para luchar contra una máquina como Enigma hiciese falta otra máquina?[24]​

A raíz de esta pregunta, Turing pudo poner en práctica sus teorías: diseñó la máquina Bombe. Bombe buscaba la configuración de los rotores de la máquina alemana, implementando una cadena de deducciones lógicas para cada combinación posible. Gracias a las mejoras del matemático,  Gordon Welchman, el 14 de marzo de 1940, el primer prototipo estaba terminado. Al cabo de un tiempo disponían con más de doscientas Bombes.[25]​ 

Los trabajos de la GC&CS, dirigidos por Turing, fueron determinantes para acortar la guerra. Algunos historiadores afirman que su trabajo acortó dos años la duración de la guerra, salvando alrededor de catorce millones de vidas.[26]​ Al finalizar la guerra, las máquinas Bombe se desmantelaron y todo el trabajo permaneció en secreto hasta los setenta. En 1974 el capitán W. F. Winterbotham escribió el libro The Ultra Secret.[27]​

De 1945 a 1948 Turing vivió en Richmond, Londres, donde trabajó en el Laboratorio Nacional de Física (NPL). En 1947 empezó a trabajar en el diseño del ACE (Automatic Computer Engine o Motor de Computación Automática). Paralelamente, existía un proyecto similar en Estados Unidos llamado EDVAC de Von Neumann. El ACE de Turing se diferenciaba en que incluía la implementación de funciones aritméticas en circuitos electrónicos. Su deseo era crear una máquina que pudiera ser configurada para hacer cálculos algebraicos, desencriptar códigos, manipular archivos y jugar al ajedrez. Aunque diseñar el ACE era factible, el secretismo que reinaba durante la guerra desembocó en retrasos para iniciar el proyecto por lo que Turing se sintió desilusionado. 

Tiempo más tarde creó el Abbreviated Code Instruction, que dio origen a los lenguajes de programación. En 1947 se tomó un año sabático en Cambridge, tiempo durante el cual escribió un trabajo pionero sobre la inteligencia artificial que no fue publicado en vida. En 1948, con la ayuda de Frederic Calland Williams, se dio, por primera vez, la demostración del principio de la máquina de Turing. 

Mientras se encontraba en Cambridge y a pesar de su ausencia, se siguió construyendo el prototipo piloto del ACE, que ejecutó su primer programa en mayo de 1950. Aunque la versión completa del ACE de Turing jamás fue construida, el diseño de otras computadoras en todo el mundo le debió mucho a su concepción.[28]​

A mediados de 1948 fue nombrado director delegado del laboratorio de computación de la Universidad de Mánchester y trabajó en el software de una de las primeras computadoras reales, la Manchester Mark I. Durante esta etapa también realizó estudios más abstractos y en su artículo de octubre de 1950 «Computing machinery and intelligence» Turing trató el problema de la inteligencia artificial y propuso un experimento que hoy se conoce como test de Turing, con la intención de definir una prueba estándar por la que una máquina podría catalogarse como «sensible» o «sintiente». En el documento, Turing sugirió que en lugar de construir un programa para simular la mente adulta, sería mejor producir uno más simple para simular la mente de un niño y luego someterlo a educación. Una forma invertida de la prueba de Turing se usa ampliamente en Internet, el test CAPTCHA que está diseñado para determinar si un usuario es un humano y no una computadora.

La prueba de Turing es un método para determinar si una máquina puede pensar.

Nace de un juego de imitación, en donde hay tres personas: un interrogador, un hombre y una mujer. El interrogador está separado de los otros dos, y solo puede comunicarse con ellos a través de un lenguaje que entiendan. El objetivo del interrogador es descubrir quién es la mujer, y quién es el hombre, mientras que el de los otros dos, es convencerlo que son la mujer. En su artículo de 1950, «Computing machinery and intelligence», Turing sustituye a uno de los interrogados por una computadora y cambia los objetivos del juego: reconocer a la máquina. 

«Una computadora puede ser llamada inteligente si logra engañar a una persona haciéndole creer que es un humano» - Alan Turing.[29]​

La forma de hacer pasar la prueba a una máquina consiste básicamente en una persona hablando con una computadora en otra habitación mediante un sistema de chat. Si la persona es incapaz de determinar si habla con un humano o con una computadora, entonces la computadora se considera inteligente.  

En el año 2014, por primera vez, el chatbot de Eugene Gootsman, logró convencer a treinta jueces que estaban participando en la prueba de que estaban chateando con un niño ucraniano de trece años.[30]​[31]​

Entre 1948 y 1950 en conjunto con un antiguo compañero, D. G. Champernowne, empezó a escribir un programa de ajedrez para un ordenador que aún no existía. En 1952 trató de implementarlo en el Ferranti Mark 1, pero por falta de potencia, el ordenador no fue capaz de ejecutar el programa. En su lugar Turing jugó una partida en la que reprodujo manualmente los cálculos que hubiera hecho el ordenador, costando alrededor de hora y media en efectuar un movimiento. Una de las partidas llegó a registrarse, y el programa perdió frente a un colega de Turing, Alick Glennie. Su test fue significativo, característicamente provocativo y una gran contribución para empezar el debate alrededor de la inteligencia artificial que aún hoy continúa.[32]​

Trabajó junto a Norbert Wiener en el desarrollo de la cibernética. Esta rama de estudios se genera a partir de la demanda de sistemas de control que exige el progresivo desarrollo de las técnicas de producción a partir del siglo XX. La cibernética pretende establecer un sistema de comunicación entre el hombre y la máquina como premisa fundamental para administrar los sistemas de control. Sus estudios profundizaron en esta relación estableciendo el concepto de interfaz y cuestionando los límites de simulación del razonamiento humano.

Turing trabajó desde 1952 hasta que falleció en 1954 en la biología matemática, concretamente en la morfogénesis. Publicó un trabajo sobre esta materia titulado «Fundamentos químicos de la morfogénesis» en 1952. Su principal interés era comprender la filotaxis de Fibonacci, es decir, la existencia de los números de Fibonacci en las estructuras vegetales. Utilizó ecuaciones de reacción-difusión que actualmente son cruciales para entender la formación de patrones en el campo de biología del desarrollo ontogenético (embriología). Sus trabajos posteriores no se publicaron hasta 1992 en el libro Obras completas de A. M. Turing.

Las teorías de Turing han ido ganando la aceptación de biólogos experimentales, como uno de los mecanismos mediante los cuales células que son genéticamente idénticas pueden diferenciarse y dar origen a organismos complejos.[33]​

La carrera profesional de Turing se vio truncada cuando lo procesaron por su homosexualidad. En 1952, Arnold Murray, un amante de Turing, ayudó a un cómplice a entrar en la casa de Turing para robarle. Turing acudió a la policía a denunciar el delito. Durante la investigación policial Turing reconoció su homosexualidad, con lo que se le imputaron los cargos de «indecencia grave y perversión sexual» (los actos de homosexualidad eran ilegales en el Reino Unido en esa época), los mismos que a Oscar Wilde más de 50 años antes.

Convencido de que no tenía de qué disculparse, no se defendió de los cargos y fue condenado. Según su ampliamente difundido proceso judicial, se le dio la opción de ir a prisión o de someterse a castración química mediante un tratamiento hormonal de reducción de la libido. Finalmente escogió las inyecciones de estrógenos, que duraron un año y le produjeron importantes alteraciones físicas, como la aparición de pechos o un apreciable aumento de peso, que lo condujeron a padecer de disfunción eréctil.


En una carta de esta época a su amigo Norman Routledge, Turing escribió en forma de falso silogismo una reflexión, relacionando el rechazo social que provoca la homosexualidad con el desafío intelectual que supone demostrar la posibilidad de inteligencia en los ordenadores. En particular, le preocupaba que los ataques a su persona pudieran oscurecer sus razonamientos sobre la inteligencia artificial:[34]​
Dos años después del juicio, en 1954, falleció por envenenamiento con cianuro, aparentemente tras comerse una manzana envenenada que no llegó a ingerir completamente, en un contexto que se estimó oficialmente como suicidio.[35]​[36]​ Varias personas pensaron que su muerte fue intencionada, aunque su madre intentó negar la causa de su muerte, atribuyéndola a una ingestión accidental provocada por la falta de precauciones de Turing en el almacenamiento de sustancias químicas de laboratorio. Los últimos años de su vida fueron amargos y reservados. Esta muerte no esclarecida ha dado lugar a diversas hipótesis, incluida la del asesinato.[34]​

El 10 de septiembre de 2009, el primer ministro del Reino Unido, Gordon Brown, emitió un comunicado declarando sus disculpas en nombre de su gobierno por el trato que recibió Alan Turing durante sus últimos años de vida. Este comunicado fue consecuencia de una movilización pública solicitando al Gobierno que ofreciera disculpas oficialmente por la persecución contra Alan Turing.[37]​[38]​ Sin embargo, en 2012 el gobierno británico de David Cameron denegó el indulto al científico,[39]​ aduciendo que la homosexualidad era considerada entonces un delito.[40]​ Finalmente, el 24 de diciembre de 2013 recibió el indulto de todo tipo de culpa, por orden de la reina Isabel II.[7]​

El 23 de junio de 2001 se inauguró una estatua de Turing en Mánchester. Se encuentra en Sackville Park, entre el edificio de la Universidad de Mánchester en la calle de Whitworth y la gay village de la calle del Canal. Coincidiendo con el 50.º aniversario de su muerte, se descubrió una placa conmemorativa en su antiguo domicilio, Hollymeade, en Wilmslow el 7 de junio de 2004.

La Association for Computing Machinery otorga anualmente el Premio Turing a personas destacadas por sus contribuciones técnicas al mundo de la computación. Este premio está ampliamente considerado como el equivalente del Premio Nobel en el mundo de la computación.

El Instituto Alan Turing fue inaugurado por el UMIST (Instituto de Ciencia y Tecnología de la Universidad de Mánchester) y la Universidad de Mánchester en el verano de 2004.

El 5 de junio de 2004 se celebró un acontecimiento conmemorativo de la vida y la obra de Turing en la Universidad de Mánchester, organizado por el British Logic Colloquium y la British Society for the History of Mathematics.

El 28 de octubre de 2004 se descubrió una estatua de bronce de Alan Turing esculpida por John W. Mills en la Universidad de Surrey. La estatua conmemora el 50.º aniversario de la muerte de Turing. Representa a Turing transportando sus libros a través del campus.[41]​

El 23 de junio de 2012, día en el que se conmemoró el centenario del nacimiento de Turing, Google presentó entre sus habituales doodles una pequeña máquina de Turing capaz de comparar dos cadenas de caracteres binarios.

Una leyenda urbana asegura que el logo de Apple Computers (mordisco de la manzana) rinde homenaje a Turing y su suicidio comiendo una manzana envenenada con cianuro. Incluso, el arco iris en el logo sería un homenaje a la homosexualidad de Turing. Sin embargo, estas suposiciones fueron desmentidas por Rob Janoff, creador del logo de Apple y de hecho, los colores ni siquiera se muestran en el mismo orden que en la bandera arco iris, dado que esta fue diseñada dos años más tarde de la creación de dicha imagen.[42]​

Cristo (del latín Christus, y este del griego antiguo Χριστός, Christós)[1]​ es una traducción del término hebreo «Mesías» (מָשִׁיחַ, Māšîaḥ), que significa «ungido»,[2]​ y que se emplea como título o epíteto de Jesús de Nazaret en el Nuevo Testamento.[3]​ En el cristianismo, Cristo se utiliza como sinónimo de Jesús.[3]​

Los seguidores de Jesús son conocidos como «cristianos» porque creen y confiesan que Jesús es el Mesías profetizado en el Antiguo Testamento,[4]​ por lo cual le llamaban «Jesús Cristo», que quiere decir «Jesús, el Mesías» (en hebreo: «Yeshua Ha'Mashiaj»), o bien, en su uso recíproco: «Cristo Jesús» («El Mesías Jesús»).

El título «Cristo» también está dentro del nombre personal «Jesucristo»,[5]​ y se menciona como un sinónimo de Jesús de Nazaret en la fe cristiana, que lo considera salvador y redentor de los hombres, el «Verbo» (o Palabra) de Dios encarnado[6]​ y «el Hijo unigénito de Dios».[7]​

Las principales creencias cristianas acerca de Jesucristo incluyen su consideración como el Hijo de Dios, constituido como Señor; que fue concebido por el Espíritu Santo y que nació de la Virgen María; que fue crucificado, muerto y sepultado durante el mandato de Poncio Pilato; que descendió a los infiernos y posteriormente resucitó de la muerte y subió a los cielos, donde se encuentra junto a Dios Padre y desde donde volverá para el Juicio Final.

La cristología, un área de la teología, se ocupa principalmente de estudiar la naturaleza divina de la persona de Jesucristo, según los evangelios canónicos y los demás escritos del Nuevo Testamento.

El título «Mesías» fue utilizado en el Libro de Daniel,[8]​ que habla de un «Mesías Príncipe» en la profecía acerca de «las setenta semanas».
También aparece en el Libro de los Salmos,[9]​ donde se habla de los reyes y príncipes que conspiran contra Yahveh y contra su ungido. Pero fundamentalmente en el libro del profeta Isaías se expresa la llamada corriente mesiánica (Is 9, 1-7) atribuida a Cristo según los escritos del Nuevo Testamento.

Jesús es llamado «el Cristo» en los cuatro evangelios del Nuevo Testamento donde se le describe como ungido con el Espíritu Santo. Algunas referencias incluyen Mateo 1:16, Mateo 27:17, Mateo 27:22, Marcos 8:29, Lucas 2:11, Lucas 9:20 y Juan 1:41. En el evangelio de Mateo se trata el tema en el siguiente pasaje:

En el evangelio de Juan, el título de «Cristo» se usa como nombre de Jesús:

En el Libro de Daniel se afirma que el mesías príncipe sería cortado, y no tendría nada.[12]​[13]​
La antigua versión de Reina-Valera traduce ‘será muerto y nada tendrá’ y en el margen de la paráfrasis ‘será echado de la posesión’. Esto se cumplió cuando, en lugar de ser aceptado como Mesías por los judíos, fue rechazado, cortado, y no recibió ninguno de los honores mesiánicos que le pertenecían, aunque, con su muerte, echó los cimientos de su futura gloria en la Tierra, obrando la redención eterna para los salvos. En la Primera Carta a los Corintios san Pablo de Tarso escribió que así como el cuerpo es uno y tiene muchos miembros, así es el Cristo: la cabeza y los miembros en el poder y la unción del Espíritu forman un solo cuerpo.[14]​

En el Libro de Juan, este título es relacionado con el de Mesías, «llamado el Cristo».[15]​

Habiendo sido rechazado como mesías en la tierra, él ha sido hecho, ya resucitado de los muertos, Señor y Cristo,[16]​ y así se cumplen los consejos de Dios con respecto a él y al hombre en él. Se revela que los santos habían sido escogidos en Cristo desde antes de la fundación del mundo. Todas las cosas en el cielo y en la tierra tienen que ser encabezadas en el Cristo,[17]​ ya que el Cristo es la cabeza del cuerpo de la Iglesia.[18]​

La palabra «ungir» ―del latín únguere― significa ‘elegir a alguien para un puesto o un cargo muy notable’ (como sumo sacerdote o rey).[19]​

La concepción hebrea del ungido o entronizado proviene de la antigua creencia que establece que untar a una persona u olear un objeto con aceite otorga cualidades extraordinarias, incluso sobrenaturales, cuando estas provienen de una autoridad divina. En el Israel de la antigüedad, la costumbre de ungir a una persona otorgaba la potestad para ejercer algún cargo importante. El término Cristo no solo se utilizaba con los sacerdotes[20]​ que eran mediadores entre Dios y la humanidad, sino también con los reyes teocráticos[21]​ que eran representantes de Dios y adquirían de esa manera dignidad sacerdotal. Más tarde se aplicó a los profetas[22]​ e incluso se vinculó con los patriarcas.[23]​
Sin embargo, en la transformación del concepto mesiánico, el uso del término se restringió al redentor y restaurador de la nación judía.[24]​[25]​

En el Nuevo Testamento, la palabra Cristo se utiliza como nombre común y como nombre propio. En ambas acepciones aparece con o sin artículo definido, en solitario o asociada a otros términos o nombres. Cuando se usa como nombre propio y, muchas veces, en los otros casos, designa a Jesús de Nazaret, el esperado Mesías de los judíos. De esta manera, para las confesiones cristianas, Jesucristo es el mesías, aquel que el Antiguo Testamento anunciaba que llegaría como plan de salvación de Dios para la humanidad. Otras religiones, sobre todo los musulmanes,[26]​ judíos ortodoxos, conservadores, y reformistas,[27]​ lo consideran solamente como un gran profeta o predicador de su pueblo ―el pueblo judío― y el fundador de la religión cristiana, en quien sus seguidores creen y afirman que es el hijo encarnado de Dios.

La palabra salvador, a su vez, era el título calificativo que los judíos aplicaban a sus sacerdotes, reyes, y profetas, ya que estos debían ser ungidos con aceites como parte del rito que los consagraba a su labor. Los seguidores de Jesús de Nazaret, considerando que este era el Mesías prometido por las profecías mesiánicas de la Tanaj, le aplicaron este título a su líder, llamándole Cristo Jesús o el Salvador. A mediados del siglo II -unos cien años después de la muerte y resurrección de Jesús de Nazaret—se les comenzó a conocer por cristianos en Antioquía, ya que se decían seguidores del Cristo.

Según algunas confesiones cristianas, como la Iglesia católica, la Iglesia ortodoxa, la Iglesia anglicana o las principales iglesias protestantes, la Salvación es una venida de Dios. Sustentan este punto de vista en las palabras del Apóstol Pedro: «Por el contrario, creemos que tanto ellos como nosotros somos salvados por la gracia del Señor Jesús».[28]​ Esta gracia se obtiene a través de la fe y el obrar cristiano, según católicos y ortodoxos, o exclusivamente por la fe, según los protestantes, es decir, en creer o confiar en que Jesucristo es el Hijo de Dios, el Salvador y el Único Perdonador de pecados.

En la carta de Pablo a los romanos se explica lo que es la salvación,[29]​ pero con más precisión en la carta del apóstol Pablo a los Efesios: «Cristo, con su muerte y su Resurrección, es quien elimina la deuda del pecado humano y vehicula en su persona esa gracia redentora».[30]​ Para el cristianismo la salvación está disponible para todos los que creen y actúan en consecuencia.

La creencia cristiana afirma que Dios se manifestó a los hombres en la persona de Jesús de Nazaret (en hebreo: Yeshúa), siendo el Hijo de Dios hecho hombre y, por tanto, el Mesías anunciado por los profetas en las escrituras, y ansiosamente esperado por Israel. Escrituras.[31]​
De hecho, Jesús mismo afirmó ser el Cristo.[31]​ En el Evangelio de Juan, cuando Jesús habla con la mujer samaritana, se registra el siguiente evento:


A raíz de esto, se narra a los samaritanos diciendo: «nosotros mismos hemos oído, y sabemos que verdaderamente éste es el Salvador del mundo, el Cristo.» (Juan 4:42)

En el Evangelio de Marcos también se narra a Jesús afirmando ser el Mesías, cuando los sacerdotes del templo estaban interrogándolo:


 El cristianismo surgió como una comunidad, la Iglesia, inspirada en las enseñanzas de Jesús de Nazaret. Según san Lucas (en Hechos de los Apóstoles 11:26), los discípulos de Jesús fueron llamados «cristianos» por primera vez en Antioquía de Siria. La misión que los unía era la prédica de estas enseñanzas por todo el mundo, prédica inicialmente llevada a cabo por sus discípulos directos, llamados apóstoles. Según los Evangelios, Dios preparó un pueblo, prefigurado en el pueblo de Israel, conducido por Moisés y los profetas y al que Cristo encabeza como jefe y salvador. Con este pueblo, Cristo realizaría una nueva alianza. El fin de este pacto es que todos conozcan a Dios Padre y a Jesucristo su Hijo y en Él tengan vida eterna (según el Evangelio de Juan 3.16).

Según el cristianismo, Jesús de Nazaret es el Cristo (el Mesías), Hijo de Dios hecho hombre (según el Evangelio de Mateo),[32]​ concebido por el Espíritu Santo y nacido de la virgen María. Después de la crucifixión, al tercer día resucitó y posteriormente subió al Cielo; y se espera su regreso al final de los tiempos en lo que se llama la «segunda venida de Cristo», o Parusía. El cristianismo explica que el sufrimiento de Jesús era necesario.[33]​ Frecuentemente se cree que el padecimiento de Jesús se desarrolló en la cruz, en realidad su padecimiento comenzó desde el huerto de Getsemaní.[34]​ En este pasaje se describe como Jesús lleno de angustia oraba intensamente, su sudor era como grandes gotas de sangre que caían hasta la tierra.

La religión cristiana se inició en el seno del judaísmo como uno de tantos movimientos mesiánicos, centrado en la persona de Jesús de Nazaret. Sus seguidores extendieron su culto por todo el mundo basándose en la idea de que Jesús había resucitado.

Los seguidores de Cristo en el mundo actual no forman un conjunto único y uniforme, sino que se agrupan en distintas confesiones, como las iglesias católica, ortodoxa, anglicana, luterana, bautista, anabaptista, menonita, presbiteriana, metodista, mormona, etc. Y aún los hay que no reconocen un vínculo con algún grupo.

La fe en Cristo de la mayoría de estas comunidades puede sintetizarse en esta antiquísima profesión de fe:

Existe un movimiento llamado ecumenismo, el cual trata de buscar la unidad de todos los seguidores de Cristo. A este respecto, dentro de la Iglesia católica, el Concilio Vaticano II, en su decreto Unitatis redintegratio, ha expresado, refiriéndose a la división de los cristianos, «abiertamente repugna a la voluntad de Cristo y es piedra de escándalo para el mundo y obstáculo para la causa de la difusión del Evangelio por todo el mundo».[35]​

Antes de su realización, el papa Juan XXIII creó el Pontificio Consejo para la Promoción de la Unidad de los Cristianos. Esta llamada ha sido continuada por los papas siguientes.[n 2]​

Teología

En el cristianismo primitivo, Jesús de Nazaret fue visto por algunos de sus contemporáneos judíos como el Mesías que se profetizaba en el Tanaj, pero tiempo después, Jesús fue visto como la imagen de Dios, separándose del judaísmo y creando su propio libro sagrado, la Biblia.   

la mayoría de los cristianos tienen como dogma la Santísima Trinidad que representa a Dios, el teólogo Arrio discrepó de esa enseñanza y dijo que Jesús estaba subordinado a Dios Padre y por lo tanto no hace parte de Dios, ese modelo llevó a un movimiento llamado Unitarismo . El teólogo Nestorio indicaba que Jesús y Dios son de naturaleza divina pero separados. El binitarismo enseña que Dios son dos personas. En algunas ramas cristianas Jesús es Dios. 

Saliendose del cristianismo, en el Islam, Jesús es uno de los tantos profetas enviados por Dios. 

Para el catolicismo, Cristo es el Hijo de Dios hecho hombre para la salvación del género humano, y esa es la «Buena Nueva»: Dios ha enviado a su Hijo.[36]​
Hijo de Dios hecho hombre: para la Iglesia católica esto significa que la segunda Persona de la Santísima Trinidad, el Hijo, se hizo hombre en el seno de María. Cristo, siendo una sola Persona divina, es perfecto Dios y perfecto hombre. Esta doctrina encuentra sus antecedentes en distintos textos de la Sagrada Escritura, entre los que se puede citar:


Se han producido dentro de la Iglesia católica distintos debates referidos a cómo deben interpretarse estas afirmaciones. Su posición oficial ha quedado fijada en las decisiones de los distintos Concilios:

El Primer Concilio de Nicea, en el año 325, el primer concilio ecuménico que la Iglesia católica pudo realizar terminadas las persecuciones que padeció sus primeros 300 años, profundizó los textos bíblicos citados, afirmando que Jesucristo es consustancial al Padre (de la misma sustancia que el Padre), es decir, verdadero Dios.

El Primer Concilio de Constantinopla, en el año 381, continuó con la profundización de la doctrina, redactando el Credo Niceno-Constantinopolitano:


Los Concilios siguientes han continuado precisando la doctrina:

Estas precisiones han surgido como respuesta a distintas doctrinas que fueron apareciendo. Por ejemplo:

En todas ellas, la Iglesia ha visto en el fondo la negación de la redención, porque creían que era necesario que Cristo fuera Dios, para poder redimir; que fuera hombre, para poder padecer; y que fuera una sola persona, para poder referir la divinidad y la humanidad «en concurrencia inefable y misteriosa en la unidad».[41]​

Para la Iglesia católica, Cristo, en el mundo actual, es «Lumen Gentium»,  «Luz de los pueblos».[42]​  Por ello san Juan Pablo II, en la homilía de comienzo de su pontificado, exclamaba: «¡No temáis! ¡Abrid, más todavía, abrid de par en par las puertas a Cristo!».[43]​

Más recientemente, el papa Francisco ha expresado:

El Catecismo de la Iglesia católica destaca que «los Padres ven en la concepción virginal el signo de que es verdaderamente el Hijo de Dios el que ha venido en una humanidad como la nuestra».[45]​

La Iglesia católica resalta el papel de María en la concepción virginal de Cristo, en su relación de fe hacia Él y en la redención por él obrada.
Los Padres de la Iglesia abordaron la íntima unión de Cristo y María en la obra de la redención. Por ejemplo:


Por un lado, la Iglesia católica sostiene que Dios ha preparado a María para tal misión, «en atención a los méritos de Cristo Jesús», preservándola del pecado original, en lo que se denomina su Inmaculada Concepción[46]​ y concediéndole multitud de gracias, las que ella misma reconoció diciendo: «Porque el Todopoderoso ha hecho en mí grandes cosas»[47]​ y a las que ella correspondió con absoluta fidelidad y entrega.[n 3]​

Por otro, ha visto en el sí de María, al aceptar el ofrecimiento del ángel a ser madre de Jesús, el sí de la humanidad, que aceptaba a través de ella la salvación que traería Cristo.[n 4]​

Por el hecho de ser madre de Cristo, que según se ha visto la Iglesia católica enseña que es la segunda Persona de la Santísima Trinidad que se hizo hombre sin perder su condición divina, la Iglesia la llama Madre de Dios.[48]​

Los evangelios detallan los hechos de la vida de Cristo más sobresalientes, sin embargo, en los mismos no pasa desapercibida la discreta presencia de María: el Hijo de Dios se hace hombre luego de su consentimiento;[49]​ los pastores y los magos encuentran al Niño Prometido junto a ella;[50]​
Cristo hace su primer milagro a su pedido;[51]​ está firme al pie de la Cruz, junto a su Hijo.[52]​
La Iglesia ha visto en las palabras de Jesús: «Mujer, ahí tienes a tu hijo» y a Juan: «Ahí tienes a tu madre»[53]​ la entrega de María como madre de todos los cristianos, representados en la persona de Juan, por lo que es llamada «Madre de la Iglesia».[54]​
Y ella, que «conservaba cuidadosamente todas las cosas en su corazón»,[55]​ perseveraba en la oración junto a la Iglesia naciente, según cuenta el libro de los Hechos de los Apóstoles.[56]​
El Apocalipsis habla de una mujer, vestida de sol, con la luna bajo sus pies y una corona de doce estrellas sobre su cabeza y que da a luz un hijo varón que derrotará al dragón infernal.[57]​

En la misma promesa del Redentor, contenida en el libro del Génesis, se habla de una mujer, de la que nacería el vencedor de la serpiente:


A este respecto comenta san Alfonso María de Ligorio: «ya desde el principio de la Humanidad, Dios predijo a la serpiente infernal la victoria y el dominio que había de ejercer sobre él nuestra reina al anunciar que vendría al mundo una mujer que lo vencería […] ¿Y quién fue esta mujer su enemiga sino María, que con su preciosa humildad y vida santísima siempre venció y abatió su poder? «En aquella mujer fue prometida la Madre de nuestro Señor Jesucristo», dice san Cipriano. Y por eso argumenta que Dios no dijo «pongo», sino «pondré», para que no se pensara que se refería a Eva».[58]​

San Agustín, comentando el pasaje donde una mujer le dice a Jesús: «dichoso el vientre que te llevó» y el Señor contestó: «mejor, dichosos los que escuchan la palabra de Dios y la cumplen»,[59]​  dice que esto significa que María, no solamente escuchó la palabra y la cumplió[60]​ sino que es más feliz por haber concebido a Cristo en su mente mediante la fe, que por haberlo llevado en su seno.[61]​ A través de ella, la misma «Palabra se hizo carne, y habitó entre nosotros».[62]​

Por esta elección de Dios y su correspondencia por parte de María, ha visto la Iglesia en ella un modelo de perfecta cristiana, y un camino para llegar a Cristo.[n 5]​[n 6]​[n 7]​

En el Evangelio de Mateo, Jesús habla de «su Iglesia».[63]​ La palabra «iglesia» viene del griego ecclesia, que significa ‘asamblea’. San Pablo de Tarso dice que la iglesia es el cuerpo de Cristo.[64]​

La Iglesia católica afirma ser ella la iglesia fundada por Cristo,[65]​ exhibiendo entre otros argumentos, la sucesión apostólica: todos los obispos católicos han sido ordenados por otro obispo, y así, remontándose hacia atrás, se llegará a uno de los apóstoles elegidos por Cristo. Dice así san Ireneo de Lyon:

Según la Iglesia, solo en ella puede encontrarse la plenitud total de los medios de salvación dados por Cristo.[66]​
Sin embargo, ella misma enseña que fuera de sus límites visibles, hay muchos elementos de santificación y de verdad.[67]​

Según el catolicismo, dentro de la sucesión apostólica que concierne a todos los obispos, está la del Obispo de Roma, el papa, sucesor de san Pedro hasta nuestros días. (Véase Lista de papas). La Iglesia católica afirma que Cristo constituyó jefe de su Iglesia a San Pedro y en él a sus sucesores:

La Iglesia enseña que el papa es el «principio y fundamento perpetuo y visible de unidad, tanto de los obispos como de la muchedumbre de los fieles».[69]​ Por esto, san Ambrosio de Milán pudo decir: «allí donde está Pedro, allí está la Iglesia».[70]​

Con referencia a esto, continúa san Ireneo de Lyon en la cita que se transcribió en la sección referida a Cristo y la Iglesia:

Y san Cipriano de Cartago:

Para la Iglesia, las enseñanzas de Dios están contenidas en la Biblia y en la transmisión oral de la predicación de los apóstoles, llamada Tradición Apostólica. A su vez, estas enseñanzas han llegado a los hombres de todos los tiempos a través del Magisterio de la Iglesia, ejercido por los obispos, sucesores de los apóstoles, en comunión con el sucesor de San Pedro, el papa.

La interpretación de la Palabra en la Iglesia católica no es libre. Tratándose de la Sagrada Escritura, por ejemplo, la Iglesia enseña que debe hacerse “estando atentos a los que los autores humanos quisieron verdaderamente afirmar y a lo que de Dios quiso manifestarnos mediante sus palabras”.[71]​

Esta interpretación es realizada por la Iglesia, “columna y fundamento de la verdad”, como dice San Pablo.[72]​ Y fue ejercida desde el comienzo, por los mismos apóstoles: “El Espíritu Santo, y nosotros mismos, hemos decidido…”.[73]​

La Iglesia primitiva no tenía Nuevo Testamento. La misma inclusión de los libros sagrados en el canon bíblico, ha sido un acto del Magisterio eclesiástico.[n 8]​ El resto de las confesiones cristianas han heredado la Biblia (el Nuevo Testamento al menos) tal como quedó fijado por la Iglesia católica.

Ya desde el comienzo del cristianismo, surgieron opiniones divididas respecto a las enseñanzas transmitidas por Jesucristo. Por ejemplo el apóstol san Juan dice, refiriéndose a los disidentes: «ellos salieron de entre nosotros, sin embargo, no eran de los nuestros».[74]​

La Iglesia entiende que Dios, al revelar su palabra a través de Cristo, constituyó al mismo tiempo una autoridad presente en todos los tiempos, encargada de interpretarla sin equivocarse, a fin de mantener “la pureza de la fe transmitida por los apóstoles”, de otra manera no habría modo de saber sin que quede lugar a dudas cuál es la interpretación correcta. Esta capacidad de la Iglesia de interpretar sin equivocarse la palabra de Cristo, la Iglesia la llama “infalibilidad”, y ella entiende que la ha recibido de Cristo, conjuntamente con la misión de difundir su palabra.[75]​

Algunos párrafos del Catecismo de la Iglesia católica donde se explica la doctrina acerca de los sacramentos:

Especial mención merece la eucaristía. La Iglesia católica cree que la eucaristía o Santa Misa fue instituida por Cristo cuando en la Última Cena dijo: «Tomad y comed: esto es mi cuerpo», «Tomad y bebed, esto es mi sangre», «haced esto en conmemoración mía».[76]​ Ella cree que en cada eucaristía se hace presente (“se re-presenta”) el sacrificio que Cristo hizo en la cruz de una vez para siempre, se perpetúa su recuerdo a través de los siglos y se aplica su fruto.[77]​ Y que el sacrificio de la cruz y el sacrificio de la eucaristía son un único sacrificio, ya que tanto en uno como en otro, Cristo es el sacerdote que ofrece el sacrificio y la víctima que es ofrecida. Se diferencian sólo en la forma en que se ofrece el sacrificio. En la cruz Cristo lo ofreció en forma cruenta, y por sí mismo, y en la Misa en forma incruenta y por ministerio de los sacerdotes.[78]​ Por esto san Juan Pablo II pudo decir que en la eucaristía “está inscrito de forma indeleble el acontecimiento de la pasión y muerte del Señor. No sólo lo evoca sino que lo hace sacramentalmente presente. Es el sacrificio de la Cruz que se perpetúa por los siglos”.[79]​

La Iglesia cree que Cristo mismo está presente en la eucaristía. Esta presencia no la entiende como la que se da en una efigie, imagen, símbolo o recordatorio, sino que ella cree que está Él en persona, vivo y entero, con su cuerpo, sangre, alma y divinidad, de una forma “verdadera, real y sustancial”.[80]​

Por esto san Juan Crisóstomo pudo decir: «Cuánta gente dice hoy: ‘Querría ver a Cristo en persona, su cara, sus vestidos, sus zapatos’. ¡Pues bien, en la eucaristía es a él al que vés, al que tocas, al que recibes! Deseabas ver sus vestidos; y es él mismo el que se te da no sólo para verle, sino para tocarlo, comerlo, acogerlo en tu corazón».[81]​

Y san Juan Pablo II: «La Iglesia ha recibido la eucaristía de Cristo, su Señor, no sólo como un don entre otros muchos, aunque sean muy valiosos, sino como el don por excelencia, porque es don de sí mismo, de su persona en su santa humanidad y, además, de su obra de salvación».[82]​

La Iglesia entiende que la eucaristía se destaca del resto de los sacramentos ya que mientras ellos tienen la misión de santificar, en la eucaristía se halla el autor mismo de la santidad.[83]​ Por ello es llamada "Santísimo Sacramento del Altar", "Santísimo Sacramento", o sencillamente "Santísimo".

Cristo ha prometido la vida eterna a quienes lo reciben en este Sacramento:


La Odisea (en griego: Ὀδύσσεια, Odýsseia) es un poema épico griego compuesto por 24 cantos, atribuido al poeta griego Homero. Se cree que fue compuesta en el siglo VIII a. C. en los asentamientos que tenía Grecia en la costa oeste del Asia Menor (actual Turquía asiática). Según otros autores, la Odisea se completa en el siglo VII a. C. a partir de poemas que solo describían partes de la obra actual. Fue originalmente escrita en lo que se ha llamado dialecto homérico. Narra la vuelta a casa, tras la guerra de Troya, del héroe griego Odiseo (al modo latino, Ulises: Ὀδυσσεὺς en griego; Vlixes en latín). Además de haber estado diez años fuera luchando, Odiseo tarda otros diez años en regresar a la isla de Ítaca, de la que era rey, período durante el cual su hijo Telémaco y su esposa Penélope han de tolerar en su palacio a los pretendientes que buscan desposarla (pues ya creían muerto a Odiseo), al mismo tiempo que consumen los bienes de la familia. 

La mejor arma de Odiseo es su mētis o astucia.[1]​ Gracias a su inteligencia — además de la ayuda provista por Palas Atenea, hija de Zeus Crónida — es capaz de escapar de los continuos problemas a los que ha de enfrentarse por designio de los dioses. Para esto, planea diversas artimañas, bien sean físicas —como pueden ser disfraces— o con audaces y engañosos discursos de los que se vale para conseguir sus objetivos.

El poema es, junto a la Ilíada, uno de los primeros textos de la épica grecolatina y por tanto de la literatura occidental. Se cree que el poema original fue transmitido por vía oral durante siglos por aedos que recitaban el poema de memoria, alterándolo consciente o inconscientemente. Era transmitido en dialectos de la Antigua Grecia. Ya en el siglo IX a. C., con la reciente aparición del alfabeto, tanto la Odisea como la Ilíada pudieron ser las primeras obras en ser transcritas, aunque la mayoría de la crítica se inclina por datarlas en el siglo VIII a. C. El texto homérico más antiguo que conocemos es la versión de Aristarco de Samotracia (siglo II a. C.). El poema está escrito usando una métrica llamada hexámetro dactílico. Cada línea de la Odisea original estaba formada por seis unidades o pies, siendo cada pie dáctilo o espondeo.[2]​ Los primeros cinco pies eran dáctilos y el último podía ser un espondeo o bien un troqueo. Los distintos pies se separan  por cesuras o pausas.

La obra consta de 24 cantos. Al igual que muchos poemas épicos antiguos, comienza in medias res: empieza en mitad de la historia, contando los hechos anteriores a base de recuerdos o narraciones del propio Odiseo. El poema está dividido en tres partes. En la Telemaquia (cantos del I al IV) se describe la situación de Ítaca con la ausencia de su rey, el sufrimiento de Telémaco y Penélope debido a los pretendientes, y cómo el joven emprende un viaje en busca de su padre. En el regreso de Odiseo (cantos del V al XII) Odiseo llega a la corte del rey Alcínoo y narra todas sus vivencias desde que salió de Troya. Finalmente, en la venganza de Odiseo (cantos del XIII al XXIV), se describe el regreso a la isla, el reconocimiento por alguno de sus esclavos y su hijo, y cómo Odiseo se venga de los pretendientes matándolos a todos. Tras aquello, Odiseo es reconocido por su esposa Penélope y recupera su reino. Por último, se firma la paz entre todos los itacenses. 

Concilio de los dioses. Exhortación de Atenea a Telémaco. Homero comienza la Odisea invocando a la Musa para que cuente lo sucedido a Odiseo después de destruir Troya. En una asamblea de los dioses griegos, Atenea aboga por la vuelta del héroe a su hogar. Odiseo lleva muchos años en la isla de la ninfa Calipso. La misma Atenea, tomando la figura de Mentes, rey de los Tafios, aconseja a Telémaco que viaje en busca de noticias de su padre.[3]​

Telémaco reúne en asamblea al pueblo de Ítaca. El palacio de Odiseo se encuentra invadido por decenas de pretendientes que, creyendo que él ha muerto, buscan la mano de su esposa: Penélope. Gracias a la ayuda de Atenea, aparecida ahora en forma de Méntor, el joven convoca una asamblea en el ágora para expulsar a los soberbios pretendientes de su hogar. Finalmente, Telémaco consigue una nave y emprende viaje a Pilos en busca de noticias sobre su padre.

Telémaco viaja a Pilos para informarse sobre su padre. La siguiente mañana, Telémaco y Atenea, que continúa en la forma de Mentor, llegan a Pilos. Allí, invitados por Néstor, participan en una hecatombe para Poseidón.[4]​El rey Néstor les relata el regreso de otros héroes desde Troya y la muerte de Agamenón, pero no tiene información específica de Odiseo. Les sugiere que vayan a Esparta a hablar con Menelao, quien acaba de regresar de largos viajes. Atenea pide a Néstor que uno de sus hijos acompañe a Telemaco a Esparta y desaparece milagrosamente. Impresionado porque un joven esté escoltado por una diosa, Néstor ordena el sacrificio de una vaca en honor de ella y arregla que su hijo Pisístrato acompañe a Telémaco a Esparta.

Telémaco viaja a Esparta para informarse sobre su padre. Continúa el viaje hasta Esparta, donde lo reciben Menelao y Helena. Menelao le cuenta acerca de su conversación con Proteo, quien le informó acerca de la suerte que había corrido Odiseo, encontrándose este en una isla retenido por una ninfa llamada Calipso. Mientras tanto, los pretendientes, sabiendo del viaje del joven, preparan una emboscada que le tenderán a su regreso.

Odiseo llega a Esqueria de los feacios. En una nueva asamblea de los dioses, Zeus toma la decisión de mandar al mensajero Hermes a la isla de Calipso para que esta deje marchar a Odiseo. La ninfa promete a Odiseo la inmortalidad si se queda, pero el héroe prefiere salir de la isla.

Tarda cuatro días en construir una balsa, y emprende el viaje al quinto día, pero es hundido por Poseidón, enfadado con Odiseo desde que el griego cegó a su hijo Polifemo. Odiseo es ayudado por la nereida Leucótea, quien le da una manta con la que debe taparse el pecho y nadar hasta la isla de los feacios.

Odiseo y Nausícaa. Atenea visita, en un sueño, a la princesa Nausícaa, hija de Alcínoo, rey de Esqueria, y la conmina a hacerse cargo de sus responsabilidades como mujer en edad de casarse. Al despertar, Nausícaa pide a su padre un carro con mulas para ir a lavar ropa al río. Mientras ella y sus esclavas descansan y otras juegan a la pelota, Odiseo despierta, las ve y pide ayuda a la princesa. Nausícaa, impresionada por su forma de hablar, acoge al héroe y le brinda alimentos, le dice que la siga hacia la casa del rey y le indica cómo pedirle a su madre, la reina, hospitalidad. Le señala un bosque consagrado a Atenea, situado en las afueras de la ciudad y donde podrá descansar. Odiseo aprovecha la ocasión para implorar a la reina que lo reciba y lo ayude a llegar a su isla patria.

Odiseo en el palacio de Alcínoo. Guiado hasta allí por Atenea, Odiseo es recibido en el palacio por Alcínoo, rey de los feacios, que lo invita al banquete que se va a celebrar. Odiseo cuenta todo lo acaecido hasta ese momento, con lo que el rey queda impresionado y le ofrece la mano de su hija, mas Odiseo no acepta, por lo que el rey cambia su ofrecimiento por ayudarlo a llegar a su isla.

Odiseo agasajado por los feacios. Se celebra una fiesta en el palacio en honor del huésped, que aún no se ha presentado. Tras una competición de atletismo, en la que Odiseo asombra al público con un gran lanzamiento de disco, comienza el banquete. El aedo Demódoco ameniza la comida con un canto sobre la guerra de Troya. Al hablar del episodio del caballo, Odiseo rompe a llorar. El rey manda al aedo que deje de cantar, y pregunta al huésped sobre su verdadera identidad.

Odiseo cuenta sus aventuras: los cicones, los lotófagos, los cíclopes. Odiseo se presenta, y comienza a relatar su historia desde que salió de Troya. 

Primero destruyeron la ciudad de Ísmaro (donde estaban los cicones), y allí perdió a bastantes compañeros. 

Más tarde, llegaron a la tierra de los lotófagos. Allí, tres compañeros comieron el loto, y perdieron el deseo de regresar, por lo que hubo de llevárselos a la fuerza. 


Posteriormente, llegaron a la isla de los cíclopes. En una caverna se encontraron con Polifemo, hijo de Poseidón, que se comió a varios de los compañeros de Odiseo. 
 Estaban atrapados en la cueva, pues estaba cerrada con una enorme piedra que les impedía salir a ellos y al ganado de Polifemo. Odiseo, con su astucia, emborrachó con vino a Polifemo, mandó afilar un palo y cegaron con él al cíclope mientras este dormía. Ya ciego y para asegurarse de que no escapasen los prisioneros, el cíclope tanteaba el lomo de sus reses a medida que iban saliendo de la cueva para ir a pastar, pero cada uno de los marinos iba vientre con vientre con una res y agarrado al vellón de ella.

Luego de escapar, Odiseo le grita su nombre a Polifemo y este le pide a su padre, Poseidón, que castigue a Odiseo.

La isla de Eolo. El palacio de Circe la hechicera. Odiseo sigue narrando cómo viajaron hasta la isla de Eolo, que trató de ayudarles a viajar hasta Ítaca. Eolo entregó a Odiseo una bolsa de piel que contenía los vientos del oeste. Al acercarse a Ítaca, sus hombres decidieron ver lo que había en la bolsa, se escaparon así los vientos y se desencadenó una tormenta que hizo desaparecer la esperanza del regreso al hogar. Tras seis días de navegación, llegaron a la isla de los Lestrigones, gigantes antropófagos que devoraron a casi todos los compañeros de Odiseo. Huyendo de allí, llegaron a la isla de Circe. La hechicera se enamoró de Odiseo y logró retenerlo allí un año, pero nunca se vio correspondida y finalmente le dejó marchar, no sin antes decirle que antes de regresar a casa tendría que pasar por el Inframundo para pedir consejo al ya difunto adivino Tiresias.

Descenso al Hades. Tras llegar al país de los Cimerios y realizar el sacrificio de varias ovejas, Odiseo visitó la morada de Hades para consultar con el adivino Tiresias, quien le profetizó un difícil regreso a Ítaca. A su encuentro salieron todos los espectros, que quisieron beber la sangre de los animales sacrificados. Odiseo se la dio en primer lugar a Tiresias, luego a su madre, Anticlea, y también bebieron la sangre varias mujeres destacadas y algunos combatientes que habían muerto durante la guerra de Troya.

Las sirenas. Escila y Caribdis. La Isla de Helios. Ogigia. De nuevo en ruta, Odiseo y sus compañeros lograron escapar de las Sirenas, cuyo canto hacía enloquecer a quien las escuchara. Para ello, siguiendo los consejos de Circe, Odiseo ordenó a sus hombres taparse los oídos con cera exceptuándolo a él, que mandó ser atado al mástil. Escaparon también de las peligrosas Caribdis y Escila. Consiguieron llegar a Trinacria (nombre griego de Sicilia), la isla del Sol. Pese a las advertencias de no tocar el ganado de Helios, los compañeros sacrificaron varias reses, lo que provocó la cólera del dios. Al hacerse de nuevo a la mar, Zeus lanzó un rayo que destruyó y hundió la nave, y solo sobrevivió Odiseo, que arribó a la isla de Calipso (lugar donde se encuentra al principio de la historia).

Los feacios despiden a Odiseo. Llegada a Ítaca. Cuando el héroe termina de contar su viaje, su regreso al hogar es dispuesto por el rey. Acompañado por navegantes feacios, Odiseo llega a Ítaca. Atenea lo disfraza de vagabundo para que no sea reconocido. Por consejo de la diosa, Odiseo va a pedir ayuda a su porquerizo: Eumeo.

Odiseo en la majada de Eumeo. Odiseo no revela su verdadera identidad a Eumeo, quien lo recibe con comida y manta. Se encuentra con la diosa Atenea, y juntos preparan la venganza contra los pretendientes.

Telémaco regresa a Ítaca. Atenea aconseja al joven Telémaco salir de Esparta y regresar a su hogar. Le advierte que los pretendientes quieren ponerle una trampa para matarlo y le dice que viaje de noche.

Mientras tanto, Eumeo relata su vida y sus orígenes al mendigo, y de cómo llegó al servicio de Odiseo.

Telémaco reconoce a Odiseo. Gracias a la ayuda de la diosa, el joven consigue eludir la trampa que los pretendientes le habían preparado a la entrada de la isla. Una vez en tierra, se dirige por consejo de la diosa a la casa de Eumeo, donde conoce al supuesto mendigo. Cuando Eumeo marcha a casa de Penélope a darle la noticia del regreso de su hijo, Odiseo revela su identidad a Telémaco, asegurándole que en verdad es su padre, a quien no ve desde hace veinte años. Tras un fuerte abrazo, planean la venganza, con la ayuda de Zeus y Atenea.

Odiseo mendiga entre los pretendientes. Al día siguiente, Odiseo, de nuevo como mendigo, se dirige a su palacio. Solo es reconocido por su perro Argos, que, ya viejo, fallece frente a su amo. Al pedir comida a los pretendientes, Odiseo es humillado e incluso golpeado por ellos.

Los pretendientes vejan a Odiseo. Aparece un mendigo real, llamado Iro, quien solía pasarse por el palacio. Riéndose de Odiseo, lo reta a una pelea. Los pretendientes aceptan que el ganador se junte a comer con ellos. Le dan 2 trozos de pan a Odiseo, que, tras quitarse su manta y dejar ver sus músculos, gana fácilmente al mendigo. A pesar de la victoria, ha de seguir soportando las vejaciones de los orgullosos pretendientes.

La esclava Euriclea reconoce a Odiseo. Odiseo, ocultando su verdadera identidad, mantiene una larga conversación con Penélope, quien ordena a su criada Euriclea que lo bañe. Euriclea, que fue nodriza del héroe cuando era niño, reconoce una cicatriz que a Odiseo, en su juventud, le hizo un jabalí cuando se encontraba cazando en el monte Parnaso. La esclava, pues, reconoce a su amo, que le hace guardar silencio para no hacer fracasar los planes de venganza.

La última cena de los pretendientes. Al día siguiente, Odiseo pide una señal, y Zeus lanza un trueno en medio del cielo azul. Este gesto es entendido por uno de los sirvientes como una señal de victoria sobre los pretendientes. Odiseo aprovecha para ver quién es fiel al desaparecido rey y, por tanto, habrá de conservar la vida. Un profeta, amigo de Telémaco, avisa a los pretendientes de que pronto los muros se mancharán con la sangre de ellos. A pesar de que algunos de ellos dan crédito a la profecía y huyen, la gran mayoría de ellos se ríe de ella.

El certamen del arco. Aparece Penélope con un arco que Odiseo dejó en casa a su marcha a Troya. Promete a los pretendientes que se casará con aquel que consiga hacer pasar la flecha por los ojos de doce hachas alineadas. Uno tras otro, los pretendientes lo intentan, pero ni siquiera son capaces de tensar el arco. Odiseo pide participar en la prueba, pero los pretendientes se lo deniegan. Tras la insistencia de Telémaco, le es permitido intentarlo. Con suma facilidad, Odiseo tensa el arco y consigue hacer pasar la flecha por los ojos de las hachas, ante el asombro de los presentes. A la señal de su padre, Telémaco se arma, preparándose para la lucha final.

La venganza. Antínoo, jefe de los pretendientes, se encuentra bebiendo cuando Odiseo le atraviesa la garganta con una saeta y le da así muerte. Ante las quejas de los demás, Odiseo responde con amenazadoras palabras, y los pretendientes temen por sus vidas. Se inicia la feroz lucha, con los numerosos pretendientes por un lado y Odiseo, su hijo y sus dos fieles criados por otro. Melantio, infiel cabrero de Odiseo, consigue armas, pero gracias a la ayuda de Atenea, todos aquellos que traicionaron a Odiseo van muriendo uno por uno. Las esclavas son colgadas del cuello en el patio del palacio, mientras que Melantio es cortado en pedazos para que se lo coman los leones. Odiseo manda a Euriclea que haga fuego y limpie el patio con azufre. La esclava avisa a las mujeres que fueron fieles al héroe, que llegan y abrazan a su amo.

Penélope reconoce a Odiseo. Después de matar a los pretendientes que se hospedaban en su casa, Odiseo manda a los presentes que vistan sus mejores trajes y bailen, para que el pueblo no sospeche lo ocurrido. Con la ayuda de Euriclea, el héroe se presenta a Penélope. Como el aspecto de Odiseo es distinto al que conocía Penélope, que además está casi convencida de que él ha muerto, el héroe no es reconocido por su esposa. Entonces, Odiseo describe el lecho conyugal, y cómo lo hizo él mismo de un olivo. Penélope, convencida, abraza a su esposo, que le narra sus aventuras. Finalmente le cuenta que aún tendrá que hacer otro viaje antes de terminar su vida en una tranquila vejez.

El pacto. Las almas de los muertos viajan al Hades, donde cuentan lo ocurrido a Agamenón y Aquiles, compañeros del héroe en la expedición de los aqueos a Troya. Odiseo marcha a casa de su padre, Laertes, que se encuentra trabajando en la huerta. El hombre se encuentra envejecido y apenado por la larga ausencia de su hijo. Para ser reconocido, Odiseo le muestra la cicatriz y recuerda los árboles que en su infancia le regaló su padre. 

Mientras, los familiares de los pretendientes se juntan en asamblea, y piden venganza por la muerte de los suyos. Odiseo, su hijo y su padre, que se encuentran en la casa de este, aceptan el reto, y da comienzo la lucha. Laertes dispara una lanza que mata al padre de Antínoo. Pero en ese momento cesa la lucha. Interviene la diosa Atenea, que anima a los itacenses a llegar a un pacto, para que juntos vivan en paz durante los años venideros.

Entre las traducciones al español cabe citar a Gonzalo Pérez (1550), Antonio de Gironella en verso (1851), Luis Segalá y Estalella (1910), Ángel María Garibay K. (1931), José Manuel Pabón y Suárez de Urbina (1982), José Luis Calvo Martínez (1988) y Carlos García Gual (2004). La única versión española que se conoce realizada por una mujer es la de la cubana Laura Mestre Hevia. El filólogo mexicano Pedro Tapia Zúñiga realizó la versión más reciente (2013), en verso.

La repercusión de la Odisea en la cultura occidental se puede ver en las numerosas adaptaciones y versiones que su argumento ha tenido en prosa, verso, teatro, cine, televisión e historieta. Además, ha legado al idioma español los términos odisea y mentor (y términos similares en otros idiomas occidentales). 

La Organización de las Naciones Unidas (ONU), también conocida simplemente como Naciones Unidas (NN. UU.), es la mayor organización internacional existente. Se creó para mantener la paz y seguridad internacionales, fomentar relaciones de amistad entre las naciones, lograr la cooperación internacional para solucionar problemas globales y servir de centro que armonice las acciones de las naciones.[1]​ Su sede está en Nueva York (Estados Unidos) y está sujeta a un régimen de extraterritorialidad. También tiene oficinas en Ginebra (Suiza), Nairobi (Kenia) y Viena (Austria).

La ONU se rige por la Carta de las Naciones Unidas, que entró en vigor el 24 de octubre de 1945 y se firmó el 25 de junio del mismo año en la ciudad estadounidense de San Francisco, por 51 países, pocos meses antes del final de la Segunda Guerra Mundial.[2]​ En el preámbulo de la Carta se mencionan explícitamente las dos guerras mundiales.[3]​

La ONU se financia por las contribuciones voluntarias de los Estados miembros. Sus principales objetivos son garantizar el cumplimiento del derecho internacional, el mantenimiento de la paz internacional, la promoción y protección de los derechos humanos, lograr el desarrollo sostenible de las naciones y la cooperación internacional en asuntos económicos, sociales, culturales y humanitarios.[4]​

Los 193 Estados miembros de las Naciones Unidas[nota 1]​ y otros organismos vinculados deliberan y deciden acerca de temas significativos y administrativos en reuniones periódicas celebradas durante el año. Los principales órganos de la ONU son la Asamblea General, el Consejo de Seguridad, el Consejo Económico y Social, la Secretaría General, el Consejo de Administración Fiduciaria y la Corte Internacional de Justicia.

La figura pública principal de la ONU es el secretario general. El actual es António Guterres de Portugal, que asumió el puesto el 1 de enero de 2017, reemplazando a Ban Ki-moon.[5]​

Los idiomas oficiales de la ONU son seis: árabe, chino mandarín, español, francés, inglés y ruso.[6]​

La ONU reemplazó a la Sociedad de Naciones (SDN), fundada en 1919, ya que dicha organización había fallado en su propósito de evitar otro conflicto internacional.

El término «Naciones Unidas» se pronunció por primera vez en plena Segunda Guerra Mundial por el entonces presidente de los Estados Unidos Franklin Roosevelt, en la Declaración de las Naciones Unidas, el 1 de enero de 1942 como una alianza de 26 países en la que sus representantes se comprometieron a defender la Carta del Atlántico y para emplear sus recursos en la guerra contra el Eje Roma-Berlín-Tokio.

La idea de la ONU fue elaborada en la declaración emitida en la conferencia de Yalta celebrada por los aliados en febrero de 1945. Allí Roosevelt sugirió el nombre de Naciones Unidas.

Aunque inspirada en la Sociedad de Naciones, la ONU se diferencia de esta tanto en su composición como en su estructura y funcionalidad. Por un lado, va a aumentar su universalización, lo que va a permitir la ampliación de la organización por medio de las grandes potencias, de los nuevos estados surgidos tras la descolonización, o de los que surgirán tras el desmembramiento de la Unión Soviética, Yugoslavia y Checoslovaquia en Europa oriental. La Sociedad de Naciones no contaba con las grandes potencias como estados miembros dificultando así el respeto mismo a su autoridad. La ONU al contar con dichas naciones recalca su propia universalidad y autoridad obligando así a los estados miembros respetar las leyes establecidas por la misma organización, evitando repercusiones importantes.

De agosto a octubre de 1944, representantes de Francia, la República de China, el Reino Unido, los Estados Unidos y la Unión Soviética celebraron la conferencia de Dumbarton Oaks para esbozar los propósitos de la organización, sus miembros, los organismos, y las disposiciones para mantener la paz, seguridad y cooperación internacional. La actual organización refleja parcialmente esta conferencia, ya que los cinco miembros permanentes del Consejo de Seguridad (que tienen poder de veto en cualquier resolución de ese Consejo) son dichos estados, o sus sucesores (la República Popular China que reemplazó a la República de China en Taiwán y la Federación de Rusia que sucedió a la Unión Soviética).

El 25 de abril de 1945 se celebró la conferencia de San Francisco (la Conferencia de las Naciones Unidas sobre Organización Internacional). Además de los gobiernos, fueron invitadas organizaciones no gubernamentales. El 26 de junio las cincuenta naciones representadas en la conferencia firmaron la Carta de las Naciones Unidas. Polonia, que no había estado representada en la conferencia, añadió su nombre más tarde entre los signatarios fundadores, para un total de 51 Estados.

La ONU comenzó su existencia después de la ratificación de la Carta por la República de China, Francia, la Unión Soviética, el Reino Unido y los Estados Unidos y la gran mayoría de los otros 46 miembros. El primer período de sesiones de la Asamblea General se celebró el 10 de enero de 1946 en Central Hall Westminster (Londres). La Sociedad de Naciones se disolvió oficialmente el 18 de abril de 1946 y cedió su misión a las Naciones Unidas.

En 1948 se proclama de la Declaración Universal de los Derechos Humanos, uno de los logros más destacados de la ONU.

Los fundadores de la ONU manifestaron tener esperanzas en que esta nueva organización sirviera para prevenir nuevas guerras. Estos deseos no se han hecho realidad en muchos casos. Desde 1947 hasta 1991, la división del mundo en zonas hostiles durante la llamada Guerra Fría hizo muy difícil este objetivo, debido al sistema de veto en el Consejo de Seguridad. Desde 1991 las misiones de paz de la ONU se han hecho más complejas abarcando aspectos no militares que asegurasen un adecuado funcionamiento de las instituciones civiles, como en las elecciones.

En la actualidad, no permanecen las condiciones internacionales que impulsaron la gestación de la ONU; debido a que, el sistema internacional está en constante cambio, los problemas han tomado nuevas formas, han surgido nuevas amenazas, entre las más sobresalientes están: narcotráfico, terrorismo, armas biológicas y químicas, proliferación de armas nucleares, degradación de medio ambiente y las pandemias (Valdés, 2007: 09); así como, nuevas formas de cooperación internacional y temas de relevancia social tales como la brecha digital. Ajustar a la ONU a la nueva realidad internacional ha sido la principal razón de la comunidad internacional y de esa manera evitar que la ONU se convierta en un organismo internacional obsoleto.

Recientemente ha habido numerosas llamadas para la reforma de la ONU.[7]​ Algunos desean que esta juegue un papel mayor o más efectivo en los asuntos mundiales, otros desean que su papel se reduzca a la labor humanitaria. Ha habido también numerosos llamamientos a ampliar la composición del Consejo de Seguridad para reflejar la situación geopolítica actual (esto es, más miembros de África, América Latina y Asia) y para que se modifique el procedimiento de elección del secretario general.

Desde 2011 y después de la adhesión de Sudán del Sur, el número de estados miembros es de 193. Están incluidos todos los estados reconocidos internacionalmente, menos:

El último país en ser admitido fue la República de Sudán del Sur, el 14 de julio de 2011.

Los llamados «casos especiales», únicos territorios no miembros, sin calidad de miembro observador y con gobierno propio son:

Ambos territorios están actualmente en libre asociación con Nueva Zelanda. Sin embargo, cada uno podría declarar su independencia solicitando su ingreso a la ONU. Esto ya ha sucedido, por ejemplo, con los Estados Federados de Micronesia, las Islas Marshall y Palaos, todos Estados en libre asociación con Estados Unidos y miembros de las Naciones Unidas.

El artículo 4, del capítulo 2 de la Carta de las Naciones Unidas establece los requisitos para ser estado miembro:


China, representada por el gobierno de la República de China (ROC), fue uno de los cinco miembros fundadores de la ONU en 1945 y formó parte de la ONU como miembro original el 24 de octubre de 1945. Sin embargo, como resultado de la Guerra Civil China, el gobierno de la ROC controlado por el Kuomintang huyó a la isla de Taiwán en 1949, y el gobierno comunista de la República Popular China (RPC), declarada el 1 de octubre de 1949, tomó el control de la mayor parte del territorio de China. Representantes del gobierno de la ROC continuaron representando a China en la ONU, a pesar del pequeño tamaño de la jurisdicción en Taiwán de la ROC (y otras islas no consideradas parte de la provincia de Taiwán) comparado con la jurisdicción en China continental de la RPC, hasta que el 25 de octubre de 1971, cuando la Asamblea General aprobó la resolución 2758, reconociendo al Gobierno de la RPC como el único representante legítimo de China en la ONU, expulsando al representante de Chiang Kai-shek como representante legítimo de China y reconociendo en cambio a la RPC. Esto, en efecto, transfirió el escaño de China en la ONU (incluyendo su asiento permanente en el Consejo de Seguridad) de la ROC a la RPC.

Desde 1991, la ROC ha solicitado repetidamente volver a participar en la ONU, únicamente como representante del pueblo de Taiwán, y no como representante de toda China, utilizando la designación de «República de China en Taiwán», «República de China (Taiwán)» o simplemente «Taiwán». Sin embargo, en 2007 un comité clave de la ONU rechazó por decimoquinta vez consecutiva la solicitud de la ROC. Al consultarle al secretario general Ban Ki-moon sobre los motivos del rechazo dijo que era legalmente imposible, debido a la resolución de la asamblea que expulsó a los nacionalistas chinos en 1971.[8]​

En la actualidad, 14 estados miembros de la ONU, además de la Santa Sede, mantienen relaciones diplomáticas con la ROC. La República Popular China, que considera Taiwán como una provincia rebelde,[9]​ se opone a que la isla-estado sea miembro de la ONU.

Todos los estados miembros de la Unión Europea (UE) forman parte a su vez de la ONU. La UE, a pesar de ser miembro de otras organizaciones internacionales, como la OMC, no forma parte de la ONU. Sin embargo, ha desarrollado misiones por encargo de la ONU en diferentes partes del mundo. Tal es el caso de la EUFOR.

Al ratificarse el Tratado de Lisboa, la UE tiene personalidad jurídica única en la sociedad internacional, desde diciembre de 2009.[10]​ El tratado especifica en lo referente a su acción en la escena internacional y las relaciones con la ONU:[11]​

El régimen jurídico de la sede de la ONU está regido por un tratado entre esta y los Estados Unidos de América (Acuerdo relativo a la sede de las Naciones Unidas, del 31 de octubre de 1947), y la Convención sobre Prerrogativas e Inmunidades de las Naciones Unidas, de 1946.

Por razones de seguridad, todo correo recibido es esterilizado. La Administración Postal de las Naciones Unidas emite sellos, con los que deben ser franqueados todos los artículos enviados desde el edificio. Los periodistas acreditados, cuando informan desde el complejo, no deben utilizar «Nueva York» como identificación de su localización en reconocimiento de su estatus de extraterritorialidad.

El complejo diseñado por un equipo internacional de arquitectos incluye los siguientes edificios: la Secretaría (una torre de 39 pisos), la Asamblea General, la Biblioteca Dag Hammarskjöld  y el área de conferencias. También hay jardines y esculturas exteriores.

Aunque la sede principal está en el complejo sobre suelo neoyorquino, la ONU y sus organismos especializados y regionales tienen otras sedes, como en: Ginebra, Suiza; La Haya, Países Bajos; Viena, Austria; Montreal, Canadá; Copenhague, Dinamarca; Bonn, Alemania; Nairobi, Kenia; París, Francia; Santiago, Chile; Adís Abeba, Etiopía; Valencia, España; San José, Costa Rica y Buenos Aires, Argentina.

La ONU tiene como idiomas de trabajo al árabe, chino mandarín, español, francés, inglés y ruso.

El Servicio de Radio de Naciones Unidas emite, además de los seis idiomas oficiales, en bengalí, portugués y suajili.

Español

Inglés

Francés

Ruso

Mandarín

Árabe

El personal y personal asociado de la ONU se encuentra protegido por la Convención sobre la Seguridad del Personal de las Naciones Unidas y del Personal Asociado, aprobada el día 9 de diciembre de 1994 por la Asamblea General de la ONU.[12]​[13]​

La financiación de las Naciones Unidas y de algunas de sus agencias especializadas está asegurada por las contribuciones obligatorias de los estados miembros. En el caso de algunas agencias especializadas, su financiación proviene de contribuciones voluntarias de estados miembros, organizaciones, empresas o particulares.

La Asamblea General establece en el presupuesto ordinario las contribuciones obligatorias durante dos años (1 924 840 250 USD en 2006)[15]​ y determina la aportación de cada miembro basándose en la capacidad de pago de los países, calculado del ingreso nacional por habitante; no obstante, para mantener un nivel de independencia, el nivel máximo de contribución está fijado en el 22 % (el nivel mínimo es un 0,01 % del total). Es importante señalar que las contribuciones obligatorias no siempre son satisfechas por los países y conforme al artículo 19 de la Carta de las Naciones Unidas se le puede quitar el derecho al voto en la Asamblea General al estado miembro cuyos atrasos de pago igualen o superen la cantidad que debiera haber contribuido en los dos años anteriores.[16]​

Las celebraciones de la ONU tienen como objetivo contribuir, en todo el mundo, al cumplimiento de los objetivos de la Carta de las Naciones Unidas y sensibilizar al público acerca de temas políticos, sociales, culturales, humanitarios, o relacionados con los derechos del hombre. Son ocasiones para promover acciones nacionales e internacionales y despertar el interés sobre los programas y actividades de las Naciones Unidas.

Se realiza una reunión cada año y cuando un tema es considerado particularmente importante de tratar en ese momento, la Asamblea General puede recomendar al Consejo de Seguridad una conferencia internacional y el Consejo de Seguridad decide si se debe hacer o no para centrar atención global y construir un consenso para una acción unificada se realiza una reunión cada año. Un ejemplo sería la Conferencia de Naciones Unidas sobre el Medio Ambiente y el Desarrollo (Cumbre de la Tierra), del 3 al 14 de junio de 1992, cuyos acuerdos dieron lugar a la adopción del programa Agenda 21 por 179 países.

En este mismo sentido de centrar la atención en temas importantes de interés internacional, la ONU declara celebraciones internacionales, como días, meses, años, etc., para promover, movilizar y coordinar eventos en todo el mundo.

La Carta de las Naciones Unidas en su artículo 26, concibió la posibilidad de un sistema de regulación de los armamentos que aseguraría «la menor desviación posible de los recursos humanos y económicos del mundo hacia los armamentos». La aparición de las armas nucleares ocurrió semanas después de la firma de la Carta y esto supuso un impulso inmediato en el desarrollo de la noción de control de armamento y de desarme. De hecho, la Asamblea General de la ONU adoptó en su primera resolución (febrero de 1946), se refería a los usos pacíficos de la energía atómica y a la eliminación de armas atómicas de destrucción masiva.

La ONU ha establecido varios foros para dirigir los temas del desarme. El principal es el Primer Comité de la Asamblea General de Naciones Unidas sobre Desarme y Seguridad internacional,[17]​[18]​ en cuya agenda se ha tomado en cuenta la prohibición completa de los ensayos nucleares, la prohibición de armas químicas, la no proliferación de las armas nucleares, el establecimiento de zonas libres de armas nucleares, el prevenir, combatir y erradicar el tráfico ilícito de armas pequeñas y ligeras en todos sus aspectos, la exploración y utilización del espacio ultraterrestre con fines pacíficos, el mantenimiento de la seguridad internacional…

En junio de 1978, el primer periodo extraordinario de sesiones de la Asamblea General dedicado al desarme estableció una Comisión de Desarme como un órgano subsidiario de la Asamblea, compuesto por todos los Estados Miembros de las Naciones Unidas. Fue creado como un órgano de deliberación, con la función de considerar diferentes problemas en la esfera del desarme y hacer recomendaciones al respecto y con la de dar seguimiento a las decisiones y recomendaciones pertinentes del periodo extraordinario de sesiones. Desde el año 2000 su agenda se ocupa sólo de dos temas sustantivos. Esta Comisión presenta un informe anual a la Asamblea General.

Las Fuerzas de paz de las Naciones Unidas (los «cascos azules») son enviadas a varias regiones donde han cesado recientemente conflictos armados, para de este modo, hacer cumplir los acuerdos de paz y disuadir a los combatientes de reanudar las hostilidades. Debido a que la ONU no mantiene un ejército independiente, los efectivos son suministrados por los Estados miembros, y su participación es opcional. La autoridad para enviar o retirar a los contingentes de mantenimiento de la paz está en manos del gobierno que los aporta, al igual que la responsabilidad en relación con la paga y cuestiones disciplinarias y de personal.

Desde su creación en 1948. Se ha reconocido que el primer propósito de las Misiones de Paz de las Naciones Unidas «los cascos azules», es mantener la paz y la seguridad internacional, y con este fin se faculta para tomar medidas colectivas eficaces para prevenir y eliminar amenazas a la paz y lograr que las controversias surgidas entre diferentes Estados se logren solucionar por medio de los canales diplomáticos y de justicia internacional que para tal fin se establezcan. Sin embargo, para tener una mejor comprensión de la finalidad, funcionamiento y alcance de las Misiones de Paz, es necesario entender su origen, sus principios y el contexto en que se desarrollan.[19]​

Las operaciones del mantenimiento de paz son establecidas por el consejo de seguridad de la ONU y contribuyen apoyar la vigilancia y resolver conflictos entre países o con comunidades hostiles dentro de un mismo país, los artífices está operaciones de mantenimiento de la paz son popular mente conocidos como cascos azules, las operaciones de mantenimiento de paz,[20]​ son especialmente útiles para recordar a las partes en conflicto que la comunidad internacional está pendiente de sus actos y de la legalidad de los mismos, sus principios no se basan con el envió de fuerzas que luchen para la culminación de un conflicto si no en hacer que las partes involucradas procuren arreglar de una manera pacífica y negociada sus controversias, las tropas de las naciones unidas portan armamento ligero y pueden hacer uso de la fuerza sólo en casos de defensa propia o en caso en que elementos armados traten de impedirles que acaten las órdenes de sus comandantes, las operaciones de mantenimiento de la paz deben contar con el consentimiento del gobierno del país en el que se despliegan y de ser posible con la autorización de los países involucrados.

La primera operación de mantenimiento de la paz, fue la UNSCOB (United Nations Commission for the Balkans), dispuesta por la Asamblea General de las Naciones Unidas, por Resolución Nro.109(II)del 21 de octubre de 1947. Se llevó a cabo en Grecia entre octubre de 1947 y febrero de 1952[21]​

Todos los estados miembros tienen la obligación legal de pagar la parte que les corresponde del costo de las actividades de mantenimiento de la paz en el marco de una fórmula compleja que ellos mismos establecieron, que incluye una sobrecarga para los cinco miembros permanentes de Consejo de Seguridad. Los países que aportan voluntariamente personal uniformado a las operaciones de mantenimiento de la paz son reembolsados por las Naciones Unidas a una tasa fija de un poco más de 1000 USD por soldado por mes. Las Naciones Unidas también reembolsan a los países por el equipo que aportan.

La ONU concede las Medallas de las Naciones Unidas a los miembros del servicio militar que hacen cumplir los acuerdos de la Organización.[22]​

Su primera misión consistió en supervisar el cese de la guerra entre iraníes y árabes en 1948 desde entonces se han puesto en marcha más de 71 operaciones de paz en las que han intervenido más de 110.000[23]​ mil personas de 120 países entre, observadores, soldados expertos, personal civil y policías de las cuales más de 3.326 lamentablemente han perdido la vida al servicio de la ONU[24]​ entre las operaciones de mantenimiento de la paz en la que han participado se encuentran la crisis de canal de Suez en 1956 en el Congo 1961, Ruanda 1994 los de Somalia Bosnia y Herzegovina entre 1992 y 1995 así como Timor Oriental 2000 al 2001[25]​ entre otros, el reconociendo a su labor pacífica los cascos azules se hicieron acreedores, durante el segundo mandato de Javier Pérez de Cuéllar como secretario general, las Fuerzas de Paz de la ONU recibieron en 1988 el premio Nobel de la Paz.[26]​ En 2001, la ONU y su secretario general Kofi Annan ganaron el premio Nobel de la Paz «por su trabajo por un mejor mundo organizado y más pacífico», en la actualidad los integrantes de las operaciones del mantenimiento de la paz realizan una gran variedad de tareas complejas como ayudar a establecer instituciones de gobernanza sostenibles así como vigilar la situación de los derechos humanos la seguridad el desarme, la desmovilización y la reintegración de los excombatientes.

El origen e inicio de la implementación de los principios se dio en la FENU I que fue la 1ra Fuerza de Emergencia de las Naciones Unidas operada por la Asamblea General durante la Crisis de Suez (1956) la cual fue la primera en disponer de fuerzas armadas, ya que las primeras y anteriores Operaciones de Mantenimiento a la Paz (OMP) fueron misiones tan solo de obtención de información y observación. Estos principios se modificarían luego en respuesta de la Crisis del Congo, rigiendo con una mínima variación hasta los años ochenta. La doctrina que es de donde se desprenden los principios junto con la búsqueda de unanimidad en el modo de accionar en caso de una situación difícil, encuentra que se adecuan al tener una afinidad en común en todas las operaciones[27]​

De acuerdo con la Carta de las Naciones Unidas nos establece que el Consejo de Seguridad es el encargado de mantener la paz y seguridad en el mundo con relación a cualquier tipo de conflicto. Este mismo Consejo de Seguridad es conformado por 15 miembros de los cuales 5 son permanentes con derecho de veto que son Reino Unido, Estados Unidos, República Francesa, Federación Rusa y la República Popular de China, mientras que los otros 10 no son permanentes que estos no permanentes son elegidos de 5 en 5 cada año por la Asamblea General de la ONU y por un periodo de duración de 2 años, y la presidencia del consejo se rota mensualmente en orden alfabético.[33]​ El Consejo de Seguridad puede tomar decisiones más bien conocidas como resoluciones y obligar a los miembros a cumplirlas ya que son los que se encargan del mantenimiento de la paz y seguridad y como antes mencionado está establecido por la Carta de las Naciones Unidas.[34]​

Cuando en caso de que llegue haber una controversia la primera acción que toma este Consejo de Seguridad es que traten de llegar a un acuerdo de las formas más pacíficas que puedan haber, así no haciendo un problema grande y tener que llegar a tomar medidas más dramáticas y/o de mayor medida. Algunas de estas otras medidas más severas que puede llegar a imponer este Consejo son; embargos, sanciones económicas, autorizar el uso de la fuerza para hacer cumplir los mandatos, ya esto en caso extremo o que no haya remedio. La Organización de las Naciones Unidas es la organización más grande para mantener la paz y seguridad internacional que se lleva a cargo por el Consejo de Seguridad. Tiene su sede en Nueva York, Estado Unidos y esta sede es un área de extraterritorialidad, siendo este un lugar sin jurisdicción alguna o se atenga a los reglamentos de este mismo país donde se encuentra, como los consulados, buques o bases militares.[36]​

Dentro de este Consejo de Seguridad existen diferentes lugares u organizaciones alrededor del mundo donde estos conflictos llegan a ser resueltos, por ejemplo; existe el Tribunal Penal Internacional Para la ex Yugoslavia y el Tribunal Penal Internacional para la Runda, donde estos dos lugares se llevan a cabo juicios de genocidio. Al igual encontramos la Comisión de Consolidación de Paz, el Comité Contra el Terrorismo, Comisión de las Naciones Unidas de Vigilancia, Verificación e Inspección, entre otros.[37]​

Al igual es un artículo muy importante que cabe resaltar es el artículo 33 de la Carta de las Naciones Unidas ya que nos establece “la prevención de los conflictos y el arreglo pacifico de controversias. Las partes en una controversia internacional tienen acceso a diversas medidas y mecanismos para la solución de las controversias, entre ellas la negociación, la investigación, la mediación, la conciliación, el arbitraje, el arreglo judicial y el recurso a organismos o acuerdos regionales.” Esto es para así poder tener una solución más rápida, más eficiente y manteniendo la paz entre las partes involucradas sin tener que llegar a medidas más estrictas y severas.[39]​

La preocupación por los derechos humanos fue una de las razones principales para la creación de las Naciones Unidas. Las atrocidades y el genocidio de la Segunda Guerra Mundial contribuyeron a un consenso para que la nueva organización debiera trabajar para prevenir tragedias similares en el futuro. En este sentido se creó un marco jurídico para considerar y actuaba sobre quejas referidas a violaciones de los derechos humanos.

La Carta de la ONU (arts. 55 y 56) obliga a todos sus miembros a promover "el respeto universal a los derechos humanos y a las libertades fundamentales de todos" y para tomar "medidas conjunta o separadamente, en cooperación con la Organización" para tal fin. La Declaración Universal de los Derechos Humanos, aunque no legalmente vinculante, fue adoptada por la Asamblea General en 1948 como un patrón de realización para todos; y consecuentemente, la Asamblea se ocupa regularmente de las cuestiones referidas a los derechos humanos. Así el 15 de marzo de 2006 la Asamblea General de la ONU votó de forma abrumadora para sustituir la Comisión de Derechos Humanos de las Naciones Unidas (UNCHR) por el Consejo de Derechos Humanos de la ONU.[40]​ Su propósito es tratar violaciones de los derechos humanos. El UNCHR había sido criticado en varias ocasiones por los miembros que la componían, concretamente, varios de sus miembros, como Sudán o Libia, poseían un dudoso historial de respeto de los derechos humanos, incluyendo a los representantes elegidos para presidir la comisión.

La Carta Internacional de Derechos Humanos, dispuso la creación de siete organismos entre los que se destacan el Comité de Derechos Humanos (HRC) y al Comité para la Eliminación de la Discriminación contra la Mujer (CEDAW). El soporte de la Secretaría General se proporciona a través de la Oficina del Alto Comisionado de las Naciones Unidas para los Derechos Humanos (OHCHR), excepto del CEDAW, que lo recibe de la División para el Adelanto de la Mujer (DAW).

Las Naciones Unidas y sus agencias son fundamentales en mantener y aplicar los principios en emanados de la Declaración universal de los Derechos Humanos; por ejemplo, el apoyo de la ONU para los países en transición a la democracia ha contribuido significativamente a la democratización por todo el mundo, y se ha manifestado en la asistencia técnica para posibilitar elecciones libres y justas, en mejorar las estructuras judiciales, en redactar constituciones, en formar funcionarios, o en transformar los movimientos armados en partidos políticos. Esto se ha visto recientemente en Afganistán y Timor Oriental.

Naciones Unidas es también un foro para apoyar los derechos de la mujer para participar plenamente en la vida política, económica y social de sus países. La ONU contribuye a elevar el significado del concepto de derechos humanos a través de sus tratados y su atención a los abusos específicos con sus resoluciones de la Asamblea General o del Consejo de Seguridad o los fallos de la Corte Internacional de Justicia (ICJ).

La ONU conjuntamente con otras organizaciones como la Cruz Roja, proporciona comida, agua potable, refugio y otros servicios humanitarios a las poblaciones que los necesitan, sean desplazados por guerra, o afectados por otros desastres. Las agencias humanitarias más importantes de la ONU son la Oficina de las Naciones Unidas para la Coordinación de Asuntos Humanitarios (OCAH): Organismo perteneciente al Secretariado General de ONU, encargado de realizar acciones de coordinación humanitaria. Apoya organismos como el Comité Permanente Interagencial (IASC por sus siglas en inglés), los Equipos Humanitarios Nacionales o locales; hace la secretaría técnica a INSARAG, grupo especializado en asesorar grupos de búsqueda y rescate; administra los fondos CERF y ERF; realiza acciones de incidencia por los afectados y afectadas, y propone políticas de atención a estos afectados, así como de prevención. Adicionalmente suministra servicios y recursos de información para fortalecer la toma de decisiones.
El Programa Mundial de Alimentos (PMA), que en 2004 repartió comida a unos 100 millones de personas,[42]​ el Alto Comisionado de las Naciones Unidas para los Refugiados (ACNUR), que hasta 2001 había contribuido a reasentar a por lo menos 25 millones de personas en diferentes países.[43]​ También se destacan el Programa de las Naciones Unidas para el Desarrollo (UNDP) que es la mayor organización internacional para garantizar asistencia técnica en el mundo, las organizaciones como ONUSIDA, OMS y el Fondo Mundial de Lucha contra el sida, la Tuberculosis y la Malaria[44]​ (también llamado Fondo Mundial), que combaten las enfermedades en el mundo, especialmente en países pobres, y que han ayudado a reducir la mortalidad infantil y maternal. Siguiendo estas iniciativas, en diciembre de 2005, la Asamblea General creó el Fondo de respuesta a emergencias (CERF), administrado por OCAH , como un sistema que mejorara la coordinación de la ayuda humanitaria, haciéndola más oportuna y responsable de las víctimas de desastres naturales o hechos por el hombre.

Naciones Unidas publica anualmente el índice de desarrollo humano (IDH), como una forma de ordenar comparativamente los países por su pobreza, la instrucción, la educación, la esperanza de vida, y otros factores como el gasto militar.

Los Objetivos de Desarrollo del Milenio ya aparecen en la Declaración del Milenio, adoptada por la Asamblea General y firmada por 192 países miembros de la ONU el 8 de septiembre de 2000, tras la Cumbre del Milenio; y en este sentido, en la Cumbre mundial de 2005 (14-16 de septiembre de 2005), los representantes de los entonces 191 miembros de la ONU, los reafirmaron como ocho objetivos a alcanzar para el año 2015.[45]​

Objetivo 1: Erradicar la pobreza extrema y el hambre:

Objetivo 2: Lograr la enseñanza primaria universal.

Objetivo 3: Promover la igualdad entre los géneros y la autonomía de la mujer.

Objetivo 4: Reducir la mortalidad infantil.

Objetivo 5: Mejorar la salud materna.

Objetivo 6: Combatir el VIH/SIDA, el paludismo y otras enfermedades.

Objetivo 7: Garantizar la sostenibilidad del medio ambiente.

Objetivo 8: Fomentar una asociación mundial para el desarrollo.

Según una investigación hecha por el Centro de Estudios latinoamericanos, publicada en la Revista Electrónica Iberoamericana (Vol. 1 n. 1) expone que América Latina, en la primera conclusión del examen, no es positiva, porque si bien se han logrado avances significativos en los puntos 4, 5 y 6 aún falta mucho camino por recorrer para llegar al fin deseado.

La realidad se ve acentuada por una América Latina llena de un sin número de contrastes, en donde se tienen a los hombres más ricos del mundo, por un lado, pero también se encuentran zonas en donde la gente no recibe los servicios más básicos, dichos ámbitos, en los que se hizo un mayor énfasis, fueron: Pobreza extrema, Mortalidad materna, educación primaria universal y cobertura de saneamiento. Sin embargo no todo es desilusión, ya que ha habido grandes avances, esto debido a una reducción de la mortalidad infantil.

Dicha investigación concluye haciendo una advertencia para que los gobiernos en América Latina presten mayor para que de una manera colaborativa, se logren las estrategias necesarias para la reducción de las cifras negativas, así mismo se pide no dejar de lado a los derechos humanos, estos, por ser unos de los mayores logros alcanzados por el hombre, es su lucha por alcanzar la felicidad de todos los ciudadanos.

El artículo 7 de la Carta de las Naciones Unidas indicaba que los órganos principales de la organización eran:

Además la Carta posibilitaba que cada órgano del poder pudiera establecer los organismos subsidiarios que estimara necesarios para el desempeño de sus funciones.

Una de las características singulares del sistema de la ONU es la duplicación de la responsabilidad. Por ejemplo, la UNODOC (Oficina de las Naciones Unidas contra la Droga y el Delito) informa a la Secretaría General, la Asamblea General supervisa el UNICRI (Instituto Interregional de las Naciones Unidas para la Investigación de la Delincuencia y la Justicia), pero el Comité Económico y Social tiene dos comisiones orgánicas distintas, la de estupefacientes por un lado, y la de prevención del delito y justicia penal por el otro.

El Sistema de las Naciones Unidas está organizado de la siguiente manera (aunque las siglas varían según los idiomas oficiales de este organismo internacional):

Desde su fundación, ha habido muchos llamados para la reforma de las Naciones Unidas pero poco consenso sobre cómo hacerlo. Algunos quieren que la ONU desempeñe un mayor papel o más eficaz en los asuntos mundiales, mientras que otros quieren que su papel se reduzca al trabajo humanitario. También se han hecho numerosos llamados para que se incremente la composición del Consejo de Seguridad de la ONU, las diferentes formas de elegir al secretario general de la ONU y una Asamblea Parlamentaria de las Naciones Unidas. Jacques Fomerand afirma que la división más duradera en las opiniones de la ONU es la «división Norte-Sur» entre las naciones ricas del Norte y las naciones pobres del Sur. Las naciones del sur tienden a favorecer una ONU más potenciada con una Asamblea General más fuerte, permitiéndoles una mayor voz en los asuntos mundiales, mientras que las naciones del Norte prefieren una ONU económicamente laissez-faire que se centre en amenazas transnacionales como el terrorismo.[60]​ Algunos críticos perciben que las Naciones Unidas solo están al servicio de los gobiernos de sus países miembros (especialmente los más poderosos) y no de los ciudadanos del común.[61]​

Después de la Segunda Guerra Mundial, el Comité Francés de Liberación Nacional fue tardíamente reconocido por Estados Unidos como el gobierno de Francia, por lo que el país fue excluido inicialmente de las conferencias que crearon la nueva organización. El futuro presidente francés, Charles de Gaulle, criticó a la ONU, considerándola una machin (artilugio), y no estaba convencido de que una alianza de seguridad global ayudaría a mantener la paz mundial, prefiriendo tratados de defensa directa entre países.[62]​ Durante la Guerra Fría, tanto Estados Unidos como la Unión Soviética acusaron repetidamente a la ONU de favorecer a la otra superpotencia. En 1953, la Unión Soviética obligó a renunciar al secretario general Trygve Lie, debido a su negativa a tratar con él, mientras que en los años cincuenta y sesenta una popular pegatina de parachoques estadounidense decía: "No se puede escribir comunismo sin la ONU"[63]​ En una declaración a veces mal citada, el presidente estadounidense George W. Bush declaró en febrero de 2003 (refiriéndose a la incertidumbre de la ONU ante las provocaciones iraquíes bajo el régimen de Saddam Hussein) que "las naciones libres no permitirán que las Naciones Unidas se desvanezcan en la historia como una sociedad de debate inefectiva e irrelevante".[64]​[65]​[66]​ En cambio, el presidente francés, François Hollande, declaró en 2012 que "Francia confía en las Naciones Unidas y sabe que ningún Estado, por poderoso que sea, puede resolver problemas urgentes, la lucha por el desarrollo y poner fin a todas las crisis... Francia quiere que las Naciones Unidas sean el centro de la gobernanza mundial".[67]​ También han surgido críticas hacia la atención de la ONU al tratamiento de los palestinos por parte de Israel, el cual es considerado sesgado en contra de Israel.[68]​En septiembre de 2015, Faisal bin Hassan Trad, de Arabia Saudita, ha sido elegido Presidente del panel del Consejo de Derechos Humanos de las Naciones Unidas que nombra a expertos independientes,[69]​ una medida criticada por grupos de derechos humanos.[70]​[71]​

Los críticos también han acusado a la ONU de ineficiencia burocrática, desperdicio y corrupción. También se ha acusado a la ONU de problemas con la planificación de sus operaciones y la contratación y manejo del personal.[72]​ En 1976, la Asamblea General creó la Unidad Conjunta de Inspección para buscar ineficiencias dentro del sistema de las Naciones Unidas. Durante los años noventa, Estados Unidos retuvo las cuotas citando la ineficiencia y sólo comenzó a pagar con la condición de que se introdujera una importante iniciativa de reformas. En 1994, la Oficina de Servicios de Supervisión Interna (OSSI) fue establecida por la Asamblea General para actuar como organismo de control de la eficiencia.[73]​ En 1994, el ex representante especial del secretario general de las Naciones Unidas en Somalia Mohamed Sahnoun publicó "Somalia: The Missed Opportunities", un libro en el que analiza las razones del fracaso de la intervención de las Naciones Unidas en Somalia en 1992, mostrando que entre el comienzo de la guerra civil somalí en 1988 y la caída del régimen de Siad Barre en enero de 1991, la ONU perdió por lo menos tres oportunidades para evitar grandes tragedias humanitarias; Cuando las Naciones Unidas trataron de prestar asistencia humanitaria, fueron totalmente superadas por las ONG, cuya competencia y dedicación contrastaban marcadamente con la excesiva cautela de la ONU y las ineficiencias burocráticas. Si no se emprendiera una reforma radical, advirtió Mohamed Sahnoun, entonces la ONU seguiría respondiendo a esa crisis con improvisación inepta.[74]​ En 2004, la ONU se enfrentó a acusaciones de que su recién terminado Programa Petróleo por Alimentos -en el cual a Irak se le permitió comercializar petróleo por necesidades básicas para aliviar la presión de las sanciones- había sufrido corrupción generalizada, incluyendo miles de millones de dólares en sobornos. Una investigación independiente creada por la ONU encontró que muchos de sus funcionarios habían estado involucrados, además de plantear preguntas "importantes" sobre el papel de Kojo Annan, el hijo de Kofi Annan.[75]​

También se han hecho críticas a varios organismos de la ONU. Una fuente de críticas radica en el poder de veto de los 5 miembros permanentes del Consejo de Seguridad, el cual ha sido utilizado para proteger los intereses geopolíticos de dichos países, impidiendo la acción de la ONU para salvaguardar la paz y seguridad internacional. Así mismo, se han hecho reproches al hecho de que la Asamblea General tenga un poder limitado y al hecho que las labores del secretario general no estén claramente definidas.[61]​

Varios organismos e individuos asociados con la ONU han ganado el Premio Nobel de la Paz en reconocimiento a su trabajo. Dos secretarios generales, Dag Hammarskjöld y Kofi Annan, recibieron cada uno el premio (en 1961 y 2001, respectivamente), al igual que Ralph Bunche (1950), un negociador de la ONU; René Cassin (1968), contribuyente a la Declaración Universal de Derechos Humanos; y el Secretario de Estado estadounidense Cordell Hull (1945) por su papel en la fundación de la organización. Lester B. Pearson, ministro de Asuntos Exteriores de Canadá, recibió el premio en 1957 por su papel en la organización de la primera fuerza de paz de la ONU para resolver la crisis de Suez. UNICEF ganó el premio en 1965, la Organización Internacional del Trabajo en 1969, las Fuerzas de Mantenimiento de la Paz de las Naciones Unidas en 1988, el Organismo Internacional de Energía Atómica (que depende de la ONU) en 2005 y la Organización para la Prohibición de Armas Químicas en 2013. El Alto Comisionado de las Naciones Unidas para los Refugiados fue galardonado en 1954 y 1981, convirtiéndose en uno de los dos únicos recipientes que ganó el premio dos veces. La ONU en su conjunto recibió el premio en 2001, compartiéndolo con Annan.[76]​

Tercer Ejército de Estados UnidosDecimoquinto Ejército de Estados Unidos

Expedición Punitiva

Primera Guerra Mundial

Segunda Guerra Mundial

George Smith Patton, Jr. (San Gabriel, California; 11 de noviembre de 1885-Heidelberg, Alemania; 21 de diciembre de 1945)[1]​ fue un general del Ejército de los Estados Unidos durante la Segunda Guerra Mundial. En sus 36 años de carrera, fue de los primeros en abogar por los carros blindados, mandando importantes unidades de ellos en el norte de África, en la invasión de Sicilia y en el escenario europeo.

Pese a que muchos han visto a Patton como un guerrero puro y feroz, lo que le ganó el sobrenombre de General Sangre y Agallas («nuestra sangre y sus agallas», decían algunos soldados), la historia lo ha dejado con la imagen de un brillante, pero solitario líder militar salpicado por insubordinaciones, transgresiones y periodos de cierta inestabilidad emocional.

Los Patton gozaban de un gran patrimonio familiar, lo que convertía a George Smith Patton, Jr. en uno de los militares más ricos de los Estados Unidos. Descendía de una larga tradición de militares que lucharon y a menudo murieron en muchos conflictos, incluyendo la Guerra de la Independencia y, en particular, el bando confederado de la Guerra Civil estadounidense (1861–1865). Sus abuelos paternos fueron el general de brigada George S. Patton (Fredericksburg, Virginia, 26 de junio de 1833-batalla de Opequon —la tercera batalla de Winchester—, 19 de septiembre de 1864) y Susan Thornton Glassell. El general de brigada sirvió en el 22.° Rgto. de Infantería de Virginia de los Estados Confederados de América durante la guerra civil.

Patton fue hijo de George Smith Patton (Charleston, Virginia —actual Virginia Occidental—, 30 de septiembre de 1856-Los Ángeles, California, junio de 1927) y Ruth Wilson. Su padre era un niño durante la guerra civil estadounidense. Se graduó en el Instituto Militar de Virginia en 1897 y más tarde inició una carrera como fiscal. Sirvió notablemente como fiscal del distrito de la ciudad de Pasadena, California, y fue el primer alcalde de San Marino, California. Se opuso firmemente al sufragio femenino, y fue amigo de John S. Mosby, héroe de la caballería de los Estados Confederados de América, que sirvió primero bajo las órdenes de J.E.B. Stuart y posteriormente como miembro de una guerrilla. Al parecer, fue su influencia lo que hizo que el joven Patton quisiera hacerse militar.

Patton estudió durante un año en el Instituto Militar de Virginia, para trasladarse posteriormente a West Point, donde se graduó en 1909.[2]​

Patton era un muchacho inteligente, que estudiaba con gran intensidad literatura clásica e historia militar, pero al parecer sufría de un caso no diagnosticado de dislexia, cuyas consecuencias le persiguieron durante toda la escolarización. Tardó mucho en aprender a leer y nunca llegó a deletrear correctamente. A causa de estas dificultades, tardó cinco años en licenciarse en West Point, aunque consiguió convertirse en adjunto del Cuerpo de Cadetes. Durante su estancia en West Point, Patton renovó su trato con su amiga de la infancia Beatrice Ayer, hija de un próspero industrial textil. Se casaron poco después de la graduación de Patton.

Tras graduarse en West Point, Patton participó en los Juegos Olímpicos de Estocolmo 1912, representando a los Estados Unidos en el primer pentatlón moderno. Patton finalizó el evento en quinto puesto. Iba en cabeza hasta la competición de tiro, en la que pareció fallar su segundo disparo. Patton afirmó que la segunda bala había pasado por el agujero hecho por la primera.

Durante la campaña en la frontera de México de 1916, mientras estaba al servicio del 13.° Regimiento de Caballería en Texas, acompañó al entonces general de brigada John J. Pershing como ayudante durante la expedición punitiva en territorio mexicano en persecución de Pancho Villa. Durante esta misión, Patton, acompañado de diez soldados del 6.° Regimiento de infantería, acabó con la vida del capitán Julio Cárdenas, comandante de la guardia personal de Villa. El éxito de Patton le brindó cierta notoriedad en los Estados Unidos.

Al entrar los Estados Unidos en la Primera Guerra Mundial, el general Pershing ascendió a Patton a capitán. Estando en Francia, Patton solicitó que se le diera el mando de una unidad de combate, y Pershing le asignó el mando de una unidad del recién creado Tank Corps estadounidense. Por la organización de una escuela de entrenamiento para tanques estadounidenses en Langres, Francia), Patton fue ascendido dos veces hasta el rango de teniente coronel, y se le puso al mando del Tank Corps, que era parte de la "Fuerza Expedicionaria estadounidense". Tomó parte en la ofensiva de St. Mihiel en septiembre de 1918 y resultó herido por fuego de ametralladora mientras ayudaba a un tanque que estaba atascado en el barro. Mientras Patton se recuperaba de sus heridas, finalizaron las hostilidades.

Mientras estaba de servicio en Washington, D.C. en 1919, Patton conoció y trabó gran amistad con Dwight D. Eisenhower, que desempeñaría un papel enorme en la futura carrera militar de Patton. A principios de los años 20, Patton solicitó al Congreso fondos apropiados para una fuerza blindada, pero no tuvo éxito. Patton escribió artículos profesionales sobre tanques y tácticas con vehículos blindados, sugiriendo nuevos métodos para usar estas armas. También continuó trabajando en la mejora de los carros de combate, con innovaciones en la comunicación por radio y en sus carrocerías. Pese a todo, y por el poco dinero invertido en innovaciones en tiempos de paz, Patton finalmente volvió al cuerpo de caballería (todavía una fuerza montada a caballo) para avanzar en su carrera.

En julio de 1932, Patton sirvió bajo las órdenes del general Douglas MacArthur, como mayor, para dispersar a los veteranos que protestaban en Washington DC, conocidos como el "Bonus Army". Es promovido a teniente coronel en marzo de 1934, posteriormente es transferido a Hawái a principios de 1935. Por sus servicios en las operaciones de Meuse-Argonne, Patton recibió el Corazón Púrpura, la Cruz por Servicio Distinguido, y fue ascendido a coronel en julio de 1938.

Patton sirvió en Hawái antes de volver a Washington para pedir una vez más al Congreso fondos para unidades blindadas. A finales de los años 30, se le asignó el mando de Fort Myer, en Virginia. Poco después de los ataques de la blitzkrieg alemana en Europa, Patton pudo convencer finalmente al Congreso de la necesidad de divisiones blindadas. Tras su aprobación, en octubre de 1940 Patton fue ascendido a general de brigada y nombrado comandante de la 2.ª Brigada Blindada. Esta brigada creció hasta convertirse en la 2.ª División Blindada, es nombrado su comandante  y en abril de 1941 es ascendido a general de división.

Durante los preparativos del ejército estadounidense previos a su entrada en la Segunda Guerra Mundial, Patton estableció el Centro de Entrenamiento en Desierto Indio, California. También mandó uno de los dos ejércitos de entrenamiento en las maniobras de Luisiana de 1941. Fort Benning, en Georgia, es famoso por la presencia del general Patton.

En 1942, el general de división Patton mandaba el I Cuerpo Blindado del ejército estadounidense, que atracó en la costa de Marruecos durante la Operación Torch. Patton y la unidad llegaron a Marruecos a bordo del crucero pesado USS Augusta (CA-31), que fue atacado por el barco francés Jean Bart al entrar en el puerto de Casablanca.

Tras la derrota del ejército estadounidense a manos del Afrika Korps alemán en la batalla del paso de Kasserine en 1943, Patton reemplaza al general de división  Lloyd Fredendall al mando del 2do. Cuerpo del Ejército estadounidense y es ascendido a teniente general en marzo de 1943. Pese a ser duro en los entrenamientos, era considerado generalmente como un hombre justo y muy querido entre sus tropas. La disciplina dio sus frutos cuando, en marzo, la contraofensiva empujaba a los alemanes hacia el este mientras el VIII Ejército británico, mandado por Bernard Montgomery, los hacía retroceder hacia el oeste desde Egipto, expulsando con éxito a los alemanes del norte de África. Patton nunca congenió con Montgomery, a quien consideraba pusilánime ("pretende adaptar la realidad a sus planes, cuando lo que hay que hacer es adaptar los planes a la realidad"), y se estableció entre ellos una dura rivalidad por la fama y la conquista en los escenarios europeos.

Como resultado de sus éxitos en el norte de África, a Patton le fue dado el mando del VII Ejército estadounidense que estaba preparándose para invadir Sicilia en 1943. Su labor era la de liberar la parte occidental de la isla mientras el VIII Ejército británico del general Montgomery debía liberar la oriental.

Decidido a impedir que su rival Montgomery se llevara la gloria, Patton avanzó rápidamente sobre el oeste siciliano, liberando Palermo para posteriormente tomar el este hasta Mesina, siempre por delante de Montgomery.

Los enardecidos discursos de Patton fueron sus principales enemigos por la relevancia y las consecuencias que generaban en el escenario bélico. Estos discursos dieron lugar a una gran controversia cuando se afirmó que uno de ellos inspiró la masacre de Biscari, en la que tropas estadounidenses asesinaron a sesenta y seis prisioneros de guerra. La carrera militar de Patton estuvo a punto de acabar en agosto de 1943 si no hubiese sido por la intervención del general Bradley, amigo de Patton, y su influencia en el también amigo suyo, Eisenhower. Patton además abofeteó a unos soldados que padecían de fatiga de combate y cuando los actos de Patton se hicieron públicos, muchas voces pidieron que dimitiera o la expulsión del ejército.[3]​

Patton fue relevado del mando del VII Ejército justo antes de las operaciones en la península itálica.

Sin embargo, pese a que Patton fue relevado temporalmente de su cargo, su prolongada estancia en Sicilia fue interpretada por los alemanes como un indicativo de una inmediata invasión del sur de Francia, y posteriormente, su estancia en El Cairo fue interpretada como la señal de una futura invasión a través de los Balcanes. El temor al general Patton ayudó a mantener ocupadas a muchas tropas alemanas, y sería un factor muy importante en los siguientes meses, gracias a que fue usado como señuelo por los Aliados.

En el período que desembocó en el desembarco de Normandía, Patton dio numerosos discursos como jefe del ficticio primer grupo militar estadounidense (FUSAG), que supuestamente pretendía invadir Francia a través de Calais. Esto formaba parte de una sofisticada campaña aliada de engaño militar, la Operación Fortitude: el FUSAG disponía de barracones vacíos y carros de combate y cañones inflables para engañar a los aviones de observación de la Luftwaffe y algunos locutores que emitían falsas comunicaciones de radio para que las captase el enemigo. 

Un mes después de la invasión de Normandía, Patton fue puesto al mando del III Ejército de Estados Unidos, situado al oeste de las fuerzas aliadas de tierra. Guio a su ejército durante la Operación Cobra y se desplazó al sur y al este, ayudando a atrapar a cientos de miles de soldados alemanes en la bolsa de Chambois, cerca de Falaise. Patton usó tácticas propias de la Blitzkrieg contra los mismos alemanes, cubriendo 900 km en dos semanas. Las fuerzas de Patton liberaron gran parte del sur de Francia y envolvieron París, mientras el general francés Philippe Leclerc de Hauteclocque (Leclerc), contra el criterio de Patton y Eisenhower, ayudaba a los insurgentes que luchaban en el interior de la ciudad, hasta finalmente liberarla.

Sin embargo, la ofensiva de Patton se detuvo bruscamente el 31 de agosto de 1944, cuando sus tropas se quedaron sin combustible a las afueras de Metz (según el propio Patton fue una conspiración para favorecer a Montgomery). El tiempo necesario para conseguir el combustible fue suficiente para que los alemanes se hicieran fuertes en la fortaleza de Metz. Durante los meses de octubre y noviembre, el III Ejército estaba prácticamente en un punto muerto frente a los alemanes, infligiéndose mutuamente numerosas bajas. Pese a todo, el 23 de noviembre Metz se rindió a los estadounidenses, la primera vez que una ciudad se rendía desde la Guerra Franco-Prusiana.

A finales de 1944, el ejército alemán inició una ofensiva desesperada a través de Bélgica, Luxemburgo y el noreste de Francia. La ofensiva de las Ardenas fue la última gran ofensiva del ejército alemán en la Segunda Guerra Mundial. El 16 de diciembre de 1944, la Wehrmacht lanzó a 29 divisiones (en total unos 250 000 hombres) hacia un punto débil en las líneas aliadas y se dirigió de forma masiva hacia el río Mosa durante uno de los peores inviernos en Europa en muchos años.

Sin consultar antes con Eisenhower y el Alto Mando Aliado, Patton dirigió repentinamente al III Ejército hacia el norte (un considerable éxito táctico y logístico), desocupando el frente para aliviar a la rodeada y sitiada 101.ª División Aerotransportada, atrapada en Bastogne. Para muchos historiadores esta fue la más brillante maniobra de Patton en la guerra, al vencer las dificultades logísticas que supone girar el eje de avance de varios Cuerpos de ejército enteros. Cabe nombrar la condecoración que fue impuesta al coronel James O'Neill, reverendo de la unidad, por parte de Patton, del cual recibió el encargo de escribir una oración en la que pedía a Dios al menos 24 horas de condiciones atmosféricas propicias para el desarrollo de las operaciones aéreas de apoyo.

Para febrero, las tropas alemanas estaban nuevamente en retirada y Patton se desplazó hasta la cuenca del Sarre en Alemania. El 14 de abril de 1945 Patton fue ascendido a general, la promoción fue defendida por el secretario estado de los Estados Unidos Henry L. Stimson en reconocimiento a sus logros de batalla durante 1944. Patton, Bradley y Eisenhower recorrieron la mina de sal Merkers, así como el campo de concentración de Ohrdruf, viendo las condiciones del campo de primera mano causó en Patton un gran disgusto. El Tercer Ejército recibió la orden de marchar hacia Baviera y Checoslovaquia, anticipando una última batalla con las fuerzas alemanas allí presentes. Según los informes, se horrorizó de ver que el Ejército Rojo tendría a Berlín. Sintiendo a la Unión Soviética como una amenaza para el ejército de los EE. UU. Patton avanzó a Pilsen, pero fue detenido por Eisenhower, para que no llegara a Praga antes de Día de la victoria el 8 de mayo, que supuso el fin de la guerra en Europa. Patton planeaba tomar Praga, pero el mando aliado decidió detener el avance estadounidense. Sin embargo, sus tropas liberaron Pilsen (6 de mayo de 1945) y la mayor parte del oeste de Bohemia, en la que fue una de las últimas acciones bélicas de Patton.

Durante una de las celebraciones a raíz de la victoria con oficialidad rusa, Patton hizo un encendido discurso de velado carácter anticomunista, lo que tuvo como consecuencia, si no directamente, el comienzo de la guerra fría con los rusos, sus aliados en la II Guerra Mundial.  Patton fue relevado discretamente del escenario político.

Tras la victoria en Europa, Patton se sintió decepcionado ante la negativa del ejército de darle otro mando de combate en el Pacífico. Descontento con su papel como gobernador militar de Baviera y deprimido por su convicción de que nunca volvería a participar en una guerra, el comportamiento de Patton se volvió cada vez más errático.

Carlo D'Este, en Patton: Un Genio para la Guerra, afirma que “parece prácticamente inevitable… que Patton experimentara algún tipo de daño cerebral tras tantas heridas en la cabeza” por una vida llena de accidentes relacionados con caballos o vehículos, especialmente uno sufrido jugando al polo en 1936.

Sea cual fuera la causa, Patton volvió a encontrarse con problemas frente a sus superiores y a la población estadounidense cuando, hablando a un grupo de reporteros, comparó a los nazis con los perdedores de las elecciones estadounidenses. Patton fue relevado pronto del mando del III Ejército y transferido al XV Ejército, una unidad de oficina que preparaba una historia de la guerra. 

Patton también llegaría a afirmar que Estados Unidos luchó contra el enemigo equivocado en la guerra.[4]​

Triste y planteándose abandonar el ejército, el general Patton asumió el XV Ejército en octubre de 1945. Pero el 9 de diciembre de 1945 sufrió gravísimas lesiones en un extraño accidente de coche. Falleció el 21 de diciembre de 1945 en Heidelberg siendo enterrado con honores en el cementerio de guerra estadounidense de Hamm, en Luxemburgo.

Patton fue sin duda uno de los grandes genios militares de los EE. UU, así como uno de los más controvertidos por sus palabras.[5]​ Muchos de sus dichos, sobre todo en contra del comunismo, condicionaron acciones y respuestas indeseadas de parte de la Unión Soviética.[cita requerida]

Fue un militar considerado un genio en tácticas y  fue el soldado aliado al que más temieron los alemanes; no obstante, su fuerte personalidad unida a sonados desatinos empáticos lo hicieron perder su meta de ser considerado como el militar más admirado de los Estados Unidos. 

Se forjó una personalidad endurecida en el rigor castrense, carismática, arriesgada, voluntariosa y valiente; su mayor enemigo fue su propia volatilidad anímica y la falta de tacto en sus relaciones interpersonales.

Desde niño sufrió de dislexia lo que lo condujo a desarrollar sus aptitudes físicas en vez de las académicas, cultivó la construcción de una imagen ideal del soldado estadounidense forjado en el rigor castrense, cultivaba además el ego propio buscando permanentemente el reconocimiento personal, desarrollando la valentía y el coraje,  la competitividad y la superación personal, llegó a ser quinto en los Juegos Olímpicos de Estocolmo 1912 en pentatlón y además fue muy buen tirador de pistola y sobresalió en natación.

Al igual que muchos otros miembros de su familia, tenía creencias religiosas complejas y a menudo afirmaba haber tenido visiones vívidas de sus ancestros. Creía firmemente en la reencarnación y muchas pruebas anecdóticas indican que creía ser la reencarnación del hábil general cartaginés Aníbal, o bien, de un legionario romano, de un comandante de campo de Napoleón y otras figuras militares históricas.

Durante las acciones de guerra, Patton trataba de infundir en sus soldados el respeto y culto de su figura por el temor y buscaba que le idolatraran de forma permanente. Odiaba al soldado cobarde y se mostraba muy complaciente con aquellos que se destacaban en acción.

Eisenhower, consciente de las fortalezas y debilidades de Patton, colocó a su lado al general Omar Bradley, cuya serenidad, aplomo, honestidad y criterio, sumados a sus habilidades tácticas, debían completar las fortalezas de Patton; a la larga, fue Bradley quien se transformó en su álter ego y pasó de subordinado a jefe directo de Patton; con todo, entre ambos militares de personalidades tan diametralmente opuestas se estableció un auténtico lazo de amistad y respeto mutuo. Este punto llegará a ser desmentido por biógrafos posteriores, los cuales afirman que el general Omar Bradley, nunca aceptó de buen grado a Patton, como demuestran muchos testimonios de los soldados del Tercer Ejército, los cuales veían a Omar Bradley como alguien altivo, despectivo y distante. 

Ambos generales nunca llegaron a tener una amistad, prueba de ello es el film Patton, del que Omar Bradley fue asesor y en el cual se muestra al General Patton como un personaje histriónico, mientras que el propio Bradley se muestra así mismo con una visión totalmente irreal de la que tenían sus propios hombres de él.

Odiaba las bromas de mal gusto, carecía del sentido del humor que destacaba a Eisenhower y era particularmente mordaz, por lo que cuando se arriesgaba a decir algo en tono jocoso o irónico, el efecto que conseguía era en contrario provocando antipatía.[6]​

Patton exhibió en ciertas ocasiones rasgos racistas al desestimar la capacidad de combate de los soldados afroestadounidenses que tuvo bajo su mando.[7]​

Mientras visitaba hospitales en Italia y alababa a los soldados heridos, abofeteó y humilló verbalmente a los soldados Paul G. Bennet y Charles H. Kuhl, convencido de que estaban exhibiendo un comportamiento cobarde. Los soldados padecían diferentes formas de fatiga de combate, y no tenían heridas visibles (aunque posteriormente se descubrió que uno de ellos sufría de disentería). A causa de esta acción, Patton fue alejado de la opinión pública durante algún tiempo y se le ordenó secretamente que se disculpara ante los soldados. 

Irónicamente, muchos psiquiatras modernos que han examinado estos incidentes aseguran que el mismo Patton podría haber sufrido de fatiga de combate. No obstante, los soldados, a pesar de que lo detestaban por la estricta aplicación de la disciplina militar, preferían estar bajo su mando que bajo otro, pues lo consideraban su mejor opción de salir vivos del escenario bélico. Terminada la guerra en Europa, pidió traslado al Frente del Pacífico ya que sabía que su personalidad se exaltaba con el escenario bélico. Sus destinos en cargos más bien honoríficos le causaron un notable desánimo.

Historial de ascensos del general Patton:

En el momento de la muerte del General Patton, estaba autorizado a lucir los siguientes premios y condecoraciones:

El diccionario es una obra donde se puede consultar palabras o términos y se proporciona su significado, definición, etimología, ortografía, fija su pronunciación, separación silábica y forma gramatical. La información que proporciona varía según el tipo de diccionario del que se trata. En muchos casos los diccionarios proporcionan el significado de las palabras, su etimología, su escritura, los sinónimos y los antónimos.


Se considera que los primeros diccionarios aparecieron en Mesopotamia. Esta afirmación viene, en gran parte, del descubrimiento de varios textos cuneiformes en la Biblioteca de Asurbanipal, en Nínive, que relacionaban palabras sumerias.
Existen varios tipos de diccionarios, según su función y su uso:

En ellos se recogen términos que se consideran correctos según la norma. Para la lengua española, el referente es el Diccionario de la lengua española (DLE), de la Real Academia Española, elaborado conjuntamente por las veintitrés Academias de la Asociación de Academias de la Lengua Española.[1]​ El Diccionario del estudiante es la obra de referencia para los estudiantes de secundaria y bachillerato.[2]​

Recogen acepciones en las palabras que no son reconocidas por el órgano competente (como la Real Academia Española) pero que, sin embargo, se usan ampliamente en la sociedad. Es el caso, por ejemplo, del Diccionario de uso del español  (DUE), de María Moliner; del Diccionario Clave, de Concepción Maldonado; y del Diccionario de uso del español actual (DEA), de Manuel Seco, Olimpia Andrés y Gabino Ramos.

En ellos se explica brevemente el significado de las palabras de una determinada lengua. Estos diccionarios no contienen, a diferencia de los bilingües, definiciones que incluyen equivalentes en otras lenguas.

Diccionarios que consisten en traducir una palabra de un idioma a otro, por ejemplo, del español al inglés y viceversa. Generalmente se usan cuando se estudia un idioma diferente al idioma materno o cuando se busca una palabra que se escribe o habla en otro idioma y que no se conoce en el idioma materno.

Son diccionarios elaborados para estudiantes nativos o extranjeros. En ellos se ofrecen definiciones más sencillas que en diccionarios concebidos para el público general y se aporta mayor información sintagmática y paradigmática en los artículos. El número de ejemplos proporcionado por cada lema también es mayor. Un ejemplo de diccionario de aprendizaje para hablantes nativos del español es el Diccionario del estudiante, ya mencionado. Algunos diccionarios de aprendizaje para estudiantes de español como lengua extranjera son el Diccionario para la enseñanza de la lengua española, de VOX-Universidad de Alcalá (DIPELE); el Diccionario Salamanca de la lengua española[3]​, de Santillana; el Diccionario de español para extranjeros, de SM; y el Diccionario para estudiantes de español, de Espasa.

Son los diccionarios en los que se facilita la vida y la información sobre el origen de las palabras de una determinada lengua. Quizá el diccionario etimológico más prestigioso de la lengua inglesa es el Oxford English Dictionary. Quizá el diccionario etimológico más célebre (aunque ya no es el más actualizado) de la lengua española es el Tesoro de la lengua castellana o española (1611), obra de Sebastián de Covarrubias y Orozco, que no es solo diccionario etimológico, sino que aporta muchísimos datos históricos de la lengua utilizada en su época.

En estos diccionarios se relacionan palabras de significado similar y opuesto, para facilitar la elección de estas al redactar textos. Los más sencillos se limitan a dar una lista de palabras para cada entrada, pero algunos más complejos indican además las diferencias de matiz con la palabra buscada, sin llegar a ser un tesauro (véase más adelante); no todas las palabras tienen antónimos. En algunos casos, los diccionarios de sinónimos y antónimos también incluyen parónimos.

Se trata de diccionarios que están dedicados a palabras o términos que pertenecen a un campo o técnica determinados como, por ejemplo, la informática, la jardinería, la ingeniería, la computación, la genética, la heráldica, la gastronomía, el lenguaje SMS, los pesos y medidas, las abreviaturas, etc. Proporcionan breve información sobre el significado de tales palabras o términos. Pueden ser también diccionarios de idiomas en los que se indica la traducción a otra lengua o a otras lenguas de las palabras o términos que incluyen.

Son diccionarios de la lengua con la particularidad de que están ordenados alfabéticamente según las últimas letras de cada palabra, en vez de las primeras. Su uso principal es buscar palabras que rimen con otra(s), para la redacción de poemas y versos. Algunos diccionarios inversos reducidos no incluyen definiciones, sino solo la lista de palabras ordenadas de esta forma.

En estos diccionarios no se ordenan palabras, sino estructuras gramaticales. Su uso principal es para personas que están aprendiendo un idioma extranjero, ya que les permite buscar estructuras gramaticales de un texto y consultar en ellos su significado y construcción.

Recogen palabras y frases cuyo significado se ha desvirtuado y no significan en la sociedad lo que un diccionario de la lengua indica. Estos diccionarios ayudan a un redactor o escritor a usar los términos correctos, sin dejarse llevar por el significado popular. A diferencia del diccionario de uso práctico anterior, su objetivo no es dar a conocer el uso vulgar de una palabra, sino advertir de este, y proponer alternativas adecuadas para fines específicos.

Los tesauros son obras en las que se relacionan numerosas palabras que guardan una relación más o menos directa con la palabra u objeto de consulta. No son, pues, diccionarios de sinónimos, ya que estos últimos incluyen únicamente palabras con un significado similar y equivalente.

Se localizan las palabras según su asociación a una idea. Se parte de ideas generales y se va concretando hasta llegar a una lista de palabras entre las que se encontrará la buscada. Se diferencia del tesauro en que en aquel las palabras se relacionan con palabras con alguna relación, mientras que en este las palabras se agrupan con ideas. Por ejemplo, para localizar el nombre de un cierto color verde que no se recuerda se busca en el grupo "naturaleza"; dentro de este, en el grupo "luz"; dentro de este, en el grupo "color", luego en el grupo "verde" y ahí, entre otros, se encuentra "glauco", un tono específico de verde.

Es una especie de tesauro. Sus características hacen que se presenten en formato electrónico (DVD o página web). Se llama conceptual porque el acceso se realiza por medio de conceptos, no solo por medio de palabras. Por ejemplo, demasiado cansada para es un concepto multipalabra. Esta característica hace que la accesibilidad sea fácil para el usuario común.

En un diccionario visual, se utilizan principalmente imágenes para ilustrar el significado de las palabras. Los diccionarios visuales pueden organizarse por temas o por lista alfabética de las palabras. Para cada tema, una imagen se etiqueta con la palabra correcta, con objeto de identificar cada componente del tema en cuestión.

El diccionario contiene información más específica y detallada y abarca temas mucho más amplios como, por ejemplo, acerca de países, continentes, océanos, personas famosas; también puede mencionar cómo se escribe cierta palabra en otros idiomas. No debe confundirse un diccionario enciclopédico con una enciclopedia. El primero tiene información breve sobre el significado de un término. Una enciclopedia proporciona mucha información relacionada con cada entrada o artículo, no solo una definición. Wikipedia es ejemplo de un tipo específico de enciclopedia: la enciclopedia en línea que pueden modificar los propios usuarios.

Los diccionarios son tradicionalmente libros. Sin embargo, también existen diccionarios en soporte digital, como CD y DVD, y se pueden consultar algunos en Internet. También se han popularizado los diccionarios electrónicos portátiles, como una aplicación dentro de un teléfono o consistentes en un pequeño dispositivo independiente con pantalla y teclado, que suele contener varios diccionarios en su interior.

Los artículos del diccionario tienen las siguientes partes:

En un diccionario:

El primer diccionario europeo dedicado íntegramente a una lengua viva y que ofrece una definición para cada entrada fue el Tesoro de la lengua castellana o española de Sebastián de Covarrubias publicado en 1611[6]​. La lengua italiana fue la primera en tener un diccionario monolingüe escrito por una academia lingüística: el Vocabolario dell'Accademia della Crusca, cuya primera edición apareció en Florencia en 1612. En francés, no fue hasta César-Pierre Richelet que apareció el primer diccionario monolingüe en francés (1680). La lengua inglesa, aunque dotada de varios diccionarios, tendrá que esperar hasta 1755 para dotarse de un diccionario exhaustivo de la lengua inglesa con el Dictionary of the English Language, publicado el 15 de abril de 1755 por Samuel Johnson.

Thomas Alva Edison (Milan, Ohio; 11 de febrero de 1847-West Orange, Nueva Jersey; 18 de octubre de 1931) fue un inventor, científico y empresario estadounidense.[1]​[2]​[3]​ Desarrolló numerosos dispositivos que han tenido gran influencia en todo el mundo, como el fonógrafo, la cámara de cine o una duradera bombilla incandescente. Apodado «El mago de Menlo Park», Edison fue uno de los primeros inventores en aplicar los principios de la producción en cadena y el trabajo en equipo a gran escala al proceso de invención, motivos por los cuales se le reconoce la creación del primer laboratorio de investigación industrial.[4]​

Edison fue un inventor prolífico que registró 1093 patentes a su nombre en Estados Unidos, además de otras en Reino Unido, Francia y Alemania. Pero más importante que sus muchas patentes fue el amplio impacto que tuvieron algunas de sus invenciones: la luz eléctrica y el suministro público de electricidad, la grabación de sonido y la cinematografía se convirtieron en nuevas y poderosas industrias en todo el mundo. Sus inventos contribuyeron en particular a las telecomunicaciones, como una máquina de voto, una batería para un automóvil eléctrico, la energía eléctrica, la grabación de música y las películas. Sus avanzados trabajos en estos campos no fueron más que una continuación de su primer trabajo como radiotelegrafista. Edison desarrolló un sistema de generación y distribución de energía eléctrica mediante corriente continua a las casas,[5]​ negocios y fábricas, un avance crucial para el mundo industrializado moderno.[6]​

Hijo de Samuel Ogden Edison, Jr. (1804-1896) y Nancy Matthews Elliott (1810-1871). Sus antepasados provenían de Ámsterdam y se establecieron en el río Passaic, en Nueva Jersey. John Edison, el abuelo del inventor, se alistó en el bando de los británicos durante la Guerra de Independencia y, a final de la misma, tuvo que refugiarse en Nueva Escocia. Después de un tiempo se trasladó a Canadá para residir en Bangham, en la zona del lago Erie. Cuando estalló la rebelión canadiense en 1837, Samuel Edison (padre del inventor) se unió a los insurgentes. Una vez más la familia se vio obligada a huir a los Estados Unidos.

En 1840 Samuel Edison estableció una pequeña maderería en Milan, Ohio. Antes de que la familia se estableciera en Milan, su esposa Nancy, una canadiense de ascendencia escocesa, había tenido cuatro hijos. Posteriormente tuvo tres más, pero murieron tres de los primeros en la década de 1840 y los sobrevivientes tenían catorce, dieciséis y dieciocho años cuando el 11 de febrero de 1847, la esposa de Samuel Edison dio a luz a su séptimo hijo. Le llamaron "Thomas" por un antepasado de la familia, y "Alva" en honor del capitán Alva Bradley.

En 1855 a los ocho años y medio Edison entra a la escuela. Después de tres meses de estar asistiendo, regresó a su casa llorando, informando que el maestro lo había calificado de alumno "estéril e improductivo". Es imposible establecer si Nancy Edison tomó muy en serio la opinión de su maestro o si pensó que ella era mejor que el profesor de su hijo. El caso es que Edison recordó durante el resto de su vida el resultado del dichoso incidente. La madre de Edison sacó al niño del colegio y lo educó en casa. Después de las tareas domésticas, le enseñó a leer y escribir correctamente, además de aritmética, y comenzó a leerle algunos libros que no estaban entre lo más común del momento: La caída del Imperio Romano, Historia de Inglaterra, Historia del mundo, y a Shakespeare y Dickens. En torno a los 9 años fue cuando Edison comenzó a leer solo, en parte impulsado por su padre, Samuel, que le daba 10 céntimos cada vez que concluía un libro.[7]​

En 1859 empezó a vender diarios en el tren matutino que iba de Port Huron a Detroit, así como verduras, mantequilla y moras. En Detroit el tren hacía una parada de seis horas, las cuales aprovechaba pasándolas en el salón de lectura de la Asociación de Jóvenes (después Biblioteca Gratuita de Detroit). Ahí, comenzaba por leer el primer libro que se encontraba en el anaquel inferior y seguía por orden con los demás hasta terminar con toda la hilera.

Edison no quedaba satisfecho con solo leer, y comenzó a realizar diversos experimentos basándose en lo que leía en los libros de Ciencia. Utilizaba un vagón vacío como laboratorio, donde también instaló una pequeña prensa de mano que se agenció cuando un amigo del Detroit Free Press le regaló algunos tipos. El resultado fue inmediato: el Grand Trunk Herald, semanario del que Edison tiraba cuatrocientos ejemplares.

Tras salvar a un niño en las vías del tren en Port Huron, el agradecido padre de la criatura J. U. Mackenzie (telegrafista de la estación) le enseñó código morse y telegrafía. A los quince años obtuvo su primer trabajo como telegrafista, reemplazando a uno de los operadores de telégrafo que habían ido a servir en la Guerra Civil.[8]​

A los 16 años, después de trabajar en varias oficinas de telégrafos, donde realizó numerosos experimentos, finalmente llegó con su primera auténtica invención, llamada "repetidor automático", que transmite señales de telégrafo entre estaciones sin personal, lo que permite que prácticamente cualquiera pueda traducir fácilmente y con precisión un código a su propio ritmo y conveniencia. Curiosamente, nunca patentó la versión inicial de esta idea.[8]​

Edison ideó un instrumento sencillo para el recuento mecánico de votos en 1868. Se podía colocar en la mesa de cada representante; tenía dos botones, uno para el voto en pro y otro para el voto en contra. Para tramitar la patente, Edison contrató al abogado Carroll D. Wright. El instrumento se llevó ante un comité del Congreso de Washington. Ahí el veredicto fue brusco pero honesto: "Joven, si hay en la tierra algún invento que no queremos aquí, es exactamente el suyo. Uno de nuestros principales intereses es evitar fraudes en las votaciones, y su aparato no haría otra cosa que favorecerlos".

En 1869, Edison y Franklin Pope ofrecieron sus servicios como ingenieros electricistas, una especialidad desconocida por entonces. Pero Edison se retiró porque sentía que no ganaba suficiente.[9]​ En Nueva York, consiguió un empleo de condiciones muy ventajosas tras reparar una grave avería en un indicador telegráfico que señalaba los precios del oro en la bolsa de valores.

Trabajó en la compañía telegráfica Western Union como inventor y reparador, aunque poco después se independizó y en 1877 llevó a cabo uno de sus más importantes inventos, el fonógrafo.

En 1876, Edison se mudó de Newark a Menlo Park, Nueva Jersey, donde reunió un grupo de ayudantes y mecánicos y estableció una "fábrica de inventos". En 1887, cuando dejó Menlo Park, contaba con una lista de casi cuatrocientas patentes.[10]​

Aunque se le atribuye la invención de la lámpara incandescente, esta en realidad solo fue perfeccionada por él, quien, tras muchos intentos consiguió un filamento que alcanzara la incandescencia sin fundirse. Este filamento no era de metal, sino de bambú carbonatado. El 21 de octubre de 1879 consiguió que su primera bombilla luciera durante 48 horas seguidas. En la víspera de Año Nuevo del mismo año, se hizo funcionar con éxito en Menlo Park el primer sistema de alumbrado, construido por Edison, constituido por cincuenta y tres focos.[11]​

En 1880 se asocia con J. P. Morgan para fundar la Edison Electric. Después J. P. Morgan adquiriría sus acciones para crear General Electric.

En el ámbito científico, descubrió el efecto Edison, patentado en 1883, que consistía en el paso de electricidad desde un filamento a una placa metálica dentro de un globo de lámpara incandescente. Aunque ni él ni los científicos de su época le dieron importancia, estableció los fundamentos de la Válvula termoiónica y de la electrónica, el denominado efecto Edison, fenómeno de vital importancia para la electrónica de los años 1950 y 1960.

En la década de 1880, la iluminación de arco en calles y espacios públicos eran un negocio en expansión en los Estados Unidos, Europa y algunas ciudades de América. La alimentación para estas luminarias era suministrada por medio de generadores de corriente continua (CC), provistos por la compañía de Edison, pero pronto tuvo que enfrentar la competencia con los sistemas de corriente alterna (CA). Con el desarrollo de los transformadores de la Westinghouse Electric se hizo posible transmitir corriente alterna a largas distancias a través de cables más delgados y más baratos, y reducir el voltaje en el destino para su distribución a los usuarios. Esto permitió que la corriente alterna se usara en las pequeñas empresas y los clientes domésticos ya que las plantas de corriente continua de Edison estaban diseñadas para suministrar energía a las grandes ciudades.[12]​

Edison sostuvo públicamente que la corriente alterna no daba buenos resultados y, sobre todo, que los altos voltajes que utilizaba eran peligrosos. Cuando George Westinghouse instaló sus primeros sistemas de corriente alterna en 1886, Edison lo atacó duramente: "Es tan cierto como la muerte que Westinghouse matará a un cliente dentro de los seis meses posteriores a la instalación de cualquier sistema, se trata de algo nuevo y requerirá una gran número de experimentos para que funcione de manera práctica".[13]​ Esta postura tan terminante ha sido explicada de varias maneras; es posible que Edison no fuese capaz de captar las bases teóricas de la corriente alterna, o bien que estuviera genuinamente preocupado por el alto voltaje de los sistemas de corriente alterna en uso, los cuales mal instalados, eran un riesgo para los clientes y una mala reputación para el desarrollo de la energía eléctrica.[14]​ Además, su compañía, la Edison Electric se basaba en la corriente continua de bajo voltaje con más de cien instalaciones hasta el momento, por lo cual cambiar el estándar implicaba una gran pérdida. Sin embargo, en 1887 la Edison Electric estaba perdiendo participación de mercado frente a Westinghouse, que ya había construido 68 centrales eléctricas de CA frente a las 121 estaciones de corriente continua de Edison y, para empeorar las cosas, la Thomson-Houston Electric Company de Lynn, Massachusetts, añadía 22 centrales eléctricas de corriente alterna.[15]​

Al mismo tiempo, las líneas de corriente alterna montadas en postes provocaron una serie de muertes en la primavera de 1888; lo que provocó la preocupación del público y numerosas notas en la prensa en contra de la corriente alterna de alto voltaje y de las codiciosas e insensibles empresas que la usaban.[16]​[17]​ Edison aprovechó esta situación y se unió al inventor Harold P. Brown en una campaña contra la CA. Esta campaña fue conocida como la guerra o batalla de las corrientes y consistió en polémicas públicas, artículos en los periódicos y una fuerte presión para que el Congreso aprobase una legislación que controlara, limitándolo, el voltaje de las instalaciones de CA. En una maniobra de propaganda, Edison y Brown intentaron mostrar que la corriente alterna era el sistema más adecuado para la recientemente inventada silla eléctrica logrando que la primera de ellas fuese alimentada por un generador de la Westinghouse. Al mismo tiempo realizaron algunas ejecuciones públicas de animales usando también la CA, si bien es falso el difundido relato de la ejecución de la elefanta Topsy, que tuvo lugar muchos años después.[18]​[19]​[20]​

Esta campaña anti CA, no cayó bien en sus propios accionistas, ya que a principios de 1890, la compañía de Edison obtenía ganancias mucho menores que sus competidoras. En 1892, la Guerra de las Corrientes llegó a su fin cuando Edison perdió el control de su empresa, la cual se fusionó con Thomson-Houston (una idea de J.P. Morgan) para crear la General Electric, que controlaba las tres cuartas partes del suministro eléctrico de los Estados Unidos y competía directamente con Westinghouse por el mercado de la corriente alterna.[21]​

En 1884, el gerente de Edison Charles Batchelor, quien había estado supervisando la instalación eléctrica en París, regresó a los Estados Unidos para administrar Edison Machine Works, una división de fabricación de Edison Electric ubicada en Nueva York, y le pidió a un colaborador, el ingeniero Nikola Tesla que lo acompañase.[22]​ Tesla emigró y comenzó a trabajar casi de inmediato en el taller de la empresa, ubicado en el Lower East Side de Manhattan, con una fuerza laboral de varios cientos de maquinistas, obreros, personal administrativo y una veintena de ingenieros.[23]​ Al igual que en París, Tesla continuó trabajando en la solución de problemas de las instalaciones y en la mejora de los generadores.[24]​ El historiador W. Bernard Carlson señala que Tesla pudo haberse encontrado no más de un par de veces con Edison,[23]​ una de ellas cuando Tesla, después de permanecer despierto toda la noche reparando las dínamos dañadas en el transatlántico SS Oregon, se encontró con Batchelor y Edison quienes bromearon sobre su "estilo parisino" de trasnochar; Tesla les contó lo que había estado haciendo, por lo que Edison lo elogió diciendo: "es un gran tipo".[25]​[26]​

Uno de los proyectos entregados a Tesla era el desarrollo de un sistema de alumbrado público basado en lámpara de arco.[27]​ La iluminación de arco era el tipo de alumbrado público más popular, pero requería altos voltajes y era incompatible con el sistema incandescente de bajo voltaje de Edison, lo que provocó que la empresa perdiera contratos en algunas ciudades. De todos modos, los diseños de Tesla nunca se pusieron en producción, debido a mejoras técnicas en el alumbrado incandescente y a un acuerdo de instalación que Edison hizo con una empresa de iluminación de arco.[28]​[29]​

Tesla había estado trabajando unos seis meses cuando renunció.[23]​ No están claros los motivos. Es posible que haya sido por una bonificación que no recibió, ya sea por rediseñar los generadores o por el sistema de iluminación de arco que se archivó. Tesla tuvo enfrentamientos previos con la empresa sobre bonificaciones impagas y, en su autobiografía, declaró que el gerente de Edison Machine Works, es decir Batchelor, le había ofrecido un bono de 50.000 dólares para diseñar "veinticuatro tipos diferentes de máquinas estándar", "pero que resultó ser una broma".[30]​ Versiones posteriores de esta historia muestran al propio Edison ofreciendo y luego renegando del trato, diciendo: "Tesla, no entiendes nuestro humor americano".[31]​ Lo extraño de estas historias es el monto de la bonificación dado que Batchelor, era tacaño con la paga[32]​ y es poco probable que la compañía tuviese una suma tan grande (más de un millón de dólares al cambio de 2010) en efectivo.[33]​[34]​ El diario de Tesla contiene solo un comentario sobre lo que sucedió al final de su empleo, una nota que garabateó en las dos páginas que abarcan desde el 7 de diciembre de 1884 hasta el 4 de enero de 1885, diciendo "Adiós a Edison Machine Works".[35]​ Al año siguiente, Tesla se asoció a la Westinghouse Electric y como parte de ella tomó partido por el uso de la corriente alterna en la guerra de las corrientes.[36]​

En 1894 los quinetoscopios de Edison llegaron por primera vez a Europa; más concretamente a Francia. Dos años después, en 1896, presentó el vitascopio en Nueva York con la pretensión de reemplazar a los quinetoscopios y acercarse al cinematógrafo inventado por los hermanos Lumière.

Los aportes de Edison al mundo del cine también fueron muy importantes. En 1889 comercializó la película en celuloide en formato de 35 mm, aunque no la pudo patentar porque un tiempo antes George Eastman ya lo había hecho; aunque sí pudo patentar sus perforaciones laterales.

Por último, en 1897, Edison comenzará la llamada «guerra de patentes» con los hermanos Lumière respecto al invento de la primera máquina de cine.

Murió el 18 de octubre de 1931, en West Orange, Nueva Jersey. Como homenaje póstumo, fueron apagadas las luces de varias ciudades durante un minuto.

Es uno de los inventores más prolíficos de la historia: la obtención de su última patente, la 1093.ª, fue a sus 83 años.[8]​

Durante su vida, Edison fue presentado como el prototipo de inventor y sus innovaciones como intuiciones de un genio. Esta imagen fue popular durante el siglo XX y dio origen a una serie de representaciones culturales que lo muestran como una mente brillante y solitaria, un héroe que encarna los valores de la libre empresa y la iniciativa individual. Por otra parte, gran número de leyendas urbanas surgidas a principios del siglo XXI, se empeñan en desmitificar a Edison e incluso presentarlo como un personaje despótico y cruel. Su contemporáneo, inventor y en un tiempo empleado, Nikola Tesla es mostrado por estas mismas leyendas como la antítesis y la víctima de Edison. En ambos casos el paradigma del inventor solitario, en lucha contra un mundo que no lo comprende, sigue presente en ambas versiones.[37]​

La historia de la ciencia y la tecnología cuestiona ambas versiones. Es verdad que Edison era, además de inventor, un consumado publicista y un sagaz hombre de negocios. Sabía usar la prensa para promover su imagen y se concentraba en aquellas innovaciones que podían comercializarse fácilmente. No era, de ningún modo, un genio solitario; empleaba decenas de empleados en sus laboratorios y se empeñaba en utilizar el método de ensayo y error. Al respecto, Tesla remarcó de manera polémica, su falta de preparación teórica.[38]​

Edison, confeso pacifista, se negó a diseñar armas ofensivas durante la Primera Guerra Mundial y se limitó a diseñar dispositivos de defensa, algo de lo cual siempre se enorgulleció.[39]​

Se lo ha acusado de antisemita[40]​ en especial por su amistad con Henry Ford, quien lo era de modo manifiesto, sin embargo, si bien compartía algunos de los estereotipos respecto del judaísmo, comunes en los Estados Unidos de finales del siglo XIX, y expresó comentarios prejuiciosos sobre los judíos, sin embargo, también se opuso a su persecución, promovió su emancipación y contrató numerosos judíos en su laboratorio.[41]​

Las finanzas son una rama de la administración y de la economía que estudia el intercambio de capital entre individuos, empresas, o Estados y con la incertidumbre y el riesgo que estas actividades conllevan.[1]​ Se dedica al estudio de la obtención de capital para la inversión en bienes productivos y de las decisiones de inversión de los ahorradores.  

Así mismo, su estudio se basa en la obtención y la administración del dinero para lograr sus respectivos objetivos, tomando en cuenta todos los riesgos que ella implica, basándose en todos y cada uno de sus componentes que la integra para mantener un buen control.

Las finanzas estudian cómo los agentes económicos (empresas, familias o Estado) deben tomar decisiones de inversión, ahorro y gasto en condiciones de incertidumbre. Al momento de tomar estas decisiones los agentes pueden optar por diversos tipos de recursos financieros tales como: dinero, bonos, acciones o derivados, incluyendo la compra de bienes de capital como maquinarias, edificios y otras infraestructuras. Ver diferencia entre ahorro e inversión.

Las finanzas va ligada con la administración por ejemplo, al considerar un nuevo producto, el administrador financiero requiere que el personal de finanzas le proporcione los cálculos de presupuesto. Y la función de las finanzas administrativas puede describirse ampliamente al considerar su papel dentro de la organización, su relación con la economía y la contabilidad y las principales actividades del administrador financiero. 

Está relacionado con las transacciones y con la administración del dinero.[2]​ En ese marco se estudia la obtención y gestión, por parte de una compañía, un individuo, o del propio Estado, de los fondos que necesita para cumplir sus objetivos, y de los criterios con que dispone de sus activos; en otras palabras, lo relativo a la obtención y gestión del dinero, así como de otros valores o sucedáneos del dinero, como lo son los títulos, los bonos, etc. Según Bodie y Merton, las finanzas «estudian la manera en que los recursos escasos se asignan a través del tiempo». Las finanzas tratan, por lo tanto, de las condiciones y la oportunidad con que se consigue el capital, de los usos de este, y los retornos que un inversionista obtiene de sus inversiones.[3]​

El estudio académico de las finanzas se divide principalmente en dos ramas,[4]​ que reflejan las posiciones respectivas de aquel que necesita fondos o dinero para realizar una inversión, llamada finanzas corporativas, y de aquel que quiere invertir su dinero dándoselo a alguien que lo quiera usar para invertir, llamada valuación de activos.[5]​ El área de finanzas corporativas estudia cómo le conviene más a un inversionista conseguir dinero, por ejemplo, si vendiendo acciones, pidiendo prestado a un banco o vendiendo deuda en el mercado. El área de valuación de activos estudia cómo le conviene más a un inversionista invertir su dinero, por ejemplo, si comprando acciones, prestando/comprando deuda, o acumulado dinero en efectivo.
Estas dos ramas de las finanzas se dividen en otras más. Algunas de las áreas más populares dentro del estudio de las finanzas son: Intermediación Financiera, Finanzas Conductistas, Microestructura de los Mercados Financieros, Desarrollo Financiero, Finanzas Internacionales, y Finanzas de Consumidor.[6]​ Una disciplina recientemente creada son las neurofinanzas, rama de la neuroeconomía, encargada del estudio de los sesgos relacionados con el manejo de la economía.[7]​

Las finanzas personales son la aplicación de las finanzas y sus principios a una persona o familia en su deseo de realizar sus actividades con la mejor distribución de dinero para ello. Así, deben reconocer cómo ocupar sus ingresos en educación, salud, alimentación, vestimenta, seguros, lujos, transporte, etc. Se deben tener en cuenta los ingresos, los gastos, los ahorros y siempre estableciendo los riesgos y los eventos futuros. Parte de las finanzas personales son los cheques, las cuentas de ahorro, las tarjetas de crédito, los préstamos, las inversiones en el mercado de valores, los planes de jubilación, los impuestos, etc.

En España la tasa de interés es conocida como el tipo de interés.

La tasa de interés afecta directamente el consumo, el comercio y la inversión, pues parte del consumo se paga mediante tarjetas de crédito, parte de la mercancía comprada y vendida por los comercios es comprada a crédito, y las inversiones siempre se apoyan con préstamos bancarios o emisión de deuda mediante bonos: Al subir la tasa de interés el consumo y la inversión disminuyen, pues individuos y empresas encuentran más difícil pagar sus deudas; al bajar la tasa de interés el consumo y la inversión aumentan por el estímulo que representa pagar menos intereses. Esta relación entre tasa de interés, consumo e inversión es utilizada por los diseñadores de políticas macroeconómicas para manipularlos para afectar crecimiento económico, empleo e inflación: Para disminuir la inflación inducen un aumento en las tasas de interés; cuando quieren aumentar el empleo, inducen su disminución. Tales acciones usualmente son llevadas a cabo mediante el Banco Central de cada país al establecer la llamada tasa de descuento - la tasa de interés a la cual presta dinero a los bancos. Este cambio en la tasa de descuento impacta los costos del dinero en los bancos y, por tanto, impacta la tasa de interés que tales bancos cobran por sus préstamos.

Hay varias fuentes de las finanzas, las deudas, las obligaciones, las utilidades brutas, los préstamos a largos plazos, el capital del préstamo, las tarjetas de créditos, los fondos de riesgos laborales, entre otros. Estas fuentes financieras son útiles en diferentes situaciones y pueden clasificarse con base al tiempo y el control.  Una empresa puede elegir las fuentes de financiación alternativa. La elección debe de ser correcta y adecuada. Esto supone un gran reto a los gerentes de finanzas de las empresas. Este proceso de selección debe de ser analizado y, comprender todas las características de cada uno, con base a lo que necesite la empresa.  Para conocer las fuentes de financiación, es necesario saber cuáles son las clasificaciones de las finanzas. Las finanzas se clasifican en dos ramas, las finanzas públicas y las privadas.  

Básicamente, este tipo de finanzas se trata de la optimización de las finanzas a nivel personal (la familia, los ahorros personales, entre otros). Están sometidas a un presupuesto. Por ejemplo, una persona puede financiar su vehículo en cualquier banco. La finanza privada es la planificación financiera a nivel individual. Se trata de la utilización de los recursos monetarios, personal y familiar, considerando los acontecimientos futuros y los riesgos asociados con estos.  

Las finanzas privadas incluyen:

Esta finanza es la encargada de tratar de optimizar los objetivos económicos de un Estado (inversión, PIB, déficit, superávit, entre otras), mediante la estimación de las necesidades futuras y la asignación de fondos de acuerdo con la disponibilidad de fondos. 

Las finanzas públicas constituyen una rama de la economía que ayuda a examinar las consecuencias de los diferentes tipos de inversiones, de los impuestos y de los gastos de los empleados de las empresas estatales o de los gobiernos. También analiza la eficacia de los procedimientos en el desarrollo técnico de la empresa.  Las finanzas públicas se encargan de las provisiones necesarias a nivel de comercio. Una empresa necesita constantemente capital, sobre la base de las diferentes inversiones a:  

Las inversiones a corto plazo se utilizan para satisfacer las necesidades actuales de una empresa. Las inversiones a largo plazo son utilizadas para la adquisición de activos fijos de la empresa, por ejemplo, tierras, maquinaria, entre otras. Y las inversiones a mediano plazo incluyen las utilidades retenidas, por ejemplo, los préstamos temporales, cambios en la empresa, entre otras. Algunas ramas de las finanzas públicas son: 

La política fiscal tiene que ver con el marco político del gobierno, después de tomar en consideración el gasto público, las vías de ingresos del gobierno, entre otras. Las finanzas juegan un papel fundamental en la organización de un gobierno y de una empresa. Sin una correcta organización, los gastos pueden elevarse de tal manera que la deuda pública crezca hasta convertirse en una deuda externa.

La especialidad financiera tiene vínculo con otras ciencias y con diversas teorías económicas y administrativas. Economía y finanzas tienen gran conocimiento en común, las finanzas pueden ser una rama de la economía, así como también de la administración, por eso se tiene el concepto de administración financiera, finanzas públicas o economía de la empresa.

Una materia aledaña a las finanzas es la contabilidad. El método de registro contable produce información acomodada y estructurada en los llamados estados financieros. Las finanzas bien pueden estar relacionadas con las matemáticas. Las llamadas matemáticas financieras otorgan las bases de cálculo necesarias para solucionar diversos situaciones financieras como el valor del dinero en el proceso de estimación y posibilidades de ocurrencia. Las finanzas tienen relación con el derecho, ya que el método financiero tiene su soporte en un comulgo de reglas, normas y leyes que se encuentra su marco legal donde operan las finanzas. 

El Antiguo Egipto o Egipto Antiguo fue una civilización de la Antigüedad, que se originó a lo largo del cauce medio y bajo del río Nilo, cuya historia abarca más de tres milenios. Se la considera una de las más importantes de la humanidad.[1]​

El área denominada Antiguo Egipto ha variado a lo largo de los siglos, pero en general se acepta que abarcaba desde el delta del Nilo en el norte, hasta Elefantina, en la primera catarata del Nilo, en el sur. Además controlaba el desierto oriental, la línea costera del mar Rojo, la península del Sinaí, y un gran territorio occidental dominando los dispersos oasis. Históricamente, estaba formado por el Alto y el Bajo Egipto, al sur y al norte respectivamente, que precedieron a la creación de un estado unificado. En su período de mayor expansión controló los reinos amorreos de Palestina y el norte de Siria, llegando hasta el Éufrates medio, y las jefaturas nubias del Sudán, hasta el Jebel Barkal, en la cuarta catarata del Nilo. Ejerció una importante influencia cultural entre los pueblos vecinos, e incluso en regiones tan alejadas como Chipre, la costa de Anatolia y la península helénica. 

La civilización egipcia se desarrolló durante más de 3500 años. Comenzó con la unificación de algunas ciudades del valle del Nilo,[2]​ alrededor del año 3200 a. C.,[3]​ y convencionalmente se da por finalizada en el año 31 a. C., cuando el Imperio romano conquistó y absorbió el Egipto ptolemaico, el cual desapareció como Estado.[4]​ Este acontecimiento no representó el primer período de dominación extranjera en Egipto, pero condujo a una transformación gradual en la vida política y religiosa del valle del Nilo, marcando el final del desarrollo independiente de su identidad cultural. Esta, sin embargo, había comenzado a diluirse paulatinamente tras las conquistas de los persas (siglo VI a. C.) y los macedonios (siglo IV a. C.), especialmente durante el período de los Ptolomeos. La llegada del cristianismo, y su expansión entre los nativos egipcios, cortó uno de las últimas supervivencias de antigua cultura egipcia. En 535, por orden de Justiniano I, fue prohibido el culto a la diosa Isis, en el templo de File con lo cual terminó una religión de más de cuatro milenios. No obstante, el idioma egipcio (llamado copto) siguió siendo utilizado, escrito en un alfabeto derivado del griego, y los egipcios nativos se identificaron plenamente con el cristianismo, en especial con la doctrina monofisita. Surgió entonces una literatura copta, de carácter cristiano, que recogía mitos, costumbres y creencias de la antigua religión tradicional. La desaparición del copto y su sustitución por el árabe, en el marco de la islamización del país después de su conquista, supuso el final definitivo de los últimos restos del Antiguo Egipto.

Egipto tiene una combinación única de características geográficas, situada en el África nororiental y confinada por Libia, Sudán y los mares Rojo y Mediterráneo. El río Nilo fue la clave para el éxito de la civilización egipcia, ya que este permitía el aprovechamiento de los recursos y ofrecía una significativa ventaja sobre otros oponentes: el limo fértil depositado a lo largo de los bancos del Nilo tras las inundaciones anuales significó para los egipcios el practicar una forma de agricultura menos laboriosa que en otras zonas, liberando a la población para dedicar más tiempo y recursos al desarrollo cultural, tecnológico y artístico.

La vida se ordenaba en torno al desarrollo de un sistema de escritura y de una literatura independientes, así como en un cuidadoso control estatal sobre los recursos naturales y humanos, caracterizado sobre todo por la irrigación de la fértil cuenca del Nilo y la explotación minera del valle y de las regiones desérticas circundantes, la organización de proyectos colectivos como las grandes obras públicas, el comercio con las regiones vecinas de África del este y central y con las del Mediterráneo oriental y, finalmente, por un poderío capaz de derrotar a cualquier enemigo, y que mantuvieron una hegemonía imperial y la dominación territorial de civilizaciones vecinas en diversos períodos. La motivación y la organización de estas actividades estaba encomendada a una burocracia de élite sociopolítica y económica, los escribas, bajo el control del Faraón, un personaje semidivino, perteneciente a una sucesión de dinastías, que garantizaba la cooperación y la unidad del pueblo egipcio en el contexto de un elaborado sistema de creencias religiosas.[5]​[6]​

Los muchos logros de los egipcios incluyen la extracción minera, la topografía y las técnicas de construcción que facilitaron el levantamiento de monumentales pirámides, templos y obeliscos, unos procedimientos matemáticos, una práctica médica eficaz, métodos de riego y técnicas de producción agrícola, las primeras naves conocidas,[7]​ la tecnología del vidrio y de la fayenza, las nuevas formas de la literatura y el tratado de paz más antiguo conocido, firmado con los hititas.[8]​ Egipto dejó un legado duradero, su arte y arquitectura fueron ampliamente copiados, y sus antigüedades se llevaron a los rincones más lejanos del mundo. Sus ruinas monumentales han inspirado la imaginación de los viajeros y escritores desde hace siglos. Un nuevo respeto por las antigüedades y excavaciones en la época moderna han llevado a la investigación científica de la civilización egipcia y a una mayor apreciación de su legado cultural.[9]​

El territorio del Antiguo Egipto estaba constituido por el Delta y el valle del río Nilo, una estrecha y larga franja en el noreste de África; un territorio fértil de menos de 60 kilómetros de ancho y 1200 kilómetros de largo, flanqueado en gran parte por el desierto del Sáhara.

El Nilo es uno de los mayores cursos fluviales del mundo. Nace en el África centro oriental (en los lagos Victoria Nyanza, Alberto nyanza y Tana) y desemboca en el mar Mediterráneo conformando el delta del Nilo.

La geografía del Egipto Antiguo es muy significativa e influyó mucho en su cultura. Egipto está situado en el noroeste de África y está muy aislado de otros países por su situación geográfica. Sus límites son: por el oeste, el desierto de Libia; por el este, el desierto de Arabia; por el norte el mar Mediterráneo y por el sur el macizo de Etiopía y el desierto de Nubia. Ese medio natural circundante limitaba los contactos con el exterior, permitiendo que una cultura original se desarrollara sin apenas influencias.

La obtención de una cronología exacta del Antiguo Egipto es una tarea compleja. Existen diversos criterios de datación entre egiptólogos, con divergencias de algunos años en los últimos períodos, de décadas al principio del Imperio Nuevo y de casi un siglo durante el Imperio Antiguo (véase: Cronología del Antiguo Egipto).

El primer problema surge por el hecho de que los egipcios no utilizaron un sistema de datación homogéneo: no tenían un concepto de una era similar al Anno Domini, o la costumbre de nombrar los años, como en Mesopotamia (véase Limmu). Databan con referencia a los reinados de los diversos faraones, solapando posiblemente los interregnos y las épocas de corregencia. Un problema añadido surge al comparar las distintas Listas Reales de faraones, pues están incompletas o con datos contradictorios, incluso en el mismo texto. Las obras del mejor historiador sobre Egipto, Manetón, se perdieron y solo las conocemos a través de epítomes de escritores posteriores como Flavio Josefo, Eusebio de Cesarea, Sexto Julio Africano o el monje Jorge Sincelo. Desafortunadamente las fechas de algunos reinados varían de uno a otro autor.

Las evidencias arqueológicas indican que la civilización egipcia comenzó alrededor del VI milenio a. C., durante el Neolítico, cuando se asentaron los primeros pobladores (véase el periodo predinástico). El río Nilo, en torno al cual se asienta la población, ha sido la línea de referencia para la cultura egipcia desde que los nómadas cazadores-recolectores comenzaron a vivir en sus riberas durante el pleistoceno. Los rastros de estos primeros pobladores quedaron en los objetos y signos grabados en las rocas a lo largo del valle del Nilo y en los oasis.

A lo largo del Nilo, en el XI milenio a. C., una cultura de recolectores de grano había sido sustituida por otra de cazadores, pescadores y recolectores que usaban herramientas de piedra. Los estudios también indican asentamientos humanos en el sudoeste de Egipto, cerca de la frontera con Sudán, antes del 8000 a. C. La evidencia geológica y estudios climatológicos sugieren que los cambios del clima, alrededor del 8000 a. C., comenzaron a desecar las tierras de caza y pastoreo de Egipto, conformándose paulatinamente el desierto del Sáhara. Las tribus de la región tendieron a agruparse cerca del río, en donde surgieron pequeños poblados que desarrollaron una economía agrícola. Hay evidencias de pastoreo y del cultivo de cereales en el este del Sáhara en el VII milenio a. C.

Alrededor del 6000 a. C., ya había aparecido en el valle del Nilo la agricultura organizada y la construcción de grandes poblados. Al mismo tiempo, en el sudoeste se dedicaban a la ganadería y también construían. El mortero de cal se usaba en el 4000 a. C. Es el denominado periodo predinástico, que empieza con la cultura de Naqada.

Entre el 5500 y el 3100 a. C., durante el Predinástico, los asentamientos pequeños prosperaron a lo largo del Nilo. En el 3300 a. C., momentos antes de la primera dinastía, Egipto estaba dividido en dos reinos, conocidos como Alto Egipto Ta Shemau y Bajo Egipto Ta Mehu.[10]​ La frontera entre ambos se situaba en la actual zona de El Cairo, al sur del delta del Nilo.

La historia de Egipto, como Estado unificado, comienza alrededor del 3050 a. C. Menes (Narmer), que unificó el Alto y el Bajo Egipto, fue su primer rey. La cultura y costumbres egipcias fueron notablemente estables y apenas variaron en casi 3000 años, incluyendo religión, expresión artística, arquitectura y estructura social.

La cronología de los reyes egipcios da comienzo en esa época. La cronología convencional es la aceptada durante el siglo XX, sin incluir cualquiera de las revisiones que se han hecho en ese tiempo. Incluso en un mismo trabajo, los arqueólogos ofrecen a menudo, como posibles, varias fechas e incluso varias cronologías, y por ello puede haber discrepancias entre las fechas mostradas en las distintas fuentes. También se dan varias posibles transcripciones de los nombres. Tradicionalmente la egiptología clasifica la historia de la civilización faraónica dividida en dinastías, siguiendo la estructura narrativa de los epítomes de la Aigyptiaká (Historia de Egipto), del sacerdote egipcio Manetón.

Los primeros pobladores de Egipto alcanzaron las riberas del río Nilo, por entonces un conglomerado de marismas y foco de paludismo, en su huida de la creciente desertización del Sáhara.

Se sabe por los restos arqueológicos que antiguamente el Sáhara tenía un clima mediterráneo, más húmedo que el actual. En los macizos del Ahaggar y el Tibesti había abundante vegetación. Para aquellos pobladores, el Sáhara sería una extensa estepa con grandes herbívoros que cazar. Las culturas saharianas son, en gran medida, desconocidas, pero no por ello inexistentes.

Las sucesivas fases del neolítico están representadas por las culturas de El Fayum, hacia el 5000 a. C., la cultura tasiense, hacia el 4500 a. C. y la cultura de Merimde, hacia el 4000 a. C. Todas ellas conocen la piedra pulimentada, la cerámica, la agricultura y la ganadería. La base de la economía era la agricultura que se realizaba aprovechando el limo, fertilizante natural que aportaban las anuales inundaciones del río Nilo.

Tras estas culturas aparecieron la badariense y la amratiense o Naqada I, entre 4000 y 3800 a. C.

Hacia el año 3600 a. C. surge la gerzeense o Naqada II, que se difunde por todo Egipto, unificándolo culturalmente. Esta consonancia cultural llevará a la unidad política, que surgirá tras un periodo de luchas y alianzas entre clanes para imponer su supremacía.

Para lograr mayor eficacia y producción, hacia 3500 a. C., comenzaron a realizarse las primeras obras de canalización y surge la escritura con jeroglíficos en Abidos. En esta época comenzaron los proto-estados:
Las primeras comunidades hicieron habitable el país y se organizaron en regiones llamadas nomos. Los habitantes del Delta tenían una organización feudal y llegaron a establecer dos reinos con dos jefes o monarcas respectivamente. Un reino estaba asentado en un lugar pantanoso, que se llamaba reino del Junco y tenía como símbolo un tallo de junco. Su capital era Buto; tenían a una cobra como tótem. El otro reino tenía como capital a Busiris y como tótem un buitre pero su símbolo era una abeja y llegó a conocerse como reino de la Abeja. Ambos reinos estaban separados por un brazo del río Nilo.

El reino de la Abeja conquistó al reino del Junco de manera que el Delta quedó unificado. Pero algunos de los vencidos huyeron a establecerse en la zona del Alto Egipto donde fundaron ciudades dándoles el mismo nombre que aquellas que habían dejado en el Delta. Por eso muchas ciudades de esta época tienen nombres semejantes en el Alto y Bajo Egipto. Esta gente fue prosperando considerablemente hasta llegar a organizarse en un Estado.

Considerado la fase final del periodo predinástico, también conocido como dinastía 0, predinástico tardío, o periodo Naqada III. Está regido por gobernantes del Alto Egipto que residirán en Tinis, se hacen representar con un serej y adoran a Horus. El nombre de estos reyes figura en la Piedra de Palermo, grabada 700 años después. En este periodo surgen las primeras auténticas ciudades, tales como Tinis, Nubet, Nejeb, Nejen, etc. Son típicos de esta época los magníficos vasos tallados en piedra, cuchillos y paletas ceremoniales, o las cabezas de mazas votivas. Narmer pudo ser el último rey de esta época, y el fundador de la dinastía I.

A finales del periodo predinástico, Egipto se encontraba dividido en pequeños reinos; los principales eran: el de Hieracómpolis (Nejen) en el Alto Egipto y el de Buto (Pe) en el Bajo Egipto. El proceso de unificación fue llevado a cabo por los reyes de Hieracómpolis.

La tradición egipcia atribuyó la unificación a Menes, quedando esto reflejado en las Listas Reales. Este personaje es, según Alan Gardiner, el rey Narmer, el primer faraón del cual se tiene constancia que reinó todo Egipto, tras una serie de luchas, tal como quedó atestiguado en la paleta de Narmer. Este periodo lo conforman las dinastías I y II.

Bajo la dinastía III la capital se estableció definitivamente en Menfis, de donde procede la denominación del país, ya que el nombre del principal templo, Hat Ka Ptah «casa del espíritu de Ptah», que pasó al griego como Aegyptos, con el tiempo designó primero al barrio en el que se encontraba, luego a toda la ciudad y más tarde al reino.

En la época de la tercera dinastía comenzó la costumbre de erigir grandes pirámides y monumentales conjuntos en piedra, gracias al faraón Dyeser. También las grandes pirámides de Guiza, atribuidas a los faraones Keops, Kefrén y Micerino se datan en este periodo.

La dinastía V marca el ascenso del alto clero y los influyentes gobernadores locales (nomarcas), y durante el largo reinando de Pepy II se acentuará una época de fuerte descentralización, denominada primer periodo intermedio de Egipto. El Imperio Antiguo comprende las dinastías III a VI.

Fue una época donde el poder estaba descentralizado y transcurre entre el Imperio Antiguo y el Imperio Medio. Comprende desde la Dinastía VII hasta mediados de la Dinastía XI, cuando Mentuhotep II reunificó el país bajo su mando. A pesar de la decadencia, esta época destacó por un gran florecimiento literario, con textos doctrinales o didácticos, que muestran el gran cambio social. El importante cambio de mentalidad, así como del crecimiento de las clases medias en las ciudades originó una nueva concepción de las creencias, reflejándose en la aparición de los denominados Textos de los Sarcófagos. Osiris se convirtió en la divinidad más popular, con Montu y Amón.
Los nomos de Heracleópolis y Tebas se constituyeron como hegemónicos, imponiéndose finalmente este último. Son las dinastías VII a XI.

Se considera que se inicia con la reunificación de Egipto bajo Mentuhotep II. Es un periodo de gran prosperidad económica y expansión exterior, con faraones pragmáticos y emprendedores. Este periodo lo conforma el final de la dinastía XI y la XII.

Se realizaron ambiciosos proyectos de irrigación en El Fayum, para regular las grandes inundaciones del Nilo (Provocadas por las grandes masas de agua del mar Mediterráneo evaporadas en los desiertos cercanos al imperio), desviándolo hacia el lago Moeris (El Fayum). También se potenciaron las relaciones comerciales con las regiones circundantes: africanas, asiáticas y mediterráneas. Las representaciones artísticas se humanizaron, y se impuso el culto al dios Amón. A mediados de 1800 a. C., los dirigentes hicsos vencieron a los faraones egipcios; lo que comenzó como una migración paulatina de libios y cananeos hacia el delta del Nilo, se transformó con el tiempo en conquista militar de casi todo el territorio egipcio, originando la caída del Imperio Medio. Los hicsos vencieron porque poseían mejores armas, y supieron utilizar el factor sorpresa.

Durante gran parte de este periodo dominaron Egipto los gobernantes hicsos, jefes de pueblos nómadas de la periferia, especialmente libios y asiáticos, que se establecieron en el delta, y tuvieron como capital la ciudad de Avaris. Finalmente, los dirigentes egipcios de Tebas declararon la independencia, siendo denominados la dinastía XVII. Proclamaron la «salvación de Egipto» y dirigieron una «guerra de liberación» contra los hicsos. Fueron las dinastías XIII a XVII, parcialmente coetáneas.

Es un periodo de gran expansión exterior, tanto en Asia —donde llegan al Éufrates— como en Kush (Nubia). La dinastía XVIII comenzó con una serie de faraones guerreros, desde Amosis I hasta Tutmosis III y Tutmosis IV. Bajo Amenofis III se detuvo la expansión y se inició un período de paz interna y externa.

Después de un período de debilidad monárquica, llegaron al poder las castas militares, la dinastía XIX o Ramésida que, fundamentalmente bajo Seti I y Ramsés II, se mostró enérgica contra los expansionistas reyes hititas.

Durante los reinados de Merenptah, sucesor de Ramsés II, y Ramsés III, de la dinastía XX, Egipto tuvo que enfrentarse a las invasiones de los pueblos del mar, originarios de diversas áreas del Mediterráneo oriental (Egeo, Anatolia), y de los libios.

Los faraones del Imperio Nuevo iniciaron una campaña de construcción a gran escala para promover al dios Amón, cuyo creciente culto se asentaba en Karnak. También construyeron monumentos para glorificar a sus propios logros, tanto reales como imaginarios. Hatshepsut utilizará tal hipérbole durante su reinado de casi veintidós años que fue muy exitoso, marcado por un largo período de paz y prosperidad, con expediciones comerciales a Punt, la restauración de las redes de comercio exterior, grandes proyectos de construcción, incluyendo un elegante templo funerario que rivaliza con la arquitectura griega de mil años más tarde, obeliscos colosales y una capilla en Karnak.




A pesar de sus logros, el heredero de Hatshepsut, su hijastro Tutmosis III, trató de borrar toda huella de su legado hacia el final del reinado, apropiándose de muchos de sus logros. Él también intentó cambiar muchas tradiciones establecidas que se habían desarrollado a lo largo de siglos. Posiblemente fue un intento inútil de evitar que otras mujeres se convirtiesen en faraón y frenar así su influencia en el reino.

Alrededor de 1350 a. C., la estabilidad del Imperio parecía amenazada, aún más cuando Amenhotep IV ascendió al trono e instituyó una serie de reformas radicales, que tuvieron un resultado caótico. Cambiando su nombre por el de Ajenatón, promovió como deidad suprema la hasta entonces oscura deidad solar Atón, iniciando una reforma religiosa tendente al monoteísmo. En parte, el monoteísmo de Ajenatón fue un producto del absolutismo real; los viejos dioses habían desaparecido, pero el rey mantenía —para su propio beneficio político— su papel tradicional como mediador entre los hombres y los deseos del nuevo dios. El faraón suprimió el culto a la mayoría de las demás deidades y, sobre todo, trató de anular el poder de los influyentes sacerdotes de Amón en Tebas, a quienes veía como corruptos. Al trasladar la capital a la nueva ciudad de Ajet-Atón (actual Amarna), Ajenatón hizo oídos sordos a los acontecimientos del Cercano Oriente (donde los hititas, Mitanni y los asirios se disputaban el control) y se concentró únicamente en la nueva religión. La nueva filosofía religiosa conllevó un nuevo estilo artístico, que resaltaba la humanidad del rey por encima de la monumentalidad.

Después de su muerte, el culto de Atón fue abandonado rápidamente, los sacerdotes de Amón recuperaron el poder y devolvieron la capital a Tebas. Bajo su influencia los faraones posteriores —Tutankamon, Ay y Horemheb— intentaron borrar toda mención de Akenatón y su «herejía», ahora conocida como el Período de Amarna.

Alrededor de 1279 a. C. ascendió al trono Ramsés II, también conocido como el Grande. El suyo sería uno de los reinados más largos de la historia egipcia. Mandó construir más templos, más estatuas y obeliscos, y engendrar más hijos que cualquier otro faraón. Audaz líder militar, Ramsés II condujo su ejército contra los hititas en la batalla de Kadesh (en la actual Siria); después de llegar a un punto muerto, finalmente aceptó un tratado de paz con el reino hitita. Es el tratado de paz más antiguo registrado, en torno a 1258 antes de Cristo. Egipto se retiró de la mayor parte de sus posesiones asiáticas dejando a los hititas competir, sin éxito, con el creciente poder emergente de Asiria y los recién llegados frigios.

La riqueza de Egipto, sin embargo, se había convertido en un objetivo tentador para la invasión; en particular, para los libios beduinos del oeste y los pueblos del mar, que formaban parte de la poderosa confederación de piratas griegos del mar Egeo. Inicialmente, el ejército fue capaz de repeler las invasiones, pero Egipto terminó por perder el control de sus territorios en el sur de Siria y Palestina, que en gran parte cayeron en poder de los asirios e hititas. El impacto de las amenazas externas se vio agravado por problemas internos como la corrupción, el robo de las tumbas reales y los disturbios populares. Después de recuperar su poder, los sumos sacerdotes del templo de Amón en Tebas habían acumulado vastas extensiones de tierra y mucha riqueza, debilitando al Estado. El país terminó dividido, dando inicio al Tercer Periodo Intermedio.

Comienza con la instauración de dos dinastías de origen libio que se repartieron Egipto: una, desde Tanis, la bíblica Zoán, en el Bajo Egipto, y otra, cuyos reyes tomaron el título de Sumos sacerdotes de Amón, desde Tebas. El periodo termina con la dominación de los reyes Cushitas. Son las dinastías, parcialmente coetáneas, XXI a XXV.

Comienza con la dinastía Saíta, sigue una dinastía nubia, un intento de invasión asirio y con dos periodos de dominación persa, así como con varias dinastías coetáneas de gobernantes egipcios independientes. Egipto se convirtió finalmente en una satrapía. Son las dinastías XXVI a XXXI.

Se inicia con la conquista de Egipto por Alejandro Magno de Macedonia en 332 a. C., y la llegada al poder en 305 a. C. de la dinastía ptolemaica, de origen macedonio. Finaliza con la incorporación de Egipto al Imperio romano tras la batalla de Actium, en el año 31 a. C.
En el año 30 a. C. muere Cleopatra y Egipto se convierte en una provincia del Imperio romano.

El 30 de julio del año 30 a. C., Octavio entró en Alejandría, liquidando definitivamente la independencia política de Egipto y convirtiéndolo en provincia romana.

Pasó a sus sucesores el Imperio bizantino después que el Imperio romano fuera repartido el año 395 en Occidente y Oriente, y permaneció en sus manos hasta la conquista por el pueblo árabe del año 640. Los últimos vestigios de la tradicional cultura del Antiguo Egipto finalizan definitivamente a comienzos del s. VI d. C., con los últimos sacerdotes de Isis, que oficiaban el templo de la isla de File, al proscribirse el culto a los «dioses paganos».

La sociedad egipcia estaba jerarquizada en tres niveles:  

El antiguo Egipto se organizaba en 2 reinos, el Alto y el Bajo Egipto. 

A partir del año 3000 a.C. se unificaron en un solo reino que tenía un gobierno monárquico, absolutista y teocrático:

El faraón era la representación de dios en la tierra y todo Egipto le pertenecía: tierras, cosechas, comercio.

Algunas funciones del faraón eran:

La economía de Egipto se basaba en la agricultura y la ganadería. La vida dependía de los cultivos de las tierras inundadas por el río Nilo. Tenían un sistema de diques, estanques y canales de riego que se extendían por todas las tierras de cultivo. En las riberas del Nilo los campesinos egipcios cultivaban muchas clases de cereales. El grano cosechado se guardaba en graneros y luego se usaba para elaborar pan y cerveza. Las cosechas principales eran de trigo, cebada y lino.

La agricultura estaba centrada en el ciclo del Nilo.Había cuatro estaciones: ¨perla blanca¨(es cuando crece el Nilo y se desborda produciendo una gran inundación, también traía lodo fértil),¨perla negra¨(es cuando el Nilo baja y deja barro negro en la orilla[que se usa en la agricultura]),¨La esmeralda verde¨(es cuando brotan las cosechas)y¨oro rojo¨( es cuando los sembríos maduran).

En los huertos se cultivaban guisantes (arveja), lentejas, cebolla, puerros, pepinos y lechugas, además de uvas, dátiles, higos y granada. Entre los animales que criaban por su carne, se encuentran los cerdos, vacas, ovejas, cabras, gansos y patos.

Los egipcios cultivaban más alimentos de los que necesitaban, y hacían intercambio de sus productos. Algunas de las materias que ellos importaban de territorios extranjeros eran el incienso, la plata, y madera fina de cedro. Gran parte de los productos del comercio egipcio se transportaba en barcos, por el Nilo y el Mediterráneo.

Durante la mayor parte de su existencia, unos tres milenios, el Antiguo Egipto fue el país más rico del mundo.

Las transacciones comerciales de los antiguos egipcios no se limitaban al intercambio de productos agrícolas o de materias primas, sino que también hay constancia de expediciones para nutrir de bienes ornamentales y joyas el tesoro real de los faraones, y de actividades de venta de esclavos, e incluso de los propios cargos administrativos o de servicio en los templos.

En el Antiguo Egipto existía la figura de los shutiu, una especie de agentes comerciales que efectuaban actividades de compraventa al servicio de las grandes instituciones faraónicas (templos, palacio real, grandes explotaciones de la corona, etc...). Pero también podían vender esclavos a simples particulares, o podían realizar transacciones comerciales al margen de las instituciones en provecho propio.

Las casi 200 tablillas de arcilla y las numerosas inscripciones descubiertas por los arqueólogos en la antigua ciudad de Balat demuestran que esta localidad, situada en pleno Sahara egipcio, fue utilizada como base de operaciones y punto de abastecimiento a las expediciones comerciales enviadas por los faraones hacia el corazón de África a finales del tercer milenio a. C. Desde este enclave en el oasis de Dajla partirían expediciones, compuestas por unos 400 hombres, cuyo objeto era buscar un pigmento que una vez obtenido se enviaba mediante caravanas al valle del Nilo.

La ruta estaría marcada desde épocas antiquísimas como prueba la presencia de depósitos de jarras situados a intervalos de 30 kilómetros en el desierto, que llegan hasta Gilf el-Kebir en el extremo sudoccidental de Egipto. Se desconoce hasta dónde llegaba la ruta, aunque los especialistas aceptan como hipótesis más probable que llegase hasta la zona del lago Chad.[11]​

Egipto estaba dividido en varios sepats (provincias, o nomos en griego) con fines administrativos. Esta división se puede remontar de nuevo al período predinástico (antes de 3100 a. C.), cuando los nomos eran ciudades-estados autónomas, y permanecieron por más de tres milenios, manteniendo sus costumbres. Bajo este sistema, el país fue dividido en 42 nomos: 20 del Bajo Egipto, mientras que el Alto Egipto abarcaba 22 nomos. Cada nomo estaba gobernado por un nomarca, gobernador provincial que ostentaba la autoridad regional.

El gobierno impuso diversos impuestos, que al no existir moneda eran pagados en especie, con trabajo o mercancías. El Tyaty (visir) era el responsable de controlar el sistema impositivo en nombre del faraón, a través de su departamento. Sus subordinados debían tener al día las reservas almacenadas y sus previsiones. Los impuestos se pagaban según el trabajo o las rentas de cada uno, los campesinos (o los terratenientes en periodos posteriores) en productos agrícolas, los artesanos con parte de su producción, y de forma similar los pescadores, cazadores, etc.

El estado requería una persona de cada casa para realizar trabajos públicos algunas semanas al año, haciendo o limpiando canales, en la construcción de templos o tumbas e incluso en la minería (esto último, solo si no había prisioneros de guerra). Los cazadores y pescadores pagaban sus impuestos con capturas del río, de los canales, y del desierto. Las familias acomodadas podían contratar sustitutos para poder satisfacer este derecho.

El egipcio antiguo constituye una parte independiente de la lengua de la (macro) familia afro-asiática. Sus parientes más cercanos son los grupos bereber, semítico y Beja. Los documentos escritos más antiguos en lengua egipcia se han fechado en el 3200 a. C., haciéndola una de las más antiguas y documentadas. Los eruditos agrupan al egipcio en siete divisiones cronológicas importantes:

Recogido en las inscripciones del último predinástico y del arcaico. La evidencia más temprana de escritura jeroglífica egipcia aparece en los recipientes de cerámica de Naqada II.

Es la lengua del Imperio Antiguo y del primer período intermedio. Los textos de las pirámides son el cuerpo mayor de la literatura de esta fase, escritos en las paredes de las tumbas de la aristocracia, que a partir de este período también muestran escrituras autobiográficas. Una de las características que lo distinguen es la triple mezcla de ideogramas, fonogramas, y de determinativos para indicar el plural. No tiene grandes diferencias con la etapa siguiente.

Esta etapa, llamada también media, se conoce por una variedad de textos en escritura jeroglífica y hierática, datadas en el Imperio Medio. Incluyen los textos funerarios inscritos en los ataúdes tales como los Textos de los Sarcófagos; textos que explican cómo conducirse en la otra vida, y que ejemplifican el punto de vista filosófico egipcio (véase el papiro de Ipuur); cuentos que detallan las aventuras de ciertos individuos, por ejemplo la historia de Sinuhe; textos médicos y científicos tales como el papiro Edwin Smith y el de Ebers; y textos poéticos que elogian a un dios o a un faraón, tal como el himno al Nilo. El idioma vernáculo comenzó a diferenciarse de la lengua escrita tal como evidencian algunos textos hieráticos del Imperio medio, pero el egipcio clásico continuó siendo usado en los escritos formales hasta el último período dinástico.

Aparecen documentos de esta etapa en la segunda parte del Imperio Nuevo. Forman un amplio conjunto de textos de literatura religiosa y secular, abarcando ejemplos famosos tales como la historia de Unamón (Wenamun) y las instrucciones del Ani. Era la lengua de la administración ramésida. No es totalmente distinto del egipcio medio, ya que aparecen muchos clasicismos en los documentos históricos y literarios de esta fase, sin embargo, la diferencia entre el clásico y el tardío es mayor que entre aquel y el antiguo. También representa mejor la lengua hablada desde el Imperio Nuevo. La ortografía jeroglífica consiguió una gran expansión de su inventario gráfico entre el periodo Tardío y el Ptolemaico.

La lengua demótica es cronológicamente la última, se comenzó a usar alrededor del 660 a. C. y se convirtió en la escritura dominante cerca del 600 a. C., usándose con fines económicos y literarios. En contraste con el hierático, que solía escribirse en papiros u ostracas, el demótico se grababa además en piedra y madera.


En los textos escritos en etapas anteriores, probablemente representó el idioma hablado de la época. Pero al ser utilizada cada vez más solamente con propósitos literarios y religiosos, la lengua escrita divergió cada vez más de la forma hablada, dando a los últimos textos demóticos un carácter artificial, similar al uso del egipcio medio clásico durante el período Ptolemaico. A inicios del siglo IV comenzó a ser reemplazado por el idioma griego en los textos oficiales: el último uso que se conoce es en el año 452 d. C., sobre los muros del templo dedicado a Isis, en File. Comparte mucho con la lengua copta posterior. 
Fue el idioma de la corte tras la conquista de Alejandro, el dialecto koiné, «lengua común», que era una variante del ático utilizada en el mundo helenístico, y que en Egipto convivió con el copto empleado por el pueblo llano.

Está testimoniado alrededor del siglo III, y aparece escrito con signos jeroglíficos, o en los alfabetos hierático y demótico. El alfabeto copto es una versión ligeramente modificada del alfabeto griego, con algunas letras propias demóticas utilizadas para representar varios sonidos no existentes en el griego. Como lengua cotidiana tuvo su apogeo desde el siglo III hasta el siglo VI, y perdura solo como lengua litúrgica de la Iglesia Ortodoxa Copta tras ser sustituido por el árabe en época islámica.

Durante años, la inscripción conocida más antigua era la Paleta de Narmer, encontrada durante excavaciones en Hieracómpolis (nombre actual, Kom el-Ahmar) en 1890, datada en el 3150 a. C. Hallazgos arqueológicos recientes revelan que los símbolos grabados en la cerámica de Gerzeh, del año 3250 a. C., se asemejan al jeroglífico tradicional. En 1998 un equipo arqueológico alemán bajo el mando de Günter Dreyer, que excavaba la tumba U-j en la necrópolis de Umm el-Qaab de Abidos, que perteneció a un rey del predinástico, recuperó trescientos rótulos de arcilla inscritos con jeroglíficos y fechados en el período de Naqada III-a, en el siglo XXXIII a. C.[12]​

Según investigaciones, la escritura egipcia apareció hacia el 3000 a. C. con la unificación del Reino del Alto y Bajo Egipto y el advenimiento del Estado. Durante largo tiempo solo estuvo compuesta por unos mil signos, los jeroglíficos, que representaban personas, animales, plantas, objetos estilizados etc. Su número no llegó a alcanzar varios miles hasta el periodo tardío.[13]​

Los egiptólogos definen al sistema egipcio como jeroglífico, y se considera como la escritura más antigua del mundo. La denominación proviene del griego hieros («sagrado») y glypho («esculpir, grabar»). Era en parte silábica, en parte ideográfica. La hierática fue una forma cursiva de los jeroglíficos y comenzó a utilizarse durante la primera dinastía (c. 2925-2775 a. C.). El término demótico, en el contexto egipcio, se refiere a la escritura y a la lengua que evolucionó durante el periodo tardío, es decir desde la 25.ª dinastía Nubia, hasta que fue desplazada en la corte por el Koiné griego en las últimas centurias a. C. Después de la conquista por Amr ibn al-As en el año 640, el idioma egipcio perduró en la lengua copta durante la Edad Media.

Alrededor del 2700 a. C., se comenzaron a usar pictogramas para representar sonidos consonantes. Sobre el 2000 a. C., se usaban 26 para representar los 24 sonidos consonantes principales. El más antiguo alfabeto conocido (c. 1800 a. C.) es un sistema abyad derivado de esos signos unilíteros, igual que otros jeroglíficos egipcios.

La escritura jeroglífica finalmente cayó en desuso como escritura de los cortesanos alrededor del siglo IV a. C, bajo los ptolomeos, sustituida por el griego, aunque perduró en los templos del Alto Egipto, custodiados por el clero egipcio. Cleopatra VII fue la única gobernante ptolemaica que dominó el idioma egipcio antiguo. Las tentativas de los europeos para descifrarla comenzaron en el siglo XV, aunque hubo tentativas anteriores por parte de eruditos árabes.

La religión egipcia, plasmada en la mitología, es un conjunto de creencias que impregnaban toda la vida egipcia, desde la época predinástica hasta la llegada del cristianismo y del islamismo en las etapas grecorromanas y árabe. Eran dirigidos por sacerdotes, y el uso de la magia y los hechizos son dudosos.

El templo era un lugar sagrado en donde solamente se admitía a los sacerdotes y sacerdotisas, aunque en las celebraciones importantes el pueblo era admitido en el patio.

La existencia de momias y pirámides fuera de Egipto, indica que las creencias y los valores de las cultura egipcia se transmitieron de una u otra forma por las rutas comerciales. Los contactos de Egipto con extranjeros incluyeron Nubia y Punt al sur, el Egeo y Grecia al norte, el Líbano y otras regiones del Oriente Próximo y Libia al oeste.

La naturaleza religiosa de la civilización egipcia influenció su contribución a las artes. Muchas de las grandes obras del Egipto antiguo representan dioses, diosas, y faraones, considerados divinos. El arte está caracterizado por la idea del orden y la simetría.

Durante los 3000 años de cultura independiente, cada animal retratado o adorado en el arte, la escritura o la religión es indígena de África. El dromedario, domesticado en Arabia, apareció en Egipto al comienzo del II milenio a. C.

Aunque el análisis del cabello de momias del Imperio Medio ha revelado evidencias de una dieta estable, las momias de circa 3200 a. C. muestran señales de anemia y desórdenes hemolíticos, síntomas del envenenamiento por metales pesados. Los compuestos de cobre, plomo, mercurio, y arsénico que fueron utilizados en pigmentos, tintes y maquillaje de la época pudieron haber causado el envenenamiento, especialmente entre la clase acomodada.[14]​[15]​

Creían en una vida de ultratumba, y se preparaban para ella, tanto siguiendo unas normas determinadas (Libro de los muertos) como preparando la tumba y el cadáver.

Creían que después de la muerte, el ka (doble en forma de espíritu) se dividía en ba (alma) y akh (espíritu). El ba vivía en la tumba del difunto y era libre de ir y venir a voluntad. El akh se dirigía directamente al inframundo donde seguía su juicio. El gran dios del inframundo Osiris se encargaba de juzgar el espíritu del difunto. Anubis colocaba el corazón del difunto en un lado de su balanza y Ma'at, la diosa de la verdad y la justicia, ponía su pluma de la verdad en el otro lado. Si el corazón y la pluma pesaban lo mismo, el akh (espíritu) se iba al gran reino en donde los buenos espíritus se mezclaban con los dioses en una vida de paz y armonía. Si no era así el difunto sufriría una eternidad de castigo. Además los egipcios creían que todo difunto debía tener una casa en su otra vida, era por esto que les construían pirámides e hipogeos a los cadáveres. También como creían que la segunda vida era casi igual a la primera y uno seguía haciendo lo mismo que en la primera, les dejaban en las tumbas sus joyas y alhajas, ropas, alimentos y juegos. El otro temor de los egipcios (además del juicio de sus almas) era que alguien saqueara la casa de su espíritu. Si su tumba era saqueada o su cadáver destruido, el ba se quedaba sin hogar y tanto este como el akh experimentarían una segunda muerte mucho peor. A veces se colocaban estatuas del difunto en las pirámides por si el ba se quedaba sin hogar, permaneciera en la estatua y evitara la segunda muerte.[16]​

Antiguamente solamente los faraones tenían derecho a participar en la vida futura, pero al llegar el nuevo imperio todos los egipcios esperaban vivir en el más allá, y se preparaban, de acuerdo a sus posibilidades económicas, su tumba y su cuerpo; a los cadáveres se le extraían los órganos, que eran depositados en los vasos canopos, y después cubrían el cuerpo con resinas para preservarlo, envolviéndolo con lino. En la cámara funeraria se depositaban alimentos y pertenencias del fallecido, para su uso en la otra vida.

Los logros del Antiguo Egipto están bien estudiados, así como su civilización que alcanzó un nivel muy alto de productividad y complejidad.

Un Estado es una organización política constituida por un conjunto de instituciones burocráticas estables, a través de las cuales ejerce el monopolio del uso de la fuerza (soberanía) aplicada a una población dentro de unos límites territoriales establecidos.[1]​[2]​[3]​[4]​[5]​[6]​[7]​

Muchas sociedades humanas han sido gobernadas por Estados durante milenios; sin embargo, la mayoría de las personas en la prehistoria vivían en sociedades sin Estado. Los primeros Estados surgieron hace unos 5500 años junto con el rápido crecimiento de las ciudades, la invención de la escritura, y la codificación de nuevas clases de religión. Con el tiempo, se desarrolló una variedad de formas diferentes de Estados, empleando una variedad de justificaciones para su existencia (como el derecho divino, la teoría del contrato social, etc.). Hoy día, sin embargo, el Estado-nación moderno es la forma predominante de Estado a que están sometidas las personas.

La palabra Estado viene del latín status,[8]​ y este del verbo stare (estar parado).[9]​ De ahí pasó a significar a algo parado, detenido, como en statu quo. El verbo stare se vincula con la raíz indoeuropea *sta-, presente en el verbo griego ίσταμαι (histamai, que se puede traducir como: establecer, poner en pie, detener, estar en pie).

Como término polisémico designa también a todo aquel país soberano, reconocido como tal en el orden internacional, así como al conjunto de atribuciones y órganos de gobierno de dicho país.[8]​

Todo Estado está dotado de territorio, población y soberanía.[10]​

El concepto de Estado difiere según los autores,[11]​ pero algunos de ellos definen el Estado como el conjunto de instituciones que poseen la autoridad y potestad para establecer las normas que regulan una sociedad, teniendo soberanía interna y externa sobre un territorio determinado.

La definición más comúnmente utilizada es la de Max Weber, en 1919, define Estado moderno como una «asociación de dominación con carácter institucional que ha tratado, con éxito, de monopolizar dentro de un territorio el monopolio de la violencia legítima como medio de dominación y que, con este fin, ha reunido todos los medios materiales en manos de sus dirigentes y ha expropiado a todos los seres humanos que antes disponían de ellos por derecho propio, sustituyéndolos con sus propias jerarquías supremas».[12]​ Las categorías generales del Estado son instituciones tales como las fuerzas armadas, burocracias administrativas, los tribunales y la policía, asumiendo pues el Estado las funciones de defensa, gobernación, justicia, seguridad y otras, como las relaciones exteriores.

Probablemente la definición más clásica de Estado, fue la citada por el jurista alemán Hermann Heller que define al Estado como una «unidad de dominación, independiente en lo exterior e interior, que actúa de modo continuo, con medios de poder propios, y claramente delimitado en lo personal y territorial». Además, el autor define que sólo se puede hablar de Estado como una construcción propia de las monarquías absolutas (ver monarquía absoluta) del siglo xv, de la Edad Moderna. «No hay Estado en la Edad Antigua», señala el autor.[13]​
Asimismo, cómo evolución del concepto se ha desarrollado el Estado de derecho por el que se incluyen dentro de la organización estatal aquellas resultantes del imperio de la ley y la división de poderes (ejecutivo, legislativo y judicial) y otras funciones que emanan directamente de la nación, como la emisión de moneda propia. 

Otra definición comúnmente aceptada del estado es la que se dio en la Convención de Montevideo sobre Derechos y Deberes de los Estados en 1933. Definió el Estado como un espacio que posee lo siguiente: una población permanente, un territorio definido y un gobierno que es capaz de mantener control efectivo sobre el territorio correspondiente y de conducir relaciones internacionales con otros estados.

Confundiendo el problema de definición es que "estado" y "gobierno" a menudo se usan como sinónimos en una conversación común e incluso en algunos discursos académicos. Según este esquema de definición, los estados son personas jurídicas de derecho internacional, los gobiernos son organizaciones de personas. La relación entre un gobierno y su estado es de representación y agencia autorizada.

Max Weber escribió en su libro La política como vocación que una característica fundamental del Estado es el reclamo del monopolio de la violencia. Su definición ampliada era que algo es «un» Estado «si y en la medida en que su personal administrativo defiende con éxito un reclamo sobre el 'monopolio del uso legítimo de la fuerza física' en la ejecución de su orden».[15]​[16]​ La policía pública y el ejército son sus principales instrumentos, pero también se puede considerar que la seguridad privada tiene el «derecho» de usar la violencia «siempre que la única fuente de este derecho percibido sea la sanción estatal».

La capacidad de un estado a menudo se mide en términos de su capacidad fiscal y legal. Capacidad fiscal significa la capacidad del estado para recuperar los impuestos para proporcionar bienes públicos, y la capacidad legal que significa la supremacía del estado como único árbitro de la resolución de conflictos y la ejecución de contratos. Sin algún tipo de coacción, el estado no podría de otro modo hacer valer su legitimidad en la esfera de influencia deseada. En los estados tempranos y en desarrollo, este papel lo desempeñaba a menudo el "bandido estacionario" que defendía a los aldeanos de los bandidos errantes, con la esperanza de que la protección incentivara a los aldeanos a invertir en la producción económica, con la esperanza de que el bandido estacionario eventualmente pueda usar su poder coercitivo para expropiar parte de esa riqueza.[17]​

En regiones donde la presencia del estado se siente mínimamente, los actores no estatales pueden usar su monopolio de la violencia para establecer la legitimidad y el orden.[18]​ Por ejemplo, la mafia siciliana se originó como una banda de protección que brindaba protección a compradores y vendedores en el mercado negro. Sin este tipo de ejecución, los participantes en el mercado no tendrían la confianza suficiente para confiar en que sus contrapartes cumplirían con los contratos vigentes y el mercado colapsaría.

Incluso en los mercados ilícitos y clandestinos (algo parecido a las sociedades sin estado), la violencia se utiliza para hacer cumplir los contratos en ausencia de una solución de conflicto legal accesible.[19]​ Charles Tilly continúa esta comparación para decir que la guerra y la construcción del estado son en realidad las mejores representaciones de lo que puede convertirse el crimen organizado.[20]​

Antes de Maquiavelo, los conceptos que referían al estado muchas veces se confundían la Dinastía gobernante con el aparato estatal. Esto lo podemos ver por ejemplo en Ibn Jaldún en su Almuqqadima.

En los Diálogos de Platón, se narra la estructura del Estado ideal, pero es Maquiavelo quien introdujo la palabra Estado en su célebre obra El Príncipe: usando el término de la lengua italiana «Stato», evolución de la palabra «Status» del idioma latín.


Si bien puede considerarse que el deseo de mandar es innato, el ser humano ha civilizado el instinto de dominación, transformándolo en la autoridad. Y ha creado el Estado para legitimarla. 

Las sociedades humanas, desde que se tiene noticia, se han organizado políticamente. Tal organización puede llamarse Estado, en tanto y en cuanto corresponde a la agregación de personas y territorio en torno a una autoridad, no siendo, sin embargo, acertado entender la noción de Estado como única y permanente a través de la historia.

De una manera general, entonces, puede definírsele como la organización en la que confluyen tres elementos, la autoridad, la población y el territorio. Pero, esta noción ambigua obliga a dejar constancia de que si bien el Estado ha existido desde la antigüedad, solo puede ser definido con precisión teniendo en cuenta el momento histórico.

Del estado de la Antigüedad no es predicable la noción de legitimidad, por cuanto surgía del hecho de que un determinado jefe (rey, tirano, príncipe) se apoderase de cierto territorio, muchas veces mal determinado, sin importar el sentimiento de vinculación de la población, generalmente invocando una investidura divina y contando con la lealtad de jefes y jefezuelos regionales. Así fueron los imperios de la antigüedad, el egipcio y el persa, entre ellos.

La civilización griega aportó una nueva noción de Estado. Dado que la forma de organización política que la caracterizó correspondía a la ciudad, la polis, se acordaba a la población una participación vinculante, más allá del sentimiento religioso y sin poderes señoriales intermedios. Además, estando cada ciudad dotada de un pequeño territorio, su defensa concernía a todos los ciudadanos, que se ocupaban de lo que hoy se llama el interés nacional. 

En el régimen feudal prevalecieron los vínculos de orden personal, desapareciendo tanto la delimitación estricta del territorio como la noción de interés general. El poder central era legítimo pero débil y los jefes locales fuertes, al punto que estos ejercían atributos propios del príncipe, como administrar justicia, recaudar impuestos, acuñar moneda y reclutar ejércitos. 

Y, finalmente, el estado moderno incorpora a la legitimidad, heredada del feudal, la noción de soberanía, un concepto revolucionario, tal como señala Jacques Huntzinger,[21]​ quien atribuye el paso histórico de una sociedad desagregada y desmigajada, pero cimentada en la religión, a una sociedad de estados organizados e independientes unos de otros. 

Pero, este estado moderno, surgido de la aspiración de los reyes a desembarazarse de los lazos feudales y de la jerarquía eclesiástica, el estado – nación, la unión de un poder central, un territorio y una población alrededor del concepto revolucionario de la soberanía, habría de conocer dos formas, dos definiciones diferentes, la primera, el estado principesco y la segunda, el estado democrático.

El estado principesco, se caracterizó por el poder personal ejercido uniformemente sobre un territorio estrictamente delimitado. El príncipe era el soberano, con atribuciones internas y externas. Dentro de su territorio, cobraba impuestos y producía leyes de carácter general, aplicadas coercitivamente, mediante el monopolio de la fuerza pública. Internacionalmente, representaba y obligaba a su Estado. 

Y el estado democrático, surgido de las revoluciones inglesa, norteamericana y francesa, trasladó la soberanía del príncipe a la nación. Sus poderes fueron asumidos por organismos surgidos de consultas a la población, mediante reglas de juego previa y claramente definidas. Y al igual que en las polis griegas, el sentimiento patriótico se desarrolló y con él los de pertenencia, civismo e interés nacional. 

Sea que se practique la democracia o sólo se adhiera verbalmente a ella, el proceso histórico descrito ha llevado a la extensión del estado - nación como forma política. Los principios desarrollados en Europa y Norteamérica se propagaron con la descolonización producida durante el siglo XX y así, tal como afirma Huntzinger, se “ha llegado a universalizar el modelo de estado – nación de tal modo que el planeta, ahora, se encuentra poblado de estados.”

Existen distintas formas de organización de un Estado, pudiendo abarcar desde concepciones "centralistas" a las "federalistas" o las "autonomistas", en las que el Estado permite a las federaciones, regiones o a otras organizaciones menores al Estado, el ejercicio de competencias que le son propias pero formando un único Estado (lo que sucede, por ejemplo, en Suiza, Alemania o EE. UU.).

(Nota: "estatidad" se utiliza aquí como equivalente a "estatalidad" o "estatalismo") 

No todos los Estados actuales surgieron de la misma manera; tampoco siguieron una evolución o un camino inexorable y único. Esto es así porque los Estados son construcciones históricas de cada sociedad. En algunos casos surgieron tempranamente, como por ejemplo el Estado Nacional inglés. En otros casos lo hicieron más tardíamente, como el Estado Nacional alemán. 

Los Estados pueden ser examinados dinámicamente usando el concepto de estatidad, aportado por Oscar Oszlak. Desde este punto de vista, aquellos van adquiriendo con el paso del tiempo ciertos atributos hasta convertirse en organizaciones que cumplen la definición de Estado.

Estas características de estatidad, enunciadas en un orden arbitrario en el sentido de que cada Estado puede adquirir estas características no necesariamente en la secuencia indicada, son las siguientes:

Así, todos los territorios atraviesan un largo proceso hasta alcanzar esa calidad de Estado pleno. Que solo será tal en la medida en que ese Estado haya logrado con éxito todos estos requisitos. Requisitos que son mínimos y necesarios para hablar de un verdadero Estado Nacional.

Todo esto hace que el Estado sea una de las más importantes formas de organización social en el mundo, ya que en cada país y en gran parte de las sociedades se postula la existencia real o ficticia de un Estado. Sin embargo, la creación de entes supraestatales como la Unión Europea ha modificado el concepto tradicional de Estado, pues este delega gran parte de sus competencias esenciales en las superiores instancias europeas (económicas, fiscales, legislativas, defensa, diplomacia...) mermándose así la soberanía original de los Estados.

Otros grupos sociales que se consideran en la actualidad como Estados no son tales por tener tan mermadas sus capacidades y funciones en favor de otras formas de organización social.

El poder muestra dos facetas distintas: estricto y legitimo. El primero cuando es aludido en el sentido de fuerza coactiva, o sea, aplicación pura de la fuerza. Mientras que el segundo se lo concibe cuando es fruto del reconocimiento de los dominados. De este modo el pueblo reconoce como autoridad a una institución por excelencia y le delega su poder.

Una primera y clásica clasificación de los Estados hace referencia a la centralización y descentralización del Poder (aspecto que no debe confundirse con el aspecto de Estados de  concentralización y desconcentralización del poder), de ahí se puede diferenciar entre Estados unitarios y Estados de estructura compleja o Estados Complejos, siendo estos últimos, generalmente, las federaciones y las confederaciones, así como otros tipos intermedios.

El Derecho Internacional da también otra clasificación de los Estados según su capacidad de obrar en las relaciones internacionales:

El reconocimiento es un acto discrecional que emana de la predisposición de los sujetos preexistentes. Este acto tiene efectos jurídicos, siendo considerados ambos sujetos internacionales, el reconocedor y el reconocido, de igual a igual puesto que se crea un vínculo entre los dos.

Hoy en día la doctrina aceptada para el reconocimiento de los Estados es la doctrina Estrada, pragmática en tanto en cuanto un sujeto no sea molesto para la sociedad internacional no va a tener dificultad para ser reconocido. Se entiende que si un sujeto reconoce a otro se va a producir contactos entre ambos, por lo que en el momento que se inician los trámites para el establecimiento de relaciones diplomáticas se supone que existe un reconocimiento internacional mutuo. Sin embargo, la ruptura de estas relaciones diplomáticas no supone la pérdida del reconocimiento. Igualmente, una simple declaración formal también es válida para reconocer a otro Estado pese a no iniciar relaciones diplomáticas.

En el ámbito normativo, hay propuestas que apuntan a necesidad de mayor integración con la creación de un Estado global, entendido como un marco político planetario con poder coercitivo y capacidad para regular las relaciones interestatales y los focos de poder extrapolíticos, con capacidad ejecutiva, legislativa y judicial capaz de imponerse a los Estados nacionales en determinados ámbitos que no pueden ser abordados desde la óptica de la soberanía nacional (medio ambiente, terrorismo, paraísos fiscales...).[22]​

El Estado es una de las instituciones que perdura sin una evolución importante en su estructura y funcionamiento, con excepción de su crecimiento. El Estado moderno fue creado con la revolución industrial, pero el mundo y la dinámica de la sociedad ha cambiado mucho desde del siglo XIX. Por ejemplo, mientras las empresas modernas, que fueron creadas durante la revolución industrial, cambian ágilmente su dinámica cada vez que el mercado lo demanda, los Estados no cambian sus leyes de la misma forma como la sociedad lo demande (véase: cálculo económico).

El enfoque crítico difiere además entre el institucionalismo y el clasismo como factor determinante de la naturaleza del Estado. Algunas concepciones como el anarquismo consideran conveniente la total desaparición de los Estados, en favor del ejercicio soberano de la libertad individual a través de asociaciones y organizaciones libres. Otras concepciones aceptan la existencia del Estado, con mayor o menor autoridad o potestad, pero difieren en cuanto cual debiera ser su forma de organización y el alcance de sus facultades: 

El anarquismo sostiene que el Estado es la estructura de poder que pretende tener el monopolio del uso de la fuerza sobre un territorio y su población, y que es reconocido como tal por los estados vecinos. Los elementos más aparentes que señalan el poder del estado son:

Se le critica la falsa ostentación de la seguridad, defensa, protección social y justicia de la población; ejerciendo en realidad un gobierno obligatorio y violentando la soberanía individual y la no coacción. Los anarquistas señalan que el Estado es una institución represora para mantener un orden económico y de poder concreto vinculado al poder público. Le atribuyen al Estado buena parte de los males que aquejan a la humanidad contemporánea como la pobreza, crisis económicas, las guerras, la injusticia social, etc.[23]​[24]​

Unas palabras que identifican plenamente lo que es para los anarquistas el Estado desde la perspectiva de Bakunin, uno de los teóricos del anarquismo moderno:

“Quien dice ‘Estado’, dice necesariamente ‘Guerra’. El Estado procura (y debe procurar) ser fuerte, más fuerte que sus vecinos; de lo contrario, será un juguete en manos de ellos. Se ve obligado a debilitar, a empobrecer a los otros Estados para imponerles su ley, su política, sus tratados comerciales, con objeto de enriquecerse a su costa. La lucha por la supremacía, que está en la base de la organización económica burguesa, es también la base de su organización política”.

Por su parte los marxistas afirman que cualquier Estado tiene un carácter de clase, y que no es más que el aparato armado y administrativo que ejerce los intereses de la clase social dominante.[25]​ Por tanto aspiran a la conquista del poder político por parte de la clase trabajadora, la destrucción del Estado burgués y la construcción de un necesario Estado obrero como paso de transición hacia el socialismo y el comunismo, una sociedad donde a largo plazo no habrá Estado por haberse superado las contradicciones y luchas entre las clases sociales.[26]​ Se discute sobre la viabilidad de la eliminación de las condiciones de la existencia burguesa, supuesto para el paso de la sociedad enajenada a la comunista.[27]​

Desde el liberalismo se aboga por la reducción del papel del Estado al mínimo necesario (Estado mínimo), desde un sentido civil para el respeto de las libertades básicas, es decir el Estado debería encargarse de la seguridad (ejército y policía para garantizar las libertades ciudadanas) y de la justicia (poder judicial independiente del poder político). En ningún caso el Estado debe servir para ejercer la coacción de quitar a unos individuos para dar a otros, y deben ser los agentes privados los que regulen el mercado a través del sistema de precios, asignando a cada cosa el valor que realmente tiene.[28]​

Bastiat expuso dos formas posibles de entender el Estado: Un estado que hace mucho pero debe tomar mucho, o bien un estado que hace poco pero también toma poco de sus ciudadanos. La tercera posibilidad de un estado que hace mucho por sus ciudadanos pero les pide poco a cambio (tercera vía) es, según Bastiat, una invención de algunos políticos irresponsables.

Las ideologías integristas defienden la concepción del Estado supeditada a la religión que profesan.

En defensa del bien común de la totalidad de la población que engloba el Estado o de la pervivencia del mismo, se utiliza frecuentemente la llamada razón de Estado, término acuñado por Nicolás Maquiavelo, por la que dicho Estado, perjudica o afecta de una u otra forma a personas o grupos de personas, en pro del resto de individuos que lo conforman, generalmente obviando las propias normas legales o morales que lo rigen. Tal es el argumento esgrimido, por ejemplo, en ciertos asesinatos selectivos o en ciertos casos de terrorismo de Estado.





Colombia, oficialmente República de Colombia, es un país soberano situado en la región noroccidental de América del Sur, actualmente es miembro de la Comunidad Andina y se constituye en un estado unitario, social y democrático de derecho cuya forma de gobierno es presidencialista. Es una república organizada políticamente en 32 departamentos descentralizados y el Distrito Capital de Bogotá, sede del Gobierno Nacional.[11]​

Incluyendo la isla de Malpelo, el cayo Roncador y el banco Serrana, el país abarca una superficie de 1 141 748 km²,[3]​ por lo que es el vigesimoquinto país más grande del mundo y el séptimo más grande de América. Reclama como mar territorial el área hasta las 12 millas náuticas de distancia,[4]​ manteniendo un diferendo limítrofe al respecto con Venezuela y Nicaragua.[12]​[13]​ Limita al oriente con Venezuela y Brasil, al sur con Perú y Ecuador y al occidente con Panamá; en cuanto a límites marítimos, colinda con Panamá, Costa Rica, Nicaragua, Honduras, Jamaica, Haití, República Dominicana y Venezuela en el mar Caribe, y con Panamá, Costa Rica y Ecuador en el océano Pacífico.[14]​

Es la única nación de América del Sur que tiene costas en el océano Pacífico y acceso al Atlántico a través del mar Caribe,[15]​ en los que posee diversas islas como el archipiélago de San Andrés, Providencia y Santa Catalina.[16]​

Es el vigesimoctavo país más poblado del mundo, con una población de 51 millones de habitantes,[17]​[18]​ además es la segunda nación con más hispanohablantes, solo detrás de México.[19]​ Posee una población multicultural, la cual refleja la influencia de la colonización europea a gran escala, pueblos nativos y mano de obra africana, con oleadas migratorias provenientes de Europa y Oriente Próximo durante el siglo XX.[20]​ El producto interno bruto de paridad de poder adquisitivo de Colombia ocupa el cuarto puesto en América Latina y el puesto 28 a nivel mundial. El PIB nominal colombiano es el cuarto más grande de América Latina y ocupa el puesto 28 a nivel mundial.[21]​

La presencia humana en Colombia se remonta a más de 14 500 años.[22]​[23]​[24]​Después de miles de años de formación cultural, en el actual territorio colombiano surgieron diversas culturas precolombinas como los muiscas, taironas y quimbayas. Al colonizar a estas culturas, España creó el Virreinato de la Nueva Granada con capital en Santafé (hoy Bogotá). En el año 1810 comenzó la Guerra de independencia, tras la cual surgió el país que actualmente se conoce como Colombia. Durante los siglos XIX y XX, el país se caracterizó por su inestabilidad y un gran número de guerras civiles;[25]​ el último de estos conflictos, conocido como conflicto armado interno, comenzó en 1960. En el año 2012, después de más cincuenta años de conflicto, el gobierno del entonces presidente Juan Manuel Santos inició conversaciones de paz con las FARC-EP. En 2016 se alcanzó un acuerdo final que a pesar de no ser aprobado en el plebiscito del 2 de octubre del mismo año, fue implementado con modificaciones en 2017. A la fecha, el Gobierno de Colombia se encuentra adelantando el proceso de implementación de los acuerdos e iniciando nuevas conversaciones con el ELN, que ha manifestado la intención de contribuir al final del conflicto.

Colombia tiene una economía diversificada y posee un importante componente de servicios. La producción económica del país está dominada por su demanda interna y el gasto en consumo de los hogares es el mayor componente del PIB.[26]​ El PIB en 2016 fue de 720 151 millones de dólares.[27]​ El índice de desarrollo humano colombiano es de 0,747 y su esperanza de vida promedio es de 75,1 años.[7]​[28]​ Colombia es parte del grupo de los CIVETS, considerados como seis principales mercados emergentes. Es miembro de la OCDE,[29]​ la ONU, la OEA, la Alianza del Pacífico, la CAN y de otras organizaciones internacionales; también es el único país de Latinoamérica que es socio global de la OTAN. Es el segundo país con mayor índice de desigualdad en América Latina, después de Brasil, y empatado con Panamá, según la base de datos del Banco Mundial.[30]​

Es la segunda nación más biodiversa del mundo, contando con 54 871 especies registradas;[31]​ no obstante, un estudio lo ubica entre los ocho países responsables de la mitad de la destrucción de biodiversidad en el mundo.[32]​ Por otra parte, es el país de América Latina con más conflictos ecológicos entre la población local y empresas multinacionales en áreas de especial protección ambiental.[33]​[34]​ Para proteger su medio ambiente el país cuenta con instrumentos como la Política Nacional de Cambio Climático y el impuesto al carbono.[35]​La producción de electricidad en Colombia proviene principalmente de fuentes de energía renovables. 69.97 % se obtiene de la generación hidroeléctrica.[36]​

La denominación de Colombia proviene del apellido del explorador genovés del siglo XV Cristóbal Colón (en italiano: Cristoforo Colombo). Fue adoptado el 15 de febrero de 1819, durante el Congreso de Angostura, para denominar al nuevo Estado que entonces comprendía los territorios de la Nueva Granada, Quito y la Capitanía General de Venezuela.[37]​

En 1830, con la secesión de Venezuela y Ecuador de la Gran Colombia, pasó a llamarse República de la Nueva Granada. Tras la adopción del federalismo con la constitución de 1858 pasó a llamarse Confederación Granadina. En 1863 adoptó el nombre de Estados Unidos de Colombia;[38]​ los cuales en 1886 se constituyeron en la República de Colombia. 

El origen del nombre también se menciona en la segunda estrofa del himno nacional:[39]​

El estudio de los primeros pobladores del territorio que hoy comprende la Nación se ha dividido en tres etapas de la época precolombina: el paleolítico (15 000-7000 a. C.), el periodo Arcaico Andino (7000 a 2000 a. C.), y el periodo formativo 2000 a. C. hasta el siglo XVI. Los primeros seres humanos que llegaron al territorio datan de aproximadamente 10 000 y 15 000 años. Los cazadores y recolectores nómadas de esta época utilizaban artefactos líticos, herramientas y armas hechas con piedra que datan de 10 450 a. C., hallados en El Abra, donde se comprobó que existían habitantes en la sabana de Bogotá en 10 500 a. C. En el siglo XV existían tres grandes familias que poblaban Colombia. La cultura Caribe se ubicaba en la costa del mar Caribe, la arawak en los ríos Caquetá, Amazonas y Putumayo, y los muiscas en la Sierra Nevada de Santa Marta y los altiplanos del centro del país. Esta última fue la que presentaba más pobladores y un significativo desarrollo en la agricultura, el uso de calendario, los jeroglíficos, y los rituales religiosos.[40]​[41]​

El cacicazgo fue la organización social que primó antes de la Era Cristiana. Se caracterizaba por su orden social basado en una estratificación de la sociedad, las tribus se agrupaban de forma similar a como lo hacía el señorío. Así, el cacique era el que tenía el máximo poder. Se han encontrado evidencias de actividades de esta época en las que se destacan las prácticas funerarias, la evidencia de diversos oficios y símbolos de mando, adornos personales, templos, estatuas, sistemas de riego y notables avances en la agricultura como las terrazas de cultivo, entre otros. Las culturas San Agustín, Tumaco, Tierradentro, Quimbaya, Zenú, Malagana, Pastos, Quillacingas, Guanes y Pijaos, basaron su orden social en el cacicazgo. Muchas de estas ya habían desaparecido o se encontraban reducidas a la llegada de los españoles. Culturas más avanzadas en su orden social surgieron después en el formativo superior, cuya organización superó a la del cacicazgo, pues era similar a federaciones de aldeas; sus mayores representantes fueron culturas como la tairona y los muiscas.[40]​[41]​

La primera expedición que zarpó rumbo a lo que hoy es Colombia fue emprendida en 1499 por Alonso de Ojeda acompañado por Juan de la Cosa y Américo Vespucio.[43]​ Costeó África, pasó por las islas Canarias hasta llegar a lo que hoy se conoce como Guyana y Venezuela, de allí partió hacia Trinidad y después a la Guajira, para regresar a España con un botín compuesto mayoritariamente de indígenas esclavizados. Fue la primera ocasión en la cual exploradores europeos divisaron y mapearon la costa continental americana. Específicamente la costa venezolana y colombiana.[44]​ En su segundo viaje fundó Santa Cruz en bahía Honda, la primera colonia europea en continente americano, la cual no prosperó.[44]​

Un viaje realizado por Rodrigo de Bastidas entre 1500 y 1501 partió desde la Guajira hasta el golfo de Urabá. Durante este recorrido, De Bastidas descubrió la desembocadura del río Magdalena. Por su parte Cristóbal Colón, en su último viaje a América, pudo haber llegado hasta cabo Tiburón, en el Chocó.[42]​

En 1508 Ojeda fue nombrado gobernador de Nueva Andalucía, reino que se suponía habría de extenderse desde el istmo de Darién hasta punta de la Vela. Por lo anterior emprendió una tercera expedición partiendo desde Santo Domingo con “dos bajales, dos berantines, y trescientos hombres” entre los que se encontraba Francisco Pizarro.[44]​ Desembarcó en lo que hoy es la bahía Cartagena de Indias donde leyó el “requerimiento” de sometimiento a los indígenas de Turbaco. Sus fuerzas posteriormente fueron diezmadas por los indígenas por lo cual tuvo que seguir avanzando por la costa del reino que se suponía debía gobernar. Intentó fundar otro asentamiento en el Golfo de Urabá, San Sebastián de Urabá, el cual fue rápidamente abandonado debido a la inclemencia del terreno y a la resistencia de los habitantes del lugar.[44]​

En 1510, Vasco Núñez de Balboa fundó Santa María la Antigua del Darién y en 1513 dio con el océano Pacífico, con la ayuda de los indígenas, quienes guiaron al conquistador.[42]​ El descubrimiento del océano pacífico fue informado a España, y tiempo después llegaron varios navíos para explorar la zona comandados por Pedro Arias Dávila, quien era también el gobernador de la región comprendida entre el cabo de la Vela y Panamá.

En un principio el modus operandi de los españoles fue establecer pequeñas colonias que se dedicaban a subyugar, saquear y esclavizar a los pueblos indígenas aledaños, pero a medida que los indígenas morían en grandes números o huían de la costa, se hizo cada vez más necesario establecer colonias permanentes pobladas por emigrantes peninsulares.[45]​

En 1525, sería fundada Santa Marta por Rodrigo de Bastidas y, en 1533, Pedro de Heredia fundó a Cartagena del Poniente. Sin embargo, los esfuerzos de colonización fueron duros y ciudades como Santa Marta se vieron sumidas en la miseria y en su casi total desaparición. En medio de la desesperación, ante el constante estado de guerra con los indígenas y los primeros ataques piratas por parte de corsarios franceses en aquellas costas, el entonces gobernador de Santa Marta Pedro Fernández de Lugo designó a uno de sus hombres de confianza, el licenciado Gonzalo Jiménez de Quesada, para que organizara una expedición que remontara todo el río Magdalena hasta llegar al Perú. La expedición estaba compuesta por más de 600 hombres, un número que sobrepasaba la población de Santa Marta en aquel momento. La expedición partió el 6 de abril de 1536.[46]​

La intención de llegar a Perú se basaba en las noticias que habían llegado del fabuloso rescate que los Incas habían pagado para liberar a su líder Atahualpa, lo cual apuntaba a la enorme riqueza de aquellas tierras.[45]​ El viaje de Jiménez de Quesada fue tortuoso y, para cuando llegó a lo que hoy en día es Bogotá, había perdido al menos dos terceras partes de sus hombres. Allí fundaría dicha ciudad en 1539.

En 1550, se estableció el Nuevo Reino de Granada, división territorial del Virreinato de Perú, y se estableció la Real Audiencia de Santafé de Bogotá, con lo cual esta ciudad pasó a ser el centro político y administrativo de la Nueva Granada.

Con la consolidación de las colonias españolas tanto en la costa como en el centro del país, se pasó de un sistema basado en concesiones privadas en las cuales la Corona otorgaba gobernaciones a los conquistadores (quienes habían financiado las expediciones por su propia cuenta) a un sistema de gobierno de fuerte carácter centralista en la cual el Rey pretendió gobernar directamente a través del Consejo de Indias.[47]​ Este periodo se caracterizó por la imposición del poder central desde España sobre los pueblos nativos y los colonizadores de estas tierras. Se impuso un sistema social, económico y político excluyente para gran parte de los involucrados.[42]​ Se comenzó a desarrollar la institución de la encomienda alrededor de 1549, aunque de manera tardía, ya que esta institución venía en declive en Perú y Nueva España. Con posterioridad crecería en auge la mita como forma de extraer tributos y servicios personales de los indígenas por parte de los colonos.[48]​

En 1599, se dio una rebelión de esclavos en Cartagena liderada por Benkos Biohó. Muchas rebeliones similares durante el siglo XVI y XVII fueron estableciendo poblaciones independientes de esclavos libertos, lo cual contribuirá a la formación de las culturas raizales y afrocolombianas.[49]​

Durante casi todo el periodo colonial Nueva Granada fue una capitanía del Virreinato del Perú. El Capitán de Nueva Granada debía gobernar lo que hoy es Colombia, además de Venezuela con la excepción de Caracas. Cali, Popayán, Nariño y Ecuador estaban bajo la autoridad del Presidente de Quito, quien tenía funciones parecidas a la de una capitanía.[47]​ Esta organización se mantuvo sin mayor cambio hasta 1717 cuando la capitanía de Nueva Granada fue ascendida a Virreinato. En 1723 los cambios se revirtieron y los territorios regresaron a ser parte del virreinato de Perú, sin embargo, en 1739 el Virreinato de la Nueva Granada se restableció. Las presidencias de Quito y Panamá quedaron adscritas al Virreinato de la Nueva Granada. En 1777 se creó la Capitanía General de Venezuela sobre la cual los virreyes granadinos tenían poco control.

El gobierno subterritorial era adelantado por los cabildos o los concejos municipales. Estos entes no eran de elección democrática pero eran uno de los pocos en los que podían servir los criollos y eran representativo al menos en el sentido que sus miembros eran habitantes del territorio en cuestión.[47]​

La zona Caribe fue objetivo de ataques de corsarios al servicio de la Corona británica. Lo anterior culminó en la Guerra del Asiento, durante la cual se dio el sitio de Cartagena de Indias. La derrota de los ingleses ayudó a consolidar el dominio español sobre la Nueva Granada.[50]​

El conflicto comenzó a gestarse a finales del siglo XVIII con la insurrección de los comuneros, la cual fue la primera manifestación de la identidad criolla, al marchar los insurgentes por la capital para protestar contra los nuevos impuestos del gobierno y reclamar su parte de la riqueza nacional, bajo el lema Viva el Rey abajo el mal gobierno.[51]​

En 1808, Napoleón Bonaparte invade la península ibérica y obliga al rey Carlos IV y al príncipe heredero Fernando VII a abdicar al trono y ceder la soberanía del imperio español a José de Bonaparte, hermano de Napoleón. Como resultado en Valencia se creó la Junta Suprema de España e Indias, como un reducto del gobierno español en oposición a la súbita invasión francesa. Dicha junta buscaba gobernar el imperio mientras se restauraba a Fernando VII al trono.

El vacío político subsecuente en América tuvo por resultado una mayor inherencia de los "criollos" (descendientes de españoles nacidos en las colonias) en el gobierno de las provincias.[52]​

Con el total colapso de la resistencia española en 1810 las provincias neogranadinas como Cartagena, Antioquia, Cundinamarca, Venezuela, Panamá y muchas otras a lo largo del contiene comenzaron a conformar sus propias juntas de gobierno siguiendo el ejemplo de las provincias españolas.

Algunas juntas buscaban gobernar provisionalmente mientras se restauraba el Gobierno español pero algunas otras, impulsados por políticos como Antonio Nariño y Camilo Torres, se declararon independientes y republicanas, influidas por la Revolución Americana y francesa e inspirados por filósofos como Rousseau y Bentham.

Entre 1811 y 1816 se instauró el primer estado republicano denominado Provincias Unidas de corte federalista, con algunas provincias como Santa Marta y Popayán permaneciendo leales a la Corona Española y rechazando la Unión.[53]​ Durante este periodo se dio una guerra civil entre centralistas, federalistas y realistas por lo cual se denomina a esta época la Patria Boba, nombre dado por Antonio Nariño.

Tras la restauración de la Monarquía Española en 1814, Fernando VII envió un ejército en 1816 a reconquistar los territorios que se habían declarado independientes. La resistencia a la reconquista fue liderada por Francisco de Paula Santander en el Casanare y posteriormente se unieron a Simón Bolívar, quien comandó un ejército independentista desde Guayana y luego desde Venezuela, el cual invadió a Colombia desde los Llanos Orientales con la meta de ocupar la provincia de Tunja, atravesando la cordillera que los separa.[54]​

El Ejército Libertador contaba con 1300 hombres de infantería y 800 de caballería, y con el apoyo del gobierno de Inglaterra. Simón Bolívar se encontraría el 12 de junio con las tropas del general Francisco de Paula Santander entre otros oficiales independentistas. El 25 de julio se llevó a cabo la batalla del Pantano de Vargas. El ejército patriota se tomó a Tunja el 4 de agosto. Después, José María Barreiro, en su retirada hacia Santa Fe, fue sorprendido por Simón Bolívar en el puente de Boyacá, donde ocurrió la batalla homónima el 7 de agosto de 1819. Tres días más tarde ocuparon la capital que el virrey Juan de Sámano había abandonado en retirada. El triunfo de Bolívar significó el fin de la dominación de España en el oriente y centro del país. En los siguientes cuatro años serían derrotados los últimos núcleos realistas que aún se resistían al proyecto emancipador.[54]​

Tras el triunfo de Bolívar y la expulsión de los españoles de América se revive el conflicto político entre quienes creían que la nueva nación debía ser de carácter centralista y quienes opinaban que debía ser federal. El inconveniente fue tal que la asamblea constituyente de Ocaña fue incapaz de redactar una constitución para el recién creado estado de la Gran Colombia conformada por los actuales países de Colombia, Ecuador, Panamá y Venezuela. Su nombre oficial era República de Colombia, pero en la historiografía se le hace referencia como Gran Colombia para diferenciarla de la actual Colombia.

Tras el fallido congreso de Ocaña, en 1828 Bolívar expide el Decreto Orgánico de la Dictadura declarándose dictador por 2 años hasta 1830. En septiembre de 1828 un grupo de disidentes políticos intentaron asesinar a Bolívar quien se encontraba con Manuela Sáenz en el Palacio de San Carlos. Manuela fue golpeada severamente por los conspiradores pero sobrevivió, mientas que Bolívar se fugó saltando por una ventana, pasando toda la noche escondido debajo del puente del Carmen, por donde cruzaba el río San Agustín (actual carrera 5ª con calle 6ª) cerca a La Candelaria, tiempo durante el cual adquiere neumonía.[57]​[58]​ Como resultado Santander fue acusado de participar en el complot, enjuiciado y exiliado. Estos sucesos, aunados a rumores sobre las gestiones realizadas por miembros del gabinete de Bolívar (según él, sin su aprobación) para encontrar un noble europeo que asumiese el rol de rey americano al estilo de una monarquía constitucional, culminaron con dos revueltas federalistas encabezadas por dos republicanos, José María Córdova en Antioquia y José María Obando en Cauca. El 16 de octubre de 1829, muere asesinado José María Córdova. Así mismo, el 4 de junio de 1830 muere asesinado Antonio José de Sucre, lo cual, junto a una enfermedad crónica, lleva a Bolívar a renunciar en mayo de 1830 para partir hacia el exilio ante el colapso del proyecto de la Gran Colombia. Finalmente fallece antes de poderse embarcar para Inglaterra, en Santa Marta en diciembre de 1830. Tras la muerte de Bolívar, Santander regresó al país llegando a ser presidente en 1832.[59]​ 

Subsecuentemente fue creada la centralista República de la Nueva Granada (1832-1858), conformada por las actuales Colombia y Panamá y que estaba organizada en provincias. En este periodo los partidarios del centralismo y la unión entre la Iglesia y el Estado sentarían las bases para la creación del Partido Conservador Colombiano, mientras que los federalistas y partidarios de la separación Iglesia-Estado se convertirían en el Partido Liberal Colombiano. Luego vino la fugaz Confederación Granadina, una república federal integrada por ocho estados federados que reemplazaron a las antiguas antiguas provincias neogranadinas de 1858 a 1863.

En 1854, un golpe político-militar llevó al poder al liberal José María Melo durante algunos meses.[60]​ Tras ser derrocado en la guerra civil de ese año, se impulsó la reducción del ejército, requisito importante para que funcionara el federalismo instaurado en 1858. Sin embargo, durante la guerra civil de 1860-1862 se dio una rebelión en la que el Estado de Cauca liderado por Tomás Cipriano de Mosquera derrocó al gobierno conservador de Mariano Ospina Rodríguez.[61]​ En 1863, entró en vigor la Constitución de Rionegro con la que el país pasó a llamarse Estados Unidos de Colombia, una república federal en la que primaba la autonomía territorial. Se caracterizó por ser un país influido por el liberalismo que impuso un sistema federal con la creación de ejércitos regionales. En 1876 los conservadores se alzaron en armas en la guerra civil de 1876 por cuestiones religiosas en la educación, pero fueron derrotados por el Gobierno.

Las elecciones presidenciales de 1880 fueron ganadas por el liberal moderado Rafael Núñez quien se opuso al federalismo y apoyó una mayor intervención del estado en la economía y en las regiones. Como resultado, se desató la guerra civil de 1884-1885 en un intento fallido de los liberales radicales para derrocar a Núñez, quien con la ayuda de los conservadores proclamó la Constitución de 1886, que acabó con el federalismo e impuso el centralismo administrativo, creando los departamentos, dándole amplios poderes al Presidente de la República, unificando el ejército a nivel nacional, aliándose con la Iglesia y renombrando el país al nombre actual República de Colombia.

La Guerra de los Mil Días se desarrolló entre 1899 y 1902 como reacción al gobierno conservador durante la presidencia de Manuel Antonio Sanclemente y José Manuel Marroquín; la guerra, la posterior quiebra del país y el interés económico de Estados Unidos terminó en la separación de Panamá en 1903.[62]​ Finalizada la guerra fue elegido Rafael Reyes (1904-1909), quien disolvió el Congreso en 1905 y lo reemplazó por una Asamblea Constituyente que le otorgó poderes dictatoriales.[63]​ Durante su gobierno se normalizó el orden interno, se estabilizó la economía y se inició un proceso de industrialización y modernización del estado.[64]​[65]​ En la década de 1920 se presentan hechos de violencia política como la Masacre de las Bananeras en 1928, llega la aviación al país y surgen los movimientos populares campesinos, obreros e indígenas.[66]​ En 1930, por primera vez desde 1886, es decir 44 años después, un liberal ganó las elecciones presidenciales. Con Enrique Olaya Herrera (1930-1934) se inició el periodo en el que los liberales ejercieron el poder durante 16 años continuos.[67]​ Durante su gobierno se dio la guerra contra Perú, originada en 1932 cuando un grupo de peruanos se tomaron la ciudad de Leticia en el Amazonas.[68]​ Aunque Colombia fortaleció su flota militar, el conflicto no se soluciona en el campo de batalla sino a través de la vía diplomática por medio del Protocolo de Río de Janeiro (1934) que puso fin al diferendo limítrofe.[69]​ El primer gobierno de Alfonso López Pumarejo se denominó 'la revolución en marcha' y fue una constante de reformas.[70]​

Entre 1946 y 1958, el país estuvo sumido en una crisis social y política, con el recrudecimiento de «La Violencia bipartidista». Se caracterizó por ser un periodo de persecuciones políticas en medio de una guerra entre liberales y conservadores. El conflicto causó entre 113 000 y 300 000 muertos[71]​ y el desplazamiento forzado de más de dos millones de personas, equivalente a casi una quinta parte de la población total de Colombia, de aproximadamente 11 millones de habitantes.[72]​ Como consecuencia, el país dejó de ser agrario para convertirse en urbano, pues en 1959 poco más de la mitad de población del país vivía en la ciudad. Históricamente, las tierras en su mayoría agrícolas, fueron adquiridas a bajo precio por terratenientes de la época. La población campesina desplazada de sus tierras se convirtió en una mano de obra agraria asalariada. La actividad industrial creció, pero esto no se reflejo en el desarrollo de la población.[73]​

Tras divisiones internas liberales en 1946, los conservadores retomaron el poder presidencial, no las mayorías en el congreso. En 1948, con el asesinato del líder liberal Jorge Eliécer Gaitán, se inició el Bogotazo y el recrudecimiento de la Violencia, guerra civil que perduró hasta principios de los años 1960.[74]​ Los conservadores mantuvieron la presidencia hasta 1953, cuando la clase política (conformada por las clases altas, que eran los ejercían en la mayoría de los casos los puestos del gobierno) propició un golpe de estado que entregó el poder al General Gustavo Rojas Pinilla. La mayoría de las guerrillas liberales y comunistas, atraídas por las propuestas de paz del Gobierno, entregaron sus armas, pero varios de sus miembros fueron asesinados posteriormente. Un acuerdo entre los partidos Liberal y Conservador, puso fin a la dictadura de Rojas Pinilla, y tras el Plebiscito de 1957 se creó el Frente Nacional como un regreso a la democracia electoral formal en la cual los partidos políticos alternaron el poder dándole fin a las décadas de guerra entre liberales y conservadores.[75]​

El Frente Nacional marcó el fin de la violencia bipartidista. Sin embargo continuaron la exclusión de otras ideologías políticas, los problemas sociales, económicos y políticos.[76]​ Surgieron las guerrillas comunistas: en 1964 las Fuerzas Armadas Revolucionarias de Colombia - Ejército del Pueblo (FARC-EP), el 7 de enero de 1965 el Ejército de Liberación Nacional (ELN), en julio de 1967 el Ejército Popular de Liberación (EPL); la guerrilla nacionalista en enero de 1974: el Movimiento 19 de abril (M-19), y la primera guerrilla indígena del continente en 1984: el Movimiento Armado Quintín Lame (MAQL).[77]​[78]​[79]​

Desde 1960 la historia de la nación se ha caracterizado por el conflicto armado interno (con varias etapas de recrudecimiento, la mayor entre 1988 y 2012),[81]​ entre el estado y diferentes actores armados (guerrillas de extrema izquierda, paramilitares de extrema derecha, carteles del narcotráfico y crimen organizado).[82]​ Según el informe "¡Basta ya!: Colombia: memorias de guerra y dignidad" (2013) 220 000 muertes por el conflicto entre 1958-2012.[83]​ Para 2020 según el Registro Único de Víctimas se cuentan 8 989 570 víctimas del conflicto en 11 202 790 eventos o hechos victimizantes.[84]​ Colombia es el país con mayor cantidad de desplazados en el mundo con 7 816 500 de personas que han huido de la violencia según el Centro de Monitoreo de Desplazamiento Interno (IMDC) en 2019.[85]​

Durante el Frente Nacional fueron elegidos cuatro presidentes, dos por el Partido Liberal y dos por el Partido Conservador. En 1974 se reanudó la contienda electoral entre liberales y conservadores.[86]​ Aparecen grupos insurgentes como las FARC-EP, el ELN, el EPL y el M-19, que conformaron la Coordinadora Guerrillera Simón Bolívar (CGSB) y grupos paramilitares (autodefensas regionales, las Convivir y posteriores AUC) para combatir a estos grupos y a partidos de oposición. Se realizaron negociaciones de paz que culminaron exitosamente en 1990 con el M-19 y en 1991 con el 95% del EPL entre otros grupos. Se promulga la Constitución Política de 1991 que rige los destinos de la nación desde el gobierno de César Gaviria (1990-1994).[87]​ En la década de 1970 se presentó la bonanza marimbera, y desde los años 1980 se dio un auge en el narcotráfico de cocaína. Los narcotraficantes líderes de los carteles de Cali y Medellín adquirieron un enorme poder influyendo en la clase dirigente del país con dinero producto de los negocios ilícitos, como el Proceso 8000 en la campaña electoral del presidente Ernesto Samper (1994-1998), siendo ejemplo de los actuales problemas de corrupción en Colombia. A finales de la década de 1980 y comienzos de la de 1990 el gobierno inició a perseguir y a extraditar a los capos de la droga. Esto desencadenó una oleada de violencia conocida como narcoterrorismo. El dinero del narcotráfico también ayudó a financiar grupos guerrilleros y paramilitares que se enfrascaron en un violento conflicto agudizado en las décadas de los 1990 y 2000.[88]​[89]​[90]​

Tras el fracaso de los diálogos de paz del Caguán, el Gobierno de Colombia y los Estados Unidos implementaron el Plan Colombia en el gobierno de Andrés Pastrana (1998-2002).[91]​Se establecen los dos periodos presidenciales de Álvaro Uribe (2002-2010) como los años con mayor cantidad de víctimas: 3.633.840 de víctimas según el Registro Único de víctimas.[92]​ Con la desmovilización de las Autodefensas Unidas de Colombia (AUC) entre 2003 y 2006, las denominadas Bandas Criminales (Bacrim) o Grupos Armados Organizados (GAO) retomaron las actividades ilícitas realizadas por los paramilitares, sobre todo, las relacionadas con el narcotráfico y la minería ilegal.[93]​[94]​[95]​ Desde 2012 hasta 2016, el gobierno de Juan Manuel Santos desarrolló un proceso de paz con la guerrilla de las FARC-EP en La Habana, Cuba, con el objetivo de encontrar una salida política al conflicto. El 2 de octubre de 2016, luego de firmar entre las partes un primer acuerdo de paz el 26 de septiembre de ese año, el gobierno convocó a un plebiscito para refrendar los acuerdos alcanzados con los insurgentes, resultando ganador el NO por estrecho margen.[96]​[97]​Tras un periodo de negociación con los promotores del NO, el gobierno y las FARC-EP acordaron los definitivos acuerdos de paz entre el gobierno de Juan Manuel Santos y las FARC-EP, el cual se firmó el 24 de noviembre de 2016.[98]​ Se presentó un descenso en la intensidad de la guerra.[99]​

En la actualidad, el Estado colombiano no ha cumplido con la mayoría de sus obligaciones, como establecer una presencia integrada en las regiones rurales e implementar programas de restitución de tierras y sustitución de cultivos ilícitos. Colombia encabeza la lista a nivel mundial de líderes sociales asesinados, según el último informe de la organización Front Line Defenders la impunidad en estos casos es del 86%. Según la Organización de las Naciones Unidas el 93% de los casos ocurrieron en lugares con donde no existe presencia del Estado, lo que genera un gran retroceso para la implementación del Acuerdo de Paz, y el asesinato de excombatientes de las FARC-EP.[100]​[101]​[102]​ El conflicto continúa con el ELN, los Grupos Armados Organizados y las Disidencias de las FARC-EP.[103]​ Se han presentado movilizaciones sociales como las protestas en 2019-2020, y las protestas de 2021.

La desigualdad en Colombia se refiere a la desigualdad económica y social existente en el país.[104]​[105]​ Según cifras del Banco Mundial, en el 2017 Colombia fue el segundo país más desigual de América Latina y el séptimo del mundo,[106]​ del total de 194 países que existen en el planeta. Pese al crecimiento económico sostenido del producto interno bruto que se ubicó entre el 6.6% entre 2006-2014, el índice de desigualdad cayó durante la época de mayor bonanza petrolera.[107]​

La corrupción ha sido tradicionalmente señalada por muchos analistas como uno de los principales problemas políticos del país. El Índice de Percepción de Corrupción 2016 (IPC), de la agencia para la Transparencia Internacional, – que califica de 0 (Muy Corrupto) a 100 (Muy Transparente) de los niveles de corrupción percibidos por el sector público en 175 países y territorios evaluados– Colombia, en ese estudio obtuvo una calificación de 37 puntos, muy por debajo del promedio global que es de 43 puntos, convirtiendo a Colombia en uno de los países más corruptos del mundo (ver gráfica). Se ubica a Colombia en el puesto 98 a nivel mundial, teniendo como base que Nueva Zelanda y Dinamarca están en el puesto 1 como los países menos corruptos del planeta.[108]​[109]​Se estima que el costo anual de la corrupción en Colombia es de más de 50 billones de pesos; aproximadamente 17 mil millones de dólares anuales, representa el 5 % del PIB y el 21 % del presupuesto nacional.[110]​[111]​

La pandemia de COVID-19 en Colombia comenzó el 6 de marzo de 2020, según informó el Ministerio de Salud y Protección Social. El primer caso confirmado en el país fue el de una mujer de 19 años, proveniente de Milán, Italia.[112]​

Colombia es una democracia en la que los ciudadanos les dan poder a los gobernantes mediante el voto para que los representen y tomen las decisiones referentes al país. Los colombianos pueden elegir a sus representantes cuando son mayores de dieciocho años y posean cédula de ciudadanía. En Colombia el hecho de acudir a las urnas no es obligatorio, ya que su sistema electoral es el sufragio voluntario. El voto obligatorio es permitido en la mayoría de países de América Latina. La abstención en el país es una de las más altas de América.[113]​[114]​[115]​[116]​ 

La Constitución Política de 1991 determina todas las normas, derechos y deberes de los colombianos y de las ramas del poder público.[117]​ La Constitución Política de Colombia ordena el poder público en tres ramas: ejecutiva, legislativa y judicial. De esta manera se busca evitar la concentración del poder. Cada rama cumple diferentes funciones y actúa de forma independiente pero armónica según la constitución.[117]​

Las relaciones exteriores son funciones del Presidente de la República como jefe de estado, que son delegadas al Ministerio de Relaciones Exteriores de Colombia o cancillería. La cancillería administra las misiones diplomáticas a otros países y representaciones ante organismos multilaterales.[118]​ En el siglo XX, Colombia practicó una política exterior de alianza incondicional con Estados Unidos, práctica que fue formulada por el expresidente Marco Fidel Suárez como la doctrina del «respice polum», mira hacia la «Estrella Polar». Dicha política fue complementada en el gobierno de Carlos Lleras Restrepo con la doctrina del «respice similia» o «mira a tus semejantes», con el propósito de diversificar relaciones con los países semejantes, y no solo con un actor privilegiado.[119]​ Sin embargo, los diferentes gobiernos han combinado ambas estrategias y el país ha participado en la construcción y desarrollo de organismos internacionales.[120]​

Colombia se adhirió a la OMC, mediante la Ley 170 de diciembre de 1994 y a partir del primero de abril de 1995, inició sus compromisos como miembro activo de dicha organización. La entrada en vigencia de la OMC y la adhesión a la misma, significó para el país aceptar todos los acuerdos en el ámbito multilateral a excepción de algunas reservas en los acuerdos sobre aeronaves.[121]​[122]​

Las labores de defensa recaen en el poder ejecutivo con el Presidente de la Nación como Comandante en jefe, quien delega sus funciones al Ministerio de Defensa, contando con las Fuerzas Militares de Colombia y la Policía Nacional de Colombia para labores de defensa y seguridad.[123]​ Por otra parte, el Departamento Administrativo de Seguridad (DAS) fue la principal agencia de inteligencia de Colombia entre 1960 y octubre de 2011, tenía la autonomía y atributos propios de cualquiera de los ministerios que forman parte del gobierno. Debido a escándalos de interceptación ilegal de llamadas, el DAS quedó disuelto en 2011; en su lugar, se constituyó como nueva agencia de inteligencia del país la Dirección Nacional de Inteligencia (DNI).

De acuerdo a la constitución y la ley, las Fuerzas Militares de Colombia tienen el monopolio de la coacción, y están conformadas por el ejército, la armada y la fuerza aérea, coordinando también las fuerzas de tarea conjunta.[124]​ Las Fuerzas Militares de Colombia son consideradas como una de las fuerzas armadas mejor preparadas del mundo.[125]​ Cuentan con un pie de fuerza pública de 885 842 efectivos (en 2019),[126]​ incluyendo, 20 651 oficiales, 45 588 suboficiales, 16 170 cadetes o alumnos, 214 000 soldados, y 15 407 civiles. Desde septiembre de 1981 el ejército participa en la Fuerza Multinacional de Paz y Observadores (MFO) en Sinaí.[127]​

En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Colombia ha firmado o ratificado:

Según la Constitución de 1991, Colombia está compuesta por 32 departamentos y un Distrito capital.[139]​ Los gobiernos departamentales se encuentran divididos en tres poderes: La rama ejecutiva, ejercida por el gobernador departamental, elegido cada cuatro años. Cada departamento tiene su propia asamblea departamental, corporación pública de elección popular regional que goza de autonomía administrativa y presupuesto propio.[140]​ Las asambleas departamentales están conformadas por no menos de 11 diputados ni más de 50, elegidos popularmente para un periodo de 4 años. Las asambleas departamentales emiten ordenanzas de obligatorio cumplimiento en su jurisdicción territorial o departamento.[141]​

Los departamentos están conformados por la asociación entre municipios. Actualmente hay 1120 municipios entre los que están el Distrito capital, y los distritos especiales[142]​Cada municipio o distrito es presidido por un alcalde, los cuales son elegidos para un período de cuatro años, de acuerdo con el calendario electoral del Consejo Nacional Electoral. En representación de la rama ejecutiva a nivel local, cada municipio elige un Concejo integrado por concejales, elegidos para períodos de cuatro años también.

Los territorios indígenas en Colombia son creados en común acuerdo entre el gobierno y las comunidades indígenas.[143]​ En casos en que los territorios indígenas abarcan más de un departamento o municipio, los gobiernos locales administran de forma conjunta con los consejos indígenas, en dichos territorios y como está establecido en los artículos 329 y 330 de la constitución de Colombia. Los territorios indígenas pueden llegar a tener carácter de entidad territorial cuando cumplen los requisitos de la ley.[143]​ Los territorios indígenas cubren un área aproximada de 30 845 231 ha, que se encuentran en mayor parte en los departamentos de Amazonas, Cauca, La Guajira, Guaviare y Vaupés, entre otros.[144]​

Colombia se encuentra ubicada en América, en el extremo noroccidental de América del Sur. El país es travesado por la cordillera de los Andes y la llanura amazónica, es el único país de América del Sur con costas sobre los océanos Atlántico y Pacífico.[15]​ Su ubicación latitudinal corresponde a 12º 27´46" norte y 4º 13´ 30" de latitud sur,[145]​ lo cual corresponde a la zona intertropical. El área total de Colombia es de 2 129 748 km² conformados por el territorio continental y las aguas marítimas.[146]​ El territorio continental de Colombia es de 1 141 748 km²[3]​ y el marítimo de 988 000 km², de los cuales 658 000 km² están en el mar Caribe y 330 000 km² en el océano Pacífico donde se encuentran las islas de Gorgona y Malpelo.[147]​

Colombia posee diferentes zonas climáticas. Por debajo de 1000 metros (3281 pies) de altura el clima es cálido (tierra caliente), donde las temperaturas están por encima de 24 °C (75.2 °F). Cerca de 82,5 % de la superficie total del país se encuentra en clima cálido.[148]​

La mayoría de la población del país vive en el clima templado (tierra templada, entre 1000 y 2000 metros de altura (3284 pies y 6562 pies), donde las temperaturas varían entre 17 y 24 °C (62.6 y 75.2 °F) y el clima frío (tierra fría, 2000 y 3000 metros de altura (6565 y 9843 pies).[148]​

En la "tierra fría" las temperaturas medias oscilan entre los 11 y 17 °C (53.6 y 62.6 °F). Más allá de la tierra fría se encuentran las condiciones alpinas de la zona boscosa y luego las praderas sin árboles de los páramos. Por encima de 4000 metros de altura (13 123 pies), donde las temperaturas son bajo cero, es la tierra helada, una zona de nieves perpetuas y hielo.[148]​

Clima tropical secoSan Andrés y Providencia

Clima oceánicoBogota

Clima ecuatorialValle del Cauca.

Clima polarNevado del Ruiz

El territorio colombiano presenta variedad en su relieve: sistema montañoso central, compuesto por las tres cordilleras andinas, sistema montañoso independiente de los Andes, las llanuras interiores y costeras y los valles interandinos.[149]​

En Colombia hay cuatro vertientes hidrográficas: la Vertiente del Pacífico, la Vertiente del Caribe, la Vertiente del Catatumbo y la Vertiente Atlántica. El Macizo colombiano es de gran importancia para el país, pues allí nacen los ríos Magdalena, Cauca, Patía y Caquetá. En la Sierra Nevada de Santa Marta nacen ríos como el Don Diego, el Ranchería y el Dibulla, los cuales desembocan en el mar Caribe y conforman la vertiente del Caribe. El río Atrato, que forma parte de esta vertiente, es uno de los más caudalosos del mundo con respecto al tamaño de su cuenca.[150]​

En un lugar conocido como el Nudo de los Pastos nacen los ríos que conforman la Vertiente del Pacífico. Una característica con respecto a las demás vertientes del Pacífico en América del Sur; el río San Juan es el más caudaloso, y el río Patía es el que mayor longitud. El río Amazonas y el río Orinoco forman la Vertiente Atlántica, la cual posee un área de 670 000 km². Los ríos que conforman el río Orinoco provienen de los Llanos Orientales, su caudal varía según los cambios climáticos. La Vertiente Amazónica se conforma por ríos selváticos caudalosos, debido a la alta pluviosidad de la zona, los cambios en los caudales se deben a las variaciones climáticas. La Vertiente del Catatumbo tiene una extensión de 18 500 km², conformada por el río Catatumbo que desemboca en el lago de Maracaibo en Venezuela. El río Zulia, río Sardinata, río Táchira y río Cucutilla, forman parte de la Vertiente del Catatumbo.[150]​[151]​ La ciénagas y lagunas en Colombia se encuentran ubicada en las cordilleras, donde existen llanuras inundables. En el país las ciénagas más extensas se encuentran en la Llanura del Caribe.[151]​

En Colombia se pueden evidenciar cinco regiones naturales por sus diferentes relieves, ecosistemas y climas.[152]​ Gracias a los diferentes ecosistemas que se pueden encontrar a lo largo de su territorio,[153]​ el país tiene el número más grande de especies por unidad de área en el planeta, siendo el segundo país más megadiverso del mundo.[154]​ Además, Colombia posee aproximadamente el 60% de los páramos existentes en el planeta, y cerca de &&&&&&&&&&031700.&&&&&031 700 humedales.[155]​ Cabe anotar que la mayor diversidad de flora y fauna dentro de sus biomas terrestres se halla en las selvas lluviosas, ubicadas en la Región Pacífica, la selva amazónica y el bosque andino.[156]​

En 1994, Colombia suscribió la Política Nacional de Biodiversidad que conformó el Sistema Naciónal de Áreas Protegidas «SINAP» que está a cargo 44 Parques Nacionales Naturales, 12 Santuarios de Fauna y Flora, 2 Reservas Nacionales Naturales, 1 Área Natural Única, 1 Vía Parque y 3 Distritos de Manejo Integrado.[157]​ Estas áreas son de suma importancia para protección de ecosistemas, la diversidad biológica, y la producción de agua. Se estima que aproximadamente el agua que se produce en estas zonas abastece a 25 millones de personas.[158]​

Según un informe del Fondo Mundial para la Naturaleza (WWF), cerca de la mitad de los ecosistemas que existen en Colombia se encuentran en estado crítico o en peligro. Asimismo, de las 1 853 especies de plantas evaluadas, 665 (36 %) se encuentran amenazadas de extinción, mientras que de 284 especies de animales terrestres evaluados, 41 están en peligro crítico, 112 amenazadas y 131 son vulnerables. Según la WWF, la degradación ambiental en Colombia es debido a la extracción de petróleo y minerales.[159]​ Por otra parte, en 2015 la minería ilegal afectaba a 21 departamentos del país,[160]​ causando considerables daños ambientales.[161]​ En cuanto a la deforestación, se estima que 280 mil hectáreas de bosque se talan cada año.

Aunque Colombia no es un gran emisor de gases de efecto invernadero gracias a que cuenta una matriz de generación de energía eléctrica “limpia”, el país ha reiterado su apoyo al acuerdo de París sobre el clima. Esto a pesar de que la producción de electricidad del país proviene principalmente de fuentes de energía renovables: 69.97 % se obtiene de la generación hidroeléctrica.[36]​ Además el gobierno ha tomado medidas para proteger su medio ambiente como la Política Nacional de Cambio Climático y el impuesto al carbono.[35]​ 

Colombia es un país de tamaño intermedio, a pesar de ello posee aproximadamente entre el 10 % o 20 % de especies de plantas a nivel mundial, con entre 45 000 o 55 000 de especies de plantas.[162]​ Una cifra muy alta para un país de esta proporción; Brasil, un país 6.5 veces más grande que Colombia posee 55 000 y en África, específicamente al sur del Sahara hay cerca de 30 000 especies.[162]​ Colombia es primera en variedad de orquídeas, posee más de 50 000 especies de flores y alrededor de 50 especies de plantas carnívoras.[163]​[164]​

El país ocupa el tercer lugar en especies vivas y primer lugar en especies de aves con 1 876 especies, esto equivale al 19 % de las especies en el mundo y a 60 % de las especies en Sudamérica. El ave nacional de Colombia es el cóndor de los Andes, que se encuentra representado en el escudo de Colombia. En el país hay 14 especies y 300 familias de mariposas, lo que lo convierte en el segundo país con más variedad de mariposas y con una variedad mayor de 250 000 de coleópteros. A nivel mundial, Colombia es primera en especies de anfibios al poseer el 15 %, el 25 % de cocodrilos entre los cuales destacan el C. acutus y el C. intermedius, el 30 % de tortugas y 222 especies de serpientes. El país se ubica en el cuarto lugar a nivel mundial en mamíferos con 456 especies descubiertas y quinto en especies de primates con 30 especies. En Colombia existen aproximadamente 1 600 especies de agua dulce y 1 200 especies de aguas marinas.

Las aguas colombianas en el mar Caribe poseen gran diversidad de arrecifes de coral, praderas de pastos marinos, manglares, fondos blandos, playas y estuarios que alimentan cientos de especies de fauna y flora. 

El Sistema de Información sobre Biodiversidad de Colombia SIBC, basado en la cartilla del Instituto de Investigación de Recursos Biológicos Alexander von Humboldt, estima que el número de especies de aves en el país es de 1 885, con un registro de 197 especies migratorias, sin embargo, que el número de especies conocidas es cambiante, pues se descubren especies nuevas regularmente.[165]​

La demografía es estudiada por el Departamento Administrativo Nacional de Estadística (DANE). Según los datos del censo nacional de 2021, el país tiene una población de 51 049 498,[166]​ que lo constituye como el cuarto país más poblado en el Continente Americano después de los Estados Unidos, Brasil y México. De ellos, el 51,2 % son mujeres y el 48,8 % son hombres.[167]​ La mayor parte de la población se encuentra en el centro (región andina) y norte (región Caribe) del país, mientras que al oriente y sur (región de los llanos orientales y amazonía, respectivamente) se encuentran zonas bastante extensas sin poblaciones grandes y generalmente despobladas. Los diez departamentos de tierras bajas del oriente (aproximadamente el 54 % del área total), tienen menos del 3 % de la población y una densidad de menos de una persona por kilómetro cuadrado.[168]​

El movimiento de población rural hacia áreas urbanas y la emigración fuera del país han sido significativos. La población urbana aumentó del 28 % de la población total en 1938, al 75 % en 2005; sin embargo en términos absolutos la población rural aumentó de 6 a 10 millones en ese período. En cuanto a la emigración, el DANE estima que alrededor de 3 331 107 colombianos viven en el exterior, principalmente en Estados Unidos, España, Venezuela y Canadá. Los más propensos a emigrar son los originarios del interior del país y de algunos centros urbanos, destacándose un contingente importante de intelectuales y talentosos que forman parte del fenómeno llamado fuga de cerebros.[169]​

De acuerdo con el Índice de Desarrollo Humano, Colombia se ubicó en el puesto 91 a nivel mundial según el informe de 2012 con un IDH de 0,719. Sin embargo, no todas las regiones de Colombia presentan el mismo nivel de desarrollo. La principal zona de alto desarrollo corresponde a la Región Andina en ciudades como Bogotá, Medellín y Cali, que constituyen el denominado «Triángulo de Oro» y la ciudad de Bucaramanga en la región andina nororiental, la Región Caribe y sus principales ciudades como Barranquilla, Cartagena y Santa Marta constituyen el segundo mayor polo de desarrollo a nivel nacional, destacándose notoriamente por su amplia industria turística y portuaria, así como su amplia diversidad poblacional.[170]​ 

El Censo realizado por el DANE en el 2005 arrojó que 86 % de la población no se considera parte de una minoría étnica, cifra dividida en 49 % de Mestizos y 37 % de Blancos,[20]​ un 10,6 % de afrocolombianos[173]​ que representan la cuarta población negra más grande de América, después de los Estados Unidos, Brasil, Ecuador y Haití. Los indígenas conforman el 3.4 % de la población nacional y los gitanos el 0,01 %.[173]​ La diversidad étnica en Colombia es el resultado de la mezcla entre españoles, amerindios y afrodescendientes. Los pocos pueblos indígenas que quedan en el país, son comúnmente olvidados y sus costumbres lentamente se van desapareciendo.[174]​ Estudios genéticos entregan proporciones que promedian entre un 61,0% a un 63,0% de aporte europeo,[175]​ un 26,0% a un 29,0% de aporte amerindio,[175]​ y de un 10% a un 11% de aporte africano.[176]​

Colombia es el país con la mayor población inmigrante de origen venezolano en el mundo, contando en 2019 con 1 626 000 personas.[177]​ Entre los inmigrantes, que no fueron muchos en comparación con otros países latinoamericanos más abiertos a la migración (como Argentina, Brasil o México), los grupos mayores son los provenientes de España,[178]​ Estados Unidos, Reino Unido, Países Bajos y Alemania, así como judíos y gitanos. A finales del siglo XIX, Barranquilla recibió una gran cantidad de inmigrantes europeos y del Medio Oriente (Líbano, Siria y Palestina), así como estadounidenses, cubanos y chinos, entre otros, que se dispersaron por toda la geografía nacional. Aunque una gran parte de la migración de Medio Oriente era cristiana (esto, debido a las políticas migratorias colombianas de principios del siglo XX), en Maicao (Guajira), se encuentra la comunidad musulmana más numerosa de Colombia. Descendientes de los inmigrantes levantinos también tienen fuerte presencia en el departamento de Córdoba, en Barranquilla, en Santa Marta, en Bogotá y en el Valle del Cauca. Inmigrantes de otros países latinoamericanos como Brasil, Chile, Ecuador, Argentina, Perú y las Antillas, entre otros, también tienen presencia, aunque mínima en Colombia,[179]​ sin embargo existe en la actualidad una tendencia migratoria importante de venezolanos en el país, principalmente por la situación económica y de inseguridad en su país.[180]​[181]​[182]​

Bogotá
Medellín
Cali

Barranquilla
Cartagena
Cúcuta

Aproximadamente el 75 % de la población vive en zonas urbanas, un porcentaje por encima de la media mundial que en 2010 se ubicó en el 51,3 % según las Naciones Unidas. Bogotá es la ciudad más poblada y el principal centro económico del país. Colombia presenta grandes aglomeraciones urbanas a lo largo de su territorio, Medellín y Cali poseen una población de más de dos millones de habitantes y Barranquilla de más de un millón. Otras veinticinco ciudades superan los doscientos mil habitantes.[169]​[183]​[184]​

El sistema educativo colombiano está regulado por el Estado a través del Ministerio de Educación de Colombia. El sistema educativo se divide en preescolar (para aquellos que tienen menos de seis años), educación básica (duración de nueve años comprende la primaria y la secundaria), educación media (duración de dos años, comprende los grados décimo y undécimo), y educación superior (universidad). La educación básica y media es evaluada por medio de los exámenes nacionales denominados Pruebas Saber (ICFES) en los grados 3°, 5°, 9° y 11°.[185]​ 

La educación superior se imparte en dos niveles: pregrado y posgrado. El nivel de pregrado tiene, a su vez, tres niveles de formación: nivel técnico profesional (relativo a programas técnicos profesionales), nivel tecnológico (relativo a programas tecnológicos), nivel profesional (relativo a programas profesionales universitarios). En el nivel de postgrado se reconocen las especializaciones, las maestrías y los doctorados.[185]​ La educación superior es evaluada mediante la Pruebas Saber (ICFES): las Pruebas Saber TyT evalúan las competencias genéricas y las competencias específicas comunes de los estudiantes de programas técnicos y tecnólogos que hayan aprobado el 75% de los créditos académicos.[186]​Las Pruebas Saber Pro es un examen que comprueba las competencias de los estudiantes que cursan el último año de los programas académicos de pregrado en las instituciones de Educación Superior (universidades).[187]​[185]​

La tasa de alfabetización total de adultos era en 2016 de aproximadamente el 94,65 %, siendo la tasa en hombres del 94,4 % y de mujeres en 94,89 %.[188]​ Colombia ocupó el puesto 59 entre 72 naciones evaluadas en las pruebas PISA de 2015.[189]​

El ICETEX es un organismo estatal de financiación para acceder a la educación superior mediante el «otorgamiento de créditos educativos y su recaudo, con recursos propios o de terceros, a la población con menores posibilidades económicas y buen rendimiento académico».[190]​ 

La Universidad Nacional de Colombia es la mayor universidad de la nación, con cerca de 53 879 alumnos. Las universidades normalmente se dividen en facultades.[191]​ Por otra parte, el Servicio Nacional de Aprendizaje (SENA) es una entidad que ofrece formación gratuita con programas técnicos, tecnológicos y complementarios.[192]​

Según el ranking: Webometrics Ranking of World Universities de 2021[193]​ elaborado por el Consejo Superior de Investigaciones Científicas español (CSIC) quien mide el impacto de las investigaciones científicas de las distintas universidades del mundo; las diez universidades Colombianas que realizan un mejor desempeño en el mundo académico son las siguientes:

Colombia es la nación con la Asistencia sanitaria universal más alta de América Latina, superando el 95% de su población en el año 2019.[195]​ Según la OMS, Colombia ocupa el puesto 22 a nivel mundial entre 191 países en cuanto al funcionamiento general de su sistema de salud.[196]​ El sistema de salud colombiano se caracteriza por estar formado por dos sistemas coexistentes: el régimen contributivo (privado) y el régimen subsidiado (gratuito) por medio del Sisbén. Ambos regímenes proporcionan cobertura universal, acceso por igual a medicamentos, procedimientos quirúrgicos, servicios médicos y odontológicos.[197]​

El Ministerio de Salud y Protección Social es el responsable de desarrollar políticas en materia de salud, así como el encargado de regular la salud en el país. La entidad promotora de salud (EPS) son empresas regionales de sanidad que se encargan de implementar los objetivos fijados por el Ministerio de Salud, de desarrollar las líneas maestras y sus protocolos, y también de supervisar la actuación de los cuidados en salud. Para cumplir con sus responsabilidades las EPS deben conformar una red de servicios para lo cual cualquier entidad promotora de salud puede contratar a clínicas, hospitales o un instituto prestador de salud (IPS) de forma independiente y autónoma o pueden garantizar el acceso a los servicios con su propia red. Además deben contratar un porcentaje determinado mínimo con instituciones prestadoras de servicios públicos.[197]​ 

El sistema de salud colombiano es descentralizado, las fuentes de financiación de la salud son principalmente del Sistema General de Participaciones (SGP), los aportes de empleadores y trabajadores al régimen contributivo que se administran por la Administradora de los Recursos del Sistema General de Seguridad Social en Salud (ADRES),[198]​ y los recursos obtenidos en los juegos de suerte y azar administrados por Coljuegos, también existen otras fuentes de financiación de menor envergadura.[199]​

La mayoría de los colombianos muere por enfermedades crónicas. La principal causa de defunción son las enfermedades isquémicas del corazón (infartos) en el 16,3 % de las muertes, seguido por las enfermedades cerebrovasculares, la tercera causa son las enfermedades pulmonares debido al consumo de cigarrillo.[200]​

La tasa de mortalidad infantil de Colombia es de 14,8 entre 100 nacidos vivos en 2017.[201]​

La ciencia y la tecnología en Colombia se comenzó a proyectar de manera de importante a principios del siglo XX, época en la cual se comenzó con pequeñas invenciones propias del país, aunque mucho antes, ya habían entrado grandes creaciones de otros lugares del mundo. Colombia es reconocida a nivel mundial, por su calidad en la medicina, específicamente la medicina estética, lo que la posicionado como una potencia en turismo de salud.[202]​[203]​[202]​[204]​[205]​ Entre los médicos más reconocidos en Colombia están el neurofisiólogo Rodolfo Llinás quien ha aportado en la busca de la cura contra la enfermedad de Alzheimer[206]​ y quién enunció la Ley de Llinás, y también el inmunólogo Manuel Elkin Patarroyo, el cual descubrió la primera vacuna sintética contra la malaria.[207]​

Otros grandes inventos relacionados con la medicina se han producido en el país, tales como el marcapasos, creado por el ingeniero electrónico Jorge Reynolds el cual ha sido de gran importancia para quienes sufren de insuficiencia cardiaca. De la misma manera fueron inventados en Colombia, el LASIK, una de las técnicas más utilizadas para la corrección de las ametropías esferocilindricas con láser en el mundo, la Válvula de Hakim, entre otros que no solo han aportado a la medicina nacional, sino a nivel mundial.[208]​ En febrero de 2013, Colombia fue es el primer país de América Latina en practicar una cirugía robótica en urología pediátrica, en el mundo solo se han realizado 10 procedimientos de este tipo.[209]​[210]​

La problemática que ha vivido la nación frente a la violencia, le ha permitido innovar en tecnología militar o de guerra, para su ejército e incluso para otros ejércitos del mundo. Uno de los inventos más llamativos fue el robot ‘Arcadio’, que está diseñado para detectar artefactos explosivos bajo tierra y desactivarlos, tales como las mina antipersona.[211]​[212]​ También se han creado por primera vez en el país aviones, específicamente de entrenamiento[213]​ y otros materiales como por el ejemplo, el cartucho de seguridad, con el fin de disminuir los índices de muertes y heridas a causa de disparos accidentales.[214]​[215]​[216]​

Colombia es uno de los ocho países latinoamericanos en tener objetos en órbita. El 7 de abril de 2007, el picosatélite Libertad I (diseñado por la Universidad Sergio Arboleda de Bogotá) fue puesto en órbita en uno de los lanzamientos hechos desde el Cosmódromo de Baikonur. El Libertad I órbita alrededor del planeta y según proyecciones seguirá haciéndolo por 5 o 6 años.
En 1955 Colombia firmó un tratado con Estados Unidos para el desarrollo de energía nuclear con fines pacíficos y en 1960 es admitida en el Organismo Internacional de Energía Atómica. Colombia es uno de los pocos países latinoamericanos en tener un reactor nuclear en operación, el IAN-R1, fabricado en Estados Unidos, su función es la producción de neutrones con fines de investigación.[217]​

En los últimos años el país ha decidido implementar nuevas maneras de innovar en tecnología siendo asesorada por países como Estados Unidos, Rusia, Corea del Sur, entre otros, pues se estima que Colombia solo utiliza el menos del 0,10 % del PIB nacional, para el sector de la ciencia, y la tecnología, y el 0,19 % del PIB en investigación y desarrollo, justamente por necesidad de invertir más dinero en el sector de defensa y seguridad.[218]​ En el año 2006 se aprobó la Ley de Ciencia, Tecnología e Innovación, con el fin de crear empresas e industrias con base en la tecnología.[219]​

Colombia es una economía emergente y una potencia económica de la región.[221]​[222]​ También forma parte del bloque de países emergentes CIVETS, de la Organización Mundial del Comercio (OMC) y de la Organización para la Cooperación y el Desarrollo Económicos (OCDE). Su PBI PPA ocupa el cuarto puesto en América Latina detrás de Argentina, México y Brasil, además de ser el 28 a nivel global. El PBI nominal colombiano es el cuarto más alto de América Latina después de los de Brasil, México y Argentina, y ocupa el puesto 29 a nivel mundial. El sector financiero en la nación ha crecido un 6.7 % entre el 2005-2010, debido a la liquidez favorable de la economía colombiana.[223]​ En 2012 el sector de servicios representó el 55.1 % del PIB de Colombia, mientras que 68 % de 23.08 millones de colombianos formaron parte de la fuerza laboral en este sector.[224]​

La economía de Colombia ha experimentado un crecimiento promedio anual de 5.5 % desde 2002.[225]​ En el 2012, 23.8 millones de colombianos sirvieron como fuerza laboral en la economía, con un ingreso promedio de US$10 700, produciendo US$500 000 millones para el Producto Interno Bruto (PIB) del país. Sin embargo, la desigualdad en la distribución de riqueza mantiene a un 29.3 % (2014) de colombianos viviendo por debajo de la línea de pobreza nacional,[226]​ a lo que se suma el deficiente sistema pensional. Desde 2011 desempleo ha marcado un dígito, (9.2 % en noviembre de 2011)[227]​ y el subempleo un 32.7 % (noviembre de 2011).

El Ministerio de Hacienda y Crédito Público define, formula y ejecuta la política económica del país. La moneda nacional es el peso colombiano. El Banco de la República es un organismo independiente que controla la cantidad de dinero y control cambiario de divisas que circula en la economía para evitar recesiones y desempleo a causa de la inflación, además de controlar el crédito interbancario. Juntos, el MHCP y el BRC regulan el funcionamiento de la economía a nivel nacional con el apoyo del Ministerio de Comercio, Industria y Turismo (MCIT).[228]​[229]​ El sector empresarial de Colombia se encuentra agremiado en la Asociación Nacional de Industriales (ANDI) que trata de mantener grupos de empresas del mismo sector económico para que actúen en común acuerdo para el desarrollo.[230]​

La agricultura es regulada por el Ministerio de Agricultura y Desarrollo Rural, que planea el desarrollo de la agricultura y la pesca en compañía del Ministerio de Hacienda y Crédito Público para el desarrollo económico del país y el sostenimiento de la población.[231]​ La agricultura se caracteriza por los cultivos tecnificados por región de caña de azúcar, café, flores, algodón, plátano, banano, sorgo, maíz, arroz, palma africana, papa, yuca, entre otros. Por la gran variedad de climas y terrenos, Colombia presenta una gran variedad de especies de fauna y flora para el consumo o utilización por parte de humanos.[232]​ Los agricultores colombianos se encuentran agremiados en la Sociedad de Agricultores de Colombia (SAC).[233]​

El café de Colombia es una indicación geográfica protegida, por la Unión Europea desde el 27 de septiembre de 2007.[234]​ Dicha denominación se le otorga al café 100 % arábigo (coffea arabica) producido en las regiones cafeteras de Colombia, delimitadas entre la latitud Norte 1° a 11°15, longitud Oeste 72° a 78° y rangos específicos de altitud que pueden superar los 2000 metros sobre el nivel del mar. El término café de Colombia, también es una marca de certificación registrada en Estados Unidos el 7 de julio de 1981,[235]​ y en Canadá el 6 de julio de 1990.[236]​ Así mismo, está reconocido como Denominación de Origen Protegida en otros países del mundo,[237]​ como Ecuador,[238]​ Bolivia[239]​ y Perú.[240]​ A nivel mundial, Colombia es el cuarto país productor de café[241]​ y el mayor productor de café suave en el mundo.[242]​ Los principales países importadores del café de Colombia son Estados Unidos, Alemania, Japón, Países Bajos y Suecia.

Por su parte la ganadería adquirió importancia desde mediados del siglo XIX, en que se introdujeron al país los primeros ejemplares de ganado Durham. A medida que avanzó el siglo de la ganadería se consolidó como el segundo renglón de importancia en la economía y dio origen a una amplia industria lechera.[243]​ El país cuenta con el hato ganadero más grande de América Latina con un inventario bovino de 26,9 millones de cabezas en 2008.[244]​ El Brahman colombiano, ideal para la producción de carne en condiciones tropicales, se destaca por tener una genética de alta calidad en el mundo.[244]​

El sector industrial ha tenido un crecimiento sostenidos en los últimos años. En el año 2000 la producción en manufacturera era de US$29 240 millones, en 2010 alcanzó aproximadamente US$80 000 millones. Se destaca en este periodo la producción de bienes de media y alta tecnología, que paso del 31,7 % en el año 2000 al 34,6 % en el 2010. La apertura económica a los mercados extranjeros han favorecido las exportaciones de productos de alta y media tecnología. Estos pasaron de $US 2251 millones a US$ 4868 en 2010, lo cual representa el 35 % de las exportaciones de la industria.[246]​

Por otra parte, el sector de la construcción en Colombia tuvo un crecimiento aproximado del 7,9 % en la primera década. En contraste con el PIB de la economía colombiana que tuvo un crecimiento del 5,9 % en 2011.[247]​ En la primera mitad de la década del año 2000 las edificaciones tuvieron un crecimiento del 8,3 %. En los años 2009, 2010 y para el 2011 se registró un crecimiento de 6,2 % en los tres primeros meses de este año. Las obras civiles han crecido durante la primera década, a un promedio de 8,0 %, se asocia este crecimiento a las necesidades que posee el país en infraestructura.[248]​

Uno de los sectores de mayor crecimiento ha sido el sector minero-energético, el cual se conforma por los subsectores de la minería, el petróleo, el gas y la energía. En la última década este sector pasó de US$ 8300 millones en 2000, a más de US$ 40 000 millones en 2011.[249]​ El sector petrolero en Colombia ha tenido un crecimiento del 3,4 % en la última década. En los últimos años la actividad exploratoria ha aumentado, en el año 2008 fue del 15,4 %, en 2009 fue del 15,4 %; 2010 16,9 % y a los tres primeros meses de 2011 un crecimiento del 17,7 %. En este periodo ha habido un fortalecimiento de la Agencia Nacional de Hidrocarburos. En cuanto al sector energético registra un crecimiento del 3,0. El sector de minas y canteras ha tenido un crecimiento del 6,5 %. El aporte al PIB de la producción de minería, petróleo y energía eléctrica equivaldría al 12.5 % del PIB total. Pero el peso de diferentes subsectores es diferente. El sector petrolero aporta el 7.1 % al PIB total, el minero un 2,7 %, el eléctrico 2,3 % y la producción de gas un 0,5 %.[249]​

El sector eléctrico está mayormente dominado por generación de energía hidráulica (67 % de la producción) y generación térmica (27.4 %) en 2010, la generación de energía eléctrica en este año fue de 56 877.6 GWH. No obstante, el potencial del país en nuevas tecnologías de energía renovable (principalmente eólica, solar y biomasa) apenas si ha sido explorado. El país tiene importantes recursos de pequeña hidráulica, eólica, y solar que permanecen en gran parte sin explotar. De acuerdo con un estudio del Programa de Asistencia en Gestión del Sector Energético del Banco Mundial, la explotación del gran potencial eólico del país podría cubrir más de la totalidad de sus necesidades actuales de energía. El 80 % de la capacidad instalada en Colombia para producción de electricidad proviene de energías renovables.[250]​[251]​[252]​

En cuanto a minería, los minerales más explotados son el oro, la plata, esmeraldas, platino, cobre, níquel, carbón. Uno de las extracciones más importantes es la mina de carbón del Cerrejón en La Guajira, que es la mina de carbón a cielo abierto más grande de América Latina.[253]​ La explotación de oro y cobre datan de la época precolombina y se desarrolló en la zona andina del país, con grupos étnicos como los Muiscas, los Quimbaya, los Tayrona y los Zénues, los cuales todavía son muy explotados artesanalmente, existiendo gran cantidad de ilegalidad,[254]​ y desde hace algunos años multinacionales han comenzado a hacer parte de este negocio. A nivel mundial es el principal productor de esmeraldas, concentrándose su explotación en el interior del país, en departamentos como Boyacá y Cundinamarca.[255]​[256]​ Por su parte, el petróleo es explotado en su mayoría por Ecopetrol. En Barrancabermeja se encuentra la refinería de petróleo más grandes del país.[257]​[258]​

En 2012, la agricultura en Colombia aportó un 6.8 % al PIB nacional y el 18 % de la fuerza laboral se dedicó a la agricultura, ganadería y pesca.[224]​ Sin embargo, el principal producto de exportación de Colombia es el petróleo, cuyas reservas estimadas son 2.377 millones de barriles en 2012,[259]​ las cuales son desarrolladas por Ecopetrol, en marzo de 2013 se produjo 1 013 481 barriles de petróleo por día.[260]​

Así mismo, se destaca la industria textil,[261]​ alimenticia,[224]​ automotriz[262]​ y la petroquímica.[263]​ También, el procesamiento de alimentos, la producción de café, aceite, bebidas, cemento, oro, carbón, esmeraldas, níquel, flores cortadas, bananas, entre otros.[224]​ Las exportaciones de Colombia ascendieron a US$ 59.96 mil millones en 2012,[224]​ El sector del comercio exterior comprendido por las importaciones y exportaciones del país, ha tenido un crecimiento mayor al 30 % en el año 2011. En los últimos años el país ha firmado diferentes acuerdos comerciales con otros países, varios de ellos actualmente vigentes.[264]​


La presencia de turistas pasó de medio millón en 2003 a 1,3 millones en 2007, lo que le significó a Colombia varios reconocimientos internacionales. En 2006, una de las mejores editoras de guías de viaje en el mundo, Lonely Planet, escogió a Colombia como uno de sus destinos top 10 mundiales de 2006. Las mejoras en la seguridad del país fueron reconocidos en noviembre de 2008 con una revisión de los consejos de viajes sobre Colombia emitidos por la Oficina de Relaciones Exteriores británica. En el 2013 ingresaron en el país 3 747 945 turistas,[265]​ la mayoría provenientes de países América y Europa.[266]​ Por su parte, el turismo interno tuvo un incremento del 10,3 % en 2012 con respecto a 2011.[267]​[268]​

Entre los lugares de interés turístico están el histórico barrio de la Candelaria en Bogotá, la ciudad amurallada y las playas de Cartagena de Indias, las islas del Rosario, las playas y el centro histórico de Santa Marta, el Parque nacional Tayrona, el desierto y las playas de la Guajira, las ciudades coloniales de Santa Fe de Antioquia, Popayán, Tunja, Villa de Leyva y Santa Cruz de Mompox (especialmente durante Semana Santa), la catedral de Las Lajas en Nariño y la catedral de Sal de Zipaquirá. Los turistas visitan el país durante numerosos festivales, ferias y carnavales como la Feria de las Flores en Medellín, el Festival Nacional del Folclor de Ibague, el Carnaval de Barranquilla, la Feria de Cali, la Feria de Manizales, el Carnaval de Negros y Blancos de Pasto, el Festival de la Leyenda Vallenata de Valledupar, las Fiestas del 20 de enero de Sincelejo, las Fiestas del Mar de Santa Marta, el Festival Iberoamericano de Teatro de Bogotá, el Reinado Nacional de la Belleza en Cartagena, entre otros.

La gran variedad de la geografía, la flora y la fauna de Colombia ha dado lugar al desarrollo de una industria ecoturística que se concentra en los parques nacionales del país. Dentro de los destinos ecoturísticos más importantes están el parque Tayrona y el Cabo de la Vela en la península de La Guajira (en la costa del Caribe), el volcán Nevado del Ruiz, el cañón del Chicamocha y el desierto de la Tatacoa (en la Región Andina), el parque nacional Amacayacu en la cuenca del río Amazonas, y las islas de Malpelo y Gorgona en el Pacífico. Colombia cuenta con siete sitios declarados Patrimonio de la Humanidad por la Unesco.

En Colombia existen algunas deficiencias o carencias en puentes, aeropuertos, puertos marítimos, transporte fluvial y ferrovías.[270]​ En respuesta a esta situación y ante el crecimiento económico y la apertura comercial, se han hecho esfuerzos por mejorar la infraestructura del país, aumentando la inversión en grandes proyectos. Cabe destacar, entre ellos, la Ruta del Sol, autopista que optimizará la conexión entre el centro del país y la costa Caribe;[271]​ la Troncal Bogotá-Buenaventura,[272]​ que incluye el túnel vehicular más largo de América[273]​ y permitirá conectar la capital con el principal puerto sobre el Pacífico; y la Transversal de Las Américas,[274]​ en la costa Caribe, entre otras grandes obras. Colombia es el país de la región que más invierte en infraestructura, solo superado por Brasil.[275]​

El gobierno delega a proveedores los servicios de acueducto y alcantarillado. La infraestructura en acueducto en Colombia cubre el 93 % del país, mientras que la cobertura en alcantarillado el 86 %,[276]​ con una continuidad promedio de 86 % registrada en el 2003.[277]​ El promedio de uso de agua urbano (l/c/d) en el año 2006 fue de 59 % según la SSPD, consumiéndose 1188 millones de metros cúbicos, de los cuales el 80 % se usó para consumo doméstico.[278]​ En 2006 la tarifa de agua y alcantarillado urbano promedio fue de US$11,40 al mes.[278]​ La porción del porcentaje de agua residual recogido y tratado fue de 25 %,[278]​ mientras que la inversión anual en agua y saneamiento fue de US$10 per cápita. El porcentaje de autofinanciamiento por parte de las empresas de servicio público fue del 26 %.[279]​

En 2021, Colombia contaba con 204.389 km de carreteras, de los cuales 32.280 km estaban pavimentados. [280]​ Al cierre de 2017, el país contaba con alrededor de 2100 km de carreteras duplicadas. [281]​ El transporte en Colombia es regulado dentro de las funciones del Ministerio de Transporte y entidades como el Instituto Nacional de Vías (INVÍAS) encargada de la Red Nacional de Vías (13 000 km),[282]​ la Aerocivil, encargada del transporte aéreo civil y de los aeropuertos,[283]​ la Dirección General Marítima (DIMAR),[284]​ entre otras y bajo la vigilancia de la Superintendencia de Puertos y Transporte.[285]​

El sistema portuario colombiano, privatizado en la década de 1990, está conformado por aproximadamente 122 instalaciones. Existen las sociedades portuarias regionales de Buenaventura (principal puerto marítimo de Colombia), Barranquilla, Tumaco, Cartagena y Santa Marta. Hay otras nueve sociedades portuarias para servicio público, siete sociedades portuarias privadas, cuarenta y cuatro muelles homologados y diez embarcaderos para pequeñas embarcaciones, entre otras menores.[287]​

Los sistemas urbanos de transporte masivo se desarrollaron primero en Medellín, Bogotá y Pereira. La congestión de tránsito en Bogotá se ha agravado por la falta de transporte ferroviario. Sin embargo, este problema se ha aliviado parcialmente por el desarrollo de TransMilenio desde el año 2000 y un sistema de restricción de vehículos basado en el número de las placas llamado Pico y Placa [cita requerida]. Desde 1995, Medellín, fue la primera ciudad del país en tener sistema de transporte masivo, además de ser la única que cuenta con un ferrocarril urbano conocido como el Metro de Medellín, que conecta la mayoría de su área metropolitana. Un sistema de teleférico alto, Metrocable, se añadió en 2004 para vincular algunos de los barrios más pobres de las montañas de Medellín con el Metro. A finales de 2011, un sistema de buses articulados, llamado Metroplús comenzó a operar en Medellín. Un sistema de buses de tránsito rápido llamado Transmetro, similar al TransMilenio en Bogotá, comenzó a funcionar en julio de 2010 en Barranquilla. Además de estas ciudades, el tercer sistema integrado de transporte masivo del país, lo ejecutó una ciudad intermedia, Pereira, con Megabús, desde 2006 y actualmente se encuentra en construcción la línea más larga de cable aéreo de transporte masivo de pasajeros Megacable, Bucaramanga (Metrolínea, desde 2009), Cali (Masivo Integrado de Occidente) y Cartagena (Transcaribe, desde 2015).

El sistema aeroportuario colombiano cuenta con alrededor de 1101 aeropuertos y aeródromos entre públicos y privados, de los cuales trece son internacionales. El principal terminal aéreo del país es el El Dorado de Bogotá, que con alrededor de 20 millones de pasajeros transportados al año es uno de los aeropuertos de mayor dinámica en América Latina. En la actualidad está en proceso de ampliación y modernización para adaptarlo a las nuevas exigencias de demanda. El país está próximo a contar con aeropuertos internacionales más aptos para el servicio de pasajeros del siglo actual, con la modernización de múltiples terminales de pasajeros, mejoramientos y ampliaciones de pista, como las presentadas en los Aeropuerto Internacional Matecaña de la ciudad de Pereira, Aeropuerto Internacional José María Córdoba de Rionegro, Aeropuerto Internacional El Edén de Armenia, Aeropuerto Internacional Alfonso Bonilla Aragón de Cali, Aeropuerto Internacional Camilo Daza de Cúcuta, Aeropuerto Internacional Ernesto Cortissoz de Barranquilla así como del principal aeródromo de la región Aeropuerto Internacional El Dorado de Bogotá, entre otros. Además, se espera que el aeropuerto Antonio Nariño de Pasto y El Caraño de Quibdó, Chocó se conviertan en aeródromos internacionales.

Las comunicaciones en Colombia son reguladas dentro de las funciones del Ministerio de Tecnologías de la Información y las Comunicaciones de Colombia y la Comisión de Regulación de Comunicaciones (CRC). En 2009 se reportaron 6.8 millones[288]​ de líneas de telefonía fija y en 2011 47.8 millones[289]​ de abonados a telefonía móvil, lo que equivale a una cobertura del 103 % de la población. El código de dominio de nivel superior geográfico en Internet (ccTLD) para Colombia es .co; este fue administrado por la Universidad de los Andes desde 1991 hasta el 2004, cuando el consejo de estado determinó que el encargado del dominio colombiano debía ser el Ministerio de Comunicaciones. En septiembre de 2009 fue otorgado un contrato de concesión a.CO Internet S.A.S, con el fin de administrar dicho dominio. En 2011 el número de usuarios de Internet fue de 25 millones, con una tasa de penetración del 56 %[290]​ sobre la población total, la tercera más alta de América Latina; así mismo, el número de suscriptores de banda ancha llegó a 2.3 millones, con tasa de penetración de 5.1 %.

El diario de mayor circulación nacional es El Tiempo, de la Casa Editorial El Tiempo (CEET).[291]​ El segundo en importancia es El Espectador. También están El Espacio, La República, Portafolio y El Nuevo Siglo. Entre los diarios regionales se destacan El Colombiano, de Medellín, El País, de Cali, El Heraldo, de Barranquilla, El meridiano (Córdoba) de Montería, El Nuevo Día de Ibague, el Diario del Sur de Pasto y Vanguardia Liberal, de Bucaramanga, entre otros.

La televisión en Colombia se encuentra en proceso de cambio de la radiodifusión analógica a la digital. En 2008 el estándar DVB-T desarrollado por Digital Video Broadcasting (DVB) fue escogido por el país. En diciembre de 2011 se actualizó a DVB-T2. El apagón analógico en Colombia se prevé para el 2022.[292]​[293]​ La televisión en Colombia es regulada por Autoridad Nacional de Televisión. Los canales públicos nacionales son Señal Colombia, Señal Institucional, los canales privados nacionales, Caracol Televisión y RCN Televisión y uno estatal operado por un consorcio privado, el Canal 1. Existe un canal privado a nivel local, llamado CityTV. Los canales públicos regionales nacionales son Canal Capital, Telecaribe, Teleantioquia, Telecafé, Telepacífico, Teveandina, Teleislas, Teleboyacá y Televisión Regional del Oriente. Los canales nacionales con emisión cerrada son ZOOM TV y Canal Congreso.

Colombia es el país de América Latina con la mayor cobertura de televisión por cable, ya que llegaba al 84,4 % de los colombianos en 2013.[294]​ Los principales prestadores del servicio de televisión por suscripción en Colombia son Claro y UNE, y los de televisión vía satélite, DirecTV y Telefónica Colombia.[295]​

En Colombia, la radio comercial se inició en la década de 1920.[296]​ La radiodifusión comenzó en septiembre de 1929, con la estatal HJN, el predecesor de Radiodifusora Nacional de Colombia, y de gestión privada, la Voz de Barranquilla (HKD).[297]​ Desde 2011, Colombia tiene tres principales cadenas de radio nacionales: la estatal Radiodifusora Nacional de Colombia y las redes privadas Caracol Radio y RCN Radio, que aparecieron en la década de 1940.[298]​

Las culturas indígenas asentadas en el país a la llegada de los españoles, la cultura europea (de España y de otros lugares de Europa), y las culturas africanas importadas durante la Colonia son la base de la cultura colombiana, la cual también comparte rasgos fundamentales con otras culturas hispanoamericanas en manifestaciones como la religión, la música, los bailes, las fiestas, las tradiciones, el dialecto, entre otras.[299]​ Culturalmente, Colombia es un país de regiones en el que la heterogeneidad obedece a variados factores como el aislamiento geográfico y la dificultad de acceso entre las diferentes zonas del país. Las subregiones o grupos culturales más importantes son los «cachacos» (ubicados en el altiplano cundiboyacense), los «paisas» (asentados en Antioquia y el Eje Cafetero), los «llaneros» (habitantes de los Llanos Orientales), los «vallunos» (zona del Valle del Cauca), los «costeños» (ubicados en la Costa Caribe), y los «santandereanos» (ubicados en los departamentos de Santander y Norte de Santander); entre otros, cuyas costumbres varían según sus influencias y ascendencias.[300]​

Carnaval de Barranquilla, Patrimonio Intangible de la Humanidad de la Unesco.

Feria de las Flores, Medellín.

Fiesta del Palenque de San Basilio, tradición afrocolombiana y Patrimonio de la Humanidad.

La Feria de Cali es la fiesta más importante del occidente colombiano.

Carnaval de Negros y Blancos, Patrimonio Cultural Inmaterial de la Humanidad.

Los orígenes de la literatura colombiana se remontan a los tiempos de la colonia española, período en el que se destacan Hernando Domínguez Camargo, con el Poema Épico a San Ignacio de Loyola, Juan Rodríguez Freyle (El Carnero) y la monja Francisca Josefa del Castillo representante del misticismo. En la literatura de post-independencia ligada al Romanticismo se destacaron Antonio Nariño, José Fernández Madrid, Camilo Torres Tenorio y Francisco Antonio Zea. En la segunda mitad del siglo XIX y a comienzos del siglo XX se popularizó el género costumbrista, teniendo como máximos exponentes a Tomás Carrasquilla, Jorge Isaacs y Rafael Pombo (destacado en el género de literatura infantil). Dentro de ese periodo, autores como José Asunción Silva, José Eustasio Rivera, León de Greiff, Porfirio Barba-Jacob y José María Vargas Vila, desarrollaron el movimiento modernista. En 1871 se estableció en Colombia la primera academia de la Lengua Española en América.[301]​

Entre 1939 y 1940 se publicaron en Bogotá siete cuadernos de poesía de considerable impacto en el país, bajo el nombre de «Piedra y Cielo», que fueron editados por el poeta Jorge Rojas.[302]​ En la década siguiente, Gonzalo Arango fundó el movimiento del nadaísmo como respuesta a la violencia de la época,[303]​ influido por el nihilismo, el existencialismo y el pensamiento de otro gran escritor colombiano, Fernando González Ochoa. Durante el llamado boom de la literatura latinoamericana surgieron escritores exitosos encabezados por el Nobel de Literatura Gabriel García Márquez y su obra magna Cien años de soledad, Eduardo Caballero Calderón, Manuel Mejía Vallejo y Álvaro Mutis, único colombiano galardonado con los premios Cervantes y Príncipe de Asturias. Otros autores contemporáneos destacados son Fernando Vallejo, ganador del Premio Rómulo Gallegos y Germán Castro Caycedo, escritor que vende el mayor número de libros en Colombia después de García Márquez.[304]​

En términos generales, las artes plásticas colombianas se dividen en cinco períodos: el arte prehispánico (que refiere al arte y los modos de pensar producidos por los pueblos originarios de América, con su forma de concebir el mundo, lo sagrado, la naturaleza y la sociedad),[305]​ el arte colonial (desde la Conquista hasta 1819), el arte republicano (siglo XIX), el arte moderno (desde Andrés de Santa María) y el arte contemporáneo (desde finales de la década de 1960 hasta nuestros días). Durante el periodo colonial existe una fusión cultural entre lo indígena, el aporte africano y el arte europeo religioso de la época. Las artes plásticas colombianas del siglo XIX no se independizaron de las concepciones estéticas coloniales por completo, aunque a finales de esa centuria aparecen los primeros intentos academicistas.

En 1886 se abrió la Escuela Nacional de Bellas Artes de Bogotá, entidad que formó a la mayoría de los artistas de comienzos del siglo XX. El evento más importante del país sobre arte colombiano es el Salón Nacional de Artistas Colombianos.[306]​

Entre los principales pintores nacionales sobresalen el impresionista Andrés de Santa María; el muralista Pedro Nel Gómez, los retratistas Ricardo Acevedo Bernal y Epifanio Garay; los paisajistas Fídolo González Camargo, Roberto Páramo, Jesús María Zamora, Ricardo Gómez Campuzano y Gonzalo Ariza; los hiperrealistas Darío Morales o David Manzur; los expresionistas Débora Arango e Ignacio Gómez Jaramillo (también muralistas). A partir de los años 1940, se destacan los pintores Alejandro Obregón, Guillermo Wiedemann, Leopoldo Richter Ómar Rayo. Entre de los artistas contemporáneos se cuentan Beatriz González, Santiago Cárdenas, Óscar Muñoz, Miguel Ángel Rojas, José Alejandro Restrepo, Álvaro Barrios y Antonio Caro.

En la escultura se destacan Rómulo Rozo, Rodrigo Arenas Betancourt, Édgar Negret, Héctor Lombana, Eduardo Ramírez Villamizar, John Castles y Nadín Ospina. Son numerosos los artistas colombianos que han desarrollado su obra tanto en pintura como en escultura, como Fernando Botero, famoso por su escultura monumental, Alejandro Obregón, Enrique Grau, Francisco Antonio Cano, Julio Abril, Luis Alberto Acuña, Santiago Martínez Delgado, entre otros que han alcanzado reconocimiento internacional. 

En fotografía se destaca Leo Matiz, Luis García Hevia, Melitón Rodríguez, Hernán Díaz, Abdú Eljaiek, Manuel H., Nereo López, Carlos Caicedo, Ignacio Gaitán, Sady González, Luis Benito Ramos y José Crisanto Lizarazo.

Colombia es en la actualidad el segundo país del mundo con mayor cantidad de hispanohablantes después de México.[19]​ El artículo diez de la Constitución Política de Colombia establece que el idioma español es el oficial del país, así como también lo son en sus respectivos territorios las lenguas y dialectos de los grupos étnicos. En el archipiélago de San Andrés y Providencia el idioma inglés es hablado por los habitantes nativos y es idioma oficial local.[307]​

Hay una gran diversidad de dialectos del español que se distinguen por diferencias léxicas (semánticas), morfológicas, sintácticas y de entonación, aunque el seseo, el yeísmo y otras características del español americano son comunes en todos los dialectos. El norte costero de Colombia comparte un grupo dialectal semejante al de otras naciones del Caribe hispano como Panamá, Venezuela, Cuba, República Dominicana, Puerto Rico y Nicaragua. El sur andino (Nariño principalmente) comparte el dialecto de la misma familia de la sierra ecuatoriana, peruana, boliviana y argentina. En la zona Paisa (Antioquia y Eje Cafetero) se maneja una variante andina propia. En el centro del país y alrededores de Bogotá se maneja lo que es considerado un dialecto de prestigio en el mundo hispano. En las diversas montañas, valles y llanuras hay una gran variedad de dialectos que incluyen el voseo y el tuteo.

Shakira (reconocida mundialmente)[308]​

J Balvin.

Carlos Vives.

Diomedes Díaz

Maluma.

Silvestre Dangond.

Juanes.

La industria musical de Colombia es la más grande de América Latina, el país es mayor productor y exportador de música de la región, anualmente genera más de 300 millones de dólares a la economía colombiana, con un crecimiento interanual positivo.[309]​

Históricamente se han reconocido como ritmos nacionales el bambuco (principios del siglo XX), la cumbia (mediados del siglo XX) y el vallenato (fines del siglo XX, inicios del XXI). Los diferentes géneros de la música folclórica de Colombia han sido influidos por elementos españoles, amerindios y africanos que formaron la etnografía del país, así como por otras corrientes latinoamericanas y anglosajonas que han constituido a la música colombiana como la más rica y diversa de América Latina, llevando en años recientes al reconocimiento de varios intérpretes a nivel internacional. La música es promovida principalmente por el apoyo de grandes compañías disqueras, empresas independientes y en menor escala por el gobierno de nacional a través del Ministerio de Cultura. De forma descentralizada, el Sistema Nacional de Cultura, a través del Consejo Nacional de Música, asesora al Gobierno en temas musicales y en representación de cada una de las seis regiones del país.[310]​

La Sociedad de Autores y Compositores de Colombia (Sayco) y la Asociación Colombiana de Intérpretes y productores fonográficos (Acinpro) son las organizaciones encargadas de recaudar y distribuir los derechos patrimoniales que genera el uso de las obras a sus autores afiliados y de extranjeros que forman parte de sociedades afiliadas a la Confederación Internacional de Sociedades de Autores y Compositores (Cisac).[311]​ Desde 1887, la composición musical patriótica que simboliza a Colombia es el himno nacional de Colombia, el cual fue adoptado oficialmente en 1920.[312]​ Muchos géneros extranjeros han sido comercialmente exitosos, llegando a ser bastante difundidos a través de las emisoras radiales nacionales. Entre estos géneros se pueden contar el merengue dominicano, la salsa, el rock, el pop, la balada romántica, entre otros. A pesar del origen foráneo de tales géneros, en términos culturales han tenido una influencia significativa. En este sentido, dichos géneros cuentan con destacados representantes musicales en el país, muchos de los cuales han tenido también gran éxito en el exterior. El género musical de origen colombiano que ha logrado mayor éxito comercial en la actualidad, es el vallenato, seguido por la cumbia y la música popular.

A partir de las fusiones entre muchos de estos géneros, han aparecido nuevos aires musicales, tales como el tropipop y la champeta. En cuanto al jazz, se destaca que es un género que aparece en el país a partir de la década de 1920, teniendo especial importancia en ciudades como Barranquilla, Cartagena, Bogotá, Cali y Medellín, a través de la radio, el cine y los clubes sociales, como lo destaca Enrique Luis Muñoz Vélez.[313]​ En el país se realizan distintos festivales de jazz como Jazz al Parque en Bogotá desde 1996, el Festival de Jazz del Teatro Libre de Bogotá, el más antiguo del país, el Barranquijazz de Barranquilla, el Festival Ajazzgo en Cali, el Festival internacional de jazz en Medellín, entre otros. El Rock también cuenta con grandes exponentes y festivales como el Festival Rock al parque en Bogotá, el Festival Internacional Altavoz en Medellín, entre otros. El Rap en Colombia cuenta con el festival más grande de Latinoamérica: el Festival Hip hop al parque de Bogotá.[314]​

Sobre la música clásica, es destacable que el país cuenta con diversas orquestas como la Filarmónica de Bogotá, la Filarmónica de Medellín, la Filarmónica de Cali, la Sinfónica Nacional de Colombia, entre otras. El país ha sido testigo del surgimiento de un sinnúmero de festivales de música clásica, además de la existencia de diversas figuras de la música instrumental y del canto lírico que se destacan a nivel internacional.[315]​

El teatro fue introducido durante la colonización española a partir de 1560 con compañías de zarzuela.[317]​ El teatro en Colombia es principalmente apoyado por el Ministerio de Cultura y por diferentes entidades de carácter estatal o privado afiliadas.

El Festival Iberoamericano de Teatro es un certamen cultural internacional que se lleva a cabo cada dos años en Bogotá. Fue dirigido y producido, hasta su muerte en agosto de 2008, por Fanny Mikey, actriz de teatro y empresaria cultural de origen argentino naturalizada colombiana.[318]​ Es el evento cultural de mayor transcendencia en Colombia y uno de los festivales de artes escénicas más grandes del mundo.

Otros eventos teatrales importantes son el Festival Internacional de Títeres la Fanfarria (Medellín), el Festival Internacional de Teatro de Manizales, el Festival Internacional de Teatro del Caribe (Santa Marta) y el Festival Artístico Nacional e Internacional de Cultura Popular «Invasión Cultural» (Bogotá).[319]​

El cine colombiano no ha logrado ser rentable como industria a lo largo de su historia, lo que ha impedido que exista continuidad en la producción y en el empleo de realizadores y técnicos.[320]​ Durante las primeras décadas del siglo XX existieron algunas compañías que intentaron mantener un nivel constante de producción, pero la falta de apoyo económico y la fuerte competencia extranjera terminaron por malograr las iniciativas. En los años 1980, se creó la Compañía de Fomento Cinematográfico (Focine), de carácter estatal, la cual permitió que se realizaran algunas producciones. Sin embargo, la compañía tuvo que ser liquidada a principios de los años 1990.[321]​ En la actualidad, se vive una creciente actividad cinematográfica gracias a la Ley de Cine aprobada en 2003, que ha permitido que renazcan las iniciativas alrededor de la actividad cinematográfica en el país.[322]​

Colombia no tiene un plato nacional. Entre los platos regionales más representativos se encuentran el sancocho, la arepa, el ajiaco santafereño, la bandeja paisa, el mote de queso, la lechona tolimense, la mamona o ternera a la llanera, el mute santandereano, el tamal, el arroz de lisa y los pescados, sobre todo en las regiones costeras.[323]​

La gastronomía colombiana es variada y cambia según cada región:

La población cristiana en Colombia se estima en un 92,5%, de los cuales un 89% son católicos, 10,8% protestantes y un 0,2% de otras denominaciones cristianas, mientras un 6,6% no presentan ninguna afiliación religiosa, según el Pew Research Center en 2010.[335]​ El catolicismo fue traído de España por los misioneros e introducido durante toda la Colonia. En la era republicana, la Constitución Política de 1886 estableció en su artículo 38 que "La Religión Católica, Apostólica, Romana, es la de la Nación".[336]​ Es una nación predominantemente católica a pesar de que la Constitución Política de 1991 estableció la libertad religiosa y eliminó el concepto de nación católica por el de laica. El archipiélago de San Andrés, Providencia, Santa Catalina y demás dependencias fueron colonizados inicialmente por puritanos ingleses, en particular por la Providence Island Company;[337]​ su población sigue siendo en parte protestante.[338]​ En 2004, una encuesta del diario El Tiempo mostró que el 10% de la población se identificaba como cristiana no evangélica, 3,5 % como evangélica y 1,9 % no profesaba creencias religiosas. El 60% de los encuestados reconoció no practicar su fe activamente.[339]​

Por otro lado, existen formas religiosas sincréticas, resultado de la fusión del catolicismo con religiones indígenas y africanas, las cuales se pueden observar en la santería y en el Carnaval de Blancos y Negros de Pasto, cuyas raíces se encuentran en los rituales precolombinos agrarios y festivales africanos. Al igual que en otros países de América Latina, comunidades protestantes de distintas denominaciones han hecho su aparición en los últimos años.[338]​

La actividad deportiva es regulada estatal y gubernamentalmente por el Ministerio del Deporte (Mindeporte), por la secretarías municipales y departamentales de recreación y deportes, así como por asociaciones independientes como federaciones, institutos y ligas en las distintas prácticas deportivas.[340]​ Los deportes apoyados por el gobierno se desarrollan dentro de la legislación educativa en centros educativos como escuelas deportivas, deporte universitario y juegos intercolegiados.[341]​

Los principales escenarios deportivos se concentran en las ciudades más pobladas, donde se realizan periódicamente los Juegos Deportivos Nacionales de Colombia.[342]​[343]​

Aunque solo se practica en la región cundiboyacense, el tejo o turmequé es considerado deporte nacional de Colombia, juego ancestral indígena arraigado en la Región Andina. El tejo está determinado por la Ley 6132 de 2000 como deporte nacional de Colombia, tiene reconocimiento por Coldeportes y el Comité Olímpico Colombiano, y está regido por la Federación Colombiana de Tejo (Fedetejo).[344]​

El fútbol es el deporte más popular de Colombia. La liga colombiana está conformada por dos divisiones, Categoría Primera A (Liga Betplay por motivos de patrocinio) y Categoría Primera B (Torneo Betplay por motivos de patrocinio), que compiten en tres torneos, uno para cada división y la Copa Colombia (Copa Betplay por motivos de patrocinio), además, los dos campeones anuales de la Categoría Primera A juegan la Superliga de Colombia (Superliga Betplay por motivos de patrocinio), todos bajo distinción de la Dimayor. La Federación Colombiana de Fútbol, afiliada a la FIFA y a la Conmebol, es el ente rector de este deporte en el país y organiza la participación de la selección en eventos internacionales.[345]​ La Federación agrupa a la División Mayor del Fútbol Colombiano (que organiza el fútbol profesional) y la Difútbol (que tiene a su cargo el fútbol aficionado).

El béisbol, popular en la Costa Caribe, le entregó al país su primer título mundial en cualquier deporte en la Copa Mundial de Béisbol de 1947 celebrada en Cartagena.[346]​[347]​ La selección colombiana de béisbol revalidaría el título mundial aficionado de béisbol en 1965, en torneo realizado también en Cartagena.[348]​ La Liga Colombiana de Béisbol Profesional, fundada en 1948 y constituida por cuatro equipos, organiza la principal competición anual en la materia. Los colombianos más destacados en las Grandes Ligas han sido Édgar Rentería y Orlando Cabrera. Desde 2009, se ha producido un boom de beisbolistas colombianos en Grandes Ligas, en 2013, un total de cinco colombianos actuaron en dicha competencia.[349]​[350]​[351]​[352]​[353]​

El boxeo es el deporte que más campeones mundiales ha producido para Colombia; el primero en coronarse fue Antonio Cervantes «Kid Pambelé» en 1972.[354]​ Otros campeones mundiales destacados fueron Rodrigo Valdez[355]​ y Miguel «Happy» Lora.[356]​ Clemente Rojas, Afonso Pérez, Jorge Eliécer Julio e Ingrit Valencia obtuvieron medallas de bronce para el país en los Juegos Olímpicos; Yuberjen Martínez obtuvo plata.[357]​[358]​[359]​

Otro deporte que goza de especial popularidad en el país es el ciclismo, el cual ha producido figuras con participaciones destacadas desde los años 1980 en las principales competencias de ruta europeas (Tour de Francia, Vuelta a España, Giro de Italia, entre otras) y latinoamericanas como Luis Herrera, Fabio Parra, Santiago Botero, Nairo Quintana, Rigoberto Urán y Egan Bernal. Martín Emilio «Cochise» Rodríguez impuso la marca mundial de la hora en 1970 y se coronó campeón mundial en los 4.000 metros persecución individual en 1971. Las principales competencias ciclísticas del país son la Vuelta a Colombia y el Clásico RCN. El ciclismo es, junto a la halterofilia, el deporte que más medallas olímpicas le ha entregado a Colombia: Mariana Pajón (oro en BMX), Rigoberto Urán (plata en ruta), María Luisa Calle (bronce en pista), Carlos Oquendo (bronce en BMX) y Carlos Ramírez (bronce en BMX).

Entre las décadas de 1960 y 1980, varios fondistas colombianos se destacaron en importantes carreras internacionales como la San Silvestre de São Paulo, Brasil: Álvaro Mejía (ganador en 1966),[360]​ Domingo Tibaduiza (vencedor en 1977)[361]​ y Víctor Mora (ganador en 1972, 1973, 1975 y 1981).[361]​[362]​

Colombia es considerada potencia mundial en patinaje de velocidad sobre patines en línea, a tal punto que de los 23 campeonatos del mundo que se han celebrado, Colombia ha sido ganadora absoluta en 11 oportunidades. Las principales figuras de este deporte han sido Luz Mery Tristán, Cecilia Baena, Jorge Botero y Kelly Martínez.[363]​

El automovilismo también ocupa un lugar importante en las preferencias de los colombianos, siendo Juan Pablo Montoya, quien logró varias figuraciones importantes en la Fórmula 1 en la década de 2000, el deportista más importante en esta disciplina.[364]​

Las disciplinas individuales han reportado los mejores logros en los Juegos Olímpicos. La participación más destacada en este certamen fue en Río 2016, donde el equipo olímpico colombiano obtuvo ocho medallas, destacándose las tres de oro obtenidas por Mariana Pajón, Óscar Figueroa y Caterine Ibargüen.[365]​ Antes de dicha participación, la mejor figuración había sido en Londres 2012, donde se obtuvieron ocho medallas, entre ellas una de oro obtenida por Mariana Pajón en BMX.[366]​ El primer medallista olímpico colombiano fue Helmut Bellingrodt, medalla de plata en tiro al jabalí en Múnich 1972, presea que volvió a obtener en Los Ángeles 1984.[367]​ Colombia ha obtenido en total cinco medallas de oro, ocho de plata y 14 de bronce, en diversos deportes como la halterofilia, el ciclismo (ruta, pista y BMX), el tiro deportivo, el taekwondo, el boxeo, la lucha, el judo y el atletismo. En los Juegos Paralímpicos, los deportistas colombianos han obtenido 6 medallas en total.

El capitalismo es un orden o sistema social y económico que se encuentra en constante movimiento, derivado del usufructo de la propiedad privada sobre el capital como herramienta de producción, que se encuentra mayormente constituido por relaciones empresariales vinculadas a las actividades de inversión y obtención de beneficios, así como de relaciones laborales, tanto autónomas como asalariadas subordinadas .[1]​

En el capitalismo, los individuos y las empresas habitualmente representadas por los mismos, llevan a cabo la producción de bienes y servicios de forma privada e interdependiente, dependiendo así de un mercado de consumo para la obtención de recursos.[2]​ El intercambio de los mismos se realiza básicamente mediante comercio libre y, por tanto, la división del trabajo se desarrolla de forma mercantil y los agentes económicos dependen de la búsqueda de beneficio.[3]​ La distribución se organiza, y las unidades de producción se fusionan o separan, de acuerdo a una dinámica basada en un sistema de precios para los bienes y servicios.[4]​ A su vez, los precios se forman mayoritariamente en un mercado que depende de la interacción entre una oferta y una demanda dadas por las elecciones de productores y consumidores,[5]​ y estos, son necesarios para la coordinación ex-post de una economía basada en el intercambio de mercancías.[6]​

El origen etimológico de la palabra capitalismo proviene de la idea de capital y su uso para la propiedad privada de los medios de producción,[7]​[8]​ sin embargo, se relaciona mayormente al capitalismo como concepto con el intercambio dentro de una economía de mercado que es su condición necesaria,[9]​[10]​ y a la propiedad privada absoluta[11]​ o burguesa[12]​[13]​ que es su corolario previo.[14]​[15]​ El origen de la palabra puede remontarse antes de 1848 pero no es hasta 1860 que llega a ser una corriente como tal y reconocida como término, según las fuentes escritas de la época.[16]​

Se denomina sociedad capitalista a toda aquella sociedad política y jurídica originada basada en una organización racional del trabajo, el dinero y la utilidad de los recursos de producción, caracteres propios de aquel sistema económico.[17]​ En el orden capitalista, la sociedad está formada por clases socioeconómicas en vez de estamentos como son propios del feudalismo y otros órdenes premodernos.[18]​ Se distingue de aquel y de otras formas sociales por la posibilidad de movilidad social de los individuos, por una estratificación social de tipo económica,[19]​ y por una distribución de la renta que depende casi enteramente de la funcionalidad de las diferentes posiciones sociales adquiridas en la estructura de producción.[20]​

El nombre de sociedad capitalista se adopta habitualmente debido a que el capital como relación de producción se convierte dentro de esta en un elemento económicamente predominante.[21]​ La discrepancia sobre las razones de este predominio divide a las ideologías políticas modernas: el enfoque liberal smithiano se centra en la utilidad que el capital como relación social provee para la producción en una sociedad comercial con una amplia división del trabajo, entendida como causa y consecuencia de la mejora de la oferta de consumo y los mayores ingresos por vía del salario respecto del trabajo autónomo,[22]​ mientras que el enfoque socialista marxista considera que el capital como relación social es precedido (y luego retroalimentado) por una institucionalizada imposibilidad social de sobrevivir sin relacionarse con los propietarios de un mayor capital físico mediante el intercambio de trabajo asalariado.[23]​

La clase social conformada por los creadores y/o propietarios que proveen de capital a la organización económica a cambio de un interés[24]​ se la describe como "capitalista", a diferencia de las funciones empresariales cuyo éxito se traduce en forma de ganancia[25]​ y de las gerenciales ejecutadas a cambio de un salario.[26]​ Vulgarmente se describe desde el siglo XVIII como "burguesía" tanto a este conjunto social como al de los empleadores de trabajo de una moderna sociedad industrial, pero la burguesía se origina en las ciudades de la sociedad rural medieval y está constituida por propietarios autoempleados cuya naturaleza da origen al capitalismo moderno.[27]​

El capitalismo, o más concretamente los sistemas económicos capitalistas, se caracterizan por la presencia de unos ciertos elementos de tipo socioeconómico, si un número importante de ellos está ausente el sistema no puede ser considerado como propiamente capitalista. Entre los factores que acaban haciendo que un sistema sea considerado capitalista están:

La Internet Encyclopedia of Philosophy define el capitalismo como un sistema económico que tiene las siguientes características:[28]​

En términos más descriptivos, los sistemas capitalistas son sistemas socio económicos donde los activos de capital están básicamente en manos privadas y son controlados por agentes o personas privadas. El trabajo es proporcionado mediante el ofrecimiento de salarios monetarios y la aceptación libre por parte de los empleados. La actividad económica frecuentemente está organizada para obtener un beneficio neto que permita a las personas propietarias que controlan los medios de producción incrementar su capital. Los bienes y servicios producidos son además distribuidos mediante mecanismos de mercado. Si bien todos los sistemas capitalistas existentes presentan un mayor o menor grado de intervención estatal y se alejan por diversas razones del modelo de mercado idealmente competitivo, razón por la cual se definen conceptos como la competitividad o el índice de libertad económica, para caracterizar hasta qué punto difieren unos sistemas capitalistas de otros.[29]​

En los sistemas capitalistas la titularidad de la mayor parte de medios de producción es privada, entendiéndose por esto su construcción sobre un régimen de bienes de capital industrial y de tenencia y uso de la tierra basado en la propiedad privada. Los medios de producción operan principalmente en función del beneficio y en la de los intereses directivos. Se acepta que en un sistema capitalista, la mayor parte de las decisiones de inversión de capital están determinadas por las expectativas de beneficio, por lo que la rentabilidad del capital invertido tiene un papel muy destacado en la vida económica. Junto con el capital, el trabajo se refiere al otro gran conjunto de elementos de producción (algunos autores añaden un factor tradicionalmente llamado «tierra» que en términos generales puede representar cualquier tipo de «recurso natural»). El papel decisivo del trabajo, junto el capital, hacen que uno de los aspectos importantes del capitalismo sea la competencia en el llamado mercado de trabajo asalariado.

Sobre la propiedad privada, los sistemas capitalistas tienden a que los recursos invertidos por los prestadores de capital para la producción económica, estén en manos de las empresas y personas particulares (accionistas). De esta forma a los particulares se les facilita el uso, empleo y control de los recursos que se utilizan la producción de bienes y servicios. En los sistemas capitalistas se busca que no existan demasiadas restricciones para las empresas sobre cómo usar mejor sus factores de producción (capital, trabajo, recursos disponibles).

Entre las características generales del capitalismo se encuentra la motivación basada en el cálculo costo-beneficio dentro de una economía de intercambio basada en el mercado, el énfasis legislativo en la protección de un tipo específico de apropiación privada (en el caso del capitalismo particularmente lockeano), o el predominio de las herramientas de producción en la determinación de las formas socioeconómicas.

El capitalismo se considera un sistema económico en el cual el dominio de la propiedad privada sobre los medios de producción desempeña un papel fundamental. Es importante comprender lo que se entiende por propiedad privada en el capitalismo ya que existen múltiples opiniones, a pesar de que este es uno de los principios básicos del capitalismo: otorga influencia económica a quienes detentan la propiedad de los medios de producción (o en este caso el capital), dando lugar a una relación voluntaria de funciones y de mando entre el empleador y el empleado. Esto crea a su vez una sociedad de clases móviles en relación con el éxito o fracaso económico en el mercado de consumo, lo que influye en el resto de la estructura social según la variable de capital acumulada; por tal razón en el capitalismo la pertenencia a una clase social es movible y no estática.

Las relaciones económicas de producción y el origen de la cadena de mando —incluyendo la empresaria por delegación— es establecida desde la titularidad privada y exclusiva de los propietarios de una empresa en función de la participación en su creación en tanto primeros propietarios del capital. La propiedad y el usufructo queda así en manos de quienes adquirieron o crearon el capital volviendo interés su óptima utilización, cuidado y acumulación, con independencia de que la aplicación productiva del capital se genere mediante la compra del trabajo, esto es, el sueldo, realizado por los asalariados de la empresa.

Una de las interpretaciones más difundidas señala que en el capitalismo, como sistema económico, predomina el capital —actividad empresarial, mental— sobre el trabajo —actividad corporal— como elemento de producción y creador de riqueza. El control privado de los bienes de capital sobre otros factores económicos tiene la característica de hacer posible negociar con las propiedades y sus intereses a través de rentas, inversiones, etc. Eso crea el otro distintivo del capitalismo que es el beneficio o ganancia como prioridad en la acción económica en función de la acumulación de capital que por vía de la compra del trabajo puede separarse del trabajo asalariado.

El capitalismo se basa ideológicamente en una economía en la cual el mercado predomina, esto usualmente se da, aunque existen importantes excepciones además de las polémicas sobre qué debe ser denominado libre mercado o libre empresa. En este se llevan a cabo las transacciones económicas entre personas, empresas y organizaciones que ofrecen productos y las que los demandan. El mercado, por medio de las leyes de la oferta y la demanda, regula los precios según los cuales se intercambian las mercancías (bienes y servicios), permite la asignación de recursos y la distribución de la riqueza entre los individuos.

La libertad de empresa propone que todas las empresas sean libres de conseguir recursos económicos y transformarlos en una nueva mercancía o servicio que será ofrecido en el mercado que estas dispongan. A su vez, son libres de escoger el negocio que deseen desarrollar y el momento para entrar o salir de este. La libertad de elección se aplica a las empresas, los trabajadores y los consumidores, pues la empresa puede manejar sus recursos como crea conveniente, los trabajadores pueden realizar un trabajo cualquiera que esté dentro de sus capacidades y los consumidores son libres de escoger lo que desean consumir, buscando que el producto escogido cumpla con sus necesidades y se encuentre dentro de los límites de su ingreso. Esto en un contexto teórico capitalista es denominado cálculo económico.

Competencia se refiere a la existencia de un gran número de empresas o personas que ofrecen y venden un producto (oferentes) en un mercado determinado. En dicho mercado también existe un gran número de personas o empresas (demandantes), las cuales, según sus preferencias y necesidades, compran o demandan esos productos o mercancías. A través de la competencia se establece una «rivalidad» o antagonismo entre productores. Los productores buscan acaparar la mayor cantidad de consumidores/compradores para sí. Para conseguir esto, utilizan estrategias de reducción de precios, mejoramiento de la calidad, etc.

Al hacer referencia a una fuerza de trabajo libre, se entiende a una mano de obra con la libertad de vender su capacidad de trabajo a cambio de un salario a cualquier patrono potencial.[30]​

El tipo de empresa actual suele resultar de una asociación. A principios del siglo XIX, las empresas eran generalmente de un individuo que invertía en ellas capitales, fueran estos propios o procedentes de préstamos, y los ponía al servicio de una capacidad técnica, que generalmente él mismo tenía. Sin embargo, el posterior desarrollo o auge del capitalismo demostraron claramente la superioridad de la empresa, que supera los límites de la personalidad individual o de la continuidad familiar.[31]​ Este sistema permite al mismo tiempo agrupar capacidades que se completan y disociar las aportaciones de capital de las aptitudes puramente técnicas, antes confundidas. Hay que distinguir dos grandes categorías de sociedades:

1. Las de personas, constituidas por un pequeño número de individuos que aportan al fondo social capitales, llamados (partes) o capacidades técnicas (caso del socio industrial opuesto al capitalista), que, como son en realidad fracciones casi materiales de la empresa no pueden ser cedidas sin el acuerdo de los copartícipes.

2. Las de capitales, en las que las partes llamadas (acciones),se consideran como simples pruebas materiales de la aportación de cierto capital por los asociados, en general numerosos y tienen por tanto la posibilidad de transmitirse o negociarse libremente en la bolsa de valores.

Teóricos y políticos han enfatizado la habilidad del capitalismo para promover el crecimiento económico buscando aumentar los beneficios, tal como se mide por el Producto Interno Bruto (PIB), utilización de la capacidad instalada o calidad de vida. Sin embargo, debe notarse el análisis de la tasa de crecimiento ha revelado que el progreso técnico y causas no asignables a la intensividad del capital o la asignación de trabajo, parecen ser responsables de gran parte de la productividad (ver productividad total de los factores). Igualmente los sistemas de economía planificada lograron entre 1945-1970 tasas muy superiores a la mayor parte de países capitalistas. Aun dejando a un lado el peso de los diferentes factores en el crecimiento económico, la posible benéfica influencia de la organización capitalista de la producción ha sido históricamente el argumento central, por ejemplo, en la propuesta de Adam Smith de dejar que el libre mercado controle los niveles de producción y de precio, y distribuya los recursos.

Diversos autores han sostenido que el rápido y consistente crecimiento de los indicadores económicos mundiales desde la revolución industrial se debe al surgimiento del capitalismo moderno.[32]​[33]​ Aun cuando parece que parte del crecimiento recogida dentro de la productividad total de los factores no necesariamente está ligada al modo de organización capitalista, sino podría deberse simplemente a factores técnicos cuyo desarrollo obedece a causas más complicas.[34]​ Los defensores de que la organización capitalista es el factor principal en el crecimiento argumentan que incrementar el PIB (per cápita) ha demostrado empíricamente una mejora en la calidad de vida de las personas, tal como mejor disponibilidad de alimentos, vivienda, vestimenta, atención médica, reducción de horas de trabajo, y libertad de trabajo para niños y ancianos.[35]​

Sí parece ampliamente demostrado, que la especialización tanto en la agricultura como en otras áreas, produce un aumento de la producción existente, y la actividad comercial de materias primas aumenta. La consecuencia de este hecho, es el incremento de la circulación de capital, que fue un estímulo a la banca, y por tanto de la riqueza de la sociedad, aumentando el ahorro y con ello la inversión. Este fue fundamentalmente el origen de la banca actual, la cual tenía dos funciones: prestar el dinero que custodiaban a cambio de un interés y la emisión de "promesas de pago al contado al portador" que circulaban como dinero.

Argumentos favorables al capitalismo también afirman que una economía capitalista brinda más oportunidades a los individuos de incrementar sus ingresos a través de nuevas profesiones o negocios que otras formas de economía. Según esta manera de pensar, este potencial es mucho mayor que en las sociedades feudales o tribales o en las sociedades socialistas.[cita requerida] Igualmente, diversos trabajos modernos han enfatizado las dificultades de los sistemas capitalistas no sometidos a regulación, los efectos de la información asimétrica, y la ocurrencia de crisis económicas cíclicas.[36]​

De acuerdo con los argumentos de los defensores del capitalismo, cada uno de los actores del mercado actuaría según su propio interés; por ejemplo, el empleador, quien posee recursos productivos y capital, buscaría maximizar el beneficio económico por medio de la acumulación y producción de mercancías. Por otra parte, los empleados, quienes estarían vendiendo su trabajo a su empleador a cambio de un salario; y, por último, los consumidores, que estarían buscando obtener la mayor satisfacción o utilidad adquiriendo lo que desean o necesitan en función a la calidad del producto y de su precio.

De acuerdo con numerosos economistas, el capitalismo podría organizarse a sí mismo como un sistema complejo sin necesidad de un mecanismo de planeamiento o guía externa.[37]​ A este fenómeno se lo llama laissez faire.[38]​ Otros economistas modernos han señalado la conveniencia de las regulaciones, especialmente si se tienen en cuenta que las economías están insertas en sistemas sociopolíticos y medioambientales que también es necesario preservar. A este respecto el propio presidente Franklin D. Roosevelt, en un mensaje al Congreso del 29 de abril de 1938 llegó a afirmar:


En cualquier caso es innegable, que para unos y otros el proceso de búsqueda de beneficios tiene un rol importante (ya se prefiera una economía con cierta regulación o una totalmente desregulada). Se admite que a partir de las transacciones entre compradores y vendedores emerge un sistema de precios, y los precios frecuentemente surgen como una señal de cuáles son las urgencias y necesidades insatisfechas de las personas, si bien algunos autores señalan que pueden existir fallos de mercado bajo circunstancias específicas. La promesa de beneficios les da a los emprendedores el incentivo para usar su conocimiento y recursos para satisfacer esas necesidades. De tal manera, las actividades de millones de personas, cada una buscando su propio interés, se coordinan y complementan entre sí.[40]​

La doctrina política que históricamente ha encabezado la defensa e implantación de este sistema económico y político ha sido el liberalismo económico y clásico del cual se considera sus padres fundadores a John Locke, Juan de Mariana y Adam Smith. El pensamiento liberal clásico sostiene en economía que la intervención del gobierno debe reducirse a su mínima expresión. Solo debe encargarse del ordenamiento jurídico que garantice el respeto de la propiedad privada, la defensa de las llamadas libertades negativas: los derechos civiles y políticos, el control de la seguridad interna y externa (justicia y protección), y eventualmente la implantación de políticas para garantizar el libre funcionamiento de los mercados, ya que la presencia del Estado en la economía perturbaría su funcionamiento. Sus representantes contemporáneos más prominentes son Ludwig von Mises y Friedrich Hayek por parte de la llamada Escuela austríaca de economía; George Stigler y Milton Friedman por parte de la llamada Escuela de Chicago, existiendo profundas diferencias entre ambas.

Existen otras tendencias dentro del pensamiento económico que asignan al Estado funciones diferentes. Por ejemplo los que se adscriben a lo sostenido por John Maynard Keynes, según el cual el Estado puede intervenir para incrementar la demanda efectiva en época de crisis. También se puede mencionar a los politólogos que dan al Estado y a otras instituciones un papel importante en controlar las deficiencias del mercado (una línea de pensamiento en este sentido es el neoinstitucionalismo).

El economista de la Escuela Kennedy de Harvard, Dani Rodrik, distingue entre tres variantes históricas del capitalismo:[41]​

La relación entre democracia y capitalismo es un área polémica en la teoría y en los movimientos políticos populares. La extensión del sufragio masculino adulto en Gran Bretaña en el siglo XIX ocurrió junto con el desarrollo del capitalismo industrial y la democracia representativa se generalizó al mismo tiempo que el capitalismo, lo que llevó a los capitalistas a postular una relación causal o mutua entre ellos.[42]​ Sin embargo, según algunos autores del siglo XX, el capitalismo también acompañó a una variedad de formaciones políticas bastante distintas de las democracias liberales, incluidos los regímenes fascistas, las monarquías absolutas y los estados de partido único.[43]​ La teoría de la paz democrática afirma que las democracias rara vez luchan contra otras democracias. Los críticos moderados argumentan que aunque el crecimiento económico bajo el capitalismo ha llevado a la democracia en el pasado, es posible que no lo haga en el futuro, ya que los regímenes autoritarios han sido capaces de gestionar el crecimiento económico utilizando algunos de los principios competitivos del capitalismo[44]​ sin hacer concesiones. a una mayor libertad política.[45]​ Los politólogos Torben Iversen y David Soskice consideran que la democracia y el capitalismo se apoyan mutuamente.[46]​

En su libro The Road to Serfdom (1944), Friedrich Hayek (1899-1992) afirmó que la comprensión del libre mercado de la libertad económica como presente en el capitalismo es un requisito de la libertad política. El mecanismo es la única forma de decidir qué producir y cómo distribuir los artículos sin utilizar la coacción. Milton Friedman, Andrew Brennan y Ronald Reagan también promovieron este punto de vista. Friedman afirmó que las operaciones económicas centralizadas siempre van acompañadas de represión política. En su opinión, las transacciones en una economía de mercado son voluntarias y la amplia diversidad que permite la actividad voluntaria es una amenaza fundamental para los líderes políticos represivos y disminuye en gran medida su poder de coacción. Algunas de las opiniones de Friedman fueron compartidas por John Maynard Keynes, quien creía que el capitalismo era vital para que la libertad sobreviviera y prosperara.[47]​ Freedom House, un grupo de expertos estadounidenses que realiza investigaciones internacionales y defiende la democracia, la libertad política y los derechos humanos, ha argumentado que "existe una correlación alta y estadísticamente significativa entre el nivel de libertad política medido por Freedom House y libertad económica medida por la encuesta del Wall Street Journal / Heritage Foundation ".[48]​

Milton Friedman, uno de los mayores partidarios de la idea de que el capitalismo promueve la libertad política, argumentó que el capitalismo competitivo permite que el poder económico y político estén separados, asegurando que no chocan entre sí. Los críticos moderados han desafiado esto recientemente, afirmando que la influencia actual que los grupos de presión han tenido en las políticas en los Estados Unidos es una contradicción. Esto ha llevado a la gente a cuestionar la idea de que el capitalismo competitivo promueve la libertad política. El sistema legal de EE. UU. permite a las corporaciones gastar cantidades de dinero no divulgadas y no reguladas en campañas políticas, cambiando los resultados a favor de intereses especiales y socavando la verdadera democracia. Como se explica en los escritos de Robin Hahnel, la pieza central de la defensa ideológica del sistema de libre mercado es el concepto de libertad económica y que los partidarios equiparan la democracia económica con la libertad económica y afirman que solo el sistema de libre mercado puede proporcionar libertad económica. Según Hahnel, hay algunas objeciones a la premisa de que el capitalismo ofrece libertad a través de la libertad económica. Estas objeciones están guiadas por preguntas críticas sobre quién o qué decide qué libertades están más protegidas. A menudo, la cuestión de la desigualdad se plantea cuando se habla de como el capitalismo promueve la democracia. Un argumento que podría sostenerse es que el crecimiento económico puede conducir a la desigualdad dado que el capital puede ser adquirido a diferentes ritmos por diferentes personas. En El capital en el siglo XXI (2013), Thomas Piketty de la Escuela de Economía de París afirmó que la desigualdad es la consecuencia inevitable del crecimiento económico en una economía capitalista y que la concentración de riqueza resultante puede desestabilizar las sociedades democráticas y socavar los ideales de justicia social sobre los que se basan.[49]​ 

Los estados con sistemas económicos capitalistas han prosperado bajo regímenes políticos considerados autoritarios u opresivos. Singapur tiene una economía de mercado abierta exitosa como resultado de su clima competitivo y favorable a las empresas y de un sólido estado de derecho. Sin embargo, a menudo es criticado por su estilo de gobierno que, aunque democrático y consistentemente uno de los menos corruptos, [83] opera en gran parte bajo un gobierno de partido único. Además, no defiende enérgicamente la libertad de expresión como lo demuestra su prensa regulada por el gobierno, y su inclinación por defender las leyes que protegen la armonía étnica y religiosa, la dignidad judicial y la reputación personal. El sector privado (capitalista) en la República Popular China ha crecido exponencialmente y ha prosperado desde sus inicios, a pesar de tener un gobierno autoritario. El gobierno de Augusto Pinochet en Chile condujo al crecimiento económico y altos niveles de desigualdad[50]​ mediante el uso de medios autoritarios para crear un entorno seguro para la inversión y el capitalismo. De manera similar, el reinado autoritario de Suharto y la extirpación del Partido Comunista de Indonesia permitieron la expansión del capitalismo en Indonesia.[51]​

Tanto los mercaderes como el comercio existen desde que existe la civilización, pero el capitalismo como sistema económico, en teoría, no apareció hasta el siglo XVII en Inglaterra sustituyendo al feudalismo. Según Adam Smith, los seres humanos siempre han tenido una fuerte tendencia a «realizar trueques, cambios e intercambios de unas cosas por otras». De esta forma al capitalismo, al igual que al dinero y la economía de mercado, se le atribuye un origen espontáneo o natural dentro de la edad moderna.[52]​

La sustitución del feudalismo tuvo como impulso a poderosas fuerzas del cambio que sirvieron para introducir de forma gradual la estructura de una sociedad de mercado, dentro de las principales fuerzas se encuentran:[53]​

Todas estas fuerzas del cambio crearon un aspecto económico en la vida de las personas que antes no existía, con estos cambios se empieza a marcar la separación del aspecto social de la vida con el aspecto económico, con este nacimiento del aspecto económico la sociedad empieza a tener fuertes transformaciones, por ejemplo, el siervo ya no está atado a la tierra sino que se convierte en un trabajador libre, el maestro gremial ahora es un empresario independiente, el señor feudal se convierte ahora en un simple arrendatario, estas transformaciones son cruciales para el nacimiento del capitalismo ya que empiezan a introducir las bases de este nuevo sistema económico. El nacimiento de estos trabajadores libres, capitalistas y terratenientes cada uno vendiendo sus servicios en el mercado del trabajo, el capital y la tierra hicieron que nacieran los "factores de producción".

El orden económico resultante de estos acontecimientos fue un sistema en el que predominaba lo comercial o mercantil, es decir, cuyo objetivo principal consistía en intercambiar bienes y no en producirlos. La importancia de la producción no se hizo patente hasta la Revolución industrial que tuvo lugar en el siglo XIX.

El camino hacia el capitalismo a partir del siglo XIII fue allanado gracias a la filosofía del Renacimiento y de la Reforma. Estos movimientos cambiaron de forma drástica la sociedad, facilitando la aparición de los modernos Estados nacionales que proporcionaron las condiciones necesarias para el crecimiento y desarrollo del capitalismo en las naciones europeas. Este crecimiento fue posible gracias a la acumulación del excedente económico que generaba el empresario privado y a la reinversión de este excedente para generar mayor crecimiento, lo cual generó industrialización en las regiones del norte.

Como se ha indicado anteriormente, existen distintas variantes del capitalismo que se diferencian de acuerdo a la relación entre el mercado, el Estado y la sociedad. Por supuesto, todas comparten características como la producción de bienes y servicios por beneficio, asignación de recursos basada principalmente en el mercado, y estructuración en torno a la acumulación de capital. Es importante destacar que entre los círculos ligados a la Escuela austríaca de economía se conoce como «capitalismo» a su variante más pura, el laissez faire.[54]​ Otros defensores del capitalismo han adoptado visiones del capitalismo más moderadas y más matizadas con respecto a su implementación práctica.

Algunas de las formas de capitalismo históricamente existentes o propuestas son:

En gran medida en la mayoría de países modernos predominan formas de capitalismo más cercanas a las dos últimas formas, la economía social de mercado y la economía mixta. El mercantilismo y el proteccionismo parecen casi universalmente abandonados aunque tuvieron su auge durante los siglos XVIII y XIX.

Esta es una forma nacionalista del capitalismo temprano que nació aproximadamente en el siglo XVI. Se caracteriza por el entrelazamiento de intereses comerciales de interés para el Estado y el imperialismo y, consecuentemente, por el uso del aparato estatal para promover las empresas nacionales en el extranjero. Un buen ejemplo lo entrega el caso del monopolio comercial impuesto por España a sus territorios de ultramar en 1504 prohibiéndoles comerciar con otras naciones.

El mercantilismo sostiene que la riqueza de las naciones se incrementa a través de una balanza comercial positiva (en que las exportaciones superan a las importaciones). Corresponde a la fase de desarrollo capitalista llamada Acumulación originaria de capital.

El capitalismo laissez faire se caracteriza por contratos voluntarios en ausencia de intervención de terceros (como pudiere ser el Estado). Los precios de los bienes y servicios son establecidos por la oferta y la demanda, llegando naturalmente a un punto de equilibrio. Implica la existencia de mercados altamente competitivos y la propiedad privada de los medios de producción. El rol del Estado se limita a la producción de seguridad y al resguardo de los derechos de propiedad.

En este sistema la intervención del Estado en la economía es mínima, pero entrega servicios importantes en cuanto a la seguridad social, prestaciones de desempleo y reconocimiento de derechos laborales a través de acuerdos nacionales de negociación colectiva. Este modelo es prominente en los países de Europa occidental y del norte, aunque variando sus configuraciones. La gran mayoría de las empresas son de propiedad privada.

Caracterizado por la dominación de corporaciones jerárquicas y burocráticas. El término «capitalismo monopolista de Estado» fue originalmente un concepto marxista para referirse a una forma de capitalismo en que la política de estado es utilizada para beneficiar y promover los intereses de corporaciones dominantes mediante la imposición de barreras competitivas y la entrega de subsidios.

Una economía mixta está basada en gran medida en el mercado, y consiste en la convivencia de la propiedad privada y la propiedad pública de los medios de producción, y en el intervencionismo a través de políticas macroeconómicas destinadas a corregir los posibles fallos de mercado, reducir el desempleo y mantener bajos los niveles de inflación. Los niveles de intervención varían entre los diferentes países, y la mayoría de las economías capitalistas son mixtas hasta cierto punto.

En términos políticos informales se considera que los sistemas capitalistas son opuestos a los sistemas de inspiración socialista. Presuntamente los sistemas socialistas difieren de los sistemas capitalistas en varias maneras: propiedad pública de los medios de producción, los recursos monetarios obtenidos mediante la producción pueden ser utilizados con fines sociales no relacionados con la inversión o la obtención de beneficios. En muchos sistemas históricos de inspiración socialista muchas decisiones importantes de producción fueron directamente planificadas por el estado lo cual dio lugar a sistemas de economía planificada.

Tampoco pueden considerarse sistemas capitalistas muchos sistemas socioeconómicos de la antigüedad y la edad media, ya que en ellos tenía un papel destacado la mano de obra forzada (como en el feudalismo) o directamente la mano de obra esclava (presente en la antigüedad, la edad moderna e incluso perduró inicialmente en las sociedades capitalistas). Tampoco existía en muchos de esos sistemas movilidad social, al tratarse de sociedades estamentarias; ni la producción estaba orientada o racionalizada a la obtención de beneficio económico o a crear sistemas de acumulación capitalista, sino que otros objetivos socialmente deseables para una parte de la sociedad podían tener mayor peso en las decisiones de producción y la actividad económica.

Comprendido también como sociedad de riesgo, ha sido un vocablo introducido por el sociólogo alemán Ulrich Beck quien comprendía que luego de Chernobyl la sociedad entró en una nueva fase de producción. El riesgo era la base angular de la sociedad que hacía a todas las clases iguales. Este proceso de desjerarquización ha llevado a un fenómeno conocido como proceso de reflexibilidad. En el capitalismo descrito por Beck, los sistemas de producción son descentralizados, en parte como resultado del proceso de reflexibilidad que da origen a formas donde el lego tiene acceso a información que en épocas anteriores eran exclusivas de los expertos. No obstante, la introducción de la tecnología para detectar y reducir ciertos riesgos, engendraba otros no tenidos en cuenta o planificados por los expertos.[55]​ Anthony Giddens explora el capitalismo del riesgo como una consecuencia del empalme entre la globalización y el mercantilismo.[56]​ Por su parte, Richard Sennet sugiere que la discursividad del riesgo es útil para que los grupos privilegiados no asuman los riesgos de sus decisiones. El ciudadano moderno debe gestionarse su propia seguridad como signo de estatus, que le permite ingresar al mundo de los buenos ciudadanos. Quienes así no pueden gestionarlo, son tildados de «incapaces» o «personas vulnerables». Ser vulnerable implica no tener autonomía respecto de otros que si pueden autoprotegerse. Este cambio en las políticas de protección se asocia a una tendencia económica que pondera y valoriza a quienes no se apegan a una empresa por muchos años. Los expertos en organizaciones o sociología laboral sugieren que las personas deben cambiar de trabajo en forma periódica debido a que ello sugiere una adaptación sana a lo diferente. Más allá de este discurso subyace una lógica de explotación que intenta romper con los lazos sociales y con el apego tradicional de un sujeto a una organización. Por ese motivo, no es extraño observar que dentro del culto al cambio prime una atmósfera de precarización laboral.[57]​ Ante el mismo problema Zygmunt Bauman acuña el término «sociedad líquida» para expresar la dinámica del capitalismo moderno. En la sociedad sólida las economías y los lazos institucionales estaban orientados a largo plazo, en forma de una producción de escala. Pero la modernidad ha cambiado a formas más descentralizadas, móviles y menos estables en los canales productivos. Eso ha dado como resultado una sociedad donde los lazos sociales son adaptables al momento y a los intereses individuales de las personas. En la sociedad líquida la seguridad es empleada como una forma discursiva que denota exclusividad y estatus social. Los medios tecnológicos vigentes son usados por los grupos privilegiados no solo para protegerse de ciertos grupos marginales, sino para demostrar ejemplaridad.[58]​[59]​[60]​

George H. Mead afirmaba que existía una fascinación por las malas noticias, los periódicos y los accidentes porque de esa forma el "yo" exorciza a la muerte. Se siente una sana alegría ante la tragedia de los demás debido a que se ha evitado ser afectado por el evento.[61]​ En este sentido, Joy Sather-Wagstaff sugiere que los desastres provocados por el hombre o naturales generan un gran trauma para la sociedad, el cual debe ser regulado por medio de la solidaridad entre las víctimas y los supervivientes. En ciertas ocasiones, el poder político intenta manipular el discurso con el fin de ganar legitimidad frente a los miembros de la comunidad. Se da, entonces, una patrimonialización del dolor que distorsiona las razones reales del desastre. Rememorar la muerte es el primer hecho político que da origen a la cultura.[62]​ Estas mismas observaciones fueron validadas por la profesora Rodanthi Tzanelli de la Universidad de Leeds, quien sostiene que el cine ha hecho de la muerte una principal mercancía (en inglés, commodity) para ser comercializado por los diferentes agentes del capitalismo al punto de imponer mensajes discursivos hegemónicos. En diversas prácticas como la visita a lugares de extrema pobreza, o a santuarios donde abunda la muerte masiva, estos dispositivos apelan al sufrimiento humano para dotar al consumidor de una realidad apocalíptica. La función de retratar la miseria ajena radica en el reforzamiento de la propia posición de clase ejercida por la élite capitalista.[63]​ Phillipe Aries por su parte sostiene que el hombre moderno ha perdido la familiaridad con la muerte y a diferencia de sus predecesores ha hecho de ella algo incontrolable, cuyos efectos adquieren una naturaleza desestabilizadora.[64]​ Por último, la muerte funcionaría según Geoffrey Skoll como un importante discurso para mantener a la masa trabajadora bajo control.[65]​ Zygmunt Bauman sostiene que el estado de hiper-vigilancia que se ha fundamentado en el uso de tecnologías cumple una doble función. Por un lado protege a los ciudadanos deseables de los indeseables, pero también sirve como criterio de exclusión donde solo unos pocos se aíslan del resto de la sociedad. La exclusividad confiere estatus a ciertos grupos y la vigilancia es el instrumento por medio del cual ese estatus se hace visible a otros quienes no poseen los recursos necesarios para protegerse.[66]​

Parte de la crítica al capitalismo es la opinión de que es un sistema caracterizado por la explotación de la fuerza de trabajo humano al constituir el trabajo como una mercancía más. Esta condición sería su principal contradicción: medios de producción privados con fuerza de trabajo colectiva, de este modo, mientras en el capitalismo se produce de forma colectiva, el disfrute de las riquezas generadas es privado, ya que el sector privado "compra" el trabajo de los obreros con el salario. La alternativa histórica al capitalismo con mayor acogida ha estado representada por el socialismo.[cita requerida]

Para el materialismo histórico (el marco teórico del marxismo), el capitalismo es un modo de producción. Los marxistas creen que las desigualdades sociales se deben a una continua lucha social, la "lucha de clases" que tendría una inevitable evolución en el comunismo, en este sistema se plantea una mejora en las relaciones socio-económicas que mejoraría las condiciones laborales de los trabajadores y evitaría la injusticia social que ellos creen que tiene lugar en el capitalismo.

Esta construcción intelectual es originaria del pensamiento de Karl Marx (Manifiesto Comunista, 1848, El Capital, 1867) y deriva de la síntesis y crítica de tres elementos: la economía clásica inglesa (Adam Smith, David Ricardo y Thomas Malthus), la filosofía idealista alemana (fundamentante la dialéctica hegeliana) y el movimiento obrero de la primera mitad del siglo XIX (representado por autores que Marx calificaba de socialistas utópicos).

Los críticos del capitalismo lo responsabilizan de generar numerosas desigualdades económicas. Tales desigualdades eran muy acusadas durante el siglo XIX, sin embargo, a lo largo de la industrialización (principalmente en el siglo XX) se experimentaron notables mejorías materiales y humanas. Los críticos del capitalismo (John A. Hobson, Imperialism, a study, Lenin, El imperialismo, fase superior del capitalismo) señalaron desde finales del siglo XIX que tales avances se obtuvieron por un lado a costa del colonialismo, que permitió el desarrollo económico de las metrópolis, y por otro lado gracias al Estado del Bienestar, que suavizó los efectos negativos del capitalismo e impulsó toda una serie de políticas cuasisocialista.

Otras críticas al capitalismo que se enlazan a décadas anteriores con el mismo matiz antiimperialista (a partir del pensamiento centro-periferia) provienen de los movimientos antiglobalización, que denuncian al modelo económico capitalista y las empresas transnacionales como el responsable de las desigualdades entre el Primer Mundo y el Tercer Mundo, teniendo el tercer mundo una economía dependiente del primero.

Desde una perspectiva no estrictamente marxista, Karl Polanyi (La gran transformación, 1944) insiste en que lo crucial en la transformación capitalista de economía, sociedad y naturaleza fue la conversión en mercancía de todos los factores de producción (tierra, o naturaleza y trabajo, o seres humanos) en beneficio del capital.

Capitalismo como religión es un escrito póstumo de 1921 del filósofo alemán Walter Benjamin que contiene una crítica profunda al capitalismo. El texto indaga en la naturaleza religiosa del capitalismo como una dogmática inhumana: la identificación del pecado y la culpa religiosa y la deuda impuesta por el capitalismo (el término alemán utilizado en el escrito Schuld significa a la vez deuda y culpa). Para Michael Löwy el escrito es una lectura anticapitalista de Max Weber.[67]​[68]​ En este sentido, se ha afirmado con relación al vínculo entre capitalismo y religión:

La crítica ecologista argumenta que un sistema basado en el crecimiento y la acumulación constante es insostenible, y que acabaría por agotar los recursos naturales del planeta, muchos de los cuales no son renovables; más aún si el consumo de estos recursos es desigual entre los países y en sus respectivas clases económicas. Hasta hace algunas décadas, se pensaba que los recursos naturales eran virtualmente inagotables y que la contaminación, pérdida de la biodiversidad y de paisajes eran costes asumibles del progreso.

Actualmente existen dos tendencias principales relacionadas con la crítica ecologista: aquella que defiende un desarrollo sostenible de la economía (que consistiría en adaptar el actual modelo al nuevo problema medioambiental) y otra que defiende un decrecimiento de la economía (que apunta directamente a nuevos sistemas de organización económica).[70]​

Como contraparte al ecologismo colectivista, surge el ecologismo de mercado con base en la libertad individual. Este ecologismo plantea la protección de los ecosistemas desde el punto de vista del capitalismo libertario. Los libertarios dicen que una clara definición de la propiedad privada en todos los recursos escasos da como resultado que cada recurso escaso sea usado más eficientemente, y por lo tanto, sea regulado por el mercado; de igual manera, el propietario siempre estaría interesado en que su tierra y animales estén sanos, y usan el ejemplo de la privatización de los elefantes en Kenia y la recuperación de la población de estos para demostrar que una economía de mercado con propiedad privada siempre tiene interés en preservar un ecosistema sano. Desde el punto de vista de los libertarios, cuando no hay derechos de propiedad definidos ocurre la denominada tragedia de los comunes, donde el recurso es usado por todos de manera irresponsable y este se agota.

La salsa es un género musical bailable resultante de la síntesis del son cubano y otros géneros de música caribeña y  estadounidenses como el jazz y el blues. La salsa se consolidó como un éxito comercial por músicos de origen puertorriqueño en Nueva York en la década de 1960, y por la labor de quien fue su principal armador, el dominicano Johnny Pacheco,[3]​ si bien sus raíces se remontan a décadas anteriores en países de la cuenca del Caribe.[4]​ 

La salsa finalmente se extendió a lo largo de[5]​ Latinoamérica, dando lugar a escenas regionales como la puertorriqueña, panameña, venezolana, cubana, dominicana, colombiana, ecuatoriana y de otros países de la región. La salsa abarca varios estilos como la salsa dura, la salsa romántica y la timba.

El director cubano Machito afirmó que la salsa era lo que él había tocado durante cuarenta años (entre 1930 y 1970 aproximadamente) antes de que el género musical se denominara así.[6]​ Por otro lado, el músico neoyorquino de ascendencia puertorriqueña Tito Puente negaba la existencia de la salsa como género en sí, afirmando que «lo que llaman salsa es lo que he tocado desde hace muchísimos años: se llama mambo, guaracha, chachachá, guaguancó, todo es música de influencia cubana».[cita requerida]

El músico Eduardo Morales define la salsa como «un nuevo giro de los ritmos tradicionales al son de la música cubana y la voz cultural de una nueva generación», «una representación de la identidad cubana e hispana en Nueva York».[cita requerida]

No obstante, aunque el son cubano es la espina dorsal de la salsa, el elemento fundamental en el surgimiento de la salsa es el papel de los músicos puertorriqueños y su cultura, tanto en la isla de Puerto Rico como en su diáspora neoyorquina. En ese sentido, se señala el peso específico de los puertorriqueños en Nueva York que, aunque minoría, eran numéricamente muy superiores a cualquier asentamiento latinoamericano.

También se aduce que el corte en el intercambio cultural entre puertorriqueños y Estados Unidos en la escena musical latina de Nueva York.[7]​

La salsa presenta las siguientes características:

Además de la percusión, la instrumentación se completa con piano, contrabajo (en muchos casos bajo eléctrico), trompetas, saxofón, trombones, flauta y violín. La influencia del jazz afrocubano viene determinada por el arreglo aunque no es una condición imprescindible en la salsa.

La célula rítmica más representativa de la salsa se llama «clave de son» que tradicionalmente es interpretada por las claves.

Los bailadores y músicos de salsa agrupan el patrón en dos partes:

A) Una parte de 3 toques de clave donde se presenta un contrarritmo intermedio.

B) Una parte de 2 toques de clave 2 sin contrarritmo.

Los números representan las negras, el signo más [+] representa el golpe de las claves, y el punto [.] representa a cada corchea.

"clave de son 3-2"

1 2 3 4 1 2 3 4

X--X--X---X-X---

"clave de son 2-3"

--X-X---X--X--X-

Existe otro patrón rítmico similar que es utilizado raramente en la salsa, y proviene del complejo de la rumba cubana. Este patrón presenta 2 contrarritmos en una de sus partes.

"clave de rumba 3-2"

1 2 3 4 1 2 3 4

X--X---X--X-X---

"clave e rumba 2-3"

1 2 3 4 1 2 3 4

--X-X---X--X---X


La clave no siempre se toca directamente, pero forma la base de otros instrumentos de percusión, así como también de la canción y el acompañamiento, que lo usan como ritmo común para sus propias frases. Por ejemplo, este es el ritmo común de la campana con clave 2-3:

.. +. +... +.. +... + clave 2-3
+. *. +. * * +. * * +. * * campana coincidente con el 2 de clave

El signo más [+] representa un golpe grave de la campana.
El asterisco [*] representa un golpe agudo de la campana.

En 1933, el músico cubano Ignacio Piñeiro utilizó por primera vez un término relacionado, en un son cubano titulado «Échale salsita».

A mediados de los años 1940, el cubano Cheo Marquetti emigró a México. De regreso a Cuba, con influencia de las salsas picantes de comida, le dio ese nombre a su agrupación Conjunto Los Salseros, con quienes grabó un par de discos para las disqueras Panart y Egrem. En 1957 fue a Caracas (Venezuela) por motivo de varios conciertos en esa ciudad y fue en Venezuela donde se comenzó a emitir en la radio la palabra «salsa» a la música que hacían los soneros cubanos de esa época y posteriormente se le designaría este nombre a lo que sería la recopilación de muchos ritmos caribeños que se comenzó a hacer en Nueva York y Puerto Rico.[9]​

La autora de música Sue Steward afirma[cita requerida] que la palabra fue originalmente usada en la música como un «llanto de apreciación para un picante particular o un solo rápido», viniendo a describir un género de música específico de la mitad de los años 1970 «cuando un grupo de músicos hispanos de Nueva York, comenzó a examinar los arreglos de las grandes bandas clásicas populares desde la era del mambo de los años 1940 y 1950». Ella menciona que la primera persona que usó el término «salsa» para referirse a este género musical en 1968 fue un disc-jockey de radio, el venezolano de nombre Phidias Danilo Escalona, quien emitía un programa radial matutino llamado La hora de la salsa en el que se difundía la música hispana producida en Nueva York como una respuesta al bombardeo de la música rock en aquellos días (la beatlemanía).[10]​

―¿Qué es lo que ustedes tocan?
―Esto que nosotros hacemos lo hacemos con sabor, es como el ketchup, que le da sabor a la comida.
―¡¿Qué es eso de ketchup?!
―Bueno, eso es una salsa que se utiliza en los Estados Unidos para darle sabor a la hamburguesa.

Bobby Cruz llamó a Pancho Cristal para bautizar con el término «salsa» el nuevo LP que estaba lanzándose al mercado, Los durísimos (1968). Esta versión es apoyada por cantantes de salsa como Rubén Blades, Tite Curet Alonso y otros más.[cita requerida]

Era la hora del almuerzo, del aderezo, del sabor, y por supuesto, del son cubano, el guaguancó, la guaracha y el son montuno.[10]​

Ed Morales también menciona la palabra como usada para animar una banda al incremento del tempo y que «pone a los bailarines en una parte alta» para agradecer un momento musical, y expresar un tipo de nacionalismo cultural, proclamando el calor y sabor de la cultura hispana. También menciona a Johnny Pacheco, que realizó un álbum llamado Salsa na’ má, que Morales tradujo como «solo necesitas un poquito de salsa o condimento».

La palabra salsa para designar la música hecha por los «hispanos» en los Estados Unidos comenzó a usarse en las calles de Nueva York a finales de los años 1960 y principios de los 1970. Por esta época, el pop latino no era un ritmo importante en la música que se escuchaba en los Estados Unidos al perder terreno frente al doo wop, al R&B y al rock and roll. El surgimiento de la salsa abre un nuevo capítulo de la música latina en la música popular estadounidense, donde jugó un papel de primer orden la orquesta Fania All-Stars, dirigida por el dominicano Johnny Pacheco quien, junto al abogado Jerry Masucci, fundó el importante sello salsero Fania Records.

Entre los años 1920 y 1950, la música afrocubana era consumida ampliamente por los sectores de origen latino, específicamente puertorriqueña, de Nueva York. Los puertorriqueños en Nueva York, fundamentaban su música en gran medida en los elementos de origen afrocubano.

Según algunos músicos e historiadores,[¿quién?] salsa es un nombre comercial dado a toda la música caribeña de influencia afrocubana y puertorriqueña en los años 1970.
La salsa se expandió a fines de los años 1960 y de los 1970 a los 1990. Nuevos instrumentos, nuevos métodos y formas musicales (como canciones de Brasil) fueron adaptados a la salsa. Nuevos estilos aparecieron como las canciones de amor de la salsa romántica. Mientras tanto, la salsa se convirtió en parte importante de la escena musical de Puerto Rico, Colombia, Ecuador República Dominicana, Japón, España, Panamá, Venezuela y lugares tan lejanos como Japón. Con la llegada del siglo XXI, la salsa se ha convertido en una de las formas más importantes de la música popular en el mundo.

La integración de las tumbadoras 
en los conjuntos que tocaban son montuno fue un elemento fundamental en la instrumentación de orquestas de baile.

A finales del año 1920, los sextetos y septetos de son alcanzaron en Cuba una notable popularidad. En 1928, Gerardo Machado con la intención de reducir la influencia de los elementos africanos en la música cubana, prohibió el uso del bongó, las congas y las comparsas de carnaval. Esto provocó que las orquestas de charangas con el uso timbales) incrementaran su popularidad.[11]​ 

Cerca de 1940, el Conjunto Llave de Rafael Ortiz introdujo las tumbadoras o congas en una orquesta,[cita requerida] instrumentos que anteriormente solo se usaban en música folclórica afrocubana. Arsenio Rodríguez popularizó el uso de las congas al integrarlas a su conjunto, introduciendo el son montuno a nivel comercial.[12]​

En los años 1940, Mario Bauzá, director y arreglista de la orquesta de Machito «Los Afro-Cubans», agregó trombones al son montuno y la guaracha. Estas innovaciones influyeron   en músicos como José Curbelo, Benny More, Bebo Valdés.[cita requerida] En el álbum Tanga (de 1943), Bauzá fusionó elementos de la música afrocubana con el jazz.

La influencia del jazz afrocubano y del mambo desarrollado por Pérez Prado en 1948, propició la introducción del saxofón en las orquestas de son montuno y guaracha. En 1955, Enrique Jorrín le agregó trompetas a las orquestas de charanga, que hasta ese momento solo usaban violín y flauta.[cita requerida]

Ya para los años 1950, la música bailable cubana, es decir el son montuno, el mambo, la rumba y el chachachá, se constituyó en un elemento de gran popularidad en los Estados Unidos y Europa.[10]​

En Nueva York, el «sonido cubano» de las bandas se fundamentó en los aportes de músicos puertorriqueños que tocaban la música cubana de moda en ese entonces. Como ejemplo,  mencionar a Machito, Tito Rodríguez, Tito Puente o incluso figuras como el director catalán Xavier Cugat. Por otro lado, y ya fuera del círculo de Nueva York, grupos como la Orquesta Aragón, la Sonora Matancera y Dámaso Pérez Prado y su mambo lograron una importante proyección a nivel internacional.[10]​

El mambo fue influido por el jazz afrocubano y el son. Las grandes bandas de este género mantuvieron viva la popularidad de la larga tradición del jazz dentro de la música latina, mientras los maestros originales del jazz se circunscribieron a los exclusivos espacios de la era del bebop.[13]​

La música latina interpretada en Nueva York desde 1960 fue liderada por músicos como Ray Barretto y Eddie Palmieri, los cuales estaban fuertemente influidos por ritmos cubanos importados como la pachanga y el chachachá. Después de la crisis de los misiles en 1962, el contacto cubano-estadounidense decayó profundamente.[13]​

En 1969 Juan Formell introdujo el bajo eléctrico en los conjuntos soneros de Cuba.[14]​

El cuatro puertorriqueño fue introducido por Yomo Toro en la orquesta de Willie Colón[cita requerida] en 1971 y el piano eléctrico en los años 1970 por Larry Harlow.[cita requerida]

En los años 1970 se incrementó la influencia puertorriqueña en el ámbito de la música latina en Nueva York y los «nuyoricans» pasaron a ser una referencia fundamental. La palabra salsa para designar la música hecha por los «latinos» en los Estados Unidos, comenzó a usarse en las calles de Nueva York a finales de los años 1960 y principios de los 1970. Por esta época, el pop latino no era una fuerza importante en la música que se escuchaba en los Estados Unidos, habiendo perdido terreno frente al doo wop, al R&B y al rock and roll. En ese contexto, el surgimiento de la salsa abrió un nuevo capítulo de la música latina, especialmente en los Estados Unidos.

La historia de la salsa, en la que participaron gran cantidad de músicos, puede rastrearse en cierta medida en la trayectoria de algunas importantes compañías discográficas. En los años 1970 Fiesta Récord, Manhattan Recording Company, y en especial Fania Records, lanzaron al estrellato una gran cantidad de «salseros» desde Nueva York, realizando giras y conciertos por todo el mundo.

La compañía Fania Records fue fundada en marzo de 1964 por el abogado y empresario Jerry Masucci y el flautista dominicano y director de orquesta Johnny Pacheco. Fania comenzó con Larry Harlow y la producción El Malo de Willie Colón y Héctor Lavoe en 1967.

Fania Records le dio el espaldarazo definitivo al género al grabar y distribuir los discos de la gran mayoría de las estrellas salseras de los años 1970. Dentro de esta empresa se formó la agrupación Fania All Stars, orquesta que agrupó una gran cantidad de músicos y cantantes de salsa como: Ray Barretto, Willie Colón, Johnny Pacheco, Rubén Blades, Héctor Lavoe, Ismael Miranda, Cheo Feliciano, Bobby Cruz, y artistas invitados como Tito Puente, Celia Cruz, y Eddie Palmieri.

La dotación instrumental de Fania All Stars representó los nuevos giros de la música caribeña en los años 1970. Además del piano y bajo, la presencia de instrumentos de percusión como timba, tumba y bongó que eran extensamente utilizados por las orquestas de Puerto Rico y Nueva York desde los años 1940. La sección de instrumentos de viento estaba constituida por tres trompetas y tres trombones, dotación bastante extraña en la tradición musical caribeña y que perfilaría el sonido particular de la Salsa hasta nuestros días. La ausencia del saxofón era notable, pues en ese momento pertenecía a conceptos musicales del pasado y a la fastuosidad de las Big Band. La sustitución del saxofón por el trombón permitía diferenciar, en algo, el sonido de la salsa del sonido cubano tradicional. Por último, se destaca la presencia del Cuatro puertorriqueño ejecutado por el músico Yomo Toro incorporado a la agrupación para traer al ámbito musical urbano la guitarra de las zonas rurales caribeñas (tanto el Tres cubano como el Cuatro puertorriqueño). El Cuatro puertorriqueño adquiría jerarquía de solista y de instrumento bandera en la Fania All Stars a la vez que se establecen las diferencias instrumentales y sonoras con la música cubana.

En 1961 Tito Rodríguez en el exitoso álbum "Returns To The Palladium - Live" presentó la canción "El que se fue" que anticipa los elementos esenciales que van a configurar la Salsa. Lo mismo ocurre con la canción "Avisale a mi contrario" del álbum "Carnival of the Americas" grabado en 1964.   

En 1965 Joe Cuba Sextet, con el cantante Cheo Feliciano, grabaron el tema «El pito (I'll never go back to Georgia)» y el mismo año Richie Ray y Bobby Cruz grabaron el tema «Comején».

En el año 1969, El Gran Combo de Puerto Rico grabó «Falsaria». Este tema, originalmente un bolero, se interpretó como salsa. También la orquesta de Willie Colón con Héctor Lavoe como vocalista, grabó «Che che cole» y otros temas importantes.

En 1971, Eddie Palmieri grabó el tema «Vámonos pa’l monte» y Cheo Feliciano, como solista, grabó «Anacaona».

En 1973 Raphy Leavitt con la Orquesta La Selecta grabaron «Jíbaro soy». A su vez, en Perú se graba el tema «Llegó la banda» de Enrique Lynch y su conjunto, la misma que año más tarde sería popularizada por Héctor Lavoe.

En 1974 Celia Cruz y Johnny Pacheco grabaron «Quimbara» e Ismael Rivera hizo lo propio con «El nazareno». Por otro lado, el festival de la Fania All Star realizado en Zaire ese mismo año fue un evento a destacar en la difusión de la salsa.

En 1975, la Dimensión Latina, de Venezuela, con Oscar de León como vocalista, grabó «Llorarás», Fruko y sus Tesos grabaron «El preso», y El Gran Combo de Puerto Rico, «Un verano en Nueva York». Héctor Lavoe inició su carrera como solista con el tema «Periódico de ayer».

En 1978 La Sonora Matancera grabó «Mala mujer». Así mismo, el dúo conformado por Willie Colón y Rubén Blades publicó el disco Siembra, que contenía temas emblemáticos de la salsa como «Pedro Navaja» y «Plástico».

En 1980 Henry Fiol lanzó sus temas «Oriente» y su versión de «La juma de ayer».

Desde Nueva York la salsa se expandió primero en América Latina (sobre todo en países como Cuba, Colombia, Ecuador, Panamá, República Dominicana, Venezuela y Puerto Rico. En los años 1980 alcanzó una importante difusión en Europa y en Japón.

Miami se convirtió en una especia de «segunda metrópoli» para la música cubana, dado el peso específico de la gran cantidad de inmigrantes cubanos. La comunidad cubana se constituyó en un referente importante en la vida de Miami, contrario a lo que pasó en Nueva York, donde primó la influencia boricua.

Durante los años 1980 la salsa se expandió a Europa y Japón. En este país surgió la Orquesta de la Luz, que alcanzó alguna popularidad en América Latina.

A fines de esta década surgió la llamada «salsa romántica», estilo que se hizo popular en Nueva York, caracterizado por melodías lentas y letras de corte romántico es decir, un concepto similar a la lírica de la balada pero en ritmo de salsa. Esta nueva manifestación de la salsa pronto fue asimilada por artistas boricuas como Frankie Ruiz, Eddie Santiago, Paquito Guzmán, Marc Anthony, Willie González, Cano Estremera; cubanos como Dan Den, Rey Ruiz, Issac Delgado y el nicaragüense Luis Enrique.

La salsa en Colombia, en los años 1970, está vinculada a agrupaciones como Fruko y sus Tesos a través de la empresa Discos Fuentes y el grupo The Latin Brothers. En 1988, la empresa discográfica Discos Musart publicó la serie de LP Salsa Colección Estelar lo que provocó un incremento de popularidad y la llevó a competir con la cumbia.

En los 1980 aparecieron grupos como Los Titanes, Grupo Niche, Orquesta Guayacán y Joe Arroyo. También en los 1980, el cubano Roberto Torres y el colombiano Humberto Corredor desarrollaron en Miami el concepto de charanga-vallenata.

Desde finales de los años 1940 y principio de los 1950, las orquestas de "música bailable tropical" como la de Alfonso Larraín (1947), la Sonora Caracas (1948), la Billo's Caracas Boys (1951) y Los Melódicos (1958) y Sexteto Juventud (1962), combinaron en sus repertorios cumbias, merengues y otros ritmos antillanos con géneros cubanos. Esto determinó el surgimiento de un movimiento que influyó posteriormente en la salsa venezolana.

En ese tenor, se puede hablar de artistas como Canelita Medina, Federico y su Combo Latino, Los Dementes o el grupo del músico Carlos Emilio Landaeta, conocido como "Pan con queso" del Sonero Clásico del Caribe.

La salsa en Venezuela contó con agrupaciones como la Sonora Maracaibo, el Grupo Mango o Dimensión Latina, de donde salieron figuras como Oscar D'León. También músicos como Nelson Pueblo agregaron influencias de música llanera a la salsa nativa.

La orquesta Nelson y sus estrellas triunfadores en la década de los años 1970, también en Cali, en Barranquilla y en general en Colombia.[15]​

La salsa registró un crecimiento regular entre los años 1970 y 2000 y ahora es popular en muchos países latinoamericanos y algunos espacios del mercado estadounidense. Entre los cantantes y grupos destacados en los años 1990 encontramos a figuras como Rey Ruiz, Luis Enrique, Jerry Rivera, Salsa Kids, Dan Den, Marc Anthony, La India, La Sonora Matancera, DLG, Gilberto Santa Rosa, Víctor Manuelle, Michael Stuart, Celia Cruz, Maelo Ruiz.

Las más recientes innovaciones en este género incluyen la mezcla de rap o reguetón con la salsa dura.

La salsa es uno de los géneros de música latina que ha influido en la música del occidente africano. Un ejemplo de esta influencia es el grupo sonero Africando, en el que músicos neoyorquinos trabajan con cantantes africanos tales como Salif Keita e Ismael Lo.

A partir de los años 1980, las orquestas de salsa fueron dejando los sonidos fuertes y las "descargas" para entrar en un sonido más cadencioso y melódico, acompañada de letras con abundantes referencias al amor y a las relaciones sexuales como motivo principal y, en algunos casos, excluyente. Esta música fue denominada «salsa sensual» o «erótica» y tuvo como máximos exponentes a Eddie Santiago, Frankie Ruiz, David Pabón, Lalo Rodríguez, Rey Ruiz, Willie González y Luis Enrique.

La categorización de la salsa sensual trajo como consecuencia que se denominara al género anterior como «salsa dura», que sufrió una baja de producción y de popularidad a la par que el nuevo género se consolidó.

A fines de los años 1990 la salsa sensual empezó a declinar en popularidad, debido principalmente al fuerte impulso de otros ritmos caribeños como el merengue dominicano y la bachata en los Estados Unidos, Centro y parte de Sudamérica, trayendo aparejada la desaparición del sello RMM, de producciones netamente románticas.

Para ese entonces la salsa había perdido a muchos de sus grandes baluartes, ya por fallecimiento (Héctor Lavoe, Ismael Rivera), como por reorientación de sus carreras hacia el jazz «latino» (Ray Barretto, Eddie Palmieri) como por la realización de grabaciones cada vez más espaciadas de quienes continuaron en el género (Rubén Blades, Willie Colón, Johnny Pacheco).

El fin de siglo trajo un resurgir de la salsa dura (que en los años 1990 estuvo representada apenas por Manny Oquendo y Libre) de la mano de grabaciones para sellos independientes o minúsculos. Fue el caso de la Orquesta La 33, que acentuó la salsa con el son montuno y la guaracha, y Jimmy Bosch, que volvió a dar protagonismo al trombón, dando así el impulso inicial para la reinstalación del sonido de la «vieja escuela» en el género: algunos ejemplos actuales son La Sucursal SA y la Orquesta Bailatino.

En todo el territorio colombiano se escucha y se baila salsa, especialmente en ciudades como Barranquilla, Bogotá, Cali, Cartagena, Eje Cafetero y Medellín. En Cali se celebra el Festival Mundial de la Salsa; en Bogotá, el festival Salsa al Parque; y en Barranquilla, el Festival de Orquestas del Carnaval de Barranquilla. En Cali existen diversas academias de salsa que compiten en certámenes internacionales.

Otras agrupaciones y solistas destacados:

La cuna del son cubano, influencia musical de la salsa. Se toca, se escucha y se baila salsa en toda la isla.

Desde la llegada de los latinoamericanos a España, comenzó la influencia de la música del Caribe en ese país. Durante la dictadura franquista, la música cubana fue prohibida (excepto en las islas Canarias) pues varias canciones fueron críticas contra la dictadura. En las islas Canarias, la salsa tuvo cierta importancia desde los años 1970 y 1980, con intérpretes como Caco Senante y otras bandas de música tradicional cubana y salsa contemporánea. En todas las islas se escucha y se baila en bares y discotecas. Se celebra el SalsaOpen, Campeonato de Salsa de Canarias[16]​ y el Gran Canaria Salsa Congress.[17]​

País productor de salsa en las ciudades con mayor cantidad de latinoamericanos, como Los Ángeles, Miami y Nueva York, la gran mayoría de origen puertorriqueño.

En Fukuoka se realiza el Festival Isla de Salsa.

Se escucha en la parte central del país y en la región sur del golfo de México. Se escucha en los estados de Veracruz, Oaxaca, Tabasco, Campeche, Yucatán, Quintana Roo y en la capital, impuesta por los sonideros.

País donde la salsa mantuvo una presencia arrolladora desde los años 1970. En todo el país se escucha y se baila salsa.

La salsa es el género más escuchado en el Perú, según un estudio del Instituto de Estudios Peruanos (IEP) de 2020, obteniendo el 21% de preferencias.[18]​[19]​ Callao es conocida por ser la cuna de la salsa peruana, donde se celebra el Festival Internacional Chim Pum Callao desde 1997.[20]​[21]​ También tiene acogida en Lima, donde hay festivales exclusivos como Una Noche de Salsa, en menor medida en las demás regiones del país. Tuvo algunos exponentes en los años 1960, 1970, 1980 y 1990, y actualmente están entrando en escena artistas salseros cuyos temas tienen alta repercusión internacional, cuyo estilo es denominado salsa urbana.

Independientemente de que en todo el territorio dominicano se escucha salsa, especialmente en la capital, Santo Domingo, es de lugar señalar que en el origen e historia del concepto Salsa, la presencia dominicana, en la figura de Johnny Pacheco  ha sido determinante y que recoge la esencia de los ritmos nacidos en el Caribe antillano de habla hispana, principalmente de Cuba, Puerto Rico y República Dominicana con sus barrios capitalinos como Villa Consuelo, Villa Juana, Borojol, Villa Duarte Los Minas y Villa Mella, entre otros.

País eminentemente salsero, específicamente en Caracas, Maracaibo, Barquisimeto, Valencia, Los Teques, Puerto La Cruz y Maracay.

La salsa por sí misma ya es una consolidación y combinación de ritmos caribeños e influencias del jazz y otros. No obstante, la misma ha sido a través del tiempo combinado con otros géneros musicales, es así que se ha combinado con el rock, el rap, el ska, la bachata, bolero, en algunos casos mariachi y una de las más significativas es la cumbia colombiana. Las primeras grabaciones que combinan estos géneros fueron hechas en México por Mike Laure a finales de los años 1950 y por Carmen Rivero creando su orquesta ―o su «sonora»― en el año 1962 y a mediados de los años 1960 surge la Sonora Santanera (con músicos mexicanos), y más tarde a finales de los años 1970 y a principios de los 1980 los colombianos Joe Rodríguez y Joe Arroyo implementaron estas combinaciones, y a la par Fruko y sus Tesos con por ejemplo su tema Como cumbiambero que soy, la cual muestra una combinación entre ambos géneros mientras es ejecutada la parte de música salsa con el coro título del tema.

Cuba es la tierra madre de ritmos que, como el son, la guaracha y el son montuno, constituyeron una referencia obligada en el desarrollo posterior de lo que sería la salsa. No obstante, esos ritmos, perduraron y evolucionaron al interior de la isla, siempre marcada por el virtuosismo de sus músicos y la capacidad de sus compositores.

El son evolucionó al ritmo de songo por bandas como Los Van Van mientras otros grupos se mantuvieron tocando salsa dura o charanga, como Son 14,la Orquesta Reve,[22]​ la Orquesta Aragón, Adalberto Álvarez, la Original de Manzanillo o las Maravillas de Florida, entre otros.

Para 1988 NG La Banda desarrolló un nuevo estilo de salsa. A nivel internacional, su proyección comenzó en Japón en 1993 y más tarde alcanzó otras partes del globo. A esta forma nueva de «salsa cubana» se le llamó timba. Este estilo de música se fundamenta en piezas musicales caracterizadas por una gran complejidad rítmica y de sonidos, solos y «descargas». Bandas como Juan Formell y los Van Van, Chucho Valdés e Irakere, han proyectado con fuerza el sonido «timbero».

También, músicos y cantantes de la diáspora cubana se han destacado en la salsa romántica, estilo popular en Nueva York desde principios de los 1980. En este tenor, podemos mencionar cantantes de como Dan Den, Rey Ruiz e Isaac Delgado.

Se baila con movimientos cadenciosos de cadera y hombros. Tanto el hombre como la mujer giran uno alrededor del otro en ambos sentidos y el movimiento de brazos y solos se ejecutan con un ritmo casi inigualable. Es rica en movimientos coreográficos, pero en general los cubanos ponen el acento fundamentalmente en el juego erótico que se establece entre la pareja de bailadores, quedando el alarde y la exhibición para la parte de la pieza conocida como montuno, cuando el cantante, el coro y la orquesta inician una especie de contrapunto.

Este juego permanente ha inducido a muchos a contraponer la forma cubana de bailar salsa a la llamada salsa en línea, de origen más bien estadounidense, donde la exhibición es el fin mismo del baile, desde el principio hasta el final de la pieza. La improvisación de pasos sin renunciar a conservar el ritmo todo el tiempo es otro rasgo distintivo del estilo cubano.

En Puerto Rico se prefiere hacer más lentos los movimientos de pies y caderas. Sin embargo, los puertorriqueños realizan muchas piruetas en los concursos de salsa.[cita requerida]

Pero los latinos en los Estados Unidos no solo conectaron sus ritmos, sino que desarrollaron un nuevo género de baile: el New York Style, determinado por la escuela cubana y puertorriqueña y ampliada por un montón de elementos de academia de baile. Desde Nueva York este estilo de baile ha encontrado también en Europa mucha divulgación junto al también estilo de baile cubano conocido como rueda de casino.

En los años 1980 se desarrolló en Cali, Colombia, un género acrobático de baile caracterizado por el rápido movimiento de los pies y las caderas, que lleva un conteo de 8 tiempos. En este estilo los giros o vueltas son de gran importancia. La reproducción de los discos pasó de una velocidad mayor a la habitual (por ejemplo, un disco de 33,3 rpm se reproducía a 45 rpm). En cambio en otras ciudades colombianas como Barranquilla y Cartagena se conservó el estilo original de baile con movimientos de hombros, caderas y pies.

También en los años 1980 se desarrolló en la costa occidental de Estados Unidos el L. A. Style (estilo de Los Ángeles), similar al New York Style, aunque se baila en un tiempo diferente (en el 1) más de la mitad del baile se realiza con los pies en el piso, pero todavía con más elementos de show. Más o menos hacia el fin del milenio se puede observar que los mexicanos que vuelven de California hacen creciente la popularidad del L. A. Style también en México.

Rueda de casino: baile cubano de grupo en círculo en el cual uno actúa como voz y va dando órdenes con vueltas y cambios de pareja que hacen este subgénero divertido y participativo, como ejemplos de vueltas están el clásico «70», hasta figuras complejas como «llévala a Matanzas».

